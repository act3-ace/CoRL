{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The following Site/Repository is currently under construction. We are still porting items and updating instructions for github site/CICD. ACT3 RL Core \u00a4 Core act3 reinforcement learning library - The Core Reinforcement Learning library is intended to enable scalable deep reinforcement learning experimentation in a manner extensible to new simulations and new ways for the learning agents to interact with them. The hope is that this makes RL research easier by removing lock-in to particular simulations. The work is released under the follow APRS approval. - Initial release of CoRL - Part #1 -Approved on 2022-05-2024 12:08:51 - PA Approval # [AFRL-2022-2455]\" Documentation - https://act3-ace.github.io/CoRL/ Framework Overview - Hyper configurable environment enabling rapid exploration and integration pathways A framework for developing highly-configurable environments and agents Develop core components in python Configure experiments/agents in json/yml Provides tooling to help validate configuration files and give useful feedback when files are misconfigured Designed with integration in mind Dramatically reduce development time to put trained agents into an integration or using a different simulation Can work with any training framework Currently limited to Ray/RLLIB due to multi-agent requirement Environment pre-written, users implement plugins Simulator Platforms & Platform Parts Glues Rewards Dones Validators - Configuration guarantees for enabling validation of user configuration going into the major components All major CoRL python components have a validator Validators are python dataclasses implemented through the pydantic library Validators check and validate user configuration arguments going into the major components If a component successfully initializes, the validators guarantee the developer that the data listed in the validator is available to them If a component doesn\u2019t initialize, a nice helpful error message is automatically produced by pydantic Adds a pseudo static typing to python classes Episode Parameter Provider (EPP) - Domain Randomization & Curriculum Learning at Environment, Platform, and Agent based on training An important tool for RL environments is the ability to randomize as much as possible Starting conditions / goal location / etc. This leads to more general agents who are more robust to noise when solving a task Another tool sometimes used in RL is curriculum learning (CL) Starting from an easier problem and gradually making the environment match the required specifications can significantly speed up training CoRL Agents and the environment all have an epp, which provides simulator or user defined parameters to be used during a specific episode Simulator classes know what parameters they expect to setup an episode Configuration parameters to the various functors can all be provided from an EPP An EPP can also update parameters over the course of training Make a goal parameter harder based on the agents win rate Open the environment up to wider bounds once the agent initially starts to learn Simulator Class - Extensible interface for transitioning between Dubins and other simulator backends Responsible for setting up the world for a agents to manipulate Setting up and configuring the simulation Creating the simulation platforms Placing those platforms in the world Responsible for knowing how to advance the simulation when requested The simulation returns a simulation state when reset or advanced that rewards or done conditions can use This state contains at least both the time and the list of simulation platforms Responsible for saving any information about the current training episode Saving video/logs Simulator Platforms + parts - Extensible base interface for parts to be added to planforms with an integration focus. Simulation platforms represent some object that can be manipulated in the simulation Car/plane/robot/etc. Have a config file to allow modes of configuration Each platform has a set of parts attached to it Parts take simulation specific code and wrap it in an interface that allows agents to read from and write to them Parts do nothing unless a user configures a connection between the agent and a part using a glue (to be explained) Parts could include things such as a throttle, a game button, a steering wheel, etc. Parts are registered to a simulator using a string Sensor_Throttle , Controller_Throttle , etc. Glues - Connecting layers to allow exposing observable state to rewards, termination/goal criteria, and agents A stateful functor Responsible for producing actions and observations for the agent May directly read/write to parts or other glues Glues reading/writing to each other is called \u201cwrapping\u201d Glues implement the composable and reusable behavior useful for developers Common glues turn any sensor part into an obs and apply actions to any controller part Wrapper glues can implement behaviors such as framestacking, delta actions May not directly read from the simulation, only interface through parts Rewards, Dones (Goal & Termination) - Composable functors common interface for sharing rewards and termination criteria in a stateful manner Composable state functors Rewards generate the current step reward for the agent Dones evaluate if the episode should stop on the current timestep These done\u2019s can be triggered for either success or failure Both Done and Reward Functors can view the entire simulation state to reward agents Done conditions typically add to the state when they trigger to signify what type of Done they are WIN/LOSE/DRAW Rewards are processed after Done conditions during an update, so rewards can read these labels There can be an arbitrary number of reward or done functors for an agent Agent + Experiment Class Agent Class Responsible for holding all of the Done/Reward/Glue functors for a given agent Can be many agent classes per platform When one agent class on a platform reaches a done, all on that platform do Different subclasses may process information in different ways or do different things Experiment Class Responsible for setting up an experiment and running it Configures and creates the environment Creates and configures the agent classes Use of this class allows for any arbitrary RL training framework to be used as the backend for training CoRL Integration and Simulator Swapping In CoRL all simulation specific components must be registered and retrieved from a plug-in library As long as a specific simulator has all of the parts registered to it that an agent needs, CoRL can swap the simulator and parts out from under an agent seamlessly As long as the parts for the two simulators have the same properties (in terms of sensed value bounds or controller inputs) there is no difference to the agent between the two and the regular environment can be used for integration Besides integration this also allows for cross simulation evaluation or training of an agent to be resumed in another simulator Benifits \u00a4 CoRL helps make RL environment development significantly easier CoRL provides hyper configurable environments/agents and experiments Instead of a new file every time a new observation is added, now just add a few lines of config Makes it possible to reuse glues/dones/rewards between different tasks if they are general Provides tools to use both domain randomization and curriculum learning through EPP An integration first focus means that integrating agents to the real world or different simulators is significantly easier Install \u00a4 Install the source - Miniconda - local host: \u00a4 Miniconda Install Instruction # Create a virtual environment to install/run code conda create -n CoRL python == 3 .10.4 # Activate the virtual environment conda activate CoRL # install poetry pip install poetry == 1 .2.1 # Install the CoRL dependencies poetry install # Pre-commit setup pre-commit install How to install pip package \u00a4 Build \u00a4 How to build the wheel file \u00a4 The following project supports building python packages via Poetry . # Create a virtual environment to install/run code conda create -n CoRL python == 3 .10.4 # Activate the virtual environment conda activate CoRL # install poetry pip install poetry == 1 .2.1 # Build the CoRL package poetry build How to build the documentations - Local \u00a4 The follow project is setup to use MKDOCS which is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. To build the documentation: mkdocs build To serve the documentation: mkdocs serve How to build the Docker containers \u00a4 The following project support development via Docker containers in VSCode. This is not strictly required but does provide the mode conveniet way to get started. Note: fuller documentation is available in the documentation folder or online docs. Setup the user env file: in code directory run the following script --> ./scripts/setup_env_docker.sh Build the Docker containers using compose: run the following command --> docker-compose build Running base examples \u00a4 python -m corl.train_rl --cfg config/experiments/cartpole_v1.yml Contributors \u00a4 AFRL Autonomy Capability Team (ACT3) AFRL ACT3 terry.wilson.11@us.af.mil bejamin.heiner@us.af.mil karl.salva@us.af.mil james.patrick@us.af.mil Training Team clong@toyon.com (ML Training) sfierro@toyon.com (ML Training / System Integration) bstieber@toyon.com (ML Training) joshua.blackburn@str.us (ML Training / System Integration) madison.blake@shield.ai (ML Infrastructure and Evaluation) AFRL Autonomy Capability Team (ACT3) Safe Autonomy (SA) Team kerianne.hobbs@us.af.mil (AFRL ACT3) terry.wilson.11@us.af.mil (AFRL ACT3) Designation Indicator \u00a4 Controlled by: Air Force Research Laboratory (AFRL) Controlled by: AFRL Autonomy Capability Team (ACT3) LDC/Distribution Statement: DIST-A POCs: terry.wilson.11@us.af.mil (AFRL ACT3) bejamin.heiner@us.af.mil (AFRL ACT3) kerianne.hobbs@us.af.mil (AFRL ACT3) Notices and Warnings \u00a4","title":"Introduction"},{"location":"#act3-rl-core","text":"Core act3 reinforcement learning library - The Core Reinforcement Learning library is intended to enable scalable deep reinforcement learning experimentation in a manner extensible to new simulations and new ways for the learning agents to interact with them. The hope is that this makes RL research easier by removing lock-in to particular simulations. The work is released under the follow APRS approval. - Initial release of CoRL - Part #1 -Approved on 2022-05-2024 12:08:51 - PA Approval # [AFRL-2022-2455]\" Documentation - https://act3-ace.github.io/CoRL/ Framework Overview - Hyper configurable environment enabling rapid exploration and integration pathways A framework for developing highly-configurable environments and agents Develop core components in python Configure experiments/agents in json/yml Provides tooling to help validate configuration files and give useful feedback when files are misconfigured Designed with integration in mind Dramatically reduce development time to put trained agents into an integration or using a different simulation Can work with any training framework Currently limited to Ray/RLLIB due to multi-agent requirement Environment pre-written, users implement plugins Simulator Platforms & Platform Parts Glues Rewards Dones Validators - Configuration guarantees for enabling validation of user configuration going into the major components All major CoRL python components have a validator Validators are python dataclasses implemented through the pydantic library Validators check and validate user configuration arguments going into the major components If a component successfully initializes, the validators guarantee the developer that the data listed in the validator is available to them If a component doesn\u2019t initialize, a nice helpful error message is automatically produced by pydantic Adds a pseudo static typing to python classes Episode Parameter Provider (EPP) - Domain Randomization & Curriculum Learning at Environment, Platform, and Agent based on training An important tool for RL environments is the ability to randomize as much as possible Starting conditions / goal location / etc. This leads to more general agents who are more robust to noise when solving a task Another tool sometimes used in RL is curriculum learning (CL) Starting from an easier problem and gradually making the environment match the required specifications can significantly speed up training CoRL Agents and the environment all have an epp, which provides simulator or user defined parameters to be used during a specific episode Simulator classes know what parameters they expect to setup an episode Configuration parameters to the various functors can all be provided from an EPP An EPP can also update parameters over the course of training Make a goal parameter harder based on the agents win rate Open the environment up to wider bounds once the agent initially starts to learn Simulator Class - Extensible interface for transitioning between Dubins and other simulator backends Responsible for setting up the world for a agents to manipulate Setting up and configuring the simulation Creating the simulation platforms Placing those platforms in the world Responsible for knowing how to advance the simulation when requested The simulation returns a simulation state when reset or advanced that rewards or done conditions can use This state contains at least both the time and the list of simulation platforms Responsible for saving any information about the current training episode Saving video/logs Simulator Platforms + parts - Extensible base interface for parts to be added to planforms with an integration focus. Simulation platforms represent some object that can be manipulated in the simulation Car/plane/robot/etc. Have a config file to allow modes of configuration Each platform has a set of parts attached to it Parts take simulation specific code and wrap it in an interface that allows agents to read from and write to them Parts do nothing unless a user configures a connection between the agent and a part using a glue (to be explained) Parts could include things such as a throttle, a game button, a steering wheel, etc. Parts are registered to a simulator using a string Sensor_Throttle , Controller_Throttle , etc. Glues - Connecting layers to allow exposing observable state to rewards, termination/goal criteria, and agents A stateful functor Responsible for producing actions and observations for the agent May directly read/write to parts or other glues Glues reading/writing to each other is called \u201cwrapping\u201d Glues implement the composable and reusable behavior useful for developers Common glues turn any sensor part into an obs and apply actions to any controller part Wrapper glues can implement behaviors such as framestacking, delta actions May not directly read from the simulation, only interface through parts Rewards, Dones (Goal & Termination) - Composable functors common interface for sharing rewards and termination criteria in a stateful manner Composable state functors Rewards generate the current step reward for the agent Dones evaluate if the episode should stop on the current timestep These done\u2019s can be triggered for either success or failure Both Done and Reward Functors can view the entire simulation state to reward agents Done conditions typically add to the state when they trigger to signify what type of Done they are WIN/LOSE/DRAW Rewards are processed after Done conditions during an update, so rewards can read these labels There can be an arbitrary number of reward or done functors for an agent Agent + Experiment Class Agent Class Responsible for holding all of the Done/Reward/Glue functors for a given agent Can be many agent classes per platform When one agent class on a platform reaches a done, all on that platform do Different subclasses may process information in different ways or do different things Experiment Class Responsible for setting up an experiment and running it Configures and creates the environment Creates and configures the agent classes Use of this class allows for any arbitrary RL training framework to be used as the backend for training CoRL Integration and Simulator Swapping In CoRL all simulation specific components must be registered and retrieved from a plug-in library As long as a specific simulator has all of the parts registered to it that an agent needs, CoRL can swap the simulator and parts out from under an agent seamlessly As long as the parts for the two simulators have the same properties (in terms of sensed value bounds or controller inputs) there is no difference to the agent between the two and the regular environment can be used for integration Besides integration this also allows for cross simulation evaluation or training of an agent to be resumed in another simulator","title":"ACT3 RL Core"},{"location":"#benifits","text":"CoRL helps make RL environment development significantly easier CoRL provides hyper configurable environments/agents and experiments Instead of a new file every time a new observation is added, now just add a few lines of config Makes it possible to reuse glues/dones/rewards between different tasks if they are general Provides tools to use both domain randomization and curriculum learning through EPP An integration first focus means that integrating agents to the real world or different simulators is significantly easier","title":"Benifits"},{"location":"#install","text":"","title":"Install"},{"location":"#install-the-source-miniconda-local-host","text":"Miniconda Install Instruction # Create a virtual environment to install/run code conda create -n CoRL python == 3 .10.4 # Activate the virtual environment conda activate CoRL # install poetry pip install poetry == 1 .2.1 # Install the CoRL dependencies poetry install # Pre-commit setup pre-commit install","title":"Install the source - Miniconda - local host:"},{"location":"#how-to-install-pip-package","text":"","title":"How to install pip package"},{"location":"#build","text":"","title":"Build"},{"location":"#how-to-build-the-wheel-file","text":"The following project supports building python packages via Poetry . # Create a virtual environment to install/run code conda create -n CoRL python == 3 .10.4 # Activate the virtual environment conda activate CoRL # install poetry pip install poetry == 1 .2.1 # Build the CoRL package poetry build","title":"How to build the wheel file"},{"location":"#how-to-build-the-documentations-local","text":"The follow project is setup to use MKDOCS which is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. To build the documentation: mkdocs build To serve the documentation: mkdocs serve","title":"How to build the documentations - Local"},{"location":"#how-to-build-the-docker-containers","text":"The following project support development via Docker containers in VSCode. This is not strictly required but does provide the mode conveniet way to get started. Note: fuller documentation is available in the documentation folder or online docs. Setup the user env file: in code directory run the following script --> ./scripts/setup_env_docker.sh Build the Docker containers using compose: run the following command --> docker-compose build","title":"How to build the Docker containers"},{"location":"#running-base-examples","text":"python -m corl.train_rl --cfg config/experiments/cartpole_v1.yml","title":"Running base examples"},{"location":"#contributors","text":"AFRL Autonomy Capability Team (ACT3) AFRL ACT3 terry.wilson.11@us.af.mil bejamin.heiner@us.af.mil karl.salva@us.af.mil james.patrick@us.af.mil Training Team clong@toyon.com (ML Training) sfierro@toyon.com (ML Training / System Integration) bstieber@toyon.com (ML Training) joshua.blackburn@str.us (ML Training / System Integration) madison.blake@shield.ai (ML Infrastructure and Evaluation) AFRL Autonomy Capability Team (ACT3) Safe Autonomy (SA) Team kerianne.hobbs@us.af.mil (AFRL ACT3) terry.wilson.11@us.af.mil (AFRL ACT3)","title":"Contributors"},{"location":"#designation-indicator","text":"Controlled by: Air Force Research Laboratory (AFRL) Controlled by: AFRL Autonomy Capability Team (ACT3) LDC/Distribution Statement: DIST-A POCs: terry.wilson.11@us.af.mil (AFRL ACT3) bejamin.heiner@us.af.mil (AFRL ACT3) kerianne.hobbs@us.af.mil (AFRL ACT3)","title":"Designation Indicator"},{"location":"#notices-and-warnings","text":"","title":"Notices and Warnings"},{"location":"change_log/","text":"Intitial release of CoRL - Part #1 -Approved on 2022-05-2024 12:08:51 - PA Approval # [AFRL-2022-2455]\"","title":"Change log"},{"location":"concepts/","text":"Experiment \u00a4 An experiment contains the full description of all of the various elements of the machine learning task, including the environment, agents, platforms, and policies. Environment \u00a4 The environment specifies the overall goal of the experiment along with the constraints and rewards that inform the solution to the environment. Agents and Platforms \u00a4 A platform is something that exists within the environment, such as an aircraft. If the platform is controlled by a trained policy, it is an agent. Done \u00a4 A done condition is a termination condition for the episode that represents either the success or failure of the agent. Most done conditions apply to a single agent; however, shared dones can affect all agents. Types of Dones \u00a4 World: Done conditions that represent the state of the external world and are applicable to all agents. Common examples include geographic or altitude limits. Task: Done conditions that apply to a single agent and represent the unique success and failure conditions for this particular agent. Common examples include position capture, rejoin success, or death shot down. Platform: Done conditions that represent the physical constraints of the platform. Common examples include speed constraints. Shared: Done conditions that consolidate the overall state of all agents to determine the final state and done status of all agents. Glue \u00a4 A glue is an observation processor that takes a measurement and converts it to a new observation. Reward \u00a4 A reward is the the benefit received by the agent for the actions taken. Parts: Sensors and Controllers \u00a4 Platforms contain multiple discrete elements, such as sensors and controllers. A sensor provides the platform with raw observations of the environment. A controller allows an agent to convert a numeric action into a control signal for the underlying simulation. Simulator \u00a4 A simulator determines how the control signals provided from the agent's action update the state of the world and return an observation to the agent. Episode Parameter Provider \u00a4 An episode parameter provider controls the episode initialization parameters used by the environment on each reset. All initialization parameters are random distributions that can be sampled to provide the initialization of a particular episode of the environment. These parameters can also have an attached updater, which allows the episode parameter provider to adjust the hyperparameters of parameter distributions. References \u00a4 References allow multiple dones, glues, or rewards to reference the same parameter value. All items configured to use the same reference utilize the same draw of the parameter distribution so that the exact value is identical. Policy \u00a4 Plugins \u00a4","title":"Concepts"},{"location":"concepts/#experiment","text":"An experiment contains the full description of all of the various elements of the machine learning task, including the environment, agents, platforms, and policies.","title":"Experiment"},{"location":"concepts/#environment","text":"The environment specifies the overall goal of the experiment along with the constraints and rewards that inform the solution to the environment.","title":"Environment"},{"location":"concepts/#agents-and-platforms","text":"A platform is something that exists within the environment, such as an aircraft. If the platform is controlled by a trained policy, it is an agent.","title":"Agents and Platforms"},{"location":"concepts/#done","text":"A done condition is a termination condition for the episode that represents either the success or failure of the agent. Most done conditions apply to a single agent; however, shared dones can affect all agents.","title":"Done"},{"location":"concepts/#types-of-dones","text":"World: Done conditions that represent the state of the external world and are applicable to all agents. Common examples include geographic or altitude limits. Task: Done conditions that apply to a single agent and represent the unique success and failure conditions for this particular agent. Common examples include position capture, rejoin success, or death shot down. Platform: Done conditions that represent the physical constraints of the platform. Common examples include speed constraints. Shared: Done conditions that consolidate the overall state of all agents to determine the final state and done status of all agents.","title":"Types of Dones"},{"location":"concepts/#glue","text":"A glue is an observation processor that takes a measurement and converts it to a new observation.","title":"Glue"},{"location":"concepts/#reward","text":"A reward is the the benefit received by the agent for the actions taken.","title":"Reward"},{"location":"concepts/#parts-sensors-and-controllers","text":"Platforms contain multiple discrete elements, such as sensors and controllers. A sensor provides the platform with raw observations of the environment. A controller allows an agent to convert a numeric action into a control signal for the underlying simulation.","title":"Parts: Sensors and Controllers"},{"location":"concepts/#simulator","text":"A simulator determines how the control signals provided from the agent's action update the state of the world and return an observation to the agent.","title":"Simulator"},{"location":"concepts/#episode-parameter-provider","text":"An episode parameter provider controls the episode initialization parameters used by the environment on each reset. All initialization parameters are random distributions that can be sampled to provide the initialization of a particular episode of the environment. These parameters can also have an attached updater, which allows the episode parameter provider to adjust the hyperparameters of parameter distributions.","title":"Episode Parameter Provider"},{"location":"concepts/#references","text":"References allow multiple dones, glues, or rewards to reference the same parameter value. All items configured to use the same reference utilize the same draw of the parameter distribution so that the exact value is identical.","title":"References"},{"location":"concepts/#policy","text":"","title":"Policy"},{"location":"concepts/#plugins","text":"","title":"Plugins"},{"location":"configuration/","text":"General Configuration \u00a4 The general configuration file contains pointers to other parts of the configuration, along with the ability to override what is provided in those referenced configurations. Required Elements \u00a4 The general configuration needs to specify the experiment and the configuration of various components. The experiment is specified using the key experiment_class with a key that is the import path to the experiment. The experiment must be a subclass of BaseExperiment . For the RllibExperiment , the other components are specified with the following syntax: rllib_configs : default : <override array> <other config> : <override array> ray_config : <override array> env_config : <override array> tune_config : <override array> The rllib_configs includes the rllib configuration for various platform configurations. Which element of the rllib_configs is selected depends upon the compute-platform provide at the command line. Overrides \u00a4 The syntax of the override array is [!include filename1, !include filename2, *anchor1, *anchor2] . The number of included files and number of anchors is arbitrary. Later elements take precedence over earlier items. The filenames refer to separate configuration files that need to follow the format for ray, rllib, tune, or environments as described below. An anchor is specified as: arbitrary_name : &anchor_name <key> : <value> The keys and values need to match the configuration for whatever the anchor represents. Ray \u00a4 Tune \u00a4 RLLIB \u00a4 Hyperparameters \u00a4 Custom Models \u00a4 Environment \u00a4 The environment needs to specify the following items: plugin_paths : A list of import paths to load into the plugin library. simulator : Described below platforms : The name of the platforms to load from the plugin library. episode_parameter_provider : Described as a common element below. dones : world : A list of world dones, each which follows the syntax for functors, described below. task : agent1 : A list of task dones for agent1, each which follows the syntax for functors, described below. agentN : The names of the agents come from the experiment simulator_reset_parameters : A mapping with parameters for the reset method of the simulator class reference_store : Described as a common element below. The environment reference store can resolve references in both the environment and agent configuration files. Simulator \u00a4 The simulator configuration includes both the class and the initialization parameters of that class. The syntax is: type : Name of the class as registered within the plugin library. config : param1_name : Value for initialization param1 param2_name : Value for initialization param2 Agent \u00a4 The agent needs to specify the following items: agent : Import path to the agent class config : parts : List of parts as described below. episode_parameter_provider : Described as a common element below. glues : List of glues, each of which follows the syntax for functors, described below. dones : List of platform dones, each of which follows the syntax for functors, described below. rewards : List of rewards, each of which follows the syntax for functors, described below. simulator_reset_parameters : A mapping with parameters for the reset method of the simulator class reference_store : Described as a common element below. The agent reference store can only resolve references in the agent configuration file. Parts \u00a4 Platforms \u00a4 Policies \u00a4 Experiment \u00a4 Common Elements \u00a4 Episode Parameter Providers \u00a4 The episode parameter provider defines a class and the initialization parameters of that class: type : import path to the episode parameter provider config : param1_name : Value for initialization param1 param2_name : Value for initialization param2 In some cases the values of the initialization parameters for an episode parameter provider for an agent needs to know the name of that agent. As the agent configuration file is agnostic to the agent name, that can not be directly encoded in the file. Therefore, use the string %%AGENT%% wherever the agent name is needed. The agent class will automatically replace this string with the agent name. Note that the environment episode parameter provider cannot use this syntax. Parameters and Updaters \u00a4 A parameter defines the class and initialization parameters of that class: type : import path to the parameter config : units : The units of this parameter, or null if not relevant update : A mapping of hyperparameters of this parameter and the updater that controls it. <other initialization parameters> : Value of these parameters. An updater defines how the hyperparameters of a parameter are modified. The updater itself has the syntax: type : import path to the updater config : param1_name : Value for initialization parameter 1 param2_name : Value for initialization parameter 2 These updaters are combined into an updater mapping that also specifies the name of the hyperparameter that they update. As an example is worth more than an explanation, consider how a BoundStepUpdater is attached to the value element of a ConstantParameter : type : corl.libraries.parameters.ConstantParameter config : units : <As is appropriate> value : <The initial value of the parameter> update : value : # Matches the name of the hyperparameter above type : BoundStepUpdater config : bound : <Desired bound> step : <Desired step> bound_type : <Desired \"min\" or \"max\"> Note that parameters with multiple hyperparameters can have multiple updaters. Functors \u00a4 A functor has the following syntax: functor : Import path to the class held by the functor. This can be a done, glue, or reward. name : Optional name for the functor. The default is usually functor.__name__; however, some functors modify this, such as `SensorBoundsCheckDone`. If any of the config elements are parameters, this name must be globally unique. config : param1_name : Value for initialization parameter 1 as a constant, ValueWithUnits, Parameter, or anything else as appropriate. param2_name : Value for initialization parameter 2 as a constant, ValueWithUnits, Parameter, or anything else as appropriate. references : param3_name : Key in the reference store param4_name : Key in the reference store The units of a configuration element can be specified as a ValueWithUnits using the syntax {\"value\": value, \"units\": units} . The collection of configuration elements and references must specify all necessary initialization parameters needed to instantiate the functor. Dones need to be a direct Functor . Rewards can be a Functor or a FunctorWrapper . Glues can be a Functor , FunctorWrapper , or FunctorMultiWrapper . Reference Store \u00a4 The reference store is a mapping between reference names and the parameter to which they reference. The values need to follow the format for a Parameter as described above, potentially with updaters.","title":"Configuration"},{"location":"configuration/#general-configuration","text":"The general configuration file contains pointers to other parts of the configuration, along with the ability to override what is provided in those referenced configurations.","title":"General Configuration"},{"location":"configuration/#required-elements","text":"The general configuration needs to specify the experiment and the configuration of various components. The experiment is specified using the key experiment_class with a key that is the import path to the experiment. The experiment must be a subclass of BaseExperiment . For the RllibExperiment , the other components are specified with the following syntax: rllib_configs : default : <override array> <other config> : <override array> ray_config : <override array> env_config : <override array> tune_config : <override array> The rllib_configs includes the rllib configuration for various platform configurations. Which element of the rllib_configs is selected depends upon the compute-platform provide at the command line.","title":"Required Elements"},{"location":"configuration/#overrides","text":"The syntax of the override array is [!include filename1, !include filename2, *anchor1, *anchor2] . The number of included files and number of anchors is arbitrary. Later elements take precedence over earlier items. The filenames refer to separate configuration files that need to follow the format for ray, rllib, tune, or environments as described below. An anchor is specified as: arbitrary_name : &anchor_name <key> : <value> The keys and values need to match the configuration for whatever the anchor represents.","title":"Overrides"},{"location":"configuration/#ray","text":"","title":"Ray"},{"location":"configuration/#tune","text":"","title":"Tune"},{"location":"configuration/#rllib","text":"","title":"RLLIB"},{"location":"configuration/#hyperparameters","text":"","title":"Hyperparameters"},{"location":"configuration/#custom-models","text":"","title":"Custom Models"},{"location":"configuration/#environment","text":"The environment needs to specify the following items: plugin_paths : A list of import paths to load into the plugin library. simulator : Described below platforms : The name of the platforms to load from the plugin library. episode_parameter_provider : Described as a common element below. dones : world : A list of world dones, each which follows the syntax for functors, described below. task : agent1 : A list of task dones for agent1, each which follows the syntax for functors, described below. agentN : The names of the agents come from the experiment simulator_reset_parameters : A mapping with parameters for the reset method of the simulator class reference_store : Described as a common element below. The environment reference store can resolve references in both the environment and agent configuration files.","title":"Environment"},{"location":"configuration/#simulator","text":"The simulator configuration includes both the class and the initialization parameters of that class. The syntax is: type : Name of the class as registered within the plugin library. config : param1_name : Value for initialization param1 param2_name : Value for initialization param2","title":"Simulator"},{"location":"configuration/#agent","text":"The agent needs to specify the following items: agent : Import path to the agent class config : parts : List of parts as described below. episode_parameter_provider : Described as a common element below. glues : List of glues, each of which follows the syntax for functors, described below. dones : List of platform dones, each of which follows the syntax for functors, described below. rewards : List of rewards, each of which follows the syntax for functors, described below. simulator_reset_parameters : A mapping with parameters for the reset method of the simulator class reference_store : Described as a common element below. The agent reference store can only resolve references in the agent configuration file.","title":"Agent"},{"location":"configuration/#parts","text":"","title":"Parts"},{"location":"configuration/#platforms","text":"","title":"Platforms"},{"location":"configuration/#policies","text":"","title":"Policies"},{"location":"configuration/#experiment","text":"","title":"Experiment"},{"location":"configuration/#common-elements","text":"","title":"Common Elements"},{"location":"configuration/#episode-parameter-providers","text":"The episode parameter provider defines a class and the initialization parameters of that class: type : import path to the episode parameter provider config : param1_name : Value for initialization param1 param2_name : Value for initialization param2 In some cases the values of the initialization parameters for an episode parameter provider for an agent needs to know the name of that agent. As the agent configuration file is agnostic to the agent name, that can not be directly encoded in the file. Therefore, use the string %%AGENT%% wherever the agent name is needed. The agent class will automatically replace this string with the agent name. Note that the environment episode parameter provider cannot use this syntax.","title":"Episode Parameter Providers"},{"location":"configuration/#parameters-and-updaters","text":"A parameter defines the class and initialization parameters of that class: type : import path to the parameter config : units : The units of this parameter, or null if not relevant update : A mapping of hyperparameters of this parameter and the updater that controls it. <other initialization parameters> : Value of these parameters. An updater defines how the hyperparameters of a parameter are modified. The updater itself has the syntax: type : import path to the updater config : param1_name : Value for initialization parameter 1 param2_name : Value for initialization parameter 2 These updaters are combined into an updater mapping that also specifies the name of the hyperparameter that they update. As an example is worth more than an explanation, consider how a BoundStepUpdater is attached to the value element of a ConstantParameter : type : corl.libraries.parameters.ConstantParameter config : units : <As is appropriate> value : <The initial value of the parameter> update : value : # Matches the name of the hyperparameter above type : BoundStepUpdater config : bound : <Desired bound> step : <Desired step> bound_type : <Desired \"min\" or \"max\"> Note that parameters with multiple hyperparameters can have multiple updaters.","title":"Parameters and Updaters"},{"location":"configuration/#functors","text":"A functor has the following syntax: functor : Import path to the class held by the functor. This can be a done, glue, or reward. name : Optional name for the functor. The default is usually functor.__name__; however, some functors modify this, such as `SensorBoundsCheckDone`. If any of the config elements are parameters, this name must be globally unique. config : param1_name : Value for initialization parameter 1 as a constant, ValueWithUnits, Parameter, or anything else as appropriate. param2_name : Value for initialization parameter 2 as a constant, ValueWithUnits, Parameter, or anything else as appropriate. references : param3_name : Key in the reference store param4_name : Key in the reference store The units of a configuration element can be specified as a ValueWithUnits using the syntax {\"value\": value, \"units\": units} . The collection of configuration elements and references must specify all necessary initialization parameters needed to instantiate the functor. Dones need to be a direct Functor . Rewards can be a Functor or a FunctorWrapper . Glues can be a Functor , FunctorWrapper , or FunctorMultiWrapper .","title":"Functors"},{"location":"configuration/#reference-store","text":"The reference store is a mapping between reference names and the parameter to which they reference. The values need to follow the format for a Parameter as described above, potentially with updaters.","title":"Reference Store"},{"location":"credits/","text":"Credits \u00a4 These projects were used to build corl . Thank you! python | pdm | copier-pdm Direct dependencies \u00a4 `](https://pypi.org/project//) |[ 0 ](https://pypi.org/project/0/) |[ 1 ](https://pypi.org/project/1/) |[ 2 ](https://pypi.org/project/2/) |[ 3 ](https://pypi.org/project/3/) |[ 4 ](https://pypi.org/project/4/) |[ 5 ](https://pypi.org/project/5/) |[ 9 ](https://pypi.org/project/9/) |[ deepmerge ](https://pypi.org/project/deepmerge/) |[ flatten-dict ](https://pypi.org/project/flatten-dict/) |[ gitpython ](https://pypi.org/project/gitpython/) |[ h5py ](https://pypi.org/project/h5py/) |[ jsonargparse ](https://pypi.org/project/jsonargparse/) |[ numpy-ringbuffer ](https://pypi.org/project/numpy-ringbuffer/) |[ pydantic ](https://pypi.org/project/pydantic/) |[ python ](https://pypi.org/project/python/) |[ ray ](https://pypi.org/project/ray/) |[ tensorboard ](https://pypi.org/project/tensorboard/) |[ tensorflow` Indirect dependencies \u00a4 absl-py | aiohttp | aiohttp_cors | aiorwlock | aiosignal | alabaster | ansicon | ansiwrap | anyio | argcomplete | asgiref | astroid | astunparse | async-timeout | attrs | babel | backoff | bashate | beautifulsoup4 | black | blacken-docs | blessed | brotli | brotlicffi | cachetools | certifi | cffi | cfgv | charset-normalizer | click | cloudpickle | colorama | colorful | commonmark | coverage | cssselect2 | cycler | detect-secrets | distlib | dm-tree | docstring-parser | docutils | entrypoints | fastapi | fastjsonschema | filelock | flake8 | flatbuffers | fonttools | frozenlist | fsspec | gast | ghp-import | gitdb | google-api-core | google-auth | google-auth-oauthlib | google-pasta | googleapis-common-protos | gpustat | grpcio | gym | h11 | html5lib | identify | idna | imageio | imagesize | importlib-metadata | importlib-resources | iniconfig | iso8601 | isort | jinja2 | jinxed | jsonschema | jupyter-client | jupyter-core | keras | keras-preprocessing | kiwisolver | kopf | kubernetes | lazy-object-proxy | libclang | libsass | lz4 | markdown | markupsafe | matplotlib | mccabe | memory-profiler | memray | mergedeep | mkdocs | mkdocs-autorefs | mkdocs-coverage | mkdocs-gen-files | mkdocs-git-revision-date-localized-plugin | mkdocs-literate-nav | mkdocs-macros-plugin | mkdocs-material | mkdocs-material-extensions | mkdocs-mermaid-plugin | mkdocs-pdf-export-plugin | mkdocs-section-index | mkdocs-with-pdf | mkdocstrings | mkdocstrings-python-legacy | mktheapidocs | msgpack | multidict | mypy | mypy-extensions | nbclient | nbformat | nest-asyncio | networkx | nodeenv | numpy | numpydoc | nvidia-ml-py | oauthlib | opencensus | opencensus-context | opentelemetry-api | opentelemetry-exporter-otlp | opentelemetry-exporter-otlp-proto-grpc | opentelemetry-proto | opentelemetry-sdk | opentelemetry-semantic-conventions | opt-einsum | packaging | pandas | papermill | pathspec | pbr | pillow | pkgutil_resolve_name | platformdirs | pluggy | pre-commit | pre-commit-hooks | prometheus-client | protobuf | psutil | py | py-spy | pyarrow | pyasn1 | pyasn1-modules | pycodestyle | pycparser | pydyf | pyflakes | pygments | pyinstrument | pylint | pymdown-extensions | pyparsing | pyphen | pyrsistent | pytest | pytest-cov | python-dateutil | python-json-logger | pytkdocs | pytz | pywavelets | pywin32 | pyyaml | pyyaml_env_tag | pyzmq | ray-cpp | requests | requests-oauthlib | rich | rope | rsa | ruamel.yaml | ruamel.yaml.clib | scikit-image | scipy | setuptools | setuptools-scm | six | smart-open | smmap | snakeviz | sniffio | snowballstemmer | soupsieve | sphinx | sphinxcontrib-applehelp | sphinxcontrib-devhelp | sphinxcontrib-htmlhelp | sphinxcontrib-jsmath | sphinxcontrib-qthelp | sphinxcontrib-serializinghtml | starlette | tabulate | tenacity | tensorboard-data-server | tensorboard-plugin-wit | tensorboardx | tensorflow-estimator | tensorflow-io-gcs-filesystem | termcolor | textwrap3 | there | tifffile | tinycss2 | toml | tomli | torch | tornado | tqdm | traitlets | types-pyyaml | typing-extensions | urllib3 | uvicorn | velin | virtualenv | watchdog | wcwidth | weasyprint | webencodings | websocket-client | werkzeug | wheel | wrapt | yapf | yarl | zipp | zopfli More credits from the author","title":"Credits"},{"location":"credits/#credits","text":"These projects were used to build corl . Thank you! python | pdm | copier-pdm","title":"Credits"},{"location":"credits/#direct-dependencies","text":"`](https://pypi.org/project//) |[ 0 ](https://pypi.org/project/0/) |[ 1 ](https://pypi.org/project/1/) |[ 2 ](https://pypi.org/project/2/) |[ 3 ](https://pypi.org/project/3/) |[ 4 ](https://pypi.org/project/4/) |[ 5 ](https://pypi.org/project/5/) |[ 9 ](https://pypi.org/project/9/) |[ deepmerge ](https://pypi.org/project/deepmerge/) |[ flatten-dict ](https://pypi.org/project/flatten-dict/) |[ gitpython ](https://pypi.org/project/gitpython/) |[ h5py ](https://pypi.org/project/h5py/) |[ jsonargparse ](https://pypi.org/project/jsonargparse/) |[ numpy-ringbuffer ](https://pypi.org/project/numpy-ringbuffer/) |[ pydantic ](https://pypi.org/project/pydantic/) |[ python ](https://pypi.org/project/python/) |[ ray ](https://pypi.org/project/ray/) |[ tensorboard ](https://pypi.org/project/tensorboard/) |[ tensorflow`","title":"Direct dependencies"},{"location":"credits/#indirect-dependencies","text":"absl-py | aiohttp | aiohttp_cors | aiorwlock | aiosignal | alabaster | ansicon | ansiwrap | anyio | argcomplete | asgiref | astroid | astunparse | async-timeout | attrs | babel | backoff | bashate | beautifulsoup4 | black | blacken-docs | blessed | brotli | brotlicffi | cachetools | certifi | cffi | cfgv | charset-normalizer | click | cloudpickle | colorama | colorful | commonmark | coverage | cssselect2 | cycler | detect-secrets | distlib | dm-tree | docstring-parser | docutils | entrypoints | fastapi | fastjsonschema | filelock | flake8 | flatbuffers | fonttools | frozenlist | fsspec | gast | ghp-import | gitdb | google-api-core | google-auth | google-auth-oauthlib | google-pasta | googleapis-common-protos | gpustat | grpcio | gym | h11 | html5lib | identify | idna | imageio | imagesize | importlib-metadata | importlib-resources | iniconfig | iso8601 | isort | jinja2 | jinxed | jsonschema | jupyter-client | jupyter-core | keras | keras-preprocessing | kiwisolver | kopf | kubernetes | lazy-object-proxy | libclang | libsass | lz4 | markdown | markupsafe | matplotlib | mccabe | memory-profiler | memray | mergedeep | mkdocs | mkdocs-autorefs | mkdocs-coverage | mkdocs-gen-files | mkdocs-git-revision-date-localized-plugin | mkdocs-literate-nav | mkdocs-macros-plugin | mkdocs-material | mkdocs-material-extensions | mkdocs-mermaid-plugin | mkdocs-pdf-export-plugin | mkdocs-section-index | mkdocs-with-pdf | mkdocstrings | mkdocstrings-python-legacy | mktheapidocs | msgpack | multidict | mypy | mypy-extensions | nbclient | nbformat | nest-asyncio | networkx | nodeenv | numpy | numpydoc | nvidia-ml-py | oauthlib | opencensus | opencensus-context | opentelemetry-api | opentelemetry-exporter-otlp | opentelemetry-exporter-otlp-proto-grpc | opentelemetry-proto | opentelemetry-sdk | opentelemetry-semantic-conventions | opt-einsum | packaging | pandas | papermill | pathspec | pbr | pillow | pkgutil_resolve_name | platformdirs | pluggy | pre-commit | pre-commit-hooks | prometheus-client | protobuf | psutil | py | py-spy | pyarrow | pyasn1 | pyasn1-modules | pycodestyle | pycparser | pydyf | pyflakes | pygments | pyinstrument | pylint | pymdown-extensions | pyparsing | pyphen | pyrsistent | pytest | pytest-cov | python-dateutil | python-json-logger | pytkdocs | pytz | pywavelets | pywin32 | pyyaml | pyyaml_env_tag | pyzmq | ray-cpp | requests | requests-oauthlib | rich | rope | rsa | ruamel.yaml | ruamel.yaml.clib | scikit-image | scipy | setuptools | setuptools-scm | six | smart-open | smmap | snakeviz | sniffio | snowballstemmer | soupsieve | sphinx | sphinxcontrib-applehelp | sphinxcontrib-devhelp | sphinxcontrib-htmlhelp | sphinxcontrib-jsmath | sphinxcontrib-qthelp | sphinxcontrib-serializinghtml | starlette | tabulate | tenacity | tensorboard-data-server | tensorboard-plugin-wit | tensorboardx | tensorflow-estimator | tensorflow-io-gcs-filesystem | termcolor | textwrap3 | there | tifffile | tinycss2 | toml | tomli | torch | tornado | tqdm | traitlets | types-pyyaml | typing-extensions | urllib3 | uvicorn | velin | virtualenv | watchdog | wcwidth | weasyprint | webencodings | websocket-client | werkzeug | wheel | wrapt | yapf | yarl | zipp | zopfli More credits from the author","title":"Indirect dependencies"},{"location":"diverse-frame-rate/","text":"Agents need not operate at the same frame rate. The BaseAgent allows a user to specify the rate at which an agent should be processed by the environment (which also afects the rate at which actions are generated by a policy). The BaseAgentParser provides a field frame_rate which will define the rate (in Hertz) that the agent will be processed (e.g. a {frame_rate: 0.5} means that the agent will be processed every other time step and have a period == 2). By default, agent frame rates are rounded to the nearest 20th of a second. The rate the simulation is run at, is the lcm of all the agents in the system (see multi_agent_env.py for the most up-to-date implementation): agent_periods = [ 1 / agent . frame_rate for agent in agents ] sim_period = get_sim_period ( agent_periods ) def get_sim_period ( agent_periods : typing . List [ float ], max_agent_rate_hz : int = 20 ): def compute_lcm ( values : typing . List [ fractions . Fraction ]) -> float : assert len ( values ) > 0 lcm = values [ 0 ] . denominator for v in values : lcm = lcm // math . gcd ( lcm , v . denominator ) * v . denominator return 1.0 / lcm agent_periods = [ fractions . Fraction ( p ) . limit_denominator ( max_agent_rate_hz ) for p in agent_periods ] sim_period = compute_lcm ( agent_periods ) return sim_period >>> get_sim_period ( agent_periods = [ .3 , .5 , .4 ]) # [Fraction(3, 10), Fraction(1, 2), Fraction(2, 5)] 0.1 >>> get_sim_period ( agent_periods = [ .33 , .5 , .4 ]) # [Fraction(1, 3), Fraction(1, 2), Fraction(2, 5)] 0.03333333333333333 >>> get_sim_period ( agent_periods = [ 3 , 2 , 1 ]) # [Fraction(3, 1), Fraction(2, 1), Fraction(1, 1)] 1.0","title":"Diverse frame rate"},{"location":"install/","text":"How to install pip package from project level with a personal access token \u00a4 In your User Settings->Access Tokens, you are able to create a Personal Access Token. Create a token with read_api scope. The name of the token will be your username, the value of the token will be your password. Be sure to save them, because you will not be able to view them again. Use this command to install the pip package: pip install corl --no-deps --index-url https://<username>:<password>@git.act3-ace.com/api/v4/projects/657/packages/pypi/simple ` The --no-deps tag` is optional. Use it if you do not want dependency packages installed with the package. The project should already have the dependencies in the package. Do NOT use the command provided by GitLab that utilizes the --extra-index-url tag . This tag will check PYPI.org first for the package, and you will get an error. How to build the docker containers and get started \u00a4 The following project support development via docker containers in vscode and on the DOD HPC. This is not strictly required but does provide the mode convenient way to get started. Note: fuller documentation is available in the documentation folder or online docs. Setup the user env file: in code directory run the following script --> ./scripts/setup_env_docker.sh Build the docker containers using compose: run the following command --> docker-compose build How to build the documentation locally \u00a4 This repository is setup to use MKDOCS which is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introductory tutorial, then check the User Guide for more information. Install Mkdocs Modules in container/virtual environment run the following command --> pip install -U -r mkdocs-requirements.txt Build Documentation: Inside docker container run the following command --> python -m mkdocs build Serve Documentation: Inside docker container run one of the following commands --> python -m mkdocs serve python -m mkdocs serve --no-livereload If using WSL: python -m mkdocs serve -a $(hostname -i):8000 --no-livereload . You will still browse to http://localhost:8000 .","title":"Install"},{"location":"install/#how-to-install-pip-package-from-project-level-with-a-personal-access-token","text":"In your User Settings->Access Tokens, you are able to create a Personal Access Token. Create a token with read_api scope. The name of the token will be your username, the value of the token will be your password. Be sure to save them, because you will not be able to view them again. Use this command to install the pip package: pip install corl --no-deps --index-url https://<username>:<password>@git.act3-ace.com/api/v4/projects/657/packages/pypi/simple ` The --no-deps tag` is optional. Use it if you do not want dependency packages installed with the package. The project should already have the dependencies in the package. Do NOT use the command provided by GitLab that utilizes the --extra-index-url tag . This tag will check PYPI.org first for the package, and you will get an error.","title":"How to install pip package from project level with a personal access token"},{"location":"install/#how-to-build-the-docker-containers-and-get-started","text":"The following project support development via docker containers in vscode and on the DOD HPC. This is not strictly required but does provide the mode convenient way to get started. Note: fuller documentation is available in the documentation folder or online docs. Setup the user env file: in code directory run the following script --> ./scripts/setup_env_docker.sh Build the docker containers using compose: run the following command --> docker-compose build","title":"How to build the docker containers and get started"},{"location":"install/#how-to-build-the-documentation-locally","text":"This repository is setup to use MKDOCS which is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introductory tutorial, then check the User Guide for more information. Install Mkdocs Modules in container/virtual environment run the following command --> pip install -U -r mkdocs-requirements.txt Build Documentation: Inside docker container run the following command --> python -m mkdocs build Serve Documentation: Inside docker container run one of the following commands --> python -m mkdocs serve python -m mkdocs serve --no-livereload If using WSL: python -m mkdocs serve -a $(hostname -i):8000 --no-livereload . You will still browse to http://localhost:8000 .","title":"How to build the documentation locally"},{"location":"license/","text":"License to Accompany corl \u00a4","title":"License"},{"location":"license/#license-to-accompany-corl","text":"","title":"License to Accompany corl"},{"location":"reference/SUMMARY/","text":"__init__.py agents __init__.py base_agent.py dones __init__.py docking_1d __init__.py dones.py done_func_base.py done_func_dict_wrapper.py done_func_multi_wrapper.py done_func_wrapper.py episode_length_done.py openai_gym_done.py sensor_bounds_check_done.py environment __init__.py default_env_rllib_callbacks.py gym_env.py multi_agent_env.py utils __init__.py env_creation.py obs_buffer.py space_sort.py episode_parameter_providers __init__.py core.py remote.py simple.py experiments __init__.py base_experiment.py benchmark_experiment.py rllib_experiment.py glues __init__.py base_dict_wrapper.py base_glue.py base_multi_wrapper.py base_wrapper.py common __init__.py arithmetic_multi_glue.py controller_glue.py observe_part_validity.py observe_sensor.py observe_sensor_repeated.py projected_quantity.py target_value.py target_value_difference.py controller_wrappers __init__.py delta_controller.py obs_relative_delta_controller.py obs_relative_delta_controller_dict.py libraries __init__.py cleanup.py collection_utils.py env_common.py env_func_base.py env_space_util.py environment_dict.py factory.py functor.py hparam_search_util.py nan_check.py observation_extractor.py observation_util.py parameters.py plugin_library.py property.py rllib_setup_util.py state_dict.py units.py models __init__.py frame_stacking.py torch_frame_stack.py parsers __init__.py yaml_loader.py policies __init__.py base_policy.py custom_policy.py random_action.py scripted_action.py rewards __init__.py base_measurement_operation.py docking_1d __init__.py docking_distance_change_reward.py docking_reward.py episode_done.py exponential_decay_from_target_value.py multi_measurement_operation.py openai_gym_reward.py reward_func_base.py reward_func_dict_wrapper.py reward_func_multi_wrapper.py reward_func_wrapper.py simulators __init__.py base_available_platforms.py base_parts.py base_platform.py base_properties.py base_simulator.py common_platform_utils.py docking_1d __init__.py available_platforms.py controllers.py entities.py platform.py properties.py sensors.py simulator.py openai_gym __init__.py gym_available_platforms.py gym_controllers.py gym_sensors.py gym_simulator.py six_dof __init__.py base_six_dof_controllers.py base_six_dof_platform.py base_six_dof_properties.py train_rl.py","title":"ACT3 CORE"},{"location":"reference/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"  init  "},{"location":"reference/train_rl/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. MainUtilACT3Core \u00a4 Contains all the procedures that allow for argument parsing to setup an experiment Source code in corl/train_rl.py class MainUtilACT3Core : \"\"\" Contains all the procedures that allow for argument parsing to setup an experiment \"\"\" DEFAULT_CONFIG_PATH = str ( pathlib . Path ( __file__ ) . parent . absolute () / 'config' / 'tasks' / 'single_lear_capture.yml' ) @staticmethod def parse_args ( alternate_argv : typing . Optional [ typing . Sequence [ str ]] = None ): \"\"\" Processes the arguments as main entry point for CoRL deep reinforcement training code Parameters ---------- alternate_argv : Sequence[str], optional Arguments that should be parsed rather than sys.argv. The default of None parses sys.argv. See https://docs.python.org/3/library/argparse.html#beyond-sys-argv. Returns ------- namespace The arguments from the parser \"\"\" parser = jsonargparse . ArgumentParser () parser . add_argument ( \"--cfg\" , help = \"an alternative way to provide arguments, the path to a json/yml file containing the running arguments\" , action = jsonargparse . ActionConfigFile ) parser . add_argument ( \"--config\" , type = str , default = MainUtilACT3Core . DEFAULT_CONFIG_PATH , help = f \"Path to config.yml file used to setup the training environment Default= { MainUtilACT3Core . DEFAULT_CONFIG_PATH } \" , ) parser . add_argument ( \"--compute-platform\" , type = str , default = \"auto\" , help = \"Compute platform [ace, hpc, local, auto] of experiment. Used to select rllib_config\" , ) parser . add_argument ( \"-pc\" , \"--platform-config\" , action = \"append\" , nargs = 2 , metavar = ( \"platform-name\" , \"platform-file\" ), help = \"the specification for a platform in the environment\" ) parser . add_argument ( \"-ac\" , \"--agent-config\" , action = \"append\" , nargs = 4 , metavar = ( \"agent-name\" , \"platform-name\" , \"configuration-file\" , \"policy-file\" ), help = \"the specification for an agent in the environment\" ) parser . add_argument ( \"-op\" , \"--other-platform\" , action = \"append\" , nargs = 2 , metavar = ( \"agent-name\" , \"platform-file\" ), help = \"help:\" ) parser . add_argument ( '--debug' , action = 'store_true' , help = \"Tells your specified experiment to switch configurations to debug mode. Experiments may ignore this flag\" ) parser . add_argument ( '--name' , action = 'store' , help = \"Tells your specified experiment to update its name. Experiments may ignore this directive.\" ) parser . add_argument ( '--output' , action = 'store' , help = \"Tells your specified experiment to update its output directory. Experiments may ignore this directive.\" ) parser . add_argument ( '--profile' , action = 'store_true' , help = \"Tells experiment to switch configuration to profile mode\" ) parser . add_argument ( '--profile-iterations' , type = int , default = 10 ) return parser . parse_args ( args = alternate_argv ) parse_args ( alternate_argv = None ) staticmethod \u00a4 Processes the arguments as main entry point for CoRL deep reinforcement training code Parameters \u00a4 alternate_argv : Sequence[str], optional Arguments that should be parsed rather than sys.argv. The default of None parses sys.argv. See https://docs.python.org/3/library/argparse.html#beyond-sys-argv . Returns \u00a4 namespace The arguments from the parser Source code in corl/train_rl.py @staticmethod def parse_args ( alternate_argv : typing . Optional [ typing . Sequence [ str ]] = None ): \"\"\" Processes the arguments as main entry point for CoRL deep reinforcement training code Parameters ---------- alternate_argv : Sequence[str], optional Arguments that should be parsed rather than sys.argv. The default of None parses sys.argv. See https://docs.python.org/3/library/argparse.html#beyond-sys-argv. Returns ------- namespace The arguments from the parser \"\"\" parser = jsonargparse . ArgumentParser () parser . add_argument ( \"--cfg\" , help = \"an alternative way to provide arguments, the path to a json/yml file containing the running arguments\" , action = jsonargparse . ActionConfigFile ) parser . add_argument ( \"--config\" , type = str , default = MainUtilACT3Core . DEFAULT_CONFIG_PATH , help = f \"Path to config.yml file used to setup the training environment Default= { MainUtilACT3Core . DEFAULT_CONFIG_PATH } \" , ) parser . add_argument ( \"--compute-platform\" , type = str , default = \"auto\" , help = \"Compute platform [ace, hpc, local, auto] of experiment. Used to select rllib_config\" , ) parser . add_argument ( \"-pc\" , \"--platform-config\" , action = \"append\" , nargs = 2 , metavar = ( \"platform-name\" , \"platform-file\" ), help = \"the specification for a platform in the environment\" ) parser . add_argument ( \"-ac\" , \"--agent-config\" , action = \"append\" , nargs = 4 , metavar = ( \"agent-name\" , \"platform-name\" , \"configuration-file\" , \"policy-file\" ), help = \"the specification for an agent in the environment\" ) parser . add_argument ( \"-op\" , \"--other-platform\" , action = \"append\" , nargs = 2 , metavar = ( \"agent-name\" , \"platform-file\" ), help = \"help:\" ) parser . add_argument ( '--debug' , action = 'store_true' , help = \"Tells your specified experiment to switch configurations to debug mode. Experiments may ignore this flag\" ) parser . add_argument ( '--name' , action = 'store' , help = \"Tells your specified experiment to update its name. Experiments may ignore this directive.\" ) parser . add_argument ( '--output' , action = 'store' , help = \"Tells your specified experiment to update its output directory. Experiments may ignore this directive.\" ) parser . add_argument ( '--profile' , action = 'store_true' , help = \"Tells experiment to switch configuration to profile mode\" ) parser . add_argument ( '--profile-iterations' , type = int , default = 10 ) return parser . parse_args ( args = alternate_argv ) main () \u00a4 Main method of the module that allows for arguments parsing for experiment setup. Source code in corl/train_rl.py def main (): \"\"\" Main method of the module that allows for arguments parsing for experiment setup. \"\"\" args = MainUtilACT3Core . parse_args () config = load_file ( config_filename = args . config ) # print(config) experiment_parse = ExperimentParse ( ** config ) experiment_class = experiment_parse . experiment_class ( ** experiment_parse . config ) print ( experiment_class . config . dict ()) if experiment_parse . auto_system_detect_class is not None and args . compute_platform == 'auto' : args . compute_platform = experiment_parse . auto_system_detect_class () . autodetect_system () # pylint: disable=not-callable elif experiment_parse . auto_system_detect_class is None and args . compute_platform == 'auto' : args . compute_platform = \"local\" experiment_class . run_experiment ( args )","title":"Train rl"},{"location":"reference/train_rl/#corl.train_rl.MainUtilACT3Core","text":"Contains all the procedures that allow for argument parsing to setup an experiment Source code in corl/train_rl.py class MainUtilACT3Core : \"\"\" Contains all the procedures that allow for argument parsing to setup an experiment \"\"\" DEFAULT_CONFIG_PATH = str ( pathlib . Path ( __file__ ) . parent . absolute () / 'config' / 'tasks' / 'single_lear_capture.yml' ) @staticmethod def parse_args ( alternate_argv : typing . Optional [ typing . Sequence [ str ]] = None ): \"\"\" Processes the arguments as main entry point for CoRL deep reinforcement training code Parameters ---------- alternate_argv : Sequence[str], optional Arguments that should be parsed rather than sys.argv. The default of None parses sys.argv. See https://docs.python.org/3/library/argparse.html#beyond-sys-argv. Returns ------- namespace The arguments from the parser \"\"\" parser = jsonargparse . ArgumentParser () parser . add_argument ( \"--cfg\" , help = \"an alternative way to provide arguments, the path to a json/yml file containing the running arguments\" , action = jsonargparse . ActionConfigFile ) parser . add_argument ( \"--config\" , type = str , default = MainUtilACT3Core . DEFAULT_CONFIG_PATH , help = f \"Path to config.yml file used to setup the training environment Default= { MainUtilACT3Core . DEFAULT_CONFIG_PATH } \" , ) parser . add_argument ( \"--compute-platform\" , type = str , default = \"auto\" , help = \"Compute platform [ace, hpc, local, auto] of experiment. Used to select rllib_config\" , ) parser . add_argument ( \"-pc\" , \"--platform-config\" , action = \"append\" , nargs = 2 , metavar = ( \"platform-name\" , \"platform-file\" ), help = \"the specification for a platform in the environment\" ) parser . add_argument ( \"-ac\" , \"--agent-config\" , action = \"append\" , nargs = 4 , metavar = ( \"agent-name\" , \"platform-name\" , \"configuration-file\" , \"policy-file\" ), help = \"the specification for an agent in the environment\" ) parser . add_argument ( \"-op\" , \"--other-platform\" , action = \"append\" , nargs = 2 , metavar = ( \"agent-name\" , \"platform-file\" ), help = \"help:\" ) parser . add_argument ( '--debug' , action = 'store_true' , help = \"Tells your specified experiment to switch configurations to debug mode. Experiments may ignore this flag\" ) parser . add_argument ( '--name' , action = 'store' , help = \"Tells your specified experiment to update its name. Experiments may ignore this directive.\" ) parser . add_argument ( '--output' , action = 'store' , help = \"Tells your specified experiment to update its output directory. Experiments may ignore this directive.\" ) parser . add_argument ( '--profile' , action = 'store_true' , help = \"Tells experiment to switch configuration to profile mode\" ) parser . add_argument ( '--profile-iterations' , type = int , default = 10 ) return parser . parse_args ( args = alternate_argv )","title":"MainUtilACT3Core"},{"location":"reference/train_rl/#corl.train_rl.MainUtilACT3Core.parse_args","text":"Processes the arguments as main entry point for CoRL deep reinforcement training code","title":"parse_args()"},{"location":"reference/train_rl/#corl.train_rl.MainUtilACT3Core.parse_args--parameters","text":"alternate_argv : Sequence[str], optional Arguments that should be parsed rather than sys.argv. The default of None parses sys.argv. See https://docs.python.org/3/library/argparse.html#beyond-sys-argv .","title":"Parameters"},{"location":"reference/train_rl/#corl.train_rl.MainUtilACT3Core.parse_args--returns","text":"namespace The arguments from the parser Source code in corl/train_rl.py @staticmethod def parse_args ( alternate_argv : typing . Optional [ typing . Sequence [ str ]] = None ): \"\"\" Processes the arguments as main entry point for CoRL deep reinforcement training code Parameters ---------- alternate_argv : Sequence[str], optional Arguments that should be parsed rather than sys.argv. The default of None parses sys.argv. See https://docs.python.org/3/library/argparse.html#beyond-sys-argv. Returns ------- namespace The arguments from the parser \"\"\" parser = jsonargparse . ArgumentParser () parser . add_argument ( \"--cfg\" , help = \"an alternative way to provide arguments, the path to a json/yml file containing the running arguments\" , action = jsonargparse . ActionConfigFile ) parser . add_argument ( \"--config\" , type = str , default = MainUtilACT3Core . DEFAULT_CONFIG_PATH , help = f \"Path to config.yml file used to setup the training environment Default= { MainUtilACT3Core . DEFAULT_CONFIG_PATH } \" , ) parser . add_argument ( \"--compute-platform\" , type = str , default = \"auto\" , help = \"Compute platform [ace, hpc, local, auto] of experiment. Used to select rllib_config\" , ) parser . add_argument ( \"-pc\" , \"--platform-config\" , action = \"append\" , nargs = 2 , metavar = ( \"platform-name\" , \"platform-file\" ), help = \"the specification for a platform in the environment\" ) parser . add_argument ( \"-ac\" , \"--agent-config\" , action = \"append\" , nargs = 4 , metavar = ( \"agent-name\" , \"platform-name\" , \"configuration-file\" , \"policy-file\" ), help = \"the specification for an agent in the environment\" ) parser . add_argument ( \"-op\" , \"--other-platform\" , action = \"append\" , nargs = 2 , metavar = ( \"agent-name\" , \"platform-file\" ), help = \"help:\" ) parser . add_argument ( '--debug' , action = 'store_true' , help = \"Tells your specified experiment to switch configurations to debug mode. Experiments may ignore this flag\" ) parser . add_argument ( '--name' , action = 'store' , help = \"Tells your specified experiment to update its name. Experiments may ignore this directive.\" ) parser . add_argument ( '--output' , action = 'store' , help = \"Tells your specified experiment to update its output directory. Experiments may ignore this directive.\" ) parser . add_argument ( '--profile' , action = 'store_true' , help = \"Tells experiment to switch configuration to profile mode\" ) parser . add_argument ( '--profile-iterations' , type = int , default = 10 ) return parser . parse_args ( args = alternate_argv )","title":"Returns"},{"location":"reference/train_rl/#corl.train_rl.main","text":"Main method of the module that allows for arguments parsing for experiment setup. Source code in corl/train_rl.py def main (): \"\"\" Main method of the module that allows for arguments parsing for experiment setup. \"\"\" args = MainUtilACT3Core . parse_args () config = load_file ( config_filename = args . config ) # print(config) experiment_parse = ExperimentParse ( ** config ) experiment_class = experiment_parse . experiment_class ( ** experiment_parse . config ) print ( experiment_class . config . dict ()) if experiment_parse . auto_system_detect_class is not None and args . compute_platform == 'auto' : args . compute_platform = experiment_parse . auto_system_detect_class () . autodetect_system () # pylint: disable=not-callable elif experiment_parse . auto_system_detect_class is None and args . compute_platform == 'auto' : args . compute_platform = \"local\" experiment_class . run_experiment ( args )","title":"main()"},{"location":"reference/agents/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Agents"},{"location":"reference/agents/base_agent/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. AgentParseBase ( BaseModel ) pydantic-model \u00a4 agent: The BaseAgent class representing the agent. config: The agent specific configuration including glues, reward, dones, parts, etc. Source code in corl/agents/base_agent.py class AgentParseBase ( BaseModel ): \"\"\" - agent: The BaseAgent class representing the agent. - config: The agent specific configuration including glues, reward, dones, parts, etc. \"\"\" agent : PyObject # this autoimports (no more plugin loader for glues) config : Dict [ str , Any ] @validator ( 'agent' ) def check_agent ( cls , v ): \"\"\"Check if agent subclass AgentBase\"\"\" if not issubclass ( v , BaseAgent ): raise ValueError ( f \"Agents must subclass BaseAgent, but is of type { v } \" ) return v check_agent ( v ) classmethod \u00a4 Check if agent subclass AgentBase Source code in corl/agents/base_agent.py @validator ( 'agent' ) def check_agent ( cls , v ): \"\"\"Check if agent subclass AgentBase\"\"\" if not issubclass ( v , BaseAgent ): raise ValueError ( f \"Agents must subclass BaseAgent, but is of type { v } \" ) return v AgentParseInfo ( BaseModel ) pydantic-model \u00a4 class_config: The agent class and its configuration platform_name: The name of the platform policy_config: Configuration of the policy of this agent Source code in corl/agents/base_agent.py class AgentParseInfo ( BaseModel ): \"\"\" - class_config: The agent class and its configuration - platform_name: The name of the platform - policy_config: Configuration of the policy of this agent \"\"\" class_config : AgentParseBase platform_name : str policy_config : Dict [ str , Any ] BaseAgent \u00a4 Base class representing an agent in an environment. Source code in corl/agents/base_agent.py class BaseAgent : # pylint: disable=too-many-public-methods \"\"\" Base class representing an agent in an environment. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseAgentParser = self . get_validator ( ** kwargs ) self . agent_glue_dict : typing . Dict [ str , BaseAgentGlue ] = {} self . agent_reward_dict = RewardDict () self . agent_done_dict = DoneDict () # self._agent_glue_obs_export_behavior = {} # Sample parameter provider # This RNG only used here. Normal use uses the one from the environment. rng , _ = gym . utils . seeding . np_random ( 0 ) self . fill_parameters ( rng = rng , default_parameters = True ) @property def platform_name ( self ) -> str : \"\"\" Returns the name of the platform this agent is attached to Returns ------- str: The name of the platform \"\"\" return self . config . platform_name @property def trainable ( self ) -> bool : \"\"\" Flag denoting if agent is trainable. Returns ------- bool: False \"\"\" return False @property def get_validator ( self ) -> typing . Type [ BaseAgentParser ]: \"\"\" Get the validator used to validate the kwargs passed to BaseAgent. Returns ------- BaseAgentParser A BaseAgent kwargs parser and validator. \"\"\" return BaseAgentParser @property def frame_rate ( self ) -> float : \"\"\"Get the frame rate this agent runs at\"\"\" return self . config . frame_rate def fill_parameters ( self , rng : Randomness , default_parameters : bool = False ) -> None : \"\"\"Sample the episode parameter provider to fill the local variable store.\"\"\" if default_parameters : current_parameters = self . config . epp . config . parameters else : current_parameters , _ = self . config . epp . get_params ( rng ) self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( rng ) for k , v in current_parameters . items ()}) def get_simulator_reset_parameters ( self ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the local parameters needed within the simulator reset\"\"\" output = copy . deepcopy ( self . config . simulator_reset_parameters ) def expand_and_update ( data : MutableMapping [ str , Any ], simulator_reset : typing . Mapping [ str , Any ]): for key , value in data . items (): if isinstance ( value , BaseModel ): expand_and_update ( vars ( value ), simulator_reset . get ( key , {})) elif isinstance ( value , collections . abc . MutableMapping ): expand_and_update ( value , simulator_reset . get ( key , {})) else : if key in simulator_reset : data [ key ] = simulator_reset [ key ] expand_and_update ( output , self . local_variable_store . get ( 'simulator_reset' , {})) return output def get_platform_parts ( self , simulator , platform_type ): \"\"\" Gets a list of the agent's platform parts. Parameters ---------- simulator: Simulator class used by environment to simulate agent actions. platform_type: Platform type enumeration corresponding to the agent's platform Returns ------- list: List of platform parts \"\"\" my_parts = [] for part in self . config . parts : tmp = PluginLibrary . FindMatch ( part . part , { \"simulator\" : simulator , \"platform_type\" : platform_type }) for ref_name , ref_key in part . references . items (): if ref_key not in self . config . reference_store : raise RuntimeError ( f 'Part reference { ref_key } must be in the agent reference store.' ) ref_value = self . config . reference_store [ ref_key ] if isinstance ( ref_value , Parameter ): raise TypeError ( f 'Part reference { ref_key } cannot be Parameter' ) part . config [ ref_name ] = ref_value my_parts . append (( tmp , part . config )) return my_parts def make_glues ( self , platform , agent_id : str , env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]]) -> None : \"\"\" Creates agent glue functors from agent configuration. Parameters ---------- platform: The platform instance associated with the glue functors. agent_id: The id of the agent associated with the glue functors. env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" self . agent_glue_dict . clear () for glue_dict in self . config . glues : created_glue = glue_dict . create_functor_object ( platform = platform , agent_name = agent_id , param_sources = [ self . local_variable_store . get ( 'glues' , {})], ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores ) glue_name = created_glue . get_unique_name () if glue_name is not None : if glue_name in self . agent_glue_dict : raise RuntimeError ( f \"The { glue_name } glue has a unique name, but it already exists\" ) else : raise RuntimeError ( f \"No glue name for { created_glue } \" ) # add the glue to the agent glue dict # self._agent_glue_obs_export_behavior[glue_name] = glue.training_obs_behavior self . agent_glue_dict [ glue_name ] = created_glue def make_rewards ( self , agent_id : str , env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]]) -> None : \"\"\" Creates agent reward functors from agent configuration. Parameters ---------- agent_id: The id of the agent associated with the reward functors. env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" tmp = [] for reward_dict in self . config . rewards : tmp . append ( reward_dict . create_functor_object ( agent_name = agent_id , param_sources = [ self . local_variable_store . get ( 'rewards' , {})], ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores ) ) self . agent_reward_dict = RewardDict ( processing_funcs = tmp ) def make_dones ( self , agent_id : str , platform_name : str , dones : typing . Iterable [ Functor ], env_params : typing . List [ typing . Sequence [ typing . Dict [ str , typing . Any ]]], env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]] ) -> None : \"\"\" Creates agent done functors from agent configuration. Parameters ---------- agent_id: The id of the agent associated with the done functors. dones: Additional done conditions to apply env_params: Parameters for the provided dones env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" tmp = [] for done_dict in chain ( self . config . dones , dones ): tmp . append ( done_dict . create_functor_object ( param_sources = [ self . local_variable_store . get ( 'dones' , {})] + env_params , ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores , agent_name = agent_id , platform_name = platform_name ) ) self . agent_done_dict = DoneDict ( processing_funcs = tmp ) def create_space ( self , space_getter : typing . Optional [ typing . Callable ] = None ): \"\"\" Creates a gym dict space from the agent's glues. Parameters ---------- space_getter (optional): A function that takes a glue_obj: BaseAgentGlue and returns a space. Default is space_getter=lambda glue_obj: glue_obj.observation_space(). Returns ------- gym.spaces.dict.Dict() A gym dict space composed of all the spaces returned by applying the space_getter to the agent glues. \"\"\" if space_getter is None : def default_getter ( glue_obj ): return glue_obj . observation_space () space_getter = default_getter return_space = gym . spaces . dict . Dict () # loop over all glue name and glue_obj pairs glue_obj : BaseAgentGlue for glue_name , glue_obj in self . agent_glue_dict . items (): # call our space getter to pick which space we want, # for example: action_space, observation_space, normalized_action_space, normalized_observation_space space_def = space_getter ( glue_obj ) # if the space is None don't add it if space_def : return_space . spaces [ glue_name ] = space_def return return_space if len ( return_space . spaces ) > 0 else None def apply_action ( self , action_dict : typing . Dict [ typing . Any , typing . Any ]): \"\"\" Applies actions to agent. Parameters ---------- action_dict (optional): A dictionary of actions to be applied to agent. Returns ------- None \"\"\" raw_action_dict = collections . OrderedDict () obs = self . get_observations () for glue_name , glue_object in self . agent_glue_dict . items (): if glue_name in action_dict : normalized_action = action_dict [ glue_name ] raw_action = glue_object . unnormalize_action ( normalized_action ) raw_action_dict [ glue_name ] = raw_action glue_object . apply_action ( raw_action , obs ) return raw_action_dict def create_next_action ( self , observation : typing . Dict , # pylint: disable=unused-argument reward : float , # pylint: disable=unused-argument done : bool , # pylint: disable=unused-argument info : typing . Dict # pylint: disable=unused-argument ): \"\"\" Creates next action agent will apply. Parameters ---------- action_dict (optional): A dictionary of actions to be applied to agent. Returns ------- None \"\"\" ... def get_observations ( self ): \"\"\" Gets combined observation from agent glues. Returns ------- OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} \"\"\" return_observation : collections . OrderedDict = collections . OrderedDict () for glue_name , glue_object in self . agent_glue_dict . items (): glue_obs = glue_object . get_observation () if glue_obs : return_observation [ glue_name ] = glue_obs return return_observation def get_info_dict ( self ): \"\"\" Gets combined observation from agent glues. Returns ------- OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} \"\"\" return_info : collections . OrderedDict = collections . OrderedDict () for glue_name , glue_object in self . agent_glue_dict . items (): glue_info = glue_object . get_info_dict () if glue_info : return_info [ glue_name ] = glue_info return return_info def get_dones ( self , observation , action , next_observation , next_state , observation_space , observation_units ): \"\"\" Get agent's done state from agent done functors. Parameters ---------- - observation: The observation dictionary used to compute action. - action: The action computed from observation. - next_observation: The observation dictionary containing observation of next_state. - next_state: The state dictionary containing the environment state after action was applied to the environment. - observation_space: The agent observation space. - observation_units: The units of the observations in the observation space. This may be None. Returns ------- DoneDict[str: bool] A dictionary of the agent's done state in the form {agent_id: done} \"\"\" return self . agent_done_dict ( observation = observation , action = action , next_observation = next_observation , next_state = next_state , observation_space = observation_space , observation_units = observation_units , ) def get_rewards ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ): \"\"\" Get agent's environment rewards from agent reward functors. Parameters ---------- - observation: The observation dictionary used to compute action. - action: The action computed from observation. - next_observation: The observation dictionary containing observation of next_state. - state: The state dictionary containing the environment state before action was applied to the environment. - next_state: The state dictionary containing the environment state after action was applied to the environment. - observation_space: The agent observation space. - observation_units: The units of the observations in the observation space. This may be None. Returns ------- RewardDict[str: Any] A dictionary of the agent's reward in the form {agent_id: reward} \"\"\" return self . agent_reward_dict ( observation = observation , action = action , next_observation = next_observation , state = state , next_state = next_state , observation_space = observation_space , observation_units = observation_units ) def post_process_trajectory ( self , agent_id , state , batch , episode , policy , reward_info ): \"\"\" calling function for sending data to on_postprocess_trajectory Arguments: agent_id str -- name of the agent state {[type]} -- current simulator state batch {[type]} -- batch from the current trajectory episode {[type]} -- the episode object policy {[type]} -- the policy that ran the trajectory reward_info {[type]} -- the info dict for rewards \"\"\" for reward_glue in self . agent_reward_dict . process_callbacks : output_value = reward_glue . post_process_trajectory ( agent_id , state , batch , episode , policy ) if output_value is not None : # If we have a bad setup the agent may be \"dead\" on first step # This in turn causes major issues when trying to index reward_info # Check that the agent exists if agent_id in reward_info : reward_info [ agent_id ] . setdefault ( reward_glue . name , {}) . setdefault ( agent_id , 0.0 ) reward_info [ agent_id ][ reward_glue . name ][ agent_id ] += output_value def get_glue ( self , name : str ) -> typing . Optional [ BaseAgentGlue ]: \"\"\" Get the glue object with the given name \"\"\" if name in self . agent_glue_dict : return self . agent_glue_dict [ name ] return None def normalize_observation ( self , obs_name : str , obs : EnvSpaceUtil . sample_type ) -> typing . Optional [ EnvSpaceUtil . sample_type ]: \"\"\" Normalize a single observation value (if a corresponding glue is found) Parameters ---------- - obs_name: The name of the observation - obs: the observation (value) Returns ------- Normalized observation value, or None (if no corresponding glue is found) \"\"\" glue_obj = self . get_glue ( obs_name ) if glue_obj is not None : # TODO: fix this if 'ObserveSensorRepeated' in obs_name : return glue_obj . normalize_observation ( copy . deepcopy ( obs )) return glue_obj . normalize_observation ( obs ) return None def normalize_observations ( self , observations : collections . OrderedDict ) -> collections . OrderedDict : \"\"\" Normalizes glue observations according to glue definition. Parameters ---------- - observations: A dictionary of observation glues. Returns ------- OrderedDict[str: typing.Union[np.ndarray, typing.Tuple, typing.Dict]] A dictionary of scaled observations in the form {glue_name: scaled_observation} \"\"\" normalized_observation_dict = collections . OrderedDict () for obs_name , obs in observations : normalized_obs = self . normalize_observation ( obs_name , obs ) if normalized_obs : normalized_observation_dict [ obs_name ] = normalized_obs return normalized_observation_dict def set_removed ( self , removed_state : bool ): \"\"\" Set the agent_removed flag for the agent's glues. Parameters ---------- - removed_state (bool): The value to set the agent_removed flag to. Returns ------- None \"\"\" for glue in self . agent_glue_dict . values (): glue . set_agent_removed ( removed_state ) frame_rate : float property readonly \u00a4 Get the frame rate this agent runs at get_validator : Type [ BaseAgentParser ] property readonly \u00a4 Get the validator used to validate the kwargs passed to BaseAgent. Returns \u00a4 BaseAgentParser A BaseAgent kwargs parser and validator. platform_name : str property readonly \u00a4 Returns the name of the platform this agent is attached to Returns \u00a4 Str The name of the platform trainable : bool property readonly \u00a4 Flag denoting if agent is trainable. Returns \u00a4 Bool False apply_action ( self , action_dict ) \u00a4 Applies actions to agent. Parameters \u00a4 action_dict (optional): A dictionary of actions to be applied to agent. Returns \u00a4 None Source code in corl/agents/base_agent.py def apply_action ( self , action_dict : typing . Dict [ typing . Any , typing . Any ]): \"\"\" Applies actions to agent. Parameters ---------- action_dict (optional): A dictionary of actions to be applied to agent. Returns ------- None \"\"\" raw_action_dict = collections . OrderedDict () obs = self . get_observations () for glue_name , glue_object in self . agent_glue_dict . items (): if glue_name in action_dict : normalized_action = action_dict [ glue_name ] raw_action = glue_object . unnormalize_action ( normalized_action ) raw_action_dict [ glue_name ] = raw_action glue_object . apply_action ( raw_action , obs ) return raw_action_dict create_next_action ( self , observation , reward , done , info ) \u00a4 Creates next action agent will apply. Parameters \u00a4 action_dict (optional): A dictionary of actions to be applied to agent. Returns \u00a4 None Source code in corl/agents/base_agent.py def create_next_action ( self , observation : typing . Dict , # pylint: disable=unused-argument reward : float , # pylint: disable=unused-argument done : bool , # pylint: disable=unused-argument info : typing . Dict # pylint: disable=unused-argument ): \"\"\" Creates next action agent will apply. Parameters ---------- action_dict (optional): A dictionary of actions to be applied to agent. Returns ------- None \"\"\" ... create_space ( self , space_getter = None ) \u00a4 Creates a gym dict space from the agent's glues. Parameters \u00a4 space_getter (optional): A function that takes a glue_obj: BaseAgentGlue and returns a space. Default is space_getter=lambda glue_obj: glue_obj.observation_space(). Returns \u00a4 gym.spaces.dict.Dict() A gym dict space composed of all the spaces returned by applying the space_getter to the agent glues. Source code in corl/agents/base_agent.py def create_space ( self , space_getter : typing . Optional [ typing . Callable ] = None ): \"\"\" Creates a gym dict space from the agent's glues. Parameters ---------- space_getter (optional): A function that takes a glue_obj: BaseAgentGlue and returns a space. Default is space_getter=lambda glue_obj: glue_obj.observation_space(). Returns ------- gym.spaces.dict.Dict() A gym dict space composed of all the spaces returned by applying the space_getter to the agent glues. \"\"\" if space_getter is None : def default_getter ( glue_obj ): return glue_obj . observation_space () space_getter = default_getter return_space = gym . spaces . dict . Dict () # loop over all glue name and glue_obj pairs glue_obj : BaseAgentGlue for glue_name , glue_obj in self . agent_glue_dict . items (): # call our space getter to pick which space we want, # for example: action_space, observation_space, normalized_action_space, normalized_observation_space space_def = space_getter ( glue_obj ) # if the space is None don't add it if space_def : return_space . spaces [ glue_name ] = space_def return return_space if len ( return_space . spaces ) > 0 else None fill_parameters ( self , rng , default_parameters = False ) \u00a4 Sample the episode parameter provider to fill the local variable store. Source code in corl/agents/base_agent.py def fill_parameters ( self , rng : Randomness , default_parameters : bool = False ) -> None : \"\"\"Sample the episode parameter provider to fill the local variable store.\"\"\" if default_parameters : current_parameters = self . config . epp . config . parameters else : current_parameters , _ = self . config . epp . get_params ( rng ) self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( rng ) for k , v in current_parameters . items ()}) get_dones ( self , observation , action , next_observation , next_state , observation_space , observation_units ) \u00a4 Get agent's done state from agent done functors. Parameters \u00a4 observation: The observation dictionary used to compute action. action: The action computed from observation. next_observation: The observation dictionary containing observation of next_state. next_state: The state dictionary containing the environment state after action was applied to the environment. observation_space: The agent observation space. observation_units: The units of the observations in the observation space. This may be None. Returns \u00a4 DoneDict[str: bool] A dictionary of the agent's done state in the form {agent_id: done} Source code in corl/agents/base_agent.py def get_dones ( self , observation , action , next_observation , next_state , observation_space , observation_units ): \"\"\" Get agent's done state from agent done functors. Parameters ---------- - observation: The observation dictionary used to compute action. - action: The action computed from observation. - next_observation: The observation dictionary containing observation of next_state. - next_state: The state dictionary containing the environment state after action was applied to the environment. - observation_space: The agent observation space. - observation_units: The units of the observations in the observation space. This may be None. Returns ------- DoneDict[str: bool] A dictionary of the agent's done state in the form {agent_id: done} \"\"\" return self . agent_done_dict ( observation = observation , action = action , next_observation = next_observation , next_state = next_state , observation_space = observation_space , observation_units = observation_units , ) get_glue ( self , name ) \u00a4 Get the glue object with the given name Source code in corl/agents/base_agent.py def get_glue ( self , name : str ) -> typing . Optional [ BaseAgentGlue ]: \"\"\" Get the glue object with the given name \"\"\" if name in self . agent_glue_dict : return self . agent_glue_dict [ name ] return None get_info_dict ( self ) \u00a4 Gets combined observation from agent glues. Returns \u00a4 OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} Source code in corl/agents/base_agent.py def get_info_dict ( self ): \"\"\" Gets combined observation from agent glues. Returns ------- OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} \"\"\" return_info : collections . OrderedDict = collections . OrderedDict () for glue_name , glue_object in self . agent_glue_dict . items (): glue_info = glue_object . get_info_dict () if glue_info : return_info [ glue_name ] = glue_info return return_info get_observations ( self ) \u00a4 Gets combined observation from agent glues. Returns \u00a4 OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} Source code in corl/agents/base_agent.py def get_observations ( self ): \"\"\" Gets combined observation from agent glues. Returns ------- OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} \"\"\" return_observation : collections . OrderedDict = collections . OrderedDict () for glue_name , glue_object in self . agent_glue_dict . items (): glue_obs = glue_object . get_observation () if glue_obs : return_observation [ glue_name ] = glue_obs return return_observation get_platform_parts ( self , simulator , platform_type ) \u00a4 Gets a list of the agent's platform parts. Parameters \u00a4 simulator: Simulator class used by environment to simulate agent actions. platform_type: Platform type enumeration corresponding to the agent's platform Returns \u00a4 List List of platform parts Source code in corl/agents/base_agent.py def get_platform_parts ( self , simulator , platform_type ): \"\"\" Gets a list of the agent's platform parts. Parameters ---------- simulator: Simulator class used by environment to simulate agent actions. platform_type: Platform type enumeration corresponding to the agent's platform Returns ------- list: List of platform parts \"\"\" my_parts = [] for part in self . config . parts : tmp = PluginLibrary . FindMatch ( part . part , { \"simulator\" : simulator , \"platform_type\" : platform_type }) for ref_name , ref_key in part . references . items (): if ref_key not in self . config . reference_store : raise RuntimeError ( f 'Part reference { ref_key } must be in the agent reference store.' ) ref_value = self . config . reference_store [ ref_key ] if isinstance ( ref_value , Parameter ): raise TypeError ( f 'Part reference { ref_key } cannot be Parameter' ) part . config [ ref_name ] = ref_value my_parts . append (( tmp , part . config )) return my_parts get_rewards ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ) \u00a4 Get agent's environment rewards from agent reward functors. Parameters \u00a4 observation: The observation dictionary used to compute action. action: The action computed from observation. next_observation: The observation dictionary containing observation of next_state. state: The state dictionary containing the environment state before action was applied to the environment. next_state: The state dictionary containing the environment state after action was applied to the environment. observation_space: The agent observation space. observation_units: The units of the observations in the observation space. This may be None. Returns \u00a4 RewardDict[str: Any] A dictionary of the agent's reward in the form {agent_id: reward} Source code in corl/agents/base_agent.py def get_rewards ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ): \"\"\" Get agent's environment rewards from agent reward functors. Parameters ---------- - observation: The observation dictionary used to compute action. - action: The action computed from observation. - next_observation: The observation dictionary containing observation of next_state. - state: The state dictionary containing the environment state before action was applied to the environment. - next_state: The state dictionary containing the environment state after action was applied to the environment. - observation_space: The agent observation space. - observation_units: The units of the observations in the observation space. This may be None. Returns ------- RewardDict[str: Any] A dictionary of the agent's reward in the form {agent_id: reward} \"\"\" return self . agent_reward_dict ( observation = observation , action = action , next_observation = next_observation , state = state , next_state = next_state , observation_space = observation_space , observation_units = observation_units ) get_simulator_reset_parameters ( self ) \u00a4 Return the local parameters needed within the simulator reset Source code in corl/agents/base_agent.py def get_simulator_reset_parameters ( self ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the local parameters needed within the simulator reset\"\"\" output = copy . deepcopy ( self . config . simulator_reset_parameters ) def expand_and_update ( data : MutableMapping [ str , Any ], simulator_reset : typing . Mapping [ str , Any ]): for key , value in data . items (): if isinstance ( value , BaseModel ): expand_and_update ( vars ( value ), simulator_reset . get ( key , {})) elif isinstance ( value , collections . abc . MutableMapping ): expand_and_update ( value , simulator_reset . get ( key , {})) else : if key in simulator_reset : data [ key ] = simulator_reset [ key ] expand_and_update ( output , self . local_variable_store . get ( 'simulator_reset' , {})) return output make_dones ( self , agent_id , platform_name , dones , env_params , env_ref_stores ) \u00a4 Creates agent done functors from agent configuration. Parameters \u00a4 agent_id: The id of the agent associated with the done functors. dones: Additional done conditions to apply env_params: Parameters for the provided dones env_ref_stores: Reference stores for items managed by the environment Returns \u00a4 None Source code in corl/agents/base_agent.py def make_dones ( self , agent_id : str , platform_name : str , dones : typing . Iterable [ Functor ], env_params : typing . List [ typing . Sequence [ typing . Dict [ str , typing . Any ]]], env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]] ) -> None : \"\"\" Creates agent done functors from agent configuration. Parameters ---------- agent_id: The id of the agent associated with the done functors. dones: Additional done conditions to apply env_params: Parameters for the provided dones env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" tmp = [] for done_dict in chain ( self . config . dones , dones ): tmp . append ( done_dict . create_functor_object ( param_sources = [ self . local_variable_store . get ( 'dones' , {})] + env_params , ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores , agent_name = agent_id , platform_name = platform_name ) ) self . agent_done_dict = DoneDict ( processing_funcs = tmp ) make_glues ( self , platform , agent_id , env_ref_stores ) \u00a4 Creates agent glue functors from agent configuration. Parameters \u00a4 platform: The platform instance associated with the glue functors. agent_id: The id of the agent associated with the glue functors. env_ref_stores: Reference stores for items managed by the environment Returns \u00a4 None Source code in corl/agents/base_agent.py def make_glues ( self , platform , agent_id : str , env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]]) -> None : \"\"\" Creates agent glue functors from agent configuration. Parameters ---------- platform: The platform instance associated with the glue functors. agent_id: The id of the agent associated with the glue functors. env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" self . agent_glue_dict . clear () for glue_dict in self . config . glues : created_glue = glue_dict . create_functor_object ( platform = platform , agent_name = agent_id , param_sources = [ self . local_variable_store . get ( 'glues' , {})], ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores ) glue_name = created_glue . get_unique_name () if glue_name is not None : if glue_name in self . agent_glue_dict : raise RuntimeError ( f \"The { glue_name } glue has a unique name, but it already exists\" ) else : raise RuntimeError ( f \"No glue name for { created_glue } \" ) # add the glue to the agent glue dict # self._agent_glue_obs_export_behavior[glue_name] = glue.training_obs_behavior self . agent_glue_dict [ glue_name ] = created_glue make_rewards ( self , agent_id , env_ref_stores ) \u00a4 Creates agent reward functors from agent configuration. Parameters \u00a4 agent_id: The id of the agent associated with the reward functors. env_ref_stores: Reference stores for items managed by the environment Returns \u00a4 None Source code in corl/agents/base_agent.py def make_rewards ( self , agent_id : str , env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]]) -> None : \"\"\" Creates agent reward functors from agent configuration. Parameters ---------- agent_id: The id of the agent associated with the reward functors. env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" tmp = [] for reward_dict in self . config . rewards : tmp . append ( reward_dict . create_functor_object ( agent_name = agent_id , param_sources = [ self . local_variable_store . get ( 'rewards' , {})], ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores ) ) self . agent_reward_dict = RewardDict ( processing_funcs = tmp ) normalize_observation ( self , obs_name , obs ) \u00a4 Normalize a single observation value (if a corresponding glue is found) Parameters \u00a4 obs_name: The name of the observation obs: the observation (value) Returns \u00a4 Normalized observation value, or None (if no corresponding glue is found) Source code in corl/agents/base_agent.py def normalize_observation ( self , obs_name : str , obs : EnvSpaceUtil . sample_type ) -> typing . Optional [ EnvSpaceUtil . sample_type ]: \"\"\" Normalize a single observation value (if a corresponding glue is found) Parameters ---------- - obs_name: The name of the observation - obs: the observation (value) Returns ------- Normalized observation value, or None (if no corresponding glue is found) \"\"\" glue_obj = self . get_glue ( obs_name ) if glue_obj is not None : # TODO: fix this if 'ObserveSensorRepeated' in obs_name : return glue_obj . normalize_observation ( copy . deepcopy ( obs )) return glue_obj . normalize_observation ( obs ) return None normalize_observations ( self , observations ) \u00a4 Normalizes glue observations according to glue definition. Parameters \u00a4 observations: A dictionary of observation glues. Returns \u00a4 OrderedDict[str: typing.Union[np.ndarray, typing.Tuple, typing.Dict]] A dictionary of scaled observations in the form {glue_name: scaled_observation} Source code in corl/agents/base_agent.py def normalize_observations ( self , observations : collections . OrderedDict ) -> collections . OrderedDict : \"\"\" Normalizes glue observations according to glue definition. Parameters ---------- - observations: A dictionary of observation glues. Returns ------- OrderedDict[str: typing.Union[np.ndarray, typing.Tuple, typing.Dict]] A dictionary of scaled observations in the form {glue_name: scaled_observation} \"\"\" normalized_observation_dict = collections . OrderedDict () for obs_name , obs in observations : normalized_obs = self . normalize_observation ( obs_name , obs ) if normalized_obs : normalized_observation_dict [ obs_name ] = normalized_obs return normalized_observation_dict post_process_trajectory ( self , agent_id , state , batch , episode , policy , reward_info ) \u00a4 calling function for sending data to on_postprocess_trajectory Source code in corl/agents/base_agent.py def post_process_trajectory ( self , agent_id , state , batch , episode , policy , reward_info ): \"\"\" calling function for sending data to on_postprocess_trajectory Arguments: agent_id str -- name of the agent state {[type]} -- current simulator state batch {[type]} -- batch from the current trajectory episode {[type]} -- the episode object policy {[type]} -- the policy that ran the trajectory reward_info {[type]} -- the info dict for rewards \"\"\" for reward_glue in self . agent_reward_dict . process_callbacks : output_value = reward_glue . post_process_trajectory ( agent_id , state , batch , episode , policy ) if output_value is not None : # If we have a bad setup the agent may be \"dead\" on first step # This in turn causes major issues when trying to index reward_info # Check that the agent exists if agent_id in reward_info : reward_info [ agent_id ] . setdefault ( reward_glue . name , {}) . setdefault ( agent_id , 0.0 ) reward_info [ agent_id ][ reward_glue . name ][ agent_id ] += output_value set_removed ( self , removed_state ) \u00a4 Set the agent_removed flag for the agent's glues. Parameters \u00a4 removed_state (bool): The value to set the agent_removed flag to. Returns \u00a4 None Source code in corl/agents/base_agent.py def set_removed ( self , removed_state : bool ): \"\"\" Set the agent_removed flag for the agent's glues. Parameters ---------- - removed_state (bool): The value to set the agent_removed flag to. Returns ------- None \"\"\" for glue in self . agent_glue_dict . values (): glue . set_agent_removed ( removed_state ) BaseAgentEppParameters ( BaseModel ) pydantic-model \u00a4 typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: glue name, parameter name typing.Dict[str, typing.Dict[str, typing.Dict[str, typing.Any]]] = {} keys: reward name, ? name, parameter name typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name typing.Dict[str, typing.Any] = {} keys: reference name typing.Dict[str, typing.Any] = {} keys: whatever the simulator wants, but it needs to be kwargs to simulator reset Source code in corl/agents/base_agent.py class BaseAgentEppParameters ( BaseModel ): \"\"\" glues: typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: glue name, parameter name rewards: typing.Dict[str, typing.Dict[str, typing.Dict[str, typing.Any]]] = {} keys: reward name, ? name, parameter name dones: typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name reference_store: typing.Dict[str, typing.Any] = {} keys: reference name simulator_reset: typing.Dict[str, typing.Any] = {} keys: whatever the simulator wants, but it needs to be kwargs to simulator reset \"\"\" glues : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} rewards : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} dones : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} reference_store : typing . Dict [ str , typing . Any ] = {} simulator_reset : typing . Dict [ str , typing . Any ] = {} @staticmethod def _validate_leaves_are_parameters ( obj ): if isinstance ( obj , dict ): for _key , value in obj . items (): BaseAgentEppParameters . _validate_leaves_are_parameters ( value ) elif not isinstance ( obj , Parameter ): raise TypeError ( f \"Invalid type: { type ( obj ) } (required type: { Parameter . __qualname__ } )\" ) @validator ( 'glues' , 'rewards' , 'dones' , 'reference_store' , 'simulator_reset' ) def validate_leaves_are_parameters ( cls , v ): \"\"\" Verifies the outer most leaf nodes of a config are parameter types \"\"\" BaseAgentEppParameters . _validate_leaves_are_parameters ( v ) return v validate_leaves_are_parameters ( v ) classmethod \u00a4 Verifies the outer most leaf nodes of a config are parameter types Source code in corl/agents/base_agent.py @validator ( 'glues' , 'rewards' , 'dones' , 'reference_store' , 'simulator_reset' ) def validate_leaves_are_parameters ( cls , v ): \"\"\" Verifies the outer most leaf nodes of a config are parameter types \"\"\" BaseAgentEppParameters . _validate_leaves_are_parameters ( v ) return v BaseAgentParser ( BaseModel ) pydantic-model \u00a4 parts: A list of part configurations. glues: A list of glue functor configurations. rewards: A optional list of reward functor configurations dones: An optional list of done functor configurations Source code in corl/agents/base_agent.py class BaseAgentParser ( BaseModel ): \"\"\" - parts: A list of part configurations. - glues: A list of glue functor configurations. - rewards: A optional list of reward functor configurations - dones: An optional list of done functor configurations \"\"\" # these can be sensors, controller, or other platform parts parts : List [ PartsFunctor ] # this uses plugin loader to load reference_store : Dict [ str , ObjectStoreElem ] = {} glues : List [ Union [ FunctorMultiWrapper , FunctorWrapper , Functor , FunctorDictWrapper ]] rewards : List [ Union [ FunctorMultiWrapper , FunctorWrapper , Functor , FunctorDictWrapper ]] = [] dones : List [ Union [ FunctorMultiWrapper , FunctorWrapper , Functor , FunctorDictWrapper ]] = [] simulator_reset_parameters : Dict [ str , Any ] = {} # frame_rate in Hz (i.e. .25 indicates this agent will process when sim_tim % 4 == 0) frame_rate : float = 1.0 agent_name : str platform_name : str multiple_workers : bool = False episode_parameter_provider : Factory episode_parameter_provider_parameters : BaseAgentEppParameters = None # type: ignore epp : EpisodeParameterProvider = None # type: ignore # this is a dictionary validated by the simulator that allows the agent to tell # the simulator what to do with this agent class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True @validator ( 'glues' , each_item = True ) def check_glues ( cls , v ): \"\"\"Check if glues subclass BaseAgentGlue\"\"\" if not issubclass ( v . functor , BaseAgentGlue ): raise TypeError ( f \"Glue functors must subclass BaseAgentGlue, but glue { v . name } is of type { v . functor } \" ) return v @validator ( 'rewards' , each_item = True ) def check_rewards ( cls , v ): \"\"\"Check if rewards subclass RewardFuncBase\"\"\" if not issubclass ( v . functor , RewardFuncBase ): raise TypeError ( f \"Reward functors must subclass RewardFuncBase, but reward { v . name } is of type { v . functor } \" ) return v @validator ( 'dones' , each_item = True ) def check_dones ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" if not issubclass ( v . functor , DoneFuncBase ): raise TypeError ( f \"Done functors must subclass DoneFuncBase, but done { v . name } is of type { v . functor } \" ) if issubclass ( v . functor , EpisodeLengthDone ): raise ValueError ( \"Cannot specify EpisodeLengthDone as it is automatically added\" ) return v resolve_factory = validator ( 'reference_store' , pre = True , each_item = True , allow_reuse = True )( Factory . resolve_factory ) @validator ( 'simulator_reset_parameters' , pre = True ) def update_units_and_parameters ( cls , v ): \"\"\"Update simulation reset parameters to meet base simulator requirements.\"\"\" return validation_helper_units_and_parameters ( v ) @validator ( 'episode_parameter_provider' ) def check_epp ( cls , epp_factory , values ): \"\"\"Check if episode parameter provider subclass EpisodeParameterProvider\"\"\" if not issubclass ( epp_factory . type , EpisodeParameterProvider ): raise TypeError ( f \"Episode parameter providers must subclass EpisodeParameterProvider, but is is of type { epp_factory . type } \" ) # replace %%AGENT%% with agent_name epp_params = flatten_dict . flatten ( epp_factory . config ) for key , value in epp_params . items (): if isinstance ( value , str ): epp_params [ key ] = value . replace ( r ' %% AGENT %% ' , values [ 'agent_name' ]) epp_factory . config = flatten_dict . unflatten ( epp_params ) return epp_factory @validator ( 'episode_parameter_provider_parameters' , always = True , pre = True ) def build_episode_parameter_provider_parameters ( cls , _v , values ) -> BaseAgentEppParameters : \"\"\"Create the episode parameter provider config\"\"\" for key in [ 'reference_store' , 'glues' , 'rewards' , 'dones' , 'simulator_reset_parameters' ]: assert key in values reference_parameters : typing . Dict [ str , Parameter ] = {} for ref_name , ref_value in values [ 'reference_store' ] . items (): if isinstance ( ref_value , Parameter ): reference_parameters [ ref_name ] = ref_value glue_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'glues' ]: functor . add_to_parameter_store ( glue_parameters ) reward_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'rewards' ]: functor . add_to_parameter_store ( reward_parameters ) done_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ]: functor . add_to_parameter_store ( done_parameters ) flat_data = flatten_dict . flatten ( values [ 'simulator_reset_parameters' ]) for key , value in flat_data . items (): if isinstance ( value , BaseModel ): flat_data [ key ] = value . dict () expanded_sim_reset_params = flatten_dict . unflatten ( flat_data ) sim_parameters_flat = { name : param for name , param in flatten_dict . flatten ( expanded_sim_reset_params ) . items () if isinstance ( param , Parameter ) } sim_parameters = flatten_dict . unflatten ( sim_parameters_flat ) return BaseAgentEppParameters ( glues = glue_parameters , rewards = reward_parameters , dones = done_parameters , reference_store = reference_parameters , simulator_reset = sim_parameters ) @validator ( 'epp' , always = True , pre = True ) def build_epp ( cls , epp , values ): \"\"\"Builds an instance of an EpisodeParameterProvider if necessary\"\"\" if epp is None : assert 'episode_parameter_provider_parameters' in values epp_params = dict ( values [ 'episode_parameter_provider_parameters' ]) flat_epp_params = flatten_dict . flatten ( epp_params ) epp = values [ 'episode_parameter_provider' ] . build ( parameters = flat_epp_params ) return epp @validator ( 'frame_rate' , always = True ) def simplify_frame_rate ( cls , v ): \"\"\"Expand the precision of the frame rate (e.g. .333 -> .3333333333334)\"\"\" f = fractions . Fraction ( v ) . limit_denominator ( 20 ) return f . numerator / f . denominator Config \u00a4 Allow arbitrary types for Parameter Source code in corl/agents/base_agent.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True build_episode_parameter_provider_parameters ( _v , values ) classmethod \u00a4 Create the episode parameter provider config Source code in corl/agents/base_agent.py @validator ( 'episode_parameter_provider_parameters' , always = True , pre = True ) def build_episode_parameter_provider_parameters ( cls , _v , values ) -> BaseAgentEppParameters : \"\"\"Create the episode parameter provider config\"\"\" for key in [ 'reference_store' , 'glues' , 'rewards' , 'dones' , 'simulator_reset_parameters' ]: assert key in values reference_parameters : typing . Dict [ str , Parameter ] = {} for ref_name , ref_value in values [ 'reference_store' ] . items (): if isinstance ( ref_value , Parameter ): reference_parameters [ ref_name ] = ref_value glue_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'glues' ]: functor . add_to_parameter_store ( glue_parameters ) reward_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'rewards' ]: functor . add_to_parameter_store ( reward_parameters ) done_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ]: functor . add_to_parameter_store ( done_parameters ) flat_data = flatten_dict . flatten ( values [ 'simulator_reset_parameters' ]) for key , value in flat_data . items (): if isinstance ( value , BaseModel ): flat_data [ key ] = value . dict () expanded_sim_reset_params = flatten_dict . unflatten ( flat_data ) sim_parameters_flat = { name : param for name , param in flatten_dict . flatten ( expanded_sim_reset_params ) . items () if isinstance ( param , Parameter ) } sim_parameters = flatten_dict . unflatten ( sim_parameters_flat ) return BaseAgentEppParameters ( glues = glue_parameters , rewards = reward_parameters , dones = done_parameters , reference_store = reference_parameters , simulator_reset = sim_parameters ) build_epp ( epp , values ) classmethod \u00a4 Builds an instance of an EpisodeParameterProvider if necessary Source code in corl/agents/base_agent.py @validator ( 'epp' , always = True , pre = True ) def build_epp ( cls , epp , values ): \"\"\"Builds an instance of an EpisodeParameterProvider if necessary\"\"\" if epp is None : assert 'episode_parameter_provider_parameters' in values epp_params = dict ( values [ 'episode_parameter_provider_parameters' ]) flat_epp_params = flatten_dict . flatten ( epp_params ) epp = values [ 'episode_parameter_provider' ] . build ( parameters = flat_epp_params ) return epp check_dones ( v ) classmethod \u00a4 Check if dones subclass DoneFuncBase Source code in corl/agents/base_agent.py @validator ( 'dones' , each_item = True ) def check_dones ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" if not issubclass ( v . functor , DoneFuncBase ): raise TypeError ( f \"Done functors must subclass DoneFuncBase, but done { v . name } is of type { v . functor } \" ) if issubclass ( v . functor , EpisodeLengthDone ): raise ValueError ( \"Cannot specify EpisodeLengthDone as it is automatically added\" ) return v check_epp ( epp_factory , values ) classmethod \u00a4 Check if episode parameter provider subclass EpisodeParameterProvider Source code in corl/agents/base_agent.py @validator ( 'episode_parameter_provider' ) def check_epp ( cls , epp_factory , values ): \"\"\"Check if episode parameter provider subclass EpisodeParameterProvider\"\"\" if not issubclass ( epp_factory . type , EpisodeParameterProvider ): raise TypeError ( f \"Episode parameter providers must subclass EpisodeParameterProvider, but is is of type { epp_factory . type } \" ) # replace %%AGENT%% with agent_name epp_params = flatten_dict . flatten ( epp_factory . config ) for key , value in epp_params . items (): if isinstance ( value , str ): epp_params [ key ] = value . replace ( r ' %% AGENT %% ' , values [ 'agent_name' ]) epp_factory . config = flatten_dict . unflatten ( epp_params ) return epp_factory check_glues ( v ) classmethod \u00a4 Check if glues subclass BaseAgentGlue Source code in corl/agents/base_agent.py @validator ( 'glues' , each_item = True ) def check_glues ( cls , v ): \"\"\"Check if glues subclass BaseAgentGlue\"\"\" if not issubclass ( v . functor , BaseAgentGlue ): raise TypeError ( f \"Glue functors must subclass BaseAgentGlue, but glue { v . name } is of type { v . functor } \" ) return v check_rewards ( v ) classmethod \u00a4 Check if rewards subclass RewardFuncBase Source code in corl/agents/base_agent.py @validator ( 'rewards' , each_item = True ) def check_rewards ( cls , v ): \"\"\"Check if rewards subclass RewardFuncBase\"\"\" if not issubclass ( v . functor , RewardFuncBase ): raise TypeError ( f \"Reward functors must subclass RewardFuncBase, but reward { v . name } is of type { v . functor } \" ) return v resolve_factory ( v ) classmethod \u00a4 Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) Source code in corl/agents/base_agent.py @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build () simplify_frame_rate ( v ) classmethod \u00a4 Expand the precision of the frame rate (e.g. .333 -> .3333333333334) Source code in corl/agents/base_agent.py @validator ( 'frame_rate' , always = True ) def simplify_frame_rate ( cls , v ): \"\"\"Expand the precision of the frame rate (e.g. .333 -> .3333333333334)\"\"\" f = fractions . Fraction ( v ) . limit_denominator ( 20 ) return f . numerator / f . denominator update_units_and_parameters ( v ) classmethod \u00a4 Update simulation reset parameters to meet base simulator requirements. Source code in corl/agents/base_agent.py @validator ( 'simulator_reset_parameters' , pre = True ) def update_units_and_parameters ( cls , v ): \"\"\"Update simulation reset parameters to meet base simulator requirements.\"\"\" return validation_helper_units_and_parameters ( v ) PartsFunctor ( BaseModel ) pydantic-model \u00a4 part: The name of the part. This should be registered to a corresponding platform part class in the plugin library. config: The specific configuration dictionary expected by the platform part. Source code in corl/agents/base_agent.py class PartsFunctor ( BaseModel ): \"\"\" - part: The name of the part. This should be registered to a corresponding platform part class in the plugin library. - config: The specific configuration dictionary expected by the platform part. \"\"\" part : str config : Dict [ str , Any ] = {} references : Dict [ str , str ] = {} PlatformParseInfo ( BaseModel ) pydantic-model \u00a4 platform_config Source code in corl/agents/base_agent.py class PlatformParseInfo ( BaseModel ): \"\"\" - platform_config \"\"\" platform_config : Dict [ str , Any ] TrainableBaseAgent ( BaseAgent ) \u00a4 Base class representing a trainable agent in an environment. Source code in corl/agents/base_agent.py class TrainableBaseAgent ( BaseAgent ): \"\"\" Base class representing a trainable agent in an environment. \"\"\" @property def trainable ( self ) -> bool : \"\"\" Flag denoting if agent is trainable. Returns ------- bool: True \"\"\" return True trainable : bool property readonly \u00a4 Flag denoting if agent is trainable. Returns \u00a4 Bool True","title":"Base agent"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.AgentParseBase","text":"agent: The BaseAgent class representing the agent. config: The agent specific configuration including glues, reward, dones, parts, etc. Source code in corl/agents/base_agent.py class AgentParseBase ( BaseModel ): \"\"\" - agent: The BaseAgent class representing the agent. - config: The agent specific configuration including glues, reward, dones, parts, etc. \"\"\" agent : PyObject # this autoimports (no more plugin loader for glues) config : Dict [ str , Any ] @validator ( 'agent' ) def check_agent ( cls , v ): \"\"\"Check if agent subclass AgentBase\"\"\" if not issubclass ( v , BaseAgent ): raise ValueError ( f \"Agents must subclass BaseAgent, but is of type { v } \" ) return v","title":"AgentParseBase"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.AgentParseBase.check_agent","text":"Check if agent subclass AgentBase Source code in corl/agents/base_agent.py @validator ( 'agent' ) def check_agent ( cls , v ): \"\"\"Check if agent subclass AgentBase\"\"\" if not issubclass ( v , BaseAgent ): raise ValueError ( f \"Agents must subclass BaseAgent, but is of type { v } \" ) return v","title":"check_agent()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.AgentParseInfo","text":"class_config: The agent class and its configuration platform_name: The name of the platform policy_config: Configuration of the policy of this agent Source code in corl/agents/base_agent.py class AgentParseInfo ( BaseModel ): \"\"\" - class_config: The agent class and its configuration - platform_name: The name of the platform - policy_config: Configuration of the policy of this agent \"\"\" class_config : AgentParseBase platform_name : str policy_config : Dict [ str , Any ]","title":"AgentParseInfo"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent","text":"Base class representing an agent in an environment. Source code in corl/agents/base_agent.py class BaseAgent : # pylint: disable=too-many-public-methods \"\"\" Base class representing an agent in an environment. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseAgentParser = self . get_validator ( ** kwargs ) self . agent_glue_dict : typing . Dict [ str , BaseAgentGlue ] = {} self . agent_reward_dict = RewardDict () self . agent_done_dict = DoneDict () # self._agent_glue_obs_export_behavior = {} # Sample parameter provider # This RNG only used here. Normal use uses the one from the environment. rng , _ = gym . utils . seeding . np_random ( 0 ) self . fill_parameters ( rng = rng , default_parameters = True ) @property def platform_name ( self ) -> str : \"\"\" Returns the name of the platform this agent is attached to Returns ------- str: The name of the platform \"\"\" return self . config . platform_name @property def trainable ( self ) -> bool : \"\"\" Flag denoting if agent is trainable. Returns ------- bool: False \"\"\" return False @property def get_validator ( self ) -> typing . Type [ BaseAgentParser ]: \"\"\" Get the validator used to validate the kwargs passed to BaseAgent. Returns ------- BaseAgentParser A BaseAgent kwargs parser and validator. \"\"\" return BaseAgentParser @property def frame_rate ( self ) -> float : \"\"\"Get the frame rate this agent runs at\"\"\" return self . config . frame_rate def fill_parameters ( self , rng : Randomness , default_parameters : bool = False ) -> None : \"\"\"Sample the episode parameter provider to fill the local variable store.\"\"\" if default_parameters : current_parameters = self . config . epp . config . parameters else : current_parameters , _ = self . config . epp . get_params ( rng ) self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( rng ) for k , v in current_parameters . items ()}) def get_simulator_reset_parameters ( self ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the local parameters needed within the simulator reset\"\"\" output = copy . deepcopy ( self . config . simulator_reset_parameters ) def expand_and_update ( data : MutableMapping [ str , Any ], simulator_reset : typing . Mapping [ str , Any ]): for key , value in data . items (): if isinstance ( value , BaseModel ): expand_and_update ( vars ( value ), simulator_reset . get ( key , {})) elif isinstance ( value , collections . abc . MutableMapping ): expand_and_update ( value , simulator_reset . get ( key , {})) else : if key in simulator_reset : data [ key ] = simulator_reset [ key ] expand_and_update ( output , self . local_variable_store . get ( 'simulator_reset' , {})) return output def get_platform_parts ( self , simulator , platform_type ): \"\"\" Gets a list of the agent's platform parts. Parameters ---------- simulator: Simulator class used by environment to simulate agent actions. platform_type: Platform type enumeration corresponding to the agent's platform Returns ------- list: List of platform parts \"\"\" my_parts = [] for part in self . config . parts : tmp = PluginLibrary . FindMatch ( part . part , { \"simulator\" : simulator , \"platform_type\" : platform_type }) for ref_name , ref_key in part . references . items (): if ref_key not in self . config . reference_store : raise RuntimeError ( f 'Part reference { ref_key } must be in the agent reference store.' ) ref_value = self . config . reference_store [ ref_key ] if isinstance ( ref_value , Parameter ): raise TypeError ( f 'Part reference { ref_key } cannot be Parameter' ) part . config [ ref_name ] = ref_value my_parts . append (( tmp , part . config )) return my_parts def make_glues ( self , platform , agent_id : str , env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]]) -> None : \"\"\" Creates agent glue functors from agent configuration. Parameters ---------- platform: The platform instance associated with the glue functors. agent_id: The id of the agent associated with the glue functors. env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" self . agent_glue_dict . clear () for glue_dict in self . config . glues : created_glue = glue_dict . create_functor_object ( platform = platform , agent_name = agent_id , param_sources = [ self . local_variable_store . get ( 'glues' , {})], ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores ) glue_name = created_glue . get_unique_name () if glue_name is not None : if glue_name in self . agent_glue_dict : raise RuntimeError ( f \"The { glue_name } glue has a unique name, but it already exists\" ) else : raise RuntimeError ( f \"No glue name for { created_glue } \" ) # add the glue to the agent glue dict # self._agent_glue_obs_export_behavior[glue_name] = glue.training_obs_behavior self . agent_glue_dict [ glue_name ] = created_glue def make_rewards ( self , agent_id : str , env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]]) -> None : \"\"\" Creates agent reward functors from agent configuration. Parameters ---------- agent_id: The id of the agent associated with the reward functors. env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" tmp = [] for reward_dict in self . config . rewards : tmp . append ( reward_dict . create_functor_object ( agent_name = agent_id , param_sources = [ self . local_variable_store . get ( 'rewards' , {})], ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores ) ) self . agent_reward_dict = RewardDict ( processing_funcs = tmp ) def make_dones ( self , agent_id : str , platform_name : str , dones : typing . Iterable [ Functor ], env_params : typing . List [ typing . Sequence [ typing . Dict [ str , typing . Any ]]], env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]] ) -> None : \"\"\" Creates agent done functors from agent configuration. Parameters ---------- agent_id: The id of the agent associated with the done functors. dones: Additional done conditions to apply env_params: Parameters for the provided dones env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" tmp = [] for done_dict in chain ( self . config . dones , dones ): tmp . append ( done_dict . create_functor_object ( param_sources = [ self . local_variable_store . get ( 'dones' , {})] + env_params , ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores , agent_name = agent_id , platform_name = platform_name ) ) self . agent_done_dict = DoneDict ( processing_funcs = tmp ) def create_space ( self , space_getter : typing . Optional [ typing . Callable ] = None ): \"\"\" Creates a gym dict space from the agent's glues. Parameters ---------- space_getter (optional): A function that takes a glue_obj: BaseAgentGlue and returns a space. Default is space_getter=lambda glue_obj: glue_obj.observation_space(). Returns ------- gym.spaces.dict.Dict() A gym dict space composed of all the spaces returned by applying the space_getter to the agent glues. \"\"\" if space_getter is None : def default_getter ( glue_obj ): return glue_obj . observation_space () space_getter = default_getter return_space = gym . spaces . dict . Dict () # loop over all glue name and glue_obj pairs glue_obj : BaseAgentGlue for glue_name , glue_obj in self . agent_glue_dict . items (): # call our space getter to pick which space we want, # for example: action_space, observation_space, normalized_action_space, normalized_observation_space space_def = space_getter ( glue_obj ) # if the space is None don't add it if space_def : return_space . spaces [ glue_name ] = space_def return return_space if len ( return_space . spaces ) > 0 else None def apply_action ( self , action_dict : typing . Dict [ typing . Any , typing . Any ]): \"\"\" Applies actions to agent. Parameters ---------- action_dict (optional): A dictionary of actions to be applied to agent. Returns ------- None \"\"\" raw_action_dict = collections . OrderedDict () obs = self . get_observations () for glue_name , glue_object in self . agent_glue_dict . items (): if glue_name in action_dict : normalized_action = action_dict [ glue_name ] raw_action = glue_object . unnormalize_action ( normalized_action ) raw_action_dict [ glue_name ] = raw_action glue_object . apply_action ( raw_action , obs ) return raw_action_dict def create_next_action ( self , observation : typing . Dict , # pylint: disable=unused-argument reward : float , # pylint: disable=unused-argument done : bool , # pylint: disable=unused-argument info : typing . Dict # pylint: disable=unused-argument ): \"\"\" Creates next action agent will apply. Parameters ---------- action_dict (optional): A dictionary of actions to be applied to agent. Returns ------- None \"\"\" ... def get_observations ( self ): \"\"\" Gets combined observation from agent glues. Returns ------- OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} \"\"\" return_observation : collections . OrderedDict = collections . OrderedDict () for glue_name , glue_object in self . agent_glue_dict . items (): glue_obs = glue_object . get_observation () if glue_obs : return_observation [ glue_name ] = glue_obs return return_observation def get_info_dict ( self ): \"\"\" Gets combined observation from agent glues. Returns ------- OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} \"\"\" return_info : collections . OrderedDict = collections . OrderedDict () for glue_name , glue_object in self . agent_glue_dict . items (): glue_info = glue_object . get_info_dict () if glue_info : return_info [ glue_name ] = glue_info return return_info def get_dones ( self , observation , action , next_observation , next_state , observation_space , observation_units ): \"\"\" Get agent's done state from agent done functors. Parameters ---------- - observation: The observation dictionary used to compute action. - action: The action computed from observation. - next_observation: The observation dictionary containing observation of next_state. - next_state: The state dictionary containing the environment state after action was applied to the environment. - observation_space: The agent observation space. - observation_units: The units of the observations in the observation space. This may be None. Returns ------- DoneDict[str: bool] A dictionary of the agent's done state in the form {agent_id: done} \"\"\" return self . agent_done_dict ( observation = observation , action = action , next_observation = next_observation , next_state = next_state , observation_space = observation_space , observation_units = observation_units , ) def get_rewards ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ): \"\"\" Get agent's environment rewards from agent reward functors. Parameters ---------- - observation: The observation dictionary used to compute action. - action: The action computed from observation. - next_observation: The observation dictionary containing observation of next_state. - state: The state dictionary containing the environment state before action was applied to the environment. - next_state: The state dictionary containing the environment state after action was applied to the environment. - observation_space: The agent observation space. - observation_units: The units of the observations in the observation space. This may be None. Returns ------- RewardDict[str: Any] A dictionary of the agent's reward in the form {agent_id: reward} \"\"\" return self . agent_reward_dict ( observation = observation , action = action , next_observation = next_observation , state = state , next_state = next_state , observation_space = observation_space , observation_units = observation_units ) def post_process_trajectory ( self , agent_id , state , batch , episode , policy , reward_info ): \"\"\" calling function for sending data to on_postprocess_trajectory Arguments: agent_id str -- name of the agent state {[type]} -- current simulator state batch {[type]} -- batch from the current trajectory episode {[type]} -- the episode object policy {[type]} -- the policy that ran the trajectory reward_info {[type]} -- the info dict for rewards \"\"\" for reward_glue in self . agent_reward_dict . process_callbacks : output_value = reward_glue . post_process_trajectory ( agent_id , state , batch , episode , policy ) if output_value is not None : # If we have a bad setup the agent may be \"dead\" on first step # This in turn causes major issues when trying to index reward_info # Check that the agent exists if agent_id in reward_info : reward_info [ agent_id ] . setdefault ( reward_glue . name , {}) . setdefault ( agent_id , 0.0 ) reward_info [ agent_id ][ reward_glue . name ][ agent_id ] += output_value def get_glue ( self , name : str ) -> typing . Optional [ BaseAgentGlue ]: \"\"\" Get the glue object with the given name \"\"\" if name in self . agent_glue_dict : return self . agent_glue_dict [ name ] return None def normalize_observation ( self , obs_name : str , obs : EnvSpaceUtil . sample_type ) -> typing . Optional [ EnvSpaceUtil . sample_type ]: \"\"\" Normalize a single observation value (if a corresponding glue is found) Parameters ---------- - obs_name: The name of the observation - obs: the observation (value) Returns ------- Normalized observation value, or None (if no corresponding glue is found) \"\"\" glue_obj = self . get_glue ( obs_name ) if glue_obj is not None : # TODO: fix this if 'ObserveSensorRepeated' in obs_name : return glue_obj . normalize_observation ( copy . deepcopy ( obs )) return glue_obj . normalize_observation ( obs ) return None def normalize_observations ( self , observations : collections . OrderedDict ) -> collections . OrderedDict : \"\"\" Normalizes glue observations according to glue definition. Parameters ---------- - observations: A dictionary of observation glues. Returns ------- OrderedDict[str: typing.Union[np.ndarray, typing.Tuple, typing.Dict]] A dictionary of scaled observations in the form {glue_name: scaled_observation} \"\"\" normalized_observation_dict = collections . OrderedDict () for obs_name , obs in observations : normalized_obs = self . normalize_observation ( obs_name , obs ) if normalized_obs : normalized_observation_dict [ obs_name ] = normalized_obs return normalized_observation_dict def set_removed ( self , removed_state : bool ): \"\"\" Set the agent_removed flag for the agent's glues. Parameters ---------- - removed_state (bool): The value to set the agent_removed flag to. Returns ------- None \"\"\" for glue in self . agent_glue_dict . values (): glue . set_agent_removed ( removed_state )","title":"BaseAgent"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.frame_rate","text":"Get the frame rate this agent runs at","title":"frame_rate"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_validator","text":"Get the validator used to validate the kwargs passed to BaseAgent.","title":"get_validator"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_validator--returns","text":"BaseAgentParser A BaseAgent kwargs parser and validator.","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.platform_name","text":"Returns the name of the platform this agent is attached to","title":"platform_name"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.platform_name--returns","text":"Str The name of the platform","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.trainable","text":"Flag denoting if agent is trainable.","title":"trainable"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.trainable--returns","text":"Bool False","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.apply_action","text":"Applies actions to agent.","title":"apply_action()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.apply_action--parameters","text":"action_dict (optional): A dictionary of actions to be applied to agent.","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.apply_action--returns","text":"None Source code in corl/agents/base_agent.py def apply_action ( self , action_dict : typing . Dict [ typing . Any , typing . Any ]): \"\"\" Applies actions to agent. Parameters ---------- action_dict (optional): A dictionary of actions to be applied to agent. Returns ------- None \"\"\" raw_action_dict = collections . OrderedDict () obs = self . get_observations () for glue_name , glue_object in self . agent_glue_dict . items (): if glue_name in action_dict : normalized_action = action_dict [ glue_name ] raw_action = glue_object . unnormalize_action ( normalized_action ) raw_action_dict [ glue_name ] = raw_action glue_object . apply_action ( raw_action , obs ) return raw_action_dict","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.create_next_action","text":"Creates next action agent will apply.","title":"create_next_action()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.create_next_action--parameters","text":"action_dict (optional): A dictionary of actions to be applied to agent.","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.create_next_action--returns","text":"None Source code in corl/agents/base_agent.py def create_next_action ( self , observation : typing . Dict , # pylint: disable=unused-argument reward : float , # pylint: disable=unused-argument done : bool , # pylint: disable=unused-argument info : typing . Dict # pylint: disable=unused-argument ): \"\"\" Creates next action agent will apply. Parameters ---------- action_dict (optional): A dictionary of actions to be applied to agent. Returns ------- None \"\"\" ...","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.create_space","text":"Creates a gym dict space from the agent's glues.","title":"create_space()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.create_space--parameters","text":"space_getter (optional): A function that takes a glue_obj: BaseAgentGlue and returns a space. Default is space_getter=lambda glue_obj: glue_obj.observation_space().","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.create_space--returns","text":"gym.spaces.dict.Dict() A gym dict space composed of all the spaces returned by applying the space_getter to the agent glues. Source code in corl/agents/base_agent.py def create_space ( self , space_getter : typing . Optional [ typing . Callable ] = None ): \"\"\" Creates a gym dict space from the agent's glues. Parameters ---------- space_getter (optional): A function that takes a glue_obj: BaseAgentGlue and returns a space. Default is space_getter=lambda glue_obj: glue_obj.observation_space(). Returns ------- gym.spaces.dict.Dict() A gym dict space composed of all the spaces returned by applying the space_getter to the agent glues. \"\"\" if space_getter is None : def default_getter ( glue_obj ): return glue_obj . observation_space () space_getter = default_getter return_space = gym . spaces . dict . Dict () # loop over all glue name and glue_obj pairs glue_obj : BaseAgentGlue for glue_name , glue_obj in self . agent_glue_dict . items (): # call our space getter to pick which space we want, # for example: action_space, observation_space, normalized_action_space, normalized_observation_space space_def = space_getter ( glue_obj ) # if the space is None don't add it if space_def : return_space . spaces [ glue_name ] = space_def return return_space if len ( return_space . spaces ) > 0 else None","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.fill_parameters","text":"Sample the episode parameter provider to fill the local variable store. Source code in corl/agents/base_agent.py def fill_parameters ( self , rng : Randomness , default_parameters : bool = False ) -> None : \"\"\"Sample the episode parameter provider to fill the local variable store.\"\"\" if default_parameters : current_parameters = self . config . epp . config . parameters else : current_parameters , _ = self . config . epp . get_params ( rng ) self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( rng ) for k , v in current_parameters . items ()})","title":"fill_parameters()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_dones","text":"Get agent's done state from agent done functors.","title":"get_dones()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_dones--parameters","text":"observation: The observation dictionary used to compute action. action: The action computed from observation. next_observation: The observation dictionary containing observation of next_state. next_state: The state dictionary containing the environment state after action was applied to the environment. observation_space: The agent observation space. observation_units: The units of the observations in the observation space. This may be None.","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_dones--returns","text":"DoneDict[str: bool] A dictionary of the agent's done state in the form {agent_id: done} Source code in corl/agents/base_agent.py def get_dones ( self , observation , action , next_observation , next_state , observation_space , observation_units ): \"\"\" Get agent's done state from agent done functors. Parameters ---------- - observation: The observation dictionary used to compute action. - action: The action computed from observation. - next_observation: The observation dictionary containing observation of next_state. - next_state: The state dictionary containing the environment state after action was applied to the environment. - observation_space: The agent observation space. - observation_units: The units of the observations in the observation space. This may be None. Returns ------- DoneDict[str: bool] A dictionary of the agent's done state in the form {agent_id: done} \"\"\" return self . agent_done_dict ( observation = observation , action = action , next_observation = next_observation , next_state = next_state , observation_space = observation_space , observation_units = observation_units , )","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_glue","text":"Get the glue object with the given name Source code in corl/agents/base_agent.py def get_glue ( self , name : str ) -> typing . Optional [ BaseAgentGlue ]: \"\"\" Get the glue object with the given name \"\"\" if name in self . agent_glue_dict : return self . agent_glue_dict [ name ] return None","title":"get_glue()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_info_dict","text":"Gets combined observation from agent glues.","title":"get_info_dict()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_info_dict--returns","text":"OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} Source code in corl/agents/base_agent.py def get_info_dict ( self ): \"\"\" Gets combined observation from agent glues. Returns ------- OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} \"\"\" return_info : collections . OrderedDict = collections . OrderedDict () for glue_name , glue_object in self . agent_glue_dict . items (): glue_info = glue_object . get_info_dict () if glue_info : return_info [ glue_name ] = glue_info return return_info","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_observations","text":"Gets combined observation from agent glues.","title":"get_observations()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_observations--returns","text":"OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} Source code in corl/agents/base_agent.py def get_observations ( self ): \"\"\" Gets combined observation from agent glues. Returns ------- OrderedDict A dictionary of glue observations in the form {glue_name: glue_observation} \"\"\" return_observation : collections . OrderedDict = collections . OrderedDict () for glue_name , glue_object in self . agent_glue_dict . items (): glue_obs = glue_object . get_observation () if glue_obs : return_observation [ glue_name ] = glue_obs return return_observation","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_platform_parts","text":"Gets a list of the agent's platform parts.","title":"get_platform_parts()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_platform_parts--parameters","text":"simulator: Simulator class used by environment to simulate agent actions. platform_type: Platform type enumeration corresponding to the agent's platform","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_platform_parts--returns","text":"List List of platform parts Source code in corl/agents/base_agent.py def get_platform_parts ( self , simulator , platform_type ): \"\"\" Gets a list of the agent's platform parts. Parameters ---------- simulator: Simulator class used by environment to simulate agent actions. platform_type: Platform type enumeration corresponding to the agent's platform Returns ------- list: List of platform parts \"\"\" my_parts = [] for part in self . config . parts : tmp = PluginLibrary . FindMatch ( part . part , { \"simulator\" : simulator , \"platform_type\" : platform_type }) for ref_name , ref_key in part . references . items (): if ref_key not in self . config . reference_store : raise RuntimeError ( f 'Part reference { ref_key } must be in the agent reference store.' ) ref_value = self . config . reference_store [ ref_key ] if isinstance ( ref_value , Parameter ): raise TypeError ( f 'Part reference { ref_key } cannot be Parameter' ) part . config [ ref_name ] = ref_value my_parts . append (( tmp , part . config )) return my_parts","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_rewards","text":"Get agent's environment rewards from agent reward functors.","title":"get_rewards()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_rewards--parameters","text":"observation: The observation dictionary used to compute action. action: The action computed from observation. next_observation: The observation dictionary containing observation of next_state. state: The state dictionary containing the environment state before action was applied to the environment. next_state: The state dictionary containing the environment state after action was applied to the environment. observation_space: The agent observation space. observation_units: The units of the observations in the observation space. This may be None.","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_rewards--returns","text":"RewardDict[str: Any] A dictionary of the agent's reward in the form {agent_id: reward} Source code in corl/agents/base_agent.py def get_rewards ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ): \"\"\" Get agent's environment rewards from agent reward functors. Parameters ---------- - observation: The observation dictionary used to compute action. - action: The action computed from observation. - next_observation: The observation dictionary containing observation of next_state. - state: The state dictionary containing the environment state before action was applied to the environment. - next_state: The state dictionary containing the environment state after action was applied to the environment. - observation_space: The agent observation space. - observation_units: The units of the observations in the observation space. This may be None. Returns ------- RewardDict[str: Any] A dictionary of the agent's reward in the form {agent_id: reward} \"\"\" return self . agent_reward_dict ( observation = observation , action = action , next_observation = next_observation , state = state , next_state = next_state , observation_space = observation_space , observation_units = observation_units )","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.get_simulator_reset_parameters","text":"Return the local parameters needed within the simulator reset Source code in corl/agents/base_agent.py def get_simulator_reset_parameters ( self ) -> typing . Dict [ str , typing . Any ]: \"\"\"Return the local parameters needed within the simulator reset\"\"\" output = copy . deepcopy ( self . config . simulator_reset_parameters ) def expand_and_update ( data : MutableMapping [ str , Any ], simulator_reset : typing . Mapping [ str , Any ]): for key , value in data . items (): if isinstance ( value , BaseModel ): expand_and_update ( vars ( value ), simulator_reset . get ( key , {})) elif isinstance ( value , collections . abc . MutableMapping ): expand_and_update ( value , simulator_reset . get ( key , {})) else : if key in simulator_reset : data [ key ] = simulator_reset [ key ] expand_and_update ( output , self . local_variable_store . get ( 'simulator_reset' , {})) return output","title":"get_simulator_reset_parameters()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_dones","text":"Creates agent done functors from agent configuration.","title":"make_dones()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_dones--parameters","text":"agent_id: The id of the agent associated with the done functors. dones: Additional done conditions to apply env_params: Parameters for the provided dones env_ref_stores: Reference stores for items managed by the environment","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_dones--returns","text":"None Source code in corl/agents/base_agent.py def make_dones ( self , agent_id : str , platform_name : str , dones : typing . Iterable [ Functor ], env_params : typing . List [ typing . Sequence [ typing . Dict [ str , typing . Any ]]], env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]] ) -> None : \"\"\" Creates agent done functors from agent configuration. Parameters ---------- agent_id: The id of the agent associated with the done functors. dones: Additional done conditions to apply env_params: Parameters for the provided dones env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" tmp = [] for done_dict in chain ( self . config . dones , dones ): tmp . append ( done_dict . create_functor_object ( param_sources = [ self . local_variable_store . get ( 'dones' , {})] + env_params , ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores , agent_name = agent_id , platform_name = platform_name ) ) self . agent_done_dict = DoneDict ( processing_funcs = tmp )","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_glues","text":"Creates agent glue functors from agent configuration.","title":"make_glues()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_glues--parameters","text":"platform: The platform instance associated with the glue functors. agent_id: The id of the agent associated with the glue functors. env_ref_stores: Reference stores for items managed by the environment","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_glues--returns","text":"None Source code in corl/agents/base_agent.py def make_glues ( self , platform , agent_id : str , env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]]) -> None : \"\"\" Creates agent glue functors from agent configuration. Parameters ---------- platform: The platform instance associated with the glue functors. agent_id: The id of the agent associated with the glue functors. env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" self . agent_glue_dict . clear () for glue_dict in self . config . glues : created_glue = glue_dict . create_functor_object ( platform = platform , agent_name = agent_id , param_sources = [ self . local_variable_store . get ( 'glues' , {})], ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores ) glue_name = created_glue . get_unique_name () if glue_name is not None : if glue_name in self . agent_glue_dict : raise RuntimeError ( f \"The { glue_name } glue has a unique name, but it already exists\" ) else : raise RuntimeError ( f \"No glue name for { created_glue } \" ) # add the glue to the agent glue dict # self._agent_glue_obs_export_behavior[glue_name] = glue.training_obs_behavior self . agent_glue_dict [ glue_name ] = created_glue","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_rewards","text":"Creates agent reward functors from agent configuration.","title":"make_rewards()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_rewards--parameters","text":"agent_id: The id of the agent associated with the reward functors. env_ref_stores: Reference stores for items managed by the environment","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.make_rewards--returns","text":"None Source code in corl/agents/base_agent.py def make_rewards ( self , agent_id : str , env_ref_stores : typing . List [ typing . Dict [ str , typing . Any ]]) -> None : \"\"\" Creates agent reward functors from agent configuration. Parameters ---------- agent_id: The id of the agent associated with the reward functors. env_ref_stores: Reference stores for items managed by the environment Returns ------- None \"\"\" tmp = [] for reward_dict in self . config . rewards : tmp . append ( reward_dict . create_functor_object ( agent_name = agent_id , param_sources = [ self . local_variable_store . get ( 'rewards' , {})], ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] + env_ref_stores ) ) self . agent_reward_dict = RewardDict ( processing_funcs = tmp )","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.normalize_observation","text":"Normalize a single observation value (if a corresponding glue is found)","title":"normalize_observation()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.normalize_observation--parameters","text":"obs_name: The name of the observation obs: the observation (value)","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.normalize_observation--returns","text":"Normalized observation value, or None (if no corresponding glue is found) Source code in corl/agents/base_agent.py def normalize_observation ( self , obs_name : str , obs : EnvSpaceUtil . sample_type ) -> typing . Optional [ EnvSpaceUtil . sample_type ]: \"\"\" Normalize a single observation value (if a corresponding glue is found) Parameters ---------- - obs_name: The name of the observation - obs: the observation (value) Returns ------- Normalized observation value, or None (if no corresponding glue is found) \"\"\" glue_obj = self . get_glue ( obs_name ) if glue_obj is not None : # TODO: fix this if 'ObserveSensorRepeated' in obs_name : return glue_obj . normalize_observation ( copy . deepcopy ( obs )) return glue_obj . normalize_observation ( obs ) return None","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.normalize_observations","text":"Normalizes glue observations according to glue definition.","title":"normalize_observations()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.normalize_observations--parameters","text":"observations: A dictionary of observation glues.","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.normalize_observations--returns","text":"OrderedDict[str: typing.Union[np.ndarray, typing.Tuple, typing.Dict]] A dictionary of scaled observations in the form {glue_name: scaled_observation} Source code in corl/agents/base_agent.py def normalize_observations ( self , observations : collections . OrderedDict ) -> collections . OrderedDict : \"\"\" Normalizes glue observations according to glue definition. Parameters ---------- - observations: A dictionary of observation glues. Returns ------- OrderedDict[str: typing.Union[np.ndarray, typing.Tuple, typing.Dict]] A dictionary of scaled observations in the form {glue_name: scaled_observation} \"\"\" normalized_observation_dict = collections . OrderedDict () for obs_name , obs in observations : normalized_obs = self . normalize_observation ( obs_name , obs ) if normalized_obs : normalized_observation_dict [ obs_name ] = normalized_obs return normalized_observation_dict","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.post_process_trajectory","text":"calling function for sending data to on_postprocess_trajectory Source code in corl/agents/base_agent.py def post_process_trajectory ( self , agent_id , state , batch , episode , policy , reward_info ): \"\"\" calling function for sending data to on_postprocess_trajectory Arguments: agent_id str -- name of the agent state {[type]} -- current simulator state batch {[type]} -- batch from the current trajectory episode {[type]} -- the episode object policy {[type]} -- the policy that ran the trajectory reward_info {[type]} -- the info dict for rewards \"\"\" for reward_glue in self . agent_reward_dict . process_callbacks : output_value = reward_glue . post_process_trajectory ( agent_id , state , batch , episode , policy ) if output_value is not None : # If we have a bad setup the agent may be \"dead\" on first step # This in turn causes major issues when trying to index reward_info # Check that the agent exists if agent_id in reward_info : reward_info [ agent_id ] . setdefault ( reward_glue . name , {}) . setdefault ( agent_id , 0.0 ) reward_info [ agent_id ][ reward_glue . name ][ agent_id ] += output_value","title":"post_process_trajectory()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.set_removed","text":"Set the agent_removed flag for the agent's glues.","title":"set_removed()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.set_removed--parameters","text":"removed_state (bool): The value to set the agent_removed flag to.","title":"Parameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgent.set_removed--returns","text":"None Source code in corl/agents/base_agent.py def set_removed ( self , removed_state : bool ): \"\"\" Set the agent_removed flag for the agent's glues. Parameters ---------- - removed_state (bool): The value to set the agent_removed flag to. Returns ------- None \"\"\" for glue in self . agent_glue_dict . values (): glue . set_agent_removed ( removed_state )","title":"Returns"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentEppParameters","text":"typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: glue name, parameter name typing.Dict[str, typing.Dict[str, typing.Dict[str, typing.Any]]] = {} keys: reward name, ? name, parameter name typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name typing.Dict[str, typing.Any] = {} keys: reference name typing.Dict[str, typing.Any] = {} keys: whatever the simulator wants, but it needs to be kwargs to simulator reset Source code in corl/agents/base_agent.py class BaseAgentEppParameters ( BaseModel ): \"\"\" glues: typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: glue name, parameter name rewards: typing.Dict[str, typing.Dict[str, typing.Dict[str, typing.Any]]] = {} keys: reward name, ? name, parameter name dones: typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name reference_store: typing.Dict[str, typing.Any] = {} keys: reference name simulator_reset: typing.Dict[str, typing.Any] = {} keys: whatever the simulator wants, but it needs to be kwargs to simulator reset \"\"\" glues : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} rewards : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} dones : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} reference_store : typing . Dict [ str , typing . Any ] = {} simulator_reset : typing . Dict [ str , typing . Any ] = {} @staticmethod def _validate_leaves_are_parameters ( obj ): if isinstance ( obj , dict ): for _key , value in obj . items (): BaseAgentEppParameters . _validate_leaves_are_parameters ( value ) elif not isinstance ( obj , Parameter ): raise TypeError ( f \"Invalid type: { type ( obj ) } (required type: { Parameter . __qualname__ } )\" ) @validator ( 'glues' , 'rewards' , 'dones' , 'reference_store' , 'simulator_reset' ) def validate_leaves_are_parameters ( cls , v ): \"\"\" Verifies the outer most leaf nodes of a config are parameter types \"\"\" BaseAgentEppParameters . _validate_leaves_are_parameters ( v ) return v","title":"BaseAgentEppParameters"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentEppParameters.validate_leaves_are_parameters","text":"Verifies the outer most leaf nodes of a config are parameter types Source code in corl/agents/base_agent.py @validator ( 'glues' , 'rewards' , 'dones' , 'reference_store' , 'simulator_reset' ) def validate_leaves_are_parameters ( cls , v ): \"\"\" Verifies the outer most leaf nodes of a config are parameter types \"\"\" BaseAgentEppParameters . _validate_leaves_are_parameters ( v ) return v","title":"validate_leaves_are_parameters()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser","text":"parts: A list of part configurations. glues: A list of glue functor configurations. rewards: A optional list of reward functor configurations dones: An optional list of done functor configurations Source code in corl/agents/base_agent.py class BaseAgentParser ( BaseModel ): \"\"\" - parts: A list of part configurations. - glues: A list of glue functor configurations. - rewards: A optional list of reward functor configurations - dones: An optional list of done functor configurations \"\"\" # these can be sensors, controller, or other platform parts parts : List [ PartsFunctor ] # this uses plugin loader to load reference_store : Dict [ str , ObjectStoreElem ] = {} glues : List [ Union [ FunctorMultiWrapper , FunctorWrapper , Functor , FunctorDictWrapper ]] rewards : List [ Union [ FunctorMultiWrapper , FunctorWrapper , Functor , FunctorDictWrapper ]] = [] dones : List [ Union [ FunctorMultiWrapper , FunctorWrapper , Functor , FunctorDictWrapper ]] = [] simulator_reset_parameters : Dict [ str , Any ] = {} # frame_rate in Hz (i.e. .25 indicates this agent will process when sim_tim % 4 == 0) frame_rate : float = 1.0 agent_name : str platform_name : str multiple_workers : bool = False episode_parameter_provider : Factory episode_parameter_provider_parameters : BaseAgentEppParameters = None # type: ignore epp : EpisodeParameterProvider = None # type: ignore # this is a dictionary validated by the simulator that allows the agent to tell # the simulator what to do with this agent class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True @validator ( 'glues' , each_item = True ) def check_glues ( cls , v ): \"\"\"Check if glues subclass BaseAgentGlue\"\"\" if not issubclass ( v . functor , BaseAgentGlue ): raise TypeError ( f \"Glue functors must subclass BaseAgentGlue, but glue { v . name } is of type { v . functor } \" ) return v @validator ( 'rewards' , each_item = True ) def check_rewards ( cls , v ): \"\"\"Check if rewards subclass RewardFuncBase\"\"\" if not issubclass ( v . functor , RewardFuncBase ): raise TypeError ( f \"Reward functors must subclass RewardFuncBase, but reward { v . name } is of type { v . functor } \" ) return v @validator ( 'dones' , each_item = True ) def check_dones ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" if not issubclass ( v . functor , DoneFuncBase ): raise TypeError ( f \"Done functors must subclass DoneFuncBase, but done { v . name } is of type { v . functor } \" ) if issubclass ( v . functor , EpisodeLengthDone ): raise ValueError ( \"Cannot specify EpisodeLengthDone as it is automatically added\" ) return v resolve_factory = validator ( 'reference_store' , pre = True , each_item = True , allow_reuse = True )( Factory . resolve_factory ) @validator ( 'simulator_reset_parameters' , pre = True ) def update_units_and_parameters ( cls , v ): \"\"\"Update simulation reset parameters to meet base simulator requirements.\"\"\" return validation_helper_units_and_parameters ( v ) @validator ( 'episode_parameter_provider' ) def check_epp ( cls , epp_factory , values ): \"\"\"Check if episode parameter provider subclass EpisodeParameterProvider\"\"\" if not issubclass ( epp_factory . type , EpisodeParameterProvider ): raise TypeError ( f \"Episode parameter providers must subclass EpisodeParameterProvider, but is is of type { epp_factory . type } \" ) # replace %%AGENT%% with agent_name epp_params = flatten_dict . flatten ( epp_factory . config ) for key , value in epp_params . items (): if isinstance ( value , str ): epp_params [ key ] = value . replace ( r ' %% AGENT %% ' , values [ 'agent_name' ]) epp_factory . config = flatten_dict . unflatten ( epp_params ) return epp_factory @validator ( 'episode_parameter_provider_parameters' , always = True , pre = True ) def build_episode_parameter_provider_parameters ( cls , _v , values ) -> BaseAgentEppParameters : \"\"\"Create the episode parameter provider config\"\"\" for key in [ 'reference_store' , 'glues' , 'rewards' , 'dones' , 'simulator_reset_parameters' ]: assert key in values reference_parameters : typing . Dict [ str , Parameter ] = {} for ref_name , ref_value in values [ 'reference_store' ] . items (): if isinstance ( ref_value , Parameter ): reference_parameters [ ref_name ] = ref_value glue_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'glues' ]: functor . add_to_parameter_store ( glue_parameters ) reward_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'rewards' ]: functor . add_to_parameter_store ( reward_parameters ) done_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ]: functor . add_to_parameter_store ( done_parameters ) flat_data = flatten_dict . flatten ( values [ 'simulator_reset_parameters' ]) for key , value in flat_data . items (): if isinstance ( value , BaseModel ): flat_data [ key ] = value . dict () expanded_sim_reset_params = flatten_dict . unflatten ( flat_data ) sim_parameters_flat = { name : param for name , param in flatten_dict . flatten ( expanded_sim_reset_params ) . items () if isinstance ( param , Parameter ) } sim_parameters = flatten_dict . unflatten ( sim_parameters_flat ) return BaseAgentEppParameters ( glues = glue_parameters , rewards = reward_parameters , dones = done_parameters , reference_store = reference_parameters , simulator_reset = sim_parameters ) @validator ( 'epp' , always = True , pre = True ) def build_epp ( cls , epp , values ): \"\"\"Builds an instance of an EpisodeParameterProvider if necessary\"\"\" if epp is None : assert 'episode_parameter_provider_parameters' in values epp_params = dict ( values [ 'episode_parameter_provider_parameters' ]) flat_epp_params = flatten_dict . flatten ( epp_params ) epp = values [ 'episode_parameter_provider' ] . build ( parameters = flat_epp_params ) return epp @validator ( 'frame_rate' , always = True ) def simplify_frame_rate ( cls , v ): \"\"\"Expand the precision of the frame rate (e.g. .333 -> .3333333333334)\"\"\" f = fractions . Fraction ( v ) . limit_denominator ( 20 ) return f . numerator / f . denominator","title":"BaseAgentParser"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.Config","text":"Allow arbitrary types for Parameter Source code in corl/agents/base_agent.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.build_episode_parameter_provider_parameters","text":"Create the episode parameter provider config Source code in corl/agents/base_agent.py @validator ( 'episode_parameter_provider_parameters' , always = True , pre = True ) def build_episode_parameter_provider_parameters ( cls , _v , values ) -> BaseAgentEppParameters : \"\"\"Create the episode parameter provider config\"\"\" for key in [ 'reference_store' , 'glues' , 'rewards' , 'dones' , 'simulator_reset_parameters' ]: assert key in values reference_parameters : typing . Dict [ str , Parameter ] = {} for ref_name , ref_value in values [ 'reference_store' ] . items (): if isinstance ( ref_value , Parameter ): reference_parameters [ ref_name ] = ref_value glue_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'glues' ]: functor . add_to_parameter_store ( glue_parameters ) reward_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'rewards' ]: functor . add_to_parameter_store ( reward_parameters ) done_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ]: functor . add_to_parameter_store ( done_parameters ) flat_data = flatten_dict . flatten ( values [ 'simulator_reset_parameters' ]) for key , value in flat_data . items (): if isinstance ( value , BaseModel ): flat_data [ key ] = value . dict () expanded_sim_reset_params = flatten_dict . unflatten ( flat_data ) sim_parameters_flat = { name : param for name , param in flatten_dict . flatten ( expanded_sim_reset_params ) . items () if isinstance ( param , Parameter ) } sim_parameters = flatten_dict . unflatten ( sim_parameters_flat ) return BaseAgentEppParameters ( glues = glue_parameters , rewards = reward_parameters , dones = done_parameters , reference_store = reference_parameters , simulator_reset = sim_parameters )","title":"build_episode_parameter_provider_parameters()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.build_epp","text":"Builds an instance of an EpisodeParameterProvider if necessary Source code in corl/agents/base_agent.py @validator ( 'epp' , always = True , pre = True ) def build_epp ( cls , epp , values ): \"\"\"Builds an instance of an EpisodeParameterProvider if necessary\"\"\" if epp is None : assert 'episode_parameter_provider_parameters' in values epp_params = dict ( values [ 'episode_parameter_provider_parameters' ]) flat_epp_params = flatten_dict . flatten ( epp_params ) epp = values [ 'episode_parameter_provider' ] . build ( parameters = flat_epp_params ) return epp","title":"build_epp()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.check_dones","text":"Check if dones subclass DoneFuncBase Source code in corl/agents/base_agent.py @validator ( 'dones' , each_item = True ) def check_dones ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" if not issubclass ( v . functor , DoneFuncBase ): raise TypeError ( f \"Done functors must subclass DoneFuncBase, but done { v . name } is of type { v . functor } \" ) if issubclass ( v . functor , EpisodeLengthDone ): raise ValueError ( \"Cannot specify EpisodeLengthDone as it is automatically added\" ) return v","title":"check_dones()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.check_epp","text":"Check if episode parameter provider subclass EpisodeParameterProvider Source code in corl/agents/base_agent.py @validator ( 'episode_parameter_provider' ) def check_epp ( cls , epp_factory , values ): \"\"\"Check if episode parameter provider subclass EpisodeParameterProvider\"\"\" if not issubclass ( epp_factory . type , EpisodeParameterProvider ): raise TypeError ( f \"Episode parameter providers must subclass EpisodeParameterProvider, but is is of type { epp_factory . type } \" ) # replace %%AGENT%% with agent_name epp_params = flatten_dict . flatten ( epp_factory . config ) for key , value in epp_params . items (): if isinstance ( value , str ): epp_params [ key ] = value . replace ( r ' %% AGENT %% ' , values [ 'agent_name' ]) epp_factory . config = flatten_dict . unflatten ( epp_params ) return epp_factory","title":"check_epp()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.check_glues","text":"Check if glues subclass BaseAgentGlue Source code in corl/agents/base_agent.py @validator ( 'glues' , each_item = True ) def check_glues ( cls , v ): \"\"\"Check if glues subclass BaseAgentGlue\"\"\" if not issubclass ( v . functor , BaseAgentGlue ): raise TypeError ( f \"Glue functors must subclass BaseAgentGlue, but glue { v . name } is of type { v . functor } \" ) return v","title":"check_glues()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.check_rewards","text":"Check if rewards subclass RewardFuncBase Source code in corl/agents/base_agent.py @validator ( 'rewards' , each_item = True ) def check_rewards ( cls , v ): \"\"\"Check if rewards subclass RewardFuncBase\"\"\" if not issubclass ( v . functor , RewardFuncBase ): raise TypeError ( f \"Reward functors must subclass RewardFuncBase, but reward { v . name } is of type { v . functor } \" ) return v","title":"check_rewards()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.resolve_factory","text":"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) Source code in corl/agents/base_agent.py @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build ()","title":"resolve_factory()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.simplify_frame_rate","text":"Expand the precision of the frame rate (e.g. .333 -> .3333333333334) Source code in corl/agents/base_agent.py @validator ( 'frame_rate' , always = True ) def simplify_frame_rate ( cls , v ): \"\"\"Expand the precision of the frame rate (e.g. .333 -> .3333333333334)\"\"\" f = fractions . Fraction ( v ) . limit_denominator ( 20 ) return f . numerator / f . denominator","title":"simplify_frame_rate()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.BaseAgentParser.update_units_and_parameters","text":"Update simulation reset parameters to meet base simulator requirements. Source code in corl/agents/base_agent.py @validator ( 'simulator_reset_parameters' , pre = True ) def update_units_and_parameters ( cls , v ): \"\"\"Update simulation reset parameters to meet base simulator requirements.\"\"\" return validation_helper_units_and_parameters ( v )","title":"update_units_and_parameters()"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.PartsFunctor","text":"part: The name of the part. This should be registered to a corresponding platform part class in the plugin library. config: The specific configuration dictionary expected by the platform part. Source code in corl/agents/base_agent.py class PartsFunctor ( BaseModel ): \"\"\" - part: The name of the part. This should be registered to a corresponding platform part class in the plugin library. - config: The specific configuration dictionary expected by the platform part. \"\"\" part : str config : Dict [ str , Any ] = {} references : Dict [ str , str ] = {}","title":"PartsFunctor"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.PlatformParseInfo","text":"platform_config Source code in corl/agents/base_agent.py class PlatformParseInfo ( BaseModel ): \"\"\" - platform_config \"\"\" platform_config : Dict [ str , Any ]","title":"PlatformParseInfo"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.TrainableBaseAgent","text":"Base class representing a trainable agent in an environment. Source code in corl/agents/base_agent.py class TrainableBaseAgent ( BaseAgent ): \"\"\" Base class representing a trainable agent in an environment. \"\"\" @property def trainable ( self ) -> bool : \"\"\" Flag denoting if agent is trainable. Returns ------- bool: True \"\"\" return True","title":"TrainableBaseAgent"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.TrainableBaseAgent.trainable","text":"Flag denoting if agent is trainable.","title":"trainable"},{"location":"reference/agents/base_agent/#corl.agents.base_agent.TrainableBaseAgent.trainable--returns","text":"Bool True","title":"Returns"},{"location":"reference/dones/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Dones"},{"location":"reference/dones/done_func_base/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. DoneFuncBase ( EnvFuncBase ) \u00a4 Base implementation for done functors Source code in corl/dones/done_func_base.py class DoneFuncBase ( EnvFuncBase ): \"\"\"Base implementation for done functors \"\"\" _ALL = \"__all__\" def __init__ ( self , ** kwargs ) -> None : self . config : DoneFuncBaseValidator = self . get_validator ( ** kwargs ) self . config . name = self . config . name if self . config . name else type ( self ) . __name__ @property def get_validator ( self ) -> typing . Type [ DoneFuncBaseValidator ]: \"\"\" get validator for this Done Functor Returns: DoneFuncBaseValidator -- validator the done functor will use to generate a configuration \"\"\" return DoneFuncBaseValidator @property def agent ( self ) -> str : \"\"\"The agent to which this done is applied\"\"\" return self . config . agent_name @property def platform ( self ) -> str : \"\"\"The platform to which this done is applied\"\"\" return self . config . platform_name @property def name ( self ) -> str : return self . config . name def _set_all_done ( self , done ): done [ DoneFuncBase . _ALL ] = False if np . any ( list ( done . values ())) and self . config . early_stop : done [ DoneFuncBase . _ALL ] = True done = done . fromkeys ( done , True ) return done @staticmethod def _get_platform_time ( platform ): sensor = [ s for s in platform . sensors if isinstance ( s , BaseTimeSensor )] if not sensor : raise ValueError ( \"Did not find time sensor type (BaseTimeSensor)\" ) update_time = sensor [ 0 ] . get_measurement ()[ 0 ] return update_time @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : ... agent : str property readonly \u00a4 The agent to which this done is applied get_validator : Type [ corl . dones . done_func_base . DoneFuncBaseValidator ] property readonly \u00a4 get validator for this Done Functor Returns: Type Description Type[corl.dones.done_func_base.DoneFuncBaseValidator] DoneFuncBaseValidator -- validator the done functor will use to generate a configuration name : str property readonly \u00a4 gets the name fo the functor Returns \u00a4 str The name of the functor platform : str property readonly \u00a4 The platform to which this done is applied DoneFuncBaseValidator ( BaseModel ) pydantic-model \u00a4 Initialize the done condition All done conditions have three common parameters, with any others being handled by subclass validators Parameters \u00a4 name : str A name applied to this done condition, by default the name of the class. agent_name : str Name of the agent to which this done condition applies platform_name : str Name of the platform to which this done condition applies early_stop : bool, optional If True, set the done condition on \" all \" once any done condition is True. This is by default False. Source code in corl/dones/done_func_base.py class DoneFuncBaseValidator ( BaseModel ): \"\"\"Initialize the done condition All done conditions have three common parameters, with any others being handled by subclass validators Parameters ---------- name : str A name applied to this done condition, by default the name of the class. agent_name : str Name of the agent to which this done condition applies platform_name : str Name of the platform to which this done condition applies early_stop : bool, optional If True, set the done condition on \"__all__\" once any done condition is True. This is by default False. \"\"\" name : str = \"\" agent_name : str platform_name : str early_stop : bool = False DoneStatusCodes ( Enum ) \u00a4 reward states for the done conditions Source code in corl/dones/done_func_base.py class DoneStatusCodes ( Enum ): \"\"\"reward states for the done conditions \"\"\" WIN = 1 PARTIAL_WIN = 2 DRAW = 3 PARTIAL_LOSS = 4 LOSE = 5 SharedDoneFuncBase ( EnvFuncBase ) \u00a4 Base implementation for global done functors Global done functors are not associated with a single agent, but the environment as a whole. They use a modified call syntax that receives the done dictionary and done info from the per-agent done functors. Source code in corl/dones/done_func_base.py class SharedDoneFuncBase ( EnvFuncBase ): \"\"\"Base implementation for global done functors Global done functors are not associated with a single agent, but the environment as a whole. They use a modified call syntax that receives the done dictionary and done info from the per-agent done functors. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : SharedDoneFuncBaseValidator = self . get_validator ( ** kwargs ) self . config . name = self . config . name if self . config . name else type ( self ) . __name__ @property def get_validator ( self ) -> typing . Type [ SharedDoneFuncBaseValidator ]: \"\"\" gets the validator for this SharedDoneFuncBase Returns: SharedDoneFuncBaseValidator -- validator for this class \"\"\" return SharedDoneFuncBaseValidator @property def name ( self ) -> str : return self . config . name @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , local_dones : DoneDict , local_done_info : OrderedDict ) -> DoneDict : ... get_validator : Type [ corl . dones . done_func_base . SharedDoneFuncBaseValidator ] property readonly \u00a4 gets the validator for this SharedDoneFuncBase Returns: Type Description Type[corl.dones.done_func_base.SharedDoneFuncBaseValidator] SharedDoneFuncBaseValidator -- validator for this class name : str property readonly \u00a4 gets the name fo the functor Returns \u00a4 str The name of the functor SharedDoneFuncBaseValidator ( BaseModel ) pydantic-model \u00a4 name : str The name of this done condition Source code in corl/dones/done_func_base.py class SharedDoneFuncBaseValidator ( BaseModel ): \"\"\" name : str The name of this done condition \"\"\" name : str = \"\"","title":"Done func base"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneFuncBase","text":"Base implementation for done functors Source code in corl/dones/done_func_base.py class DoneFuncBase ( EnvFuncBase ): \"\"\"Base implementation for done functors \"\"\" _ALL = \"__all__\" def __init__ ( self , ** kwargs ) -> None : self . config : DoneFuncBaseValidator = self . get_validator ( ** kwargs ) self . config . name = self . config . name if self . config . name else type ( self ) . __name__ @property def get_validator ( self ) -> typing . Type [ DoneFuncBaseValidator ]: \"\"\" get validator for this Done Functor Returns: DoneFuncBaseValidator -- validator the done functor will use to generate a configuration \"\"\" return DoneFuncBaseValidator @property def agent ( self ) -> str : \"\"\"The agent to which this done is applied\"\"\" return self . config . agent_name @property def platform ( self ) -> str : \"\"\"The platform to which this done is applied\"\"\" return self . config . platform_name @property def name ( self ) -> str : return self . config . name def _set_all_done ( self , done ): done [ DoneFuncBase . _ALL ] = False if np . any ( list ( done . values ())) and self . config . early_stop : done [ DoneFuncBase . _ALL ] = True done = done . fromkeys ( done , True ) return done @staticmethod def _get_platform_time ( platform ): sensor = [ s for s in platform . sensors if isinstance ( s , BaseTimeSensor )] if not sensor : raise ValueError ( \"Did not find time sensor type (BaseTimeSensor)\" ) update_time = sensor [ 0 ] . get_measurement ()[ 0 ] return update_time @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : ...","title":"DoneFuncBase"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneFuncBase.agent","text":"The agent to which this done is applied","title":"agent"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneFuncBase.get_validator","text":"get validator for this Done Functor Returns: Type Description Type[corl.dones.done_func_base.DoneFuncBaseValidator] DoneFuncBaseValidator -- validator the done functor will use to generate a configuration","title":"get_validator"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneFuncBase.name","text":"gets the name fo the functor","title":"name"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneFuncBase.name--returns","text":"str The name of the functor","title":"Returns"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneFuncBase.platform","text":"The platform to which this done is applied","title":"platform"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneFuncBaseValidator","text":"Initialize the done condition All done conditions have three common parameters, with any others being handled by subclass validators","title":"DoneFuncBaseValidator"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneFuncBaseValidator--parameters","text":"name : str A name applied to this done condition, by default the name of the class. agent_name : str Name of the agent to which this done condition applies platform_name : str Name of the platform to which this done condition applies early_stop : bool, optional If True, set the done condition on \" all \" once any done condition is True. This is by default False. Source code in corl/dones/done_func_base.py class DoneFuncBaseValidator ( BaseModel ): \"\"\"Initialize the done condition All done conditions have three common parameters, with any others being handled by subclass validators Parameters ---------- name : str A name applied to this done condition, by default the name of the class. agent_name : str Name of the agent to which this done condition applies platform_name : str Name of the platform to which this done condition applies early_stop : bool, optional If True, set the done condition on \"__all__\" once any done condition is True. This is by default False. \"\"\" name : str = \"\" agent_name : str platform_name : str early_stop : bool = False","title":"Parameters"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.DoneStatusCodes","text":"reward states for the done conditions Source code in corl/dones/done_func_base.py class DoneStatusCodes ( Enum ): \"\"\"reward states for the done conditions \"\"\" WIN = 1 PARTIAL_WIN = 2 DRAW = 3 PARTIAL_LOSS = 4 LOSE = 5","title":"DoneStatusCodes"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.SharedDoneFuncBase","text":"Base implementation for global done functors Global done functors are not associated with a single agent, but the environment as a whole. They use a modified call syntax that receives the done dictionary and done info from the per-agent done functors. Source code in corl/dones/done_func_base.py class SharedDoneFuncBase ( EnvFuncBase ): \"\"\"Base implementation for global done functors Global done functors are not associated with a single agent, but the environment as a whole. They use a modified call syntax that receives the done dictionary and done info from the per-agent done functors. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : SharedDoneFuncBaseValidator = self . get_validator ( ** kwargs ) self . config . name = self . config . name if self . config . name else type ( self ) . __name__ @property def get_validator ( self ) -> typing . Type [ SharedDoneFuncBaseValidator ]: \"\"\" gets the validator for this SharedDoneFuncBase Returns: SharedDoneFuncBaseValidator -- validator for this class \"\"\" return SharedDoneFuncBaseValidator @property def name ( self ) -> str : return self . config . name @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , local_dones : DoneDict , local_done_info : OrderedDict ) -> DoneDict : ...","title":"SharedDoneFuncBase"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.SharedDoneFuncBase.get_validator","text":"gets the validator for this SharedDoneFuncBase Returns: Type Description Type[corl.dones.done_func_base.SharedDoneFuncBaseValidator] SharedDoneFuncBaseValidator -- validator for this class","title":"get_validator"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.SharedDoneFuncBase.name","text":"gets the name fo the functor","title":"name"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.SharedDoneFuncBase.name--returns","text":"str The name of the functor","title":"Returns"},{"location":"reference/dones/done_func_base/#corl.dones.done_func_base.SharedDoneFuncBaseValidator","text":"name : str The name of this done condition Source code in corl/dones/done_func_base.py class SharedDoneFuncBaseValidator ( BaseModel ): \"\"\" name : str The name of this done condition \"\"\" name : str = \"\"","title":"SharedDoneFuncBaseValidator"},{"location":"reference/dones/done_func_dict_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseDictWrapperDone ( DoneFuncBase ) \u00a4 A base object that dones can inherit in order to \"wrap\" multiple done instances, addressed by keys Source code in corl/dones/done_func_dict_wrapper.py class BaseDictWrapperDone ( DoneFuncBase ): \"\"\"A base object that dones can inherit in order to \"wrap\" multiple done instances, addressed by keys \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseDictWrapperDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseDictWrapperDoneValidator ]: return BaseDictWrapperDoneValidator def dones ( self ) -> typing . Dict [ str , DoneFuncBase ]: \"\"\"Get the wrapped done instance dict \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : ... get_validator : Type [ corl . dones . done_func_dict_wrapper . BaseDictWrapperDoneValidator ] property readonly \u00a4 get validator for this Done Functor Returns: Type Description Type[corl.dones.done_func_dict_wrapper.BaseDictWrapperDoneValidator] DoneFuncBaseValidator -- validator the done functor will use to generate a configuration dones ( self ) \u00a4 Get the wrapped done instance dict Source code in corl/dones/done_func_dict_wrapper.py def dones ( self ) -> typing . Dict [ str , DoneFuncBase ]: \"\"\"Get the wrapped done instance dict \"\"\" return self . config . wrapped BaseDictWrapperDoneValidator ( DoneFuncBaseValidator ) pydantic-model \u00a4 wrapped - the wrapped done instances and their keys Source code in corl/dones/done_func_dict_wrapper.py class BaseDictWrapperDoneValidator ( DoneFuncBaseValidator ): \"\"\" wrapped - the wrapped done instances and their keys \"\"\" wrapped : typing . Dict [ str , DoneFuncBase ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Done func dict wrapper"},{"location":"reference/dones/done_func_dict_wrapper/#corl.dones.done_func_dict_wrapper.BaseDictWrapperDone","text":"A base object that dones can inherit in order to \"wrap\" multiple done instances, addressed by keys Source code in corl/dones/done_func_dict_wrapper.py class BaseDictWrapperDone ( DoneFuncBase ): \"\"\"A base object that dones can inherit in order to \"wrap\" multiple done instances, addressed by keys \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseDictWrapperDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseDictWrapperDoneValidator ]: return BaseDictWrapperDoneValidator def dones ( self ) -> typing . Dict [ str , DoneFuncBase ]: \"\"\"Get the wrapped done instance dict \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : ...","title":"BaseDictWrapperDone"},{"location":"reference/dones/done_func_dict_wrapper/#corl.dones.done_func_dict_wrapper.BaseDictWrapperDone.get_validator","text":"get validator for this Done Functor Returns: Type Description Type[corl.dones.done_func_dict_wrapper.BaseDictWrapperDoneValidator] DoneFuncBaseValidator -- validator the done functor will use to generate a configuration","title":"get_validator"},{"location":"reference/dones/done_func_dict_wrapper/#corl.dones.done_func_dict_wrapper.BaseDictWrapperDone.dones","text":"Get the wrapped done instance dict Source code in corl/dones/done_func_dict_wrapper.py def dones ( self ) -> typing . Dict [ str , DoneFuncBase ]: \"\"\"Get the wrapped done instance dict \"\"\" return self . config . wrapped","title":"dones()"},{"location":"reference/dones/done_func_dict_wrapper/#corl.dones.done_func_dict_wrapper.BaseDictWrapperDoneValidator","text":"wrapped - the wrapped done instances and their keys Source code in corl/dones/done_func_dict_wrapper.py class BaseDictWrapperDoneValidator ( DoneFuncBaseValidator ): \"\"\" wrapped - the wrapped done instances and their keys \"\"\" wrapped : typing . Dict [ str , DoneFuncBase ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseDictWrapperDoneValidator"},{"location":"reference/dones/done_func_multi_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseMultiWrapperDone ( DoneFuncBase ) \u00a4 A base object that dones can inherit in order to \"wrap\" a single done instance Source code in corl/dones/done_func_multi_wrapper.py class BaseMultiWrapperDone ( DoneFuncBase ): \"\"\"A base object that dones can inherit in order to \"wrap\" a single done instance \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseMultiWrapperDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseMultiWrapperDoneValidator ]: return BaseMultiWrapperDoneValidator def dones ( self ) -> typing . List [ DoneFuncBase ]: \"\"\"Get the wrapped done instance \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : ... get_validator : Type [ corl . dones . done_func_multi_wrapper . BaseMultiWrapperDoneValidator ] property readonly \u00a4 get validator for this Done Functor Returns: Type Description Type[corl.dones.done_func_multi_wrapper.BaseMultiWrapperDoneValidator] DoneFuncBaseValidator -- validator the done functor will use to generate a configuration dones ( self ) \u00a4 Get the wrapped done instance Source code in corl/dones/done_func_multi_wrapper.py def dones ( self ) -> typing . List [ DoneFuncBase ]: \"\"\"Get the wrapped done instance \"\"\" return self . config . wrapped BaseMultiWrapperDoneValidator ( DoneFuncBaseValidator ) pydantic-model \u00a4 wrapped - the wrapped done instance Source code in corl/dones/done_func_multi_wrapper.py class BaseMultiWrapperDoneValidator ( DoneFuncBaseValidator ): \"\"\" wrapped - the wrapped done instance \"\"\" wrapped : typing . List [ DoneFuncBase ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Done func multi wrapper"},{"location":"reference/dones/done_func_multi_wrapper/#corl.dones.done_func_multi_wrapper.BaseMultiWrapperDone","text":"A base object that dones can inherit in order to \"wrap\" a single done instance Source code in corl/dones/done_func_multi_wrapper.py class BaseMultiWrapperDone ( DoneFuncBase ): \"\"\"A base object that dones can inherit in order to \"wrap\" a single done instance \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseMultiWrapperDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseMultiWrapperDoneValidator ]: return BaseMultiWrapperDoneValidator def dones ( self ) -> typing . List [ DoneFuncBase ]: \"\"\"Get the wrapped done instance \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : ...","title":"BaseMultiWrapperDone"},{"location":"reference/dones/done_func_multi_wrapper/#corl.dones.done_func_multi_wrapper.BaseMultiWrapperDone.get_validator","text":"get validator for this Done Functor Returns: Type Description Type[corl.dones.done_func_multi_wrapper.BaseMultiWrapperDoneValidator] DoneFuncBaseValidator -- validator the done functor will use to generate a configuration","title":"get_validator"},{"location":"reference/dones/done_func_multi_wrapper/#corl.dones.done_func_multi_wrapper.BaseMultiWrapperDone.dones","text":"Get the wrapped done instance Source code in corl/dones/done_func_multi_wrapper.py def dones ( self ) -> typing . List [ DoneFuncBase ]: \"\"\"Get the wrapped done instance \"\"\" return self . config . wrapped","title":"dones()"},{"location":"reference/dones/done_func_multi_wrapper/#corl.dones.done_func_multi_wrapper.BaseMultiWrapperDoneValidator","text":"wrapped - the wrapped done instance Source code in corl/dones/done_func_multi_wrapper.py class BaseMultiWrapperDoneValidator ( DoneFuncBaseValidator ): \"\"\" wrapped - the wrapped done instance \"\"\" wrapped : typing . List [ DoneFuncBase ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseMultiWrapperDoneValidator"},{"location":"reference/dones/done_func_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseWrapperDone ( DoneFuncBase ) \u00a4 A base object that dones can inherit in order to \"wrap\" a single done instance Source code in corl/dones/done_func_wrapper.py class BaseWrapperDone ( DoneFuncBase ): \"\"\"A base object that dones can inherit in order to \"wrap\" a single done instance \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseWrapperDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseWrapperDoneValidator ]: return BaseWrapperDoneValidator def done ( self ) -> DoneFuncBase : \"\"\"Get the wrapped done instance \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : ... get_validator : Type [ corl . dones . done_func_wrapper . BaseWrapperDoneValidator ] property readonly \u00a4 get validator for this Done Functor Returns: Type Description Type[corl.dones.done_func_wrapper.BaseWrapperDoneValidator] DoneFuncBaseValidator -- validator the done functor will use to generate a configuration done ( self ) \u00a4 Get the wrapped done instance Source code in corl/dones/done_func_wrapper.py def done ( self ) -> DoneFuncBase : \"\"\"Get the wrapped done instance \"\"\" return self . config . wrapped BaseWrapperDoneValidator ( DoneFuncBaseValidator ) pydantic-model \u00a4 wrapped - the wrapped done instance Source code in corl/dones/done_func_wrapper.py class BaseWrapperDoneValidator ( DoneFuncBaseValidator ): \"\"\" wrapped - the wrapped done instance \"\"\" wrapped : DoneFuncBase class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Done func wrapper"},{"location":"reference/dones/done_func_wrapper/#corl.dones.done_func_wrapper.BaseWrapperDone","text":"A base object that dones can inherit in order to \"wrap\" a single done instance Source code in corl/dones/done_func_wrapper.py class BaseWrapperDone ( DoneFuncBase ): \"\"\"A base object that dones can inherit in order to \"wrap\" a single done instance \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseWrapperDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseWrapperDoneValidator ]: return BaseWrapperDoneValidator def done ( self ) -> DoneFuncBase : \"\"\"Get the wrapped done instance \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : ...","title":"BaseWrapperDone"},{"location":"reference/dones/done_func_wrapper/#corl.dones.done_func_wrapper.BaseWrapperDone.get_validator","text":"get validator for this Done Functor Returns: Type Description Type[corl.dones.done_func_wrapper.BaseWrapperDoneValidator] DoneFuncBaseValidator -- validator the done functor will use to generate a configuration","title":"get_validator"},{"location":"reference/dones/done_func_wrapper/#corl.dones.done_func_wrapper.BaseWrapperDone.done","text":"Get the wrapped done instance Source code in corl/dones/done_func_wrapper.py def done ( self ) -> DoneFuncBase : \"\"\"Get the wrapped done instance \"\"\" return self . config . wrapped","title":"done()"},{"location":"reference/dones/done_func_wrapper/#corl.dones.done_func_wrapper.BaseWrapperDoneValidator","text":"wrapped - the wrapped done instance Source code in corl/dones/done_func_wrapper.py class BaseWrapperDoneValidator ( DoneFuncBaseValidator ): \"\"\" wrapped - the wrapped done instance \"\"\" wrapped : DoneFuncBase class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseWrapperDoneValidator"},{"location":"reference/dones/episode_length_done/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. EpisodeLengthDone ( DoneFuncBase ) \u00a4 CheckEpisodeLengthDone Just checks to see if we hit the timeout time and notes in the code via a done condition... Note this is largely a debug item Source code in corl/dones/episode_length_done.py class EpisodeLengthDone ( DoneFuncBase ): \"\"\" CheckEpisodeLengthDone Just checks to see if we hit the timeout time and notes in the code via a done condition... Note this is largely a debug item \"\"\" REQUIRED_UNITS = { 'horizon' : Time . Second } def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeLengthDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ EpisodeLengthDoneValidator ]: \"\"\"Returns the validator for this done condition\"\"\" return EpisodeLengthDoneValidator def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : done = DoneDict () try : done [ self . platform ] = next_state [ 'sim_time' ] >= self . config . horizon if done [ self . platform ]: next_state . episode_state [ self . platform ][ self . name ] = DoneStatusCodes . DRAW except ValueError : # Missing platform should trigger some other done condition done [ self . platform ] = False self . _set_all_done ( done ) if done [ self . platform ]: next_state . episode_state [ self . platform ][ self . name ] = DoneStatusCodes . DRAW return done get_validator : Type [ corl . dones . episode_length_done . EpisodeLengthDoneValidator ] property readonly \u00a4 Returns the validator for this done condition EpisodeLengthDoneValidator ( DoneFuncBaseValidator ) pydantic-model \u00a4 Initialize an EpisodeLengthDone Parameters \u00a4 horizon : float, optional The max expected length for horizon (in seconds), by default 1000 Source code in corl/dones/episode_length_done.py class EpisodeLengthDoneValidator ( DoneFuncBaseValidator ): \"\"\"Initialize an EpisodeLengthDone Parameters ---------- horizon : float, optional The max expected length for horizon (in seconds), by default 1000 \"\"\" horizon : float = 1000","title":"Episode length done"},{"location":"reference/dones/episode_length_done/#corl.dones.episode_length_done.EpisodeLengthDone","text":"CheckEpisodeLengthDone Just checks to see if we hit the timeout time and notes in the code via a done condition... Note this is largely a debug item Source code in corl/dones/episode_length_done.py class EpisodeLengthDone ( DoneFuncBase ): \"\"\" CheckEpisodeLengthDone Just checks to see if we hit the timeout time and notes in the code via a done condition... Note this is largely a debug item \"\"\" REQUIRED_UNITS = { 'horizon' : Time . Second } def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeLengthDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ EpisodeLengthDoneValidator ]: \"\"\"Returns the validator for this done condition\"\"\" return EpisodeLengthDoneValidator def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : done = DoneDict () try : done [ self . platform ] = next_state [ 'sim_time' ] >= self . config . horizon if done [ self . platform ]: next_state . episode_state [ self . platform ][ self . name ] = DoneStatusCodes . DRAW except ValueError : # Missing platform should trigger some other done condition done [ self . platform ] = False self . _set_all_done ( done ) if done [ self . platform ]: next_state . episode_state [ self . platform ][ self . name ] = DoneStatusCodes . DRAW return done","title":"EpisodeLengthDone"},{"location":"reference/dones/episode_length_done/#corl.dones.episode_length_done.EpisodeLengthDone.get_validator","text":"Returns the validator for this done condition","title":"get_validator"},{"location":"reference/dones/episode_length_done/#corl.dones.episode_length_done.EpisodeLengthDoneValidator","text":"Initialize an EpisodeLengthDone","title":"EpisodeLengthDoneValidator"},{"location":"reference/dones/episode_length_done/#corl.dones.episode_length_done.EpisodeLengthDoneValidator--parameters","text":"horizon : float, optional The max expected length for horizon (in seconds), by default 1000 Source code in corl/dones/episode_length_done.py class EpisodeLengthDoneValidator ( DoneFuncBaseValidator ): \"\"\"Initialize an EpisodeLengthDone Parameters ---------- horizon : float, optional The max expected length for horizon (in seconds), by default 1000 \"\"\" horizon : float = 1000","title":"Parameters"},{"location":"reference/dones/openai_gym_done/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Done condition for OpenAIGymSimulator OpenAIGymDone ( DoneFuncBase ) \u00a4 A done functor that simply mirrors the done condition coming from the sim state this only works with the OpenAIGymSimulator Source code in corl/dones/openai_gym_done.py class OpenAIGymDone ( DoneFuncBase ): \"\"\" A done functor that simply mirrors the done condition coming from the sim state this only works with the OpenAIGymSimulator \"\"\" def __call__ ( self , observation , action , next_observation , next_state , observation_space , observation_units , ): done = DoneDict () done [ self . config . agent_name ] = next_state . dones [ self . config . agent_name ] self . _set_all_done ( done ) return done","title":"Openai gym done"},{"location":"reference/dones/openai_gym_done/#corl.dones.openai_gym_done.OpenAIGymDone","text":"A done functor that simply mirrors the done condition coming from the sim state this only works with the OpenAIGymSimulator Source code in corl/dones/openai_gym_done.py class OpenAIGymDone ( DoneFuncBase ): \"\"\" A done functor that simply mirrors the done condition coming from the sim state this only works with the OpenAIGymSimulator \"\"\" def __call__ ( self , observation , action , next_observation , next_state , observation_space , observation_units , ): done = DoneDict () done [ self . config . agent_name ] = next_state . dones [ self . config . agent_name ] self . _set_all_done ( done ) return done","title":"OpenAIGymDone"},{"location":"reference/dones/sensor_bounds_check_done/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. SensorBoundsCheckDone ( DoneFuncBase ) \u00a4 Checks to see if a specified state parameter is within bounds Source code in corl/dones/sensor_bounds_check_done.py class SensorBoundsCheckDone ( DoneFuncBase ): \"\"\" Checks to see if a specified state parameter is within bounds \"\"\" # True means that clients should pass ValueWithUnits rather than evaluated value REQUIRED_UNITS = { 'min_value' : True , 'max_value' : True , 'sensor_name' : NoneUnitType . NoneUnit } def __init__ ( self , ** kwargs ) -> None : self . config : SensorBoundsCheckDoneValidator super () . __init__ ( ** kwargs ) if self . config . name == type ( self ) . __name__ : self . config . name = self . config . sensor_name + '_Done' @property def get_validator ( self ) -> typing . Type [ SensorBoundsCheckDoneValidator ]: \"\"\"Returns the validator for this done condition\"\"\" return SensorBoundsCheckDoneValidator def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : done = DoneDict () # Find Target Platform platform = get_platform_by_name ( next_state , self . platform , allow_invalid = True ) # platform does not exist if platform is None : done [ self . platform ] = False return done # Find target Sensor sensor = get_sensor_by_name ( platform , self . config . sensor_name ) if not isinstance ( sensor . measurement_properties , BoxProp ): raise TypeError ( f 'Can only do bounds checking on BoxProp, received { type ( sensor . measurement_properties ) . __name__ } ' ) # Get measured value measurement = sensor . get_measurement () if len ( measurement ) != 1 : raise ValueError ( \"Sensor measurement has more than one element\" ) measured_value = ValueWithUnits ( value = measurement [ 0 ], units = sensor . measurement_properties . unit [ 0 ]) converted_value = measured_value . as_units ( self . config . min_value . units ) # Determine if done done [ self . platform ] = converted_value < self . config . min_value . value or self . config . max_value . value < converted_value if done [ self . platform ]: next_state . episode_state [ self . platform ][ self . name ] = DoneStatusCodes . LOSE self . _set_all_done ( done ) return done get_validator : Type [ corl . dones . sensor_bounds_check_done . SensorBoundsCheckDoneValidator ] property readonly \u00a4 Returns the validator for this done condition SensorBoundsCheckDoneValidator ( DoneFuncBaseValidator ) pydantic-model \u00a4 Initialize an SensorBoundsCheckDone Parameters \u00a4 min_value : float The minimum allowed value of this sensor max_value : float The maximum allowed value of this sensor sensor_name : str The name of the sensor to check Source code in corl/dones/sensor_bounds_check_done.py class SensorBoundsCheckDoneValidator ( DoneFuncBaseValidator ): \"\"\"Initialize an SensorBoundsCheckDone Parameters ---------- min_value : float The minimum allowed value of this sensor max_value : float The maximum allowed value of this sensor sensor_name : str The name of the sensor to check \"\"\" min_value : ValueWithUnits max_value : ValueWithUnits sensor_name : str @validator ( 'max_value' ) def min_max_consistent ( cls , v , values ): \"\"\"Validate that the maximum is bigger than the minimum.\"\"\" if 'min_value' not in values : # min_value failed, so let that error message be provided return v if v . units != values [ 'min_value' ] . units : raise ValueError ( f 'Inconsistent units for min and max: { values [ \"min_value\" ] . units } , { v . units } ' ) if v . value <= values [ 'min_value' ] . value : raise ValueError ( f 'Minimum bound { values [ \"min_value\" ] . value } exceeds maximum bound { v . value } ' ) return v min_max_consistent ( v , values ) classmethod \u00a4 Validate that the maximum is bigger than the minimum. Source code in corl/dones/sensor_bounds_check_done.py @validator ( 'max_value' ) def min_max_consistent ( cls , v , values ): \"\"\"Validate that the maximum is bigger than the minimum.\"\"\" if 'min_value' not in values : # min_value failed, so let that error message be provided return v if v . units != values [ 'min_value' ] . units : raise ValueError ( f 'Inconsistent units for min and max: { values [ \"min_value\" ] . units } , { v . units } ' ) if v . value <= values [ 'min_value' ] . value : raise ValueError ( f 'Minimum bound { values [ \"min_value\" ] . value } exceeds maximum bound { v . value } ' ) return v","title":"Sensor bounds check done"},{"location":"reference/dones/sensor_bounds_check_done/#corl.dones.sensor_bounds_check_done.SensorBoundsCheckDone","text":"Checks to see if a specified state parameter is within bounds Source code in corl/dones/sensor_bounds_check_done.py class SensorBoundsCheckDone ( DoneFuncBase ): \"\"\" Checks to see if a specified state parameter is within bounds \"\"\" # True means that clients should pass ValueWithUnits rather than evaluated value REQUIRED_UNITS = { 'min_value' : True , 'max_value' : True , 'sensor_name' : NoneUnitType . NoneUnit } def __init__ ( self , ** kwargs ) -> None : self . config : SensorBoundsCheckDoneValidator super () . __init__ ( ** kwargs ) if self . config . name == type ( self ) . __name__ : self . config . name = self . config . sensor_name + '_Done' @property def get_validator ( self ) -> typing . Type [ SensorBoundsCheckDoneValidator ]: \"\"\"Returns the validator for this done condition\"\"\" return SensorBoundsCheckDoneValidator def __call__ ( self , observation : OrderedDict , action : OrderedDict , next_observation : OrderedDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> DoneDict : done = DoneDict () # Find Target Platform platform = get_platform_by_name ( next_state , self . platform , allow_invalid = True ) # platform does not exist if platform is None : done [ self . platform ] = False return done # Find target Sensor sensor = get_sensor_by_name ( platform , self . config . sensor_name ) if not isinstance ( sensor . measurement_properties , BoxProp ): raise TypeError ( f 'Can only do bounds checking on BoxProp, received { type ( sensor . measurement_properties ) . __name__ } ' ) # Get measured value measurement = sensor . get_measurement () if len ( measurement ) != 1 : raise ValueError ( \"Sensor measurement has more than one element\" ) measured_value = ValueWithUnits ( value = measurement [ 0 ], units = sensor . measurement_properties . unit [ 0 ]) converted_value = measured_value . as_units ( self . config . min_value . units ) # Determine if done done [ self . platform ] = converted_value < self . config . min_value . value or self . config . max_value . value < converted_value if done [ self . platform ]: next_state . episode_state [ self . platform ][ self . name ] = DoneStatusCodes . LOSE self . _set_all_done ( done ) return done","title":"SensorBoundsCheckDone"},{"location":"reference/dones/sensor_bounds_check_done/#corl.dones.sensor_bounds_check_done.SensorBoundsCheckDone.get_validator","text":"Returns the validator for this done condition","title":"get_validator"},{"location":"reference/dones/sensor_bounds_check_done/#corl.dones.sensor_bounds_check_done.SensorBoundsCheckDoneValidator","text":"Initialize an SensorBoundsCheckDone","title":"SensorBoundsCheckDoneValidator"},{"location":"reference/dones/sensor_bounds_check_done/#corl.dones.sensor_bounds_check_done.SensorBoundsCheckDoneValidator--parameters","text":"min_value : float The minimum allowed value of this sensor max_value : float The maximum allowed value of this sensor sensor_name : str The name of the sensor to check Source code in corl/dones/sensor_bounds_check_done.py class SensorBoundsCheckDoneValidator ( DoneFuncBaseValidator ): \"\"\"Initialize an SensorBoundsCheckDone Parameters ---------- min_value : float The minimum allowed value of this sensor max_value : float The maximum allowed value of this sensor sensor_name : str The name of the sensor to check \"\"\" min_value : ValueWithUnits max_value : ValueWithUnits sensor_name : str @validator ( 'max_value' ) def min_max_consistent ( cls , v , values ): \"\"\"Validate that the maximum is bigger than the minimum.\"\"\" if 'min_value' not in values : # min_value failed, so let that error message be provided return v if v . units != values [ 'min_value' ] . units : raise ValueError ( f 'Inconsistent units for min and max: { values [ \"min_value\" ] . units } , { v . units } ' ) if v . value <= values [ 'min_value' ] . value : raise ValueError ( f 'Minimum bound { values [ \"min_value\" ] . value } exceeds maximum bound { v . value } ' ) return v","title":"Parameters"},{"location":"reference/dones/sensor_bounds_check_done/#corl.dones.sensor_bounds_check_done.SensorBoundsCheckDoneValidator.min_max_consistent","text":"Validate that the maximum is bigger than the minimum. Source code in corl/dones/sensor_bounds_check_done.py @validator ( 'max_value' ) def min_max_consistent ( cls , v , values ): \"\"\"Validate that the maximum is bigger than the minimum.\"\"\" if 'min_value' not in values : # min_value failed, so let that error message be provided return v if v . units != values [ 'min_value' ] . units : raise ValueError ( f 'Inconsistent units for min and max: { values [ \"min_value\" ] . units } , { v . units } ' ) if v . value <= values [ 'min_value' ] . value : raise ValueError ( f 'Minimum bound { values [ \"min_value\" ] . value } exceeds maximum bound { v . value } ' ) return v","title":"min_max_consistent()"},{"location":"reference/dones/docking_1d/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Docking 1D"},{"location":"reference/dones/docking_1d/dones/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. This module defines functions that determine terminal conditions for the 1D Docking environment. DockingDoneFunction ( DoneFuncBase ) \u00a4 A done function that determines if deputy has successfully docked with the cheif or not. Source code in corl/dones/docking_1d/dones.py class DockingDoneFunction ( DoneFuncBase ): \"\"\" A done function that determines if deputy has successfully docked with the cheif or not. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : DockingDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ): \"\"\" Parameters ---------- cls : constructor function Returns ------- SuccessfulDockingDoneValidator config validator for the SuccessfulDockingDoneFunction \"\"\" return DockingDoneValidator def __call__ ( self , observation , action , next_observation , next_state , observation_space , observation_units ): \"\"\" Parameters ---------- observation : np.ndarray np.ndarray describing the current observation action : np.ndarray np.ndarray describing the current action next_observation : np.ndarray np.ndarray describing the incoming observation next_state : np.ndarray np.ndarray describing the incoming state Returns ------- done : DoneDict dictionary containing the condition condition for the current agent \"\"\" done = DoneDict () deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) velocity_sensor = get_sensor_by_name ( deputy , self . config . velocity_sensor_name ) position = position_sensor . get_measurement () velocity = velocity_sensor . get_measurement () chief_position = np . array ([ 0 ]) docking_region_radius = self . config . docking_region_radius distance = abs ( position - chief_position ) in_docking = distance <= docking_region_radius max_velocity_exceeded = self . config . velocity_threshold > velocity successful_dock = in_docking and not max_velocity_exceeded crash = ( in_docking and max_velocity_exceeded ) or position < 0 # pylint: disable=R1706 done [ self . agent ] = False if crash : done [ self . agent ] = True next_state . episode_state [ self . agent ][ self . name ] = DoneStatusCodes . LOSE elif successful_dock : done [ self . agent ] = True next_state . episode_state [ self . agent ][ self . name ] = DoneStatusCodes . WIN self . _set_all_done ( done ) return done get_validator property readonly \u00a4 Parameters \u00a4 cls : constructor function Returns \u00a4 SuccessfulDockingDoneValidator config validator for the SuccessfulDockingDoneFunction __call__ ( self , observation , action , next_observation , next_state , observation_space , observation_units ) special \u00a4 Parameters \u00a4 observation : np.ndarray np.ndarray describing the current observation action : np.ndarray np.ndarray describing the current action next_observation : np.ndarray np.ndarray describing the incoming observation next_state : np.ndarray np.ndarray describing the incoming state Returns \u00a4 done : DoneDict dictionary containing the condition condition for the current agent Source code in corl/dones/docking_1d/dones.py def __call__ ( self , observation , action , next_observation , next_state , observation_space , observation_units ): \"\"\" Parameters ---------- observation : np.ndarray np.ndarray describing the current observation action : np.ndarray np.ndarray describing the current action next_observation : np.ndarray np.ndarray describing the incoming observation next_state : np.ndarray np.ndarray describing the incoming state Returns ------- done : DoneDict dictionary containing the condition condition for the current agent \"\"\" done = DoneDict () deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) velocity_sensor = get_sensor_by_name ( deputy , self . config . velocity_sensor_name ) position = position_sensor . get_measurement () velocity = velocity_sensor . get_measurement () chief_position = np . array ([ 0 ]) docking_region_radius = self . config . docking_region_radius distance = abs ( position - chief_position ) in_docking = distance <= docking_region_radius max_velocity_exceeded = self . config . velocity_threshold > velocity successful_dock = in_docking and not max_velocity_exceeded crash = ( in_docking and max_velocity_exceeded ) or position < 0 # pylint: disable=R1706 done [ self . agent ] = False if crash : done [ self . agent ] = True next_state . episode_state [ self . agent ][ self . name ] = DoneStatusCodes . LOSE elif successful_dock : done [ self . agent ] = True next_state . episode_state [ self . agent ][ self . name ] = DoneStatusCodes . WIN self . _set_all_done ( done ) return done DockingDoneValidator ( DoneFuncBaseValidator ) pydantic-model \u00a4 This class validates that the config contains the docking region and crash velocity data needed for computations in the SuccessfulDockingDoneFunction. Source code in corl/dones/docking_1d/dones.py class DockingDoneValidator ( DoneFuncBaseValidator ): \"\"\" This class validates that the config contains the docking region and crash velocity data needed for computations in the SuccessfulDockingDoneFunction. \"\"\" docking_region_radius : float velocity_threshold : float position_sensor_name : str velocity_sensor_name : str","title":"Dones"},{"location":"reference/dones/docking_1d/dones/#corl.dones.docking_1d.dones.DockingDoneFunction","text":"A done function that determines if deputy has successfully docked with the cheif or not. Source code in corl/dones/docking_1d/dones.py class DockingDoneFunction ( DoneFuncBase ): \"\"\" A done function that determines if deputy has successfully docked with the cheif or not. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : DockingDoneValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ): \"\"\" Parameters ---------- cls : constructor function Returns ------- SuccessfulDockingDoneValidator config validator for the SuccessfulDockingDoneFunction \"\"\" return DockingDoneValidator def __call__ ( self , observation , action , next_observation , next_state , observation_space , observation_units ): \"\"\" Parameters ---------- observation : np.ndarray np.ndarray describing the current observation action : np.ndarray np.ndarray describing the current action next_observation : np.ndarray np.ndarray describing the incoming observation next_state : np.ndarray np.ndarray describing the incoming state Returns ------- done : DoneDict dictionary containing the condition condition for the current agent \"\"\" done = DoneDict () deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) velocity_sensor = get_sensor_by_name ( deputy , self . config . velocity_sensor_name ) position = position_sensor . get_measurement () velocity = velocity_sensor . get_measurement () chief_position = np . array ([ 0 ]) docking_region_radius = self . config . docking_region_radius distance = abs ( position - chief_position ) in_docking = distance <= docking_region_radius max_velocity_exceeded = self . config . velocity_threshold > velocity successful_dock = in_docking and not max_velocity_exceeded crash = ( in_docking and max_velocity_exceeded ) or position < 0 # pylint: disable=R1706 done [ self . agent ] = False if crash : done [ self . agent ] = True next_state . episode_state [ self . agent ][ self . name ] = DoneStatusCodes . LOSE elif successful_dock : done [ self . agent ] = True next_state . episode_state [ self . agent ][ self . name ] = DoneStatusCodes . WIN self . _set_all_done ( done ) return done","title":"DockingDoneFunction"},{"location":"reference/dones/docking_1d/dones/#corl.dones.docking_1d.dones.DockingDoneFunction.get_validator","text":"","title":"get_validator"},{"location":"reference/dones/docking_1d/dones/#corl.dones.docking_1d.dones.DockingDoneFunction.get_validator--parameters","text":"cls : constructor function","title":"Parameters"},{"location":"reference/dones/docking_1d/dones/#corl.dones.docking_1d.dones.DockingDoneFunction.get_validator--returns","text":"SuccessfulDockingDoneValidator config validator for the SuccessfulDockingDoneFunction","title":"Returns"},{"location":"reference/dones/docking_1d/dones/#corl.dones.docking_1d.dones.DockingDoneFunction.__call__","text":"","title":"__call__()"},{"location":"reference/dones/docking_1d/dones/#corl.dones.docking_1d.dones.DockingDoneFunction.__call__--parameters","text":"observation : np.ndarray np.ndarray describing the current observation action : np.ndarray np.ndarray describing the current action next_observation : np.ndarray np.ndarray describing the incoming observation next_state : np.ndarray np.ndarray describing the incoming state","title":"Parameters"},{"location":"reference/dones/docking_1d/dones/#corl.dones.docking_1d.dones.DockingDoneFunction.__call__--returns","text":"done : DoneDict dictionary containing the condition condition for the current agent Source code in corl/dones/docking_1d/dones.py def __call__ ( self , observation , action , next_observation , next_state , observation_space , observation_units ): \"\"\" Parameters ---------- observation : np.ndarray np.ndarray describing the current observation action : np.ndarray np.ndarray describing the current action next_observation : np.ndarray np.ndarray describing the incoming observation next_state : np.ndarray np.ndarray describing the incoming state Returns ------- done : DoneDict dictionary containing the condition condition for the current agent \"\"\" done = DoneDict () deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) velocity_sensor = get_sensor_by_name ( deputy , self . config . velocity_sensor_name ) position = position_sensor . get_measurement () velocity = velocity_sensor . get_measurement () chief_position = np . array ([ 0 ]) docking_region_radius = self . config . docking_region_radius distance = abs ( position - chief_position ) in_docking = distance <= docking_region_radius max_velocity_exceeded = self . config . velocity_threshold > velocity successful_dock = in_docking and not max_velocity_exceeded crash = ( in_docking and max_velocity_exceeded ) or position < 0 # pylint: disable=R1706 done [ self . agent ] = False if crash : done [ self . agent ] = True next_state . episode_state [ self . agent ][ self . name ] = DoneStatusCodes . LOSE elif successful_dock : done [ self . agent ] = True next_state . episode_state [ self . agent ][ self . name ] = DoneStatusCodes . WIN self . _set_all_done ( done ) return done","title":"Returns"},{"location":"reference/dones/docking_1d/dones/#corl.dones.docking_1d.dones.DockingDoneValidator","text":"This class validates that the config contains the docking region and crash velocity data needed for computations in the SuccessfulDockingDoneFunction. Source code in corl/dones/docking_1d/dones.py class DockingDoneValidator ( DoneFuncBaseValidator ): \"\"\" This class validates that the config contains the docking region and crash velocity data needed for computations in the SuccessfulDockingDoneFunction. \"\"\" docking_region_radius : float velocity_threshold : float position_sensor_name : str velocity_sensor_name : str","title":"DockingDoneValidator"},{"location":"reference/environment/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Environment"},{"location":"reference/environment/default_env_rllib_callbacks/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. EnvironmentDefaultCallbacks EnvironmentDefaultCallbacks ( DefaultCallbacks ) \u00a4 This is the default class for callbacks to be use in the Environment class. To make your own custom callbacks set the EnvironmentCallbacks in your derived class to be a class that subclasses this class (EnvironmentDefaultCallbacks) Make sure you call the super function for all derived functions or else there will be unexpected callback behavior Source code in corl/environment/default_env_rllib_callbacks.py class EnvironmentDefaultCallbacks ( DefaultCallbacks ): \"\"\" This is the default class for callbacks to be use in the Environment class. To make your own custom callbacks set the EnvironmentCallbacks in your derived class to be a class that subclasses this class (EnvironmentDefaultCallbacks) Make sure you call the super function for all derived functions or else there will be unexpected callback behavior \"\"\" DEFAULT_METRIC_OPS = { \"min\" : np . min , \"max\" : np . max , \"median\" : np . median , \"mean\" : np . mean , \"var\" : np . var , \"std\" : np . std , \"sum\" : np . sum , \"nonzero\" : np . count_nonzero , } def on_episode_start ( self , * , worker , base_env : BaseEnv , policies : typing . Dict [ PolicyID , Policy ], episode : Episode , ** kwargs , ) -> None : \"\"\"Callback run on the rollout worker before each episode starts. Args: worker: Reference to the current rollout worker. base_env: BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling `base_env.get_sub_environments()`. policies: Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. episode: Episode object which contains the episode's state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. kwargs: Forward compatibility placeholder. \"\"\" super () . on_episode_start ( worker = worker , base_env = base_env , policies = policies , episode = episode , ** kwargs ) episode . user_data [ \"rewards_accumulator\" ] = defaultdict ( float ) # default dict with default value of 0.0 def on_episode_step ( self , * , worker , base_env : BaseEnv , policies : typing . Optional [ typing . Dict [ PolicyID , Policy ]] = None , episode : Episode , ** kwargs , ) -> None : \"\"\"Runs on each episode step. Args: worker: Reference to the current rollout worker. base_env: BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling `base_env.get_sub_environments()`. policies: Mapping of policy id to policy objects. In single agent mode there will only be a single \"default_policy\". episode: Episode object which contains episode state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. kwargs: Forward compatibility placeholder. \"\"\" super () . on_episode_step ( worker = worker , base_env = base_env , policies = policies , episode = episode , ** kwargs ) env = base_env . get_sub_environments ()[ episode . env_id ] if env . reward_info : rewards_accumulator = episode . user_data [ \"rewards_accumulator\" ] for reward_name , reward_val in flatten ( env . reward_info , reducer = \"path\" ) . items (): key = f \"rewards_cumulative/ { reward_name } \" rewards_accumulator [ key ] += reward_val def on_episode_end ( # pylint: disable=too-many-branches self , * , worker , base_env : BaseEnv , # pylint: disable=too-many-branches policies : typing . Dict [ PolicyID , Policy ], episode , ** kwargs ) -> None : \"\"\" on_episode_end stores the custom metrics in RLLIB. Note this is on a per glue basis. 1. read the training information for the current episode 2. For each metric in each platform interface in each environment update metric container Parameters ---------- worker: RolloutWorker Reference to the current rollout worker. base_env: BaseEnv BaseEnv running the episode. The underlying env object can be gotten by calling base_env.get_sub_environments(). policies: dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. episode: MultiAgentEpisode Episode object which contains episode state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. \"\"\" # Issue a warning if the episode is short # This, should get outputing in the log and may help identify any setup issues if episode . length < SHORT_EPISODE_THRESHOLD : msg = f \"Episode { str ( episode . episode_id ) } length { episode . length } is less than warn threshold { str ( SHORT_EPISODE_THRESHOLD ) } \" if \"params\" in episode . user_data : msg += \" \\n params: \\n \" for key in episode . user_data [ \"params\" ] . keys (): msg += f \" { key } : { str ( episode . user_data [ 'params' ][ key ]) } \" msg += \" \\n \" else : msg += \" \\n Params not provied in episode user_data\" warnings . warn ( msg ) env = base_env . get_sub_environments ()[ episode . env_id ] if env . glue_info : # pylint: disable=too-many-nested-blocks for glue_name , metric_val in flatten ( env . glue_info , reducer = \"path\" ) . items (): episode . custom_metrics [ glue_name ] = metric_val if env . reward_info : for reward_name , reward_val in flatten ( env . reward_info , reducer = \"path\" ) . items (): key = f \"rewards/ { reward_name } \" episode . custom_metrics [ key ] = reward_val log_done_info ( env , episode ) log_done_status ( env , episode ) # Variables for key , value in flatten ( env . local_variable_store , reducer = \"path\" ) . items (): try : episode . custom_metrics [ f 'variable/env/ { key } ' ] = float ( value . value ) except ValueError : pass for agent_name , agent_data in env . agent_dict . items (): for key , value in flatten ( agent_data . local_variable_store , reducer = \"path\" ) . items (): try : episode . custom_metrics [ f 'variable/ { agent_name } / { key } ' ] = float ( value . value ) except ValueError : pass # Episode Parameter Providers metrics = env . config . epp . compute_metrics () for k , v in metrics . items (): episode . custom_metrics [ f 'adr/env/ { k } ' ] = v for agent_name , agent_data in env . agent_dict . items (): metrics = agent_data . config . epp . compute_metrics () for k , v in metrics . items (): episode . custom_metrics [ f 'adr/ { agent_name } / { k } ' ] = v # Cumulative Rewards for key , value in episode . user_data [ \"rewards_accumulator\" ] . items (): episode . custom_metrics [ key ] = value def on_postprocess_trajectory ( self , * , worker , episode , agent_id : AgentID , policy_id : PolicyID , policies : typing . Dict [ PolicyID , Policy ], postprocessed_batch : SampleBatch , original_batches : typing . Dict [ AgentID , typing . Tuple [ Policy , SampleBatch ]], ** kwargs ) -> None : \"\"\" Called immediately after a policy's postprocess_fn is called. You can use this callback to do additional postprocessing for a policy, including looking at the trajectory data of other agents in multi-agent settings. Parameters ---------- worker: RolloutWorker Reference to the current rollout worker. episode: MultiAgentEpisode Episode object. agent_id: str Id of the current agent. policy_id: str Id of the current policy for the agent. policies: dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. postprocessed_batch: SampleBatch The postprocessed sample batch for this agent. You can mutate this object to apply your own trajectory postprocessing. original_batches: dict Mapping of agents to their unpostprocessed trajectory data. You should not mutate this object. \"\"\" super () . on_postprocess_trajectory ( worker = worker , episode = episode , agent_id = agent_id , policy_id = policy_id , policies = policies , postprocessed_batch = postprocessed_batch , original_batches = original_batches ) episode . worker . foreach_env ( lambda env : env . post_process_trajectory ( agent_id , postprocessed_batch , episode , policies [ policy_id ]) ) # type: ignore def on_train_result ( self , * , trainer , result : dict , ** kwargs ) -> None : \"\"\" Called at the end of Trainable.train(). Parameters ---------- trainer: Trainer Current trainer instance. result: dict Dict of results returned from trainer.train() call. You can mutate this object to add additional metrics. \"\"\" rng , _ = seeding . np_random ( seed = trainer . iteration ) # Environment EPP assert result [ 'config' ][ 'env' ] == ACT3MultiAgentEnv . __name__ for epp in result [ 'config' ][ 'env_config' ][ 'epp_registry' ] . values (): epp . update ( result , rng ) on_episode_end ( self , * , worker , base_env , policies , episode , ** kwargs ) \u00a4 on_episode_end stores the custom metrics in RLLIB. Note this is on a per glue basis. read the training information for the current episode For each metric in each platform interface in each environment update metric container Parameters \u00a4 RolloutWorker Reference to the current rollout worker. BaseEnv BaseEnv running the episode. The underlying env object can be gotten by calling base_env.get_sub_environments(). dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. MultiAgentEpisode Episode object which contains episode state. You can use the episode.user_data dict to store temporary data, and episode.custom_metrics to store custom metrics for the episode. Source code in corl/environment/default_env_rllib_callbacks.py def on_episode_end ( # pylint: disable=too-many-branches self , * , worker , base_env : BaseEnv , # pylint: disable=too-many-branches policies : typing . Dict [ PolicyID , Policy ], episode , ** kwargs ) -> None : \"\"\" on_episode_end stores the custom metrics in RLLIB. Note this is on a per glue basis. 1. read the training information for the current episode 2. For each metric in each platform interface in each environment update metric container Parameters ---------- worker: RolloutWorker Reference to the current rollout worker. base_env: BaseEnv BaseEnv running the episode. The underlying env object can be gotten by calling base_env.get_sub_environments(). policies: dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. episode: MultiAgentEpisode Episode object which contains episode state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. \"\"\" # Issue a warning if the episode is short # This, should get outputing in the log and may help identify any setup issues if episode . length < SHORT_EPISODE_THRESHOLD : msg = f \"Episode { str ( episode . episode_id ) } length { episode . length } is less than warn threshold { str ( SHORT_EPISODE_THRESHOLD ) } \" if \"params\" in episode . user_data : msg += \" \\n params: \\n \" for key in episode . user_data [ \"params\" ] . keys (): msg += f \" { key } : { str ( episode . user_data [ 'params' ][ key ]) } \" msg += \" \\n \" else : msg += \" \\n Params not provied in episode user_data\" warnings . warn ( msg ) env = base_env . get_sub_environments ()[ episode . env_id ] if env . glue_info : # pylint: disable=too-many-nested-blocks for glue_name , metric_val in flatten ( env . glue_info , reducer = \"path\" ) . items (): episode . custom_metrics [ glue_name ] = metric_val if env . reward_info : for reward_name , reward_val in flatten ( env . reward_info , reducer = \"path\" ) . items (): key = f \"rewards/ { reward_name } \" episode . custom_metrics [ key ] = reward_val log_done_info ( env , episode ) log_done_status ( env , episode ) # Variables for key , value in flatten ( env . local_variable_store , reducer = \"path\" ) . items (): try : episode . custom_metrics [ f 'variable/env/ { key } ' ] = float ( value . value ) except ValueError : pass for agent_name , agent_data in env . agent_dict . items (): for key , value in flatten ( agent_data . local_variable_store , reducer = \"path\" ) . items (): try : episode . custom_metrics [ f 'variable/ { agent_name } / { key } ' ] = float ( value . value ) except ValueError : pass # Episode Parameter Providers metrics = env . config . epp . compute_metrics () for k , v in metrics . items (): episode . custom_metrics [ f 'adr/env/ { k } ' ] = v for agent_name , agent_data in env . agent_dict . items (): metrics = agent_data . config . epp . compute_metrics () for k , v in metrics . items (): episode . custom_metrics [ f 'adr/ { agent_name } / { k } ' ] = v # Cumulative Rewards for key , value in episode . user_data [ \"rewards_accumulator\" ] . items (): episode . custom_metrics [ key ] = value on_episode_start ( self , * , worker , base_env , policies , episode , ** kwargs ) \u00a4 Callback run on the rollout worker before each episode starts. Parameters: Name Type Description Default worker Reference to the current rollout worker. required base_env BaseEnv BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling base_env.get_sub_environments() . required policies Dict[str, ray.rllib.policy.policy.Policy] Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. required episode Episode Episode object which contains the episode's state. You can use the episode.user_data dict to store temporary data, and episode.custom_metrics to store custom metrics for the episode. required kwargs Forward compatibility placeholder. {} Source code in corl/environment/default_env_rllib_callbacks.py def on_episode_start ( self , * , worker , base_env : BaseEnv , policies : typing . Dict [ PolicyID , Policy ], episode : Episode , ** kwargs , ) -> None : \"\"\"Callback run on the rollout worker before each episode starts. Args: worker: Reference to the current rollout worker. base_env: BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling `base_env.get_sub_environments()`. policies: Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. episode: Episode object which contains the episode's state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. kwargs: Forward compatibility placeholder. \"\"\" super () . on_episode_start ( worker = worker , base_env = base_env , policies = policies , episode = episode , ** kwargs ) episode . user_data [ \"rewards_accumulator\" ] = defaultdict ( float ) # default dict with default value of 0.0 on_episode_step ( self , * , worker , base_env , policies = None , episode , ** kwargs ) \u00a4 Runs on each episode step. Parameters: Name Type Description Default worker Reference to the current rollout worker. required base_env BaseEnv BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling base_env.get_sub_environments() . required policies Optional[Dict[str, ray.rllib.policy.policy.Policy]] Mapping of policy id to policy objects. In single agent mode there will only be a single \"default_policy\". None episode Episode Episode object which contains episode state. You can use the episode.user_data dict to store temporary data, and episode.custom_metrics to store custom metrics for the episode. required kwargs Forward compatibility placeholder. {} Source code in corl/environment/default_env_rllib_callbacks.py def on_episode_step ( self , * , worker , base_env : BaseEnv , policies : typing . Optional [ typing . Dict [ PolicyID , Policy ]] = None , episode : Episode , ** kwargs , ) -> None : \"\"\"Runs on each episode step. Args: worker: Reference to the current rollout worker. base_env: BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling `base_env.get_sub_environments()`. policies: Mapping of policy id to policy objects. In single agent mode there will only be a single \"default_policy\". episode: Episode object which contains episode state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. kwargs: Forward compatibility placeholder. \"\"\" super () . on_episode_step ( worker = worker , base_env = base_env , policies = policies , episode = episode , ** kwargs ) env = base_env . get_sub_environments ()[ episode . env_id ] if env . reward_info : rewards_accumulator = episode . user_data [ \"rewards_accumulator\" ] for reward_name , reward_val in flatten ( env . reward_info , reducer = \"path\" ) . items (): key = f \"rewards_cumulative/ { reward_name } \" rewards_accumulator [ key ] += reward_val on_postprocess_trajectory ( self , * , worker , episode , agent_id , policy_id , policies , postprocessed_batch , original_batches , ** kwargs ) \u00a4 Called immediately after a policy's postprocess_fn is called. You can use this callback to do additional postprocessing for a policy, including looking at the trajectory data of other agents in multi-agent settings. Parameters \u00a4 RolloutWorker Reference to the current rollout worker. MultiAgentEpisode Episode object. str Id of the current agent. str Id of the current policy for the agent. dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. SampleBatch The postprocessed sample batch for this agent. You can mutate this object to apply your own trajectory postprocessing. dict Mapping of agents to their unpostprocessed trajectory data. You should not mutate this object. Source code in corl/environment/default_env_rllib_callbacks.py def on_postprocess_trajectory ( self , * , worker , episode , agent_id : AgentID , policy_id : PolicyID , policies : typing . Dict [ PolicyID , Policy ], postprocessed_batch : SampleBatch , original_batches : typing . Dict [ AgentID , typing . Tuple [ Policy , SampleBatch ]], ** kwargs ) -> None : \"\"\" Called immediately after a policy's postprocess_fn is called. You can use this callback to do additional postprocessing for a policy, including looking at the trajectory data of other agents in multi-agent settings. Parameters ---------- worker: RolloutWorker Reference to the current rollout worker. episode: MultiAgentEpisode Episode object. agent_id: str Id of the current agent. policy_id: str Id of the current policy for the agent. policies: dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. postprocessed_batch: SampleBatch The postprocessed sample batch for this agent. You can mutate this object to apply your own trajectory postprocessing. original_batches: dict Mapping of agents to their unpostprocessed trajectory data. You should not mutate this object. \"\"\" super () . on_postprocess_trajectory ( worker = worker , episode = episode , agent_id = agent_id , policy_id = policy_id , policies = policies , postprocessed_batch = postprocessed_batch , original_batches = original_batches ) episode . worker . foreach_env ( lambda env : env . post_process_trajectory ( agent_id , postprocessed_batch , episode , policies [ policy_id ]) ) # type: ignore on_train_result ( self , * , trainer , result , ** kwargs ) \u00a4 Called at the end of Trainable.train(). Parameters \u00a4 Trainer Current trainer instance. dict Dict of results returned from trainer.train() call. You can mutate this object to add additional metrics. Source code in corl/environment/default_env_rllib_callbacks.py def on_train_result ( self , * , trainer , result : dict , ** kwargs ) -> None : \"\"\" Called at the end of Trainable.train(). Parameters ---------- trainer: Trainer Current trainer instance. result: dict Dict of results returned from trainer.train() call. You can mutate this object to add additional metrics. \"\"\" rng , _ = seeding . np_random ( seed = trainer . iteration ) # Environment EPP assert result [ 'config' ][ 'env' ] == ACT3MultiAgentEnv . __name__ for epp in result [ 'config' ][ 'env_config' ][ 'epp_registry' ] . values (): epp . update ( result , rng ) log_done_info ( env , episode ) \u00a4 Log done info to done_results/{platform}/{done_name} Source code in corl/environment/default_env_rllib_callbacks.py def log_done_info ( env , episode ): \"\"\" Log done info to done_results/{platform}/{done_name} \"\"\" if env . done_info : platform_done_info : typing . Dict [ str , typing . Dict [ str , bool ]] = {} for agent_id , agent_data in env . done_info . items (): plat_name = env . agent_dict [ agent_id ] . platform_name platform_done_info . setdefault ( plat_name , {}) for done_name , done_data in agent_data . items (): if done_name == '__all__' : continue platform_done_info [ plat_name ] . setdefault ( done_name , False ) if done_data [ plat_name ] is not None : platform_done_info [ plat_name ][ done_name ] |= done_data [ plat_name ] for plat_name , done_data in platform_done_info . items (): for done_name , done_value in done_data . items (): episode . custom_metrics [ f 'done_results/ { plat_name } / { done_name } ' ] = int ( done_value ) episode . custom_metrics [ f 'done_results/ { plat_name } /NoDone' ] = int ( not any ( platform_done_info [ plat_name ] . values ())) log_done_status ( env , episode ) \u00a4 Log done status codes to done_status/{platform}/{status} Source code in corl/environment/default_env_rllib_callbacks.py def log_done_status ( env , episode ): \"\"\" Log done status codes to done_status/{platform}/{status} \"\"\" if env . state . episode_state : for platform , data in env . state . episode_state . items (): if not data : continue episode_codes = set ( data . values ()) for status in DoneStatusCodes : metric_key = f \"done_status/ { platform } / { status } \" if status in episode_codes : episode . custom_metrics [ metric_key ] = 1 else : episode . custom_metrics [ metric_key ] = 0","title":"Default env rllib callbacks"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks","text":"This is the default class for callbacks to be use in the Environment class. To make your own custom callbacks set the EnvironmentCallbacks in your derived class to be a class that subclasses this class (EnvironmentDefaultCallbacks) Make sure you call the super function for all derived functions or else there will be unexpected callback behavior Source code in corl/environment/default_env_rllib_callbacks.py class EnvironmentDefaultCallbacks ( DefaultCallbacks ): \"\"\" This is the default class for callbacks to be use in the Environment class. To make your own custom callbacks set the EnvironmentCallbacks in your derived class to be a class that subclasses this class (EnvironmentDefaultCallbacks) Make sure you call the super function for all derived functions or else there will be unexpected callback behavior \"\"\" DEFAULT_METRIC_OPS = { \"min\" : np . min , \"max\" : np . max , \"median\" : np . median , \"mean\" : np . mean , \"var\" : np . var , \"std\" : np . std , \"sum\" : np . sum , \"nonzero\" : np . count_nonzero , } def on_episode_start ( self , * , worker , base_env : BaseEnv , policies : typing . Dict [ PolicyID , Policy ], episode : Episode , ** kwargs , ) -> None : \"\"\"Callback run on the rollout worker before each episode starts. Args: worker: Reference to the current rollout worker. base_env: BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling `base_env.get_sub_environments()`. policies: Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. episode: Episode object which contains the episode's state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. kwargs: Forward compatibility placeholder. \"\"\" super () . on_episode_start ( worker = worker , base_env = base_env , policies = policies , episode = episode , ** kwargs ) episode . user_data [ \"rewards_accumulator\" ] = defaultdict ( float ) # default dict with default value of 0.0 def on_episode_step ( self , * , worker , base_env : BaseEnv , policies : typing . Optional [ typing . Dict [ PolicyID , Policy ]] = None , episode : Episode , ** kwargs , ) -> None : \"\"\"Runs on each episode step. Args: worker: Reference to the current rollout worker. base_env: BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling `base_env.get_sub_environments()`. policies: Mapping of policy id to policy objects. In single agent mode there will only be a single \"default_policy\". episode: Episode object which contains episode state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. kwargs: Forward compatibility placeholder. \"\"\" super () . on_episode_step ( worker = worker , base_env = base_env , policies = policies , episode = episode , ** kwargs ) env = base_env . get_sub_environments ()[ episode . env_id ] if env . reward_info : rewards_accumulator = episode . user_data [ \"rewards_accumulator\" ] for reward_name , reward_val in flatten ( env . reward_info , reducer = \"path\" ) . items (): key = f \"rewards_cumulative/ { reward_name } \" rewards_accumulator [ key ] += reward_val def on_episode_end ( # pylint: disable=too-many-branches self , * , worker , base_env : BaseEnv , # pylint: disable=too-many-branches policies : typing . Dict [ PolicyID , Policy ], episode , ** kwargs ) -> None : \"\"\" on_episode_end stores the custom metrics in RLLIB. Note this is on a per glue basis. 1. read the training information for the current episode 2. For each metric in each platform interface in each environment update metric container Parameters ---------- worker: RolloutWorker Reference to the current rollout worker. base_env: BaseEnv BaseEnv running the episode. The underlying env object can be gotten by calling base_env.get_sub_environments(). policies: dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. episode: MultiAgentEpisode Episode object which contains episode state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. \"\"\" # Issue a warning if the episode is short # This, should get outputing in the log and may help identify any setup issues if episode . length < SHORT_EPISODE_THRESHOLD : msg = f \"Episode { str ( episode . episode_id ) } length { episode . length } is less than warn threshold { str ( SHORT_EPISODE_THRESHOLD ) } \" if \"params\" in episode . user_data : msg += \" \\n params: \\n \" for key in episode . user_data [ \"params\" ] . keys (): msg += f \" { key } : { str ( episode . user_data [ 'params' ][ key ]) } \" msg += \" \\n \" else : msg += \" \\n Params not provied in episode user_data\" warnings . warn ( msg ) env = base_env . get_sub_environments ()[ episode . env_id ] if env . glue_info : # pylint: disable=too-many-nested-blocks for glue_name , metric_val in flatten ( env . glue_info , reducer = \"path\" ) . items (): episode . custom_metrics [ glue_name ] = metric_val if env . reward_info : for reward_name , reward_val in flatten ( env . reward_info , reducer = \"path\" ) . items (): key = f \"rewards/ { reward_name } \" episode . custom_metrics [ key ] = reward_val log_done_info ( env , episode ) log_done_status ( env , episode ) # Variables for key , value in flatten ( env . local_variable_store , reducer = \"path\" ) . items (): try : episode . custom_metrics [ f 'variable/env/ { key } ' ] = float ( value . value ) except ValueError : pass for agent_name , agent_data in env . agent_dict . items (): for key , value in flatten ( agent_data . local_variable_store , reducer = \"path\" ) . items (): try : episode . custom_metrics [ f 'variable/ { agent_name } / { key } ' ] = float ( value . value ) except ValueError : pass # Episode Parameter Providers metrics = env . config . epp . compute_metrics () for k , v in metrics . items (): episode . custom_metrics [ f 'adr/env/ { k } ' ] = v for agent_name , agent_data in env . agent_dict . items (): metrics = agent_data . config . epp . compute_metrics () for k , v in metrics . items (): episode . custom_metrics [ f 'adr/ { agent_name } / { k } ' ] = v # Cumulative Rewards for key , value in episode . user_data [ \"rewards_accumulator\" ] . items (): episode . custom_metrics [ key ] = value def on_postprocess_trajectory ( self , * , worker , episode , agent_id : AgentID , policy_id : PolicyID , policies : typing . Dict [ PolicyID , Policy ], postprocessed_batch : SampleBatch , original_batches : typing . Dict [ AgentID , typing . Tuple [ Policy , SampleBatch ]], ** kwargs ) -> None : \"\"\" Called immediately after a policy's postprocess_fn is called. You can use this callback to do additional postprocessing for a policy, including looking at the trajectory data of other agents in multi-agent settings. Parameters ---------- worker: RolloutWorker Reference to the current rollout worker. episode: MultiAgentEpisode Episode object. agent_id: str Id of the current agent. policy_id: str Id of the current policy for the agent. policies: dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. postprocessed_batch: SampleBatch The postprocessed sample batch for this agent. You can mutate this object to apply your own trajectory postprocessing. original_batches: dict Mapping of agents to their unpostprocessed trajectory data. You should not mutate this object. \"\"\" super () . on_postprocess_trajectory ( worker = worker , episode = episode , agent_id = agent_id , policy_id = policy_id , policies = policies , postprocessed_batch = postprocessed_batch , original_batches = original_batches ) episode . worker . foreach_env ( lambda env : env . post_process_trajectory ( agent_id , postprocessed_batch , episode , policies [ policy_id ]) ) # type: ignore def on_train_result ( self , * , trainer , result : dict , ** kwargs ) -> None : \"\"\" Called at the end of Trainable.train(). Parameters ---------- trainer: Trainer Current trainer instance. result: dict Dict of results returned from trainer.train() call. You can mutate this object to add additional metrics. \"\"\" rng , _ = seeding . np_random ( seed = trainer . iteration ) # Environment EPP assert result [ 'config' ][ 'env' ] == ACT3MultiAgentEnv . __name__ for epp in result [ 'config' ][ 'env_config' ][ 'epp_registry' ] . values (): epp . update ( result , rng )","title":"EnvironmentDefaultCallbacks"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks.on_episode_end","text":"on_episode_end stores the custom metrics in RLLIB. Note this is on a per glue basis. read the training information for the current episode For each metric in each platform interface in each environment update metric container","title":"on_episode_end()"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks.on_episode_end--parameters","text":"RolloutWorker Reference to the current rollout worker. BaseEnv BaseEnv running the episode. The underlying env object can be gotten by calling base_env.get_sub_environments(). dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. MultiAgentEpisode Episode object which contains episode state. You can use the episode.user_data dict to store temporary data, and episode.custom_metrics to store custom metrics for the episode. Source code in corl/environment/default_env_rllib_callbacks.py def on_episode_end ( # pylint: disable=too-many-branches self , * , worker , base_env : BaseEnv , # pylint: disable=too-many-branches policies : typing . Dict [ PolicyID , Policy ], episode , ** kwargs ) -> None : \"\"\" on_episode_end stores the custom metrics in RLLIB. Note this is on a per glue basis. 1. read the training information for the current episode 2. For each metric in each platform interface in each environment update metric container Parameters ---------- worker: RolloutWorker Reference to the current rollout worker. base_env: BaseEnv BaseEnv running the episode. The underlying env object can be gotten by calling base_env.get_sub_environments(). policies: dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. episode: MultiAgentEpisode Episode object which contains episode state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. \"\"\" # Issue a warning if the episode is short # This, should get outputing in the log and may help identify any setup issues if episode . length < SHORT_EPISODE_THRESHOLD : msg = f \"Episode { str ( episode . episode_id ) } length { episode . length } is less than warn threshold { str ( SHORT_EPISODE_THRESHOLD ) } \" if \"params\" in episode . user_data : msg += \" \\n params: \\n \" for key in episode . user_data [ \"params\" ] . keys (): msg += f \" { key } : { str ( episode . user_data [ 'params' ][ key ]) } \" msg += \" \\n \" else : msg += \" \\n Params not provied in episode user_data\" warnings . warn ( msg ) env = base_env . get_sub_environments ()[ episode . env_id ] if env . glue_info : # pylint: disable=too-many-nested-blocks for glue_name , metric_val in flatten ( env . glue_info , reducer = \"path\" ) . items (): episode . custom_metrics [ glue_name ] = metric_val if env . reward_info : for reward_name , reward_val in flatten ( env . reward_info , reducer = \"path\" ) . items (): key = f \"rewards/ { reward_name } \" episode . custom_metrics [ key ] = reward_val log_done_info ( env , episode ) log_done_status ( env , episode ) # Variables for key , value in flatten ( env . local_variable_store , reducer = \"path\" ) . items (): try : episode . custom_metrics [ f 'variable/env/ { key } ' ] = float ( value . value ) except ValueError : pass for agent_name , agent_data in env . agent_dict . items (): for key , value in flatten ( agent_data . local_variable_store , reducer = \"path\" ) . items (): try : episode . custom_metrics [ f 'variable/ { agent_name } / { key } ' ] = float ( value . value ) except ValueError : pass # Episode Parameter Providers metrics = env . config . epp . compute_metrics () for k , v in metrics . items (): episode . custom_metrics [ f 'adr/env/ { k } ' ] = v for agent_name , agent_data in env . agent_dict . items (): metrics = agent_data . config . epp . compute_metrics () for k , v in metrics . items (): episode . custom_metrics [ f 'adr/ { agent_name } / { k } ' ] = v # Cumulative Rewards for key , value in episode . user_data [ \"rewards_accumulator\" ] . items (): episode . custom_metrics [ key ] = value","title":"Parameters"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks.on_episode_start","text":"Callback run on the rollout worker before each episode starts. Parameters: Name Type Description Default worker Reference to the current rollout worker. required base_env BaseEnv BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling base_env.get_sub_environments() . required policies Dict[str, ray.rllib.policy.policy.Policy] Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. required episode Episode Episode object which contains the episode's state. You can use the episode.user_data dict to store temporary data, and episode.custom_metrics to store custom metrics for the episode. required kwargs Forward compatibility placeholder. {} Source code in corl/environment/default_env_rllib_callbacks.py def on_episode_start ( self , * , worker , base_env : BaseEnv , policies : typing . Dict [ PolicyID , Policy ], episode : Episode , ** kwargs , ) -> None : \"\"\"Callback run on the rollout worker before each episode starts. Args: worker: Reference to the current rollout worker. base_env: BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling `base_env.get_sub_environments()`. policies: Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. episode: Episode object which contains the episode's state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. kwargs: Forward compatibility placeholder. \"\"\" super () . on_episode_start ( worker = worker , base_env = base_env , policies = policies , episode = episode , ** kwargs ) episode . user_data [ \"rewards_accumulator\" ] = defaultdict ( float ) # default dict with default value of 0.0","title":"on_episode_start()"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks.on_episode_step","text":"Runs on each episode step. Parameters: Name Type Description Default worker Reference to the current rollout worker. required base_env BaseEnv BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling base_env.get_sub_environments() . required policies Optional[Dict[str, ray.rllib.policy.policy.Policy]] Mapping of policy id to policy objects. In single agent mode there will only be a single \"default_policy\". None episode Episode Episode object which contains episode state. You can use the episode.user_data dict to store temporary data, and episode.custom_metrics to store custom metrics for the episode. required kwargs Forward compatibility placeholder. {} Source code in corl/environment/default_env_rllib_callbacks.py def on_episode_step ( self , * , worker , base_env : BaseEnv , policies : typing . Optional [ typing . Dict [ PolicyID , Policy ]] = None , episode : Episode , ** kwargs , ) -> None : \"\"\"Runs on each episode step. Args: worker: Reference to the current rollout worker. base_env: BaseEnv running the episode. The underlying sub environment objects can be retrieved by calling `base_env.get_sub_environments()`. policies: Mapping of policy id to policy objects. In single agent mode there will only be a single \"default_policy\". episode: Episode object which contains episode state. You can use the `episode.user_data` dict to store temporary data, and `episode.custom_metrics` to store custom metrics for the episode. kwargs: Forward compatibility placeholder. \"\"\" super () . on_episode_step ( worker = worker , base_env = base_env , policies = policies , episode = episode , ** kwargs ) env = base_env . get_sub_environments ()[ episode . env_id ] if env . reward_info : rewards_accumulator = episode . user_data [ \"rewards_accumulator\" ] for reward_name , reward_val in flatten ( env . reward_info , reducer = \"path\" ) . items (): key = f \"rewards_cumulative/ { reward_name } \" rewards_accumulator [ key ] += reward_val","title":"on_episode_step()"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks.on_postprocess_trajectory","text":"Called immediately after a policy's postprocess_fn is called. You can use this callback to do additional postprocessing for a policy, including looking at the trajectory data of other agents in multi-agent settings.","title":"on_postprocess_trajectory()"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks.on_postprocess_trajectory--parameters","text":"RolloutWorker Reference to the current rollout worker. MultiAgentEpisode Episode object. str Id of the current agent. str Id of the current policy for the agent. dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. SampleBatch The postprocessed sample batch for this agent. You can mutate this object to apply your own trajectory postprocessing. dict Mapping of agents to their unpostprocessed trajectory data. You should not mutate this object. Source code in corl/environment/default_env_rllib_callbacks.py def on_postprocess_trajectory ( self , * , worker , episode , agent_id : AgentID , policy_id : PolicyID , policies : typing . Dict [ PolicyID , Policy ], postprocessed_batch : SampleBatch , original_batches : typing . Dict [ AgentID , typing . Tuple [ Policy , SampleBatch ]], ** kwargs ) -> None : \"\"\" Called immediately after a policy's postprocess_fn is called. You can use this callback to do additional postprocessing for a policy, including looking at the trajectory data of other agents in multi-agent settings. Parameters ---------- worker: RolloutWorker Reference to the current rollout worker. episode: MultiAgentEpisode Episode object. agent_id: str Id of the current agent. policy_id: str Id of the current policy for the agent. policies: dict Mapping of policy id to policy objects. In single agent mode there will only be a single \"default\" policy. postprocessed_batch: SampleBatch The postprocessed sample batch for this agent. You can mutate this object to apply your own trajectory postprocessing. original_batches: dict Mapping of agents to their unpostprocessed trajectory data. You should not mutate this object. \"\"\" super () . on_postprocess_trajectory ( worker = worker , episode = episode , agent_id = agent_id , policy_id = policy_id , policies = policies , postprocessed_batch = postprocessed_batch , original_batches = original_batches ) episode . worker . foreach_env ( lambda env : env . post_process_trajectory ( agent_id , postprocessed_batch , episode , policies [ policy_id ]) ) # type: ignore","title":"Parameters"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks.on_train_result","text":"Called at the end of Trainable.train().","title":"on_train_result()"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.EnvironmentDefaultCallbacks.on_train_result--parameters","text":"Trainer Current trainer instance. dict Dict of results returned from trainer.train() call. You can mutate this object to add additional metrics. Source code in corl/environment/default_env_rllib_callbacks.py def on_train_result ( self , * , trainer , result : dict , ** kwargs ) -> None : \"\"\" Called at the end of Trainable.train(). Parameters ---------- trainer: Trainer Current trainer instance. result: dict Dict of results returned from trainer.train() call. You can mutate this object to add additional metrics. \"\"\" rng , _ = seeding . np_random ( seed = trainer . iteration ) # Environment EPP assert result [ 'config' ][ 'env' ] == ACT3MultiAgentEnv . __name__ for epp in result [ 'config' ][ 'env_config' ][ 'epp_registry' ] . values (): epp . update ( result , rng )","title":"Parameters"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.log_done_info","text":"Log done info to done_results/{platform}/{done_name} Source code in corl/environment/default_env_rllib_callbacks.py def log_done_info ( env , episode ): \"\"\" Log done info to done_results/{platform}/{done_name} \"\"\" if env . done_info : platform_done_info : typing . Dict [ str , typing . Dict [ str , bool ]] = {} for agent_id , agent_data in env . done_info . items (): plat_name = env . agent_dict [ agent_id ] . platform_name platform_done_info . setdefault ( plat_name , {}) for done_name , done_data in agent_data . items (): if done_name == '__all__' : continue platform_done_info [ plat_name ] . setdefault ( done_name , False ) if done_data [ plat_name ] is not None : platform_done_info [ plat_name ][ done_name ] |= done_data [ plat_name ] for plat_name , done_data in platform_done_info . items (): for done_name , done_value in done_data . items (): episode . custom_metrics [ f 'done_results/ { plat_name } / { done_name } ' ] = int ( done_value ) episode . custom_metrics [ f 'done_results/ { plat_name } /NoDone' ] = int ( not any ( platform_done_info [ plat_name ] . values ()))","title":"log_done_info()"},{"location":"reference/environment/default_env_rllib_callbacks/#corl.environment.default_env_rllib_callbacks.log_done_status","text":"Log done status codes to done_status/{platform}/{status} Source code in corl/environment/default_env_rllib_callbacks.py def log_done_status ( env , episode ): \"\"\" Log done status codes to done_status/{platform}/{status} \"\"\" if env . state . episode_state : for platform , data in env . state . episode_state . items (): if not data : continue episode_codes = set ( data . values ()) for status in DoneStatusCodes : metric_key = f \"done_status/ { platform } / { status } \" if status in episode_codes : episode . custom_metrics [ metric_key ] = 1 else : episode . custom_metrics [ metric_key ] = 0","title":"log_done_status()"},{"location":"reference/environment/gym_env/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Gym env"},{"location":"reference/environment/multi_agent_env/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. ACT3MultiAgentEnv ( MultiAgentEnv ) \u00a4 ACT3MultiAgentEnv create a RLLIB MultiAgentEnv environment. The following class is intended to wrap the interactions with RLLIB and the backend simulator environment. All items here are intended to be common parts for running the RLLIB environment with ${simulator} being the unique interaction parts. Includes wrapping the creation of the simulator specific to run Includes interactions with the dones, rewards, and glues etc... Source code in corl/environment/multi_agent_env.py class ACT3MultiAgentEnv ( MultiAgentEnv ): \"\"\" ACT3MultiAgentEnv create a RLLIB MultiAgentEnv environment. The following class is intended to wrap the interactions with RLLIB and the backend simulator environment. All items here are intended to be common parts for running the RLLIB environment with ${simulator} being the unique interaction parts. 1. Includes wrapping the creation of the simulator specific to run 2. Includes interactions with the dones, rewards, and glues 3. etc... \"\"\" episode_parameter_provider_name : str = 'environment' def __init__ ( self , config : EnvContext ) -> None : # pylint: disable=too-many-statements, super-init-not-called \"\"\" __init__ initializes the rllib multi agent environment Parameters ---------- config : ray.rllib.env.env_context.EnvContext Passed in configuration for setting items up. Must have a 'simulator' key whose value is a BaseIntegrator type \"\"\" try : config_vars = vars ( config ) except TypeError : config_vars = {} self . config : ACT3MultiAgentEnvValidator = self . get_validator ( ** config , ** config_vars ) # Random numbers self . seed ( self . config . seed ) # setup default instance variables self . _actions : list = [] self . _obs_buffer = ObsBuffer () self . _reward : RewardDict = RewardDict () self . _done : DoneDict = DoneDict () self . _info : OrderedDict = OrderedDict () self . _episode_length : int = 0 self . _episode : int = 0 self . _episode_id : typing . Union [ int , None ] # agent glue dict is a mapping from agent id to a dict with keys for the glue names # and values of the actual glue object self . _agent_glue_dict : OrderedDict = OrderedDict () self . _agent_glue_obs_export_behavior : OrderedDict = OrderedDict () # Create the logger self . _logger = logging . getLogger ( ACT3MultiAgentEnv . __name__ ) # Extra simulation init args # assign the new output_path with the worker index back to the config for the sim/integration output_path extra_sim_init_args : typing . Dict [ str , typing . Any ] = { \"output_path\" : str ( self . config . output_path ), \"worker_index\" : self . config . worker_index , \"vector_index\" : self . config . vector_index if self . config . vector_index else 0 , } self . agent_dict , extra_sim_init_args [ \"agent_configs\" ] = env_creation . create_agent_sim_configs ( self . config . agents , self . config . agent_platforms , self . config . simulator . type , self . config . platforms , self . config . epp_registry , multiple_workers = ( self . config . num_workers > 0 ) ) def compute_lcm ( values : typing . List [ fractions . Fraction ]) -> float : assert len ( values ) > 0 lcm = values [ 0 ] . denominator for v in values : lcm = lcm // math . gcd ( lcm , v . denominator ) * v . denominator return 1.0 / lcm max_rate = self . config . max_agent_rate self . _agent_periods = { agent_id : fractions . Fraction ( 1.0 / agent . frame_rate ) . limit_denominator ( max_rate ) for agent_id , agent in self . agent_dict . items () } self . _agent_process_time : typing . Dict [ str , float ] = defaultdict ( lambda : sys . float_info . min ) self . sim_period = compute_lcm ( list ( self . _agent_periods . values ())) extra_sim_init_args [ 'frame_rate' ] = 1.0 / self . sim_period for agent_name , platform in self . config . other_platforms . items (): extra_sim_init_args [ \"agent_configs\" ][ agent_name ] = { \"platform_config\" : platform , \"parts_list\" : [], } # Debug logging self . _logger . debug ( f \"output_path : { self . config . output_path } \" ) # Sample parameter provider default_parameters = self . config . epp . config . parameters self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( self . rng ) for k , v in default_parameters . items ()}) for agent in self . agent_dict . values (): agent . fill_parameters ( rng = self . rng , default_parameters = True ) # Create the simulator for this gym environment # ---- oddity from other simulator bases HLP if not hasattr ( self , \"_simulator\" ): class SimulatorWrapper ( self . config . simulator . type ): # type: ignore \"\"\"Wrapper that injects platforms/time into state dict\"\"\" def _clear_data ( self ) -> None : if 'sim_time' in self . _state : del self . _state [ 'sim_time' ] if 'sim_platforms' in self . _state : del self . _state [ 'sim_platforms' ] def _inject_data ( self , state : StateDict ) -> StateDict : \"\"\"Ensures that time/platforms exists in state\"\"\" if 'sim_time' not in state : state [ 'sim_time' ] = self . sim_time if 'sim_platforms' not in state : state [ 'sim_platforms' ] = self . platforms return state def step ( self ) -> StateDict : \"\"\"Steps the simulation - injects data into StateDict\"\"\" self . _clear_data () return self . _inject_data ( super () . step ()) def reset ( self , * args , ** kwargs ) -> StateDict : \"\"\"Resets the simulation - injects data into StateDict\"\"\" self . _clear_data () return self . _inject_data ( super () . reset ( * args , ** kwargs )) simulator_factory = copy . deepcopy ( self . config . simulator ) simulator_factory . type = SimulatorWrapper # type: ignore self . _simulator : BaseSimulator = simulator_factory . build ( ** extra_sim_init_args ) self . _state , self . _sim_reset_args = self . _reset_simulator ( extra_sim_init_args [ \"agent_configs\" ]) # Make the glue objects from the glue mapping now that we have a simulator created self . _make_glues () # create dictionary to hold done history self . __setup_state_history () # Create the observation and action space now that we have the glue self . _observation_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . observation_space ()) self . _action_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . action_space ()) gym_space_sort ( self . _action_space ) self . _normalized_observation_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . normalized_observation_space () if glue_obj . config . training_export_behavior == TrainingExportBehavior . INCLUDE else None ) self . _normalized_action_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . normalized_action_space () ) gym_space_sort ( self . _normalized_action_space ) self . _observation_units = self . __create_space ( space_getter = lambda glue_obj : glue_obj . observation_units () if hasattr ( glue_obj , \"observation_units\" ) else None ) self . _shared_done : DoneDict = DoneDict () self . _done_info : OrderedDict = OrderedDict () self . _reward_info : OrderedDict = OrderedDict () self . _episode_init_params : dict self . done_string = \"\" self . _agent_ids = set ( self . _action_space . spaces . keys ()) self . _skip_action = False @property def get_validator ( self ) -> typing . Type [ ACT3MultiAgentEnvValidator ]: \"\"\"Get the validator for this class.\"\"\" return ACT3MultiAgentEnvValidator def reset ( self ): # Sample parameter provider current_parameters , self . _episode_id = self . config . epp . get_params ( self . rng ) self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( self . rng ) for k , v in current_parameters . items ()}) for agent in self . agent_dict . values (): agent . fill_parameters ( self . rng ) # 3. Reset the Done and Reward dictionaries for the next iteration self . _make_rewards () self . _make_dones () self . _shared_done : DoneDict = self . _make_shared_dones () self . _reward : RewardDict = RewardDict () self . _done : DoneDict = DoneDict () self . _done_info . clear () self . set_default_done_reward () # 4. Reset the simulation/integration self . _state , self . _sim_reset_args = self . _reset_simulator () self . _episode_length = 0 self . _actions . clear () self . _episode += 1 self . _agent_process_time . clear () ##################################################################### # Make glue sections - Given the state of the simulation we need to # update the platform interfaces. ##################################################################### self . _make_glues () ##################################################################### # get observations # For each configured agent read the observations/measurements ##################################################################### agent_list = list ( self . agent_dict . keys ()) self . _obs_buffer . next_observation = self . __get_observations_from_glues ( agent_list ) self . _obs_buffer . update_obs_pointer () # The following loop guarantees that durring training that the glue # states start with valid values for rates. The number of recommended # steps for sim is at least equal to the depth of the rate observation # tree (ex: speed - 2, acceleration - 3, jerk - 4) - recommend defaulting # to 4 as we do not go higher thank jerk # 1 step is always added for the inital obs in reset warmup = self . config . sim_warmup_steps for _ in range ( warmup ): self . _state = self . _simulator . step () self . _obs_buffer . next_observation = self . __get_observations_from_glues ( agent_list ) self . _obs_buffer . update_obs_pointer () self . __setup_state_history () for platform in self . _state . sim_platforms : self . _state . step_state [ platform . name ] = None self . _state . episode_history [ platform . name ] . clear () self . _state . episode_state [ platform . name ] = OrderedDict () # Sanity Checks and Scale # The current deep sanity check will not raise error if values are from sample are different from space during reset if self . config . deep_sanity_check : try : self . __sanity_check ( self . _observation_space , self . _obs_buffer . observation ) except ValueError as err : self . _save_state_pickle ( err ) else : if not self . _observation_space . contains ( self . _obs_buffer . observation ): raise ValueError ( 'obs not contained in obs space' ) self . _create_actions ( self . agent_dict , self . _obs_buffer . observation ) ##################################################################### # return results to RLLIB - Note that RLLIB does not do a recursive # isinstance call and as such need to make sure items are # OrderedDicts ##################################################################### trainable_observations , _ = self . create_training_observations ( agent_list , self . _obs_buffer ) return trainable_observations def _reset_simulator ( self , agent_configs = None ) -> typing . Tuple [ StateDict , typing . Dict [ str , typing . Any ]]: sim_reset_args = copy . deepcopy ( self . config . simulator_reset_parameters ) v_store = self . local_variable_store deepmerge . always_merger . merge ( sim_reset_args , v_store . get ( 'simulator_reset' , {})) self . _process_references ( sim_reset_args , v_store ) for agent_data in self . agent_dict . values (): deepmerge . always_merger . merge ( sim_reset_args , { 'platforms' : { agent_data . platform_name : agent_data . get_simulator_reset_parameters () }} ) if agent_configs is not None : sim_reset_args [ \"agent_configs_reset\" ] = agent_configs return self . _simulator . reset ( sim_reset_args ), sim_reset_args def _process_references ( self , sim_reset_args : dict , v_store : dict ) -> None : \"\"\"Process the reference store look ups for the position data Parameters ---------- sim_reset_args : dict The simulator reset parameters v_store : dict The variable store \"\"\" plat_str = \"platforms\" pos_str = \"position\" ref_str = \"reference\" if plat_str in sim_reset_args : for plat_k , plat_v in sim_reset_args [ plat_str ] . items (): if pos_str in plat_v : for position_k , position_v in plat_v [ pos_str ] . items (): if isinstance ( position_v , dict ) and ref_str in position_v : sim_reset_args [ plat_str ][ plat_k ][ pos_str ][ position_k ] = v_store [ \"reference_store\" ] . get ( position_v [ ref_str ], self . config . reference_store [ position_v [ ref_str ]] ) def _get_operable_agents ( self ): \"\"\"Determines which agents are operable in the sim, this becomes stale after the simulation is stepped\"\"\" operable_platform_names = [ item . name for item in self . state . sim_platforms if item . operable and not self . state . episode_state . get ( item . name , {}) ] operable_agents = {} for agent_name , agent in self . agent_dict . items (): if agent . platform_name in operable_platform_names : operable_agents [ agent_name ] = agent else : agent . set_removed ( True ) return operable_agents def step ( self , action_dict : dict ) -> typing . Tuple [ OrderedDict , OrderedDict , OrderedDict , OrderedDict ]: # pylint: disable=R0912, R0914, R0915 \"\"\"Returns observations from ready agents. The returns are dicts mapping from agent_id strings to values. The number of agents in the env can vary over time. obs (StateDict): New observations for each ready agent. episode is just started, the value will be None. dones (StateDict): Done values for each ready agent. The special key \"__all__\" (required) is used to indicate env termination. infos (StateDict): Optional info values for each agent id. \"\"\" self . _episode_length += 1 operable_agents = self . _get_operable_agents () # look to add this bugsplat, but this check won't work for multi fps # if set(operable_agents.keys()) != set(action_dict.keys()): # raise RuntimeError(\"Operable_agents and action_dict keys differ!\" # f\"operable={set(operable_agents.keys())} != act={set(action_dict.keys())} \" # \"If this happens that means either your platform is not setting non operable correctly\" # \" (if extra keys are in operable set) or you do not have a done condition covering \" # \"a condition where your platform is going non operable. (if extra keys in act)\") if self . _skip_action : raw_action_dict = {} else : raw_action_dict = self . __apply_action ( operable_agents , action_dict ) # Save current action for future debugging self . _actions . append ( action_dict ) try : self . _state = self . _simulator . step () except ValueError as err : self . _save_state_pickle ( err ) # MTB - Changing to not replace operable_agents variable # We calculate observations on agents operable after sim step # - This is done because otherwise observations would be invalid # Calculate Dones/Rewards on agents operable before sim step # - This is done because if an agent \"dies\" it needs to have a final done calculated operable_agents_after_step = self . _get_operable_agents () ##################################################################### # get next observations - For each configured platform read the # observations/measurements ##################################################################### self . _obs_buffer . next_observation = self . __get_observations_from_glues ( operable_agents_after_step . keys ()) self . _info . clear () self . __get_info_from_glue ( operable_agents_after_step . keys ()) ##################################################################### # Process the done conditions # 1. Reset the rewards from the last step # 2. loops over all agents and processes the reward conditions per # agent ##################################################################### agents_done = self . __get_done_from_agents ( operable_agents . keys (), raw_action_dict = raw_action_dict ) expected_done_keys = set ( operable_agents . keys ()) expected_done_keys . add ( '__all__' ) if set ( agents_done . keys ()) != expected_done_keys : raise RuntimeError ( f 'Local dones do not match expected keys. Received \" { agents_done . keys () } \". Expected \" { expected_done_keys } \".' ) # compute if done all if not agents_done [ '__all__' ]: agent_dones = [ v for k , v in agents_done . items () if k != '__all__' ] if self . config . end_episode_on_first_agent_done : agents_done [ '__all__' ] = any ( agent_dones ) else : agents_done [ '__all__' ] = all ( agent_dones ) shared_dones , shared_done_info = self . _shared_done ( observation = self . _obs_buffer . observation , action = raw_action_dict , next_observation = self . _obs_buffer . next_observation , next_state = self . _state , observation_space = self . _observation_space , observation_units = self . _observation_units , local_dones = copy . deepcopy ( agents_done ), local_done_info = copy . deepcopy ( self . _done_info ) ) if shared_dones . keys (): if set ( shared_dones . keys ()) != expected_done_keys : raise RuntimeError ( f 'Shared dones do not match expected keys. Received \" { shared_dones . keys () } \". Expected \" { expected_done_keys } \".' ) for key in expected_done_keys : agents_done [ key ] |= shared_dones [ key ] assert shared_done_info is not None local_done_info_keys = set ( self . _done_info . keys ()) shared_done_info_keys = set ( shared_done_info ) common_keys = local_done_info_keys & shared_done_info_keys if common_keys : raise RuntimeError ( f 'Dones have common names: \" { common_keys } \"' ) for done_name , done_keys in shared_done_info . items (): for agent_name , done_status in done_keys . items (): self . _done_info [ agent_name ][ done_name ] = OrderedDict ([( self . agent_dict [ agent_name ] . platform_name , done_status )]) # compute if done all if not agents_done [ '__all__' ]: agent_dones = [ v for k , v in agents_done . items () if k != '__all__' ] if self . config . end_episode_on_first_agent_done : agents_done [ '__all__' ] = any ( agent_dones ) else : agents_done [ '__all__' ] = all ( agent_dones ) # Tell the simulator to mark the episode complete if agents_done [ '__all__' ]: self . _simulator . mark_episode_done ( self . _done_info , self . _state . episode_state ) self . _reward . reset () if agents_done [ '__all__' ]: agents_to_process_this_timestep = list ( operable_agents . keys ()) else : def do_process_agent ( self , agent_id ) -> bool : frame_rate = self . _agent_periods [ agent_id ] . numerator / self . _agent_periods [ agent_id ] . denominator return self . _state . sim_time >= self . _agent_process_time [ agent_id ] + frame_rate - self . config . timestep_epsilon agents_to_process_this_timestep = list ( filter ( partial ( do_process_agent , self ), operable_agents . keys ())) for agent_id in agents_to_process_this_timestep : self . _agent_process_time [ agent_id ] = self . _state . sim_time reward = self . __get_reward_from_agents ( agents_to_process_this_timestep , raw_action_dict = raw_action_dict ) self . _simulator . save_episode_information ( self . done_info , self . reward_info , self . _obs_buffer . observation ) # copy over observation from next to previous - There is no real reason to deep # copy here. The process of getting a new observation from the glue copies. All # we need to do is maintain the order of two buffers!!!. # Tested with: They are different and decreasing as expected # print(f\"C: {self._obs_buffer.observation['blue0']['ObserveSensor_Sensor_Fuel']}\") # print(f\"N: {self._obs_buffer.next_observation['blue0']['ObserveSensor_Sensor_Fuel']}\") self . _obs_buffer . update_obs_pointer () # Sanity checks and Scale - ensure run first time and run only every N times... # Same as RLLIB - This can add a bit of time as we are exploring complex dictionaries # default to every time if not specified... Once the limits are good we it is # recommended to increase this for training if self . config . deep_sanity_check : if self . _episode_length % self . config . sanity_check_obs == 0 : try : self . __sanity_check ( self . _observation_space , self . _obs_buffer . observation ) except ValueError as err : self . _save_state_pickle ( err ) else : if not self . _observation_space . contains ( self . _obs_buffer . observation ): raise ValueError ( 'obs not contained in obs space' ) complete_trainable_observations , complete_unnormalized_observations = self . create_training_observations ( operable_agents , self . _obs_buffer ) trainable_observations = OrderedDict () for agent_id in agents_to_process_this_timestep : trainable_observations [ agent_id ] = complete_trainable_observations [ agent_id ] trainable_rewards = get_dictionary_subset ( reward , agents_to_process_this_timestep ) trainable_dones = get_dictionary_subset ( agents_done , [ \"__all__\" ] + agents_to_process_this_timestep ) trainable_info = get_dictionary_subset ( self . _info , agents_to_process_this_timestep ) # add platform obs and env data to trainable_info (for use by custom policies) for agent_id in agents_to_process_this_timestep : if agent_id not in trainable_info : trainable_info [ agent_id ] = {} trainable_info [ agent_id ][ 'env' ] = { 'sim_period' : self . sim_period } trainable_info [ agent_id ][ 'platform_obs' ] = {} plat_name = self . agent_dict [ agent_id ] . platform_name for platform_agent in self . agent_dict : if self . agent_dict [ platform_agent ] . platform_name == plat_name : trainable_info [ agent_id ][ 'platform_obs' ][ platform_agent ] = complete_unnormalized_observations [ platform_agent ] # if not done all, delete any platforms from simulation that are done, so they don't interfere platforms_deleted = set () if not agents_done [ '__all__' ]: for agent_key , value in agents_done . items (): if agent_key != '__all__' and value : plat_name = self . agent_dict [ agent_key ] . platform_name if plat_name not in platforms_deleted : self . simulator . delete_platform ( plat_name ) platforms_deleted . add ( plat_name ) # if a platform has been deleted, we need to make sure that all agents on that platform # are also done for agent_key in trainable_dones . keys (): if agent_key == '__all__' : continue plat_name = self . agent_dict [ agent_key ] . platform_name if plat_name in platforms_deleted : trainable_dones [ agent_key ] = True ##################################################################### # return results to RLLIB - Note that RLLIB does not do a recursive # isinstance call and as such need to make sure items are # OrderedDicts ##################################################################### return trainable_observations , trainable_rewards , trainable_dones , trainable_info def __get_done_from_agents ( self , alive_agents : typing . Iterable [ str ], raw_action_dict ): def or_merge ( config , path , base , nxt ): # pylint: disable=unused-argument return base or nxt merge_strategies = copy . deepcopy ( deepmerge . DEFAULT_TYPE_SPECIFIC_MERGE_STRATEGIES ) merge_strategies . append (( bool , or_merge )) or_merger = deepmerge . Merger ( merge_strategies , [], []) done = OrderedDict () done [ \"__all__\" ] = False for agent_id in alive_agents : agent_class = self . agent_dict [ agent_id ] platform_done , done_info = agent_class . get_dones ( observation = self . _obs_buffer . observation , action = raw_action_dict , next_observation = self . _obs_buffer . next_observation , next_state = self . _state , observation_space = self . _observation_space , observation_units = self . _observation_units ) done [ agent_id ] = platform_done [ agent_class . platform_name ] # get around reduction done [ \"__all__\" ] = done [ \"__all__\" ] if done [ \"__all__\" ] else platform_done . get ( \"__all__\" , False ) or_merger . merge ( self . _done_info . setdefault ( agent_id , {}), done_info ) # self._done_info[agent_id] = done_info return done def __get_reward_from_agents ( self , alive_agents : typing . Iterable [ str ], raw_action_dict ): reward = OrderedDict () for agent_id in alive_agents : agent_class = self . agent_dict [ agent_id ] agent_reward , reward_info = agent_class . get_rewards ( observation = self . _obs_buffer . observation , action = raw_action_dict , next_observation = self . _obs_buffer . next_observation , state = self . _state , next_state = self . _state , observation_space = self . _observation_space , observation_units = self . _observation_units ) # it is possible to have a HL policy that does not compute an reward # in this case just return a zero for reward value if agent_id in agent_reward : reward [ agent_id ] = agent_reward [ agent_id ] else : reward [ agent_id ] = 0 self . _reward_info [ agent_id ] = reward_info return reward def set_default_done_reward ( self ): \"\"\" Populate the done/rewards with default values \"\"\" for key in self . agent_dict . keys (): # pylint: disable=C0201 self . _done [ key ] = False self . _reward [ key ] = 0 # pylint: disable=protected-access self . _shared_done [ key ] = False self . _done [ DoneFuncBase . _ALL ] = False # pylint: disable=protected-access self . _shared_done [ DoneFuncBase . _ALL ] = False # pylint: disable=protected-access def create_training_observations ( self , alive_agents : typing . Iterable [ str ], observations : ObsBuffer ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" Filters and normalizes observations (the sample of the space) using the glue normalize functions. Parameters ---------- alive_agents: The agents that are still alive observations: The observations Returns ------- OrderedDict: the filtered/normalized observation samples \"\"\" this_steps_obs = OrderedDict () for agent_id in alive_agents : if agent_id in observations . observation : this_steps_obs [ agent_id ] = observations . observation [ agent_id ] elif agent_id in observations . next_observation : this_steps_obs [ agent_id ] = observations . next_observation [ agent_id ] else : raise RuntimeError ( \"ERROR: create_training_observations tried to retrieve obs for this training step\" f \" but { agent_id =} was not able to be found in either the current obs data or the \" \" obs from the previous timestep as a fallback\" ) def do_export ( agent_id , obs_name , _obs ): if agent_id in alive_agents : glue_obj = self . agent_dict [ agent_id ] . get_glue ( obs_name ) if glue_obj is not None : return glue_obj . config . training_export_behavior == TrainingExportBehavior . INCLUDE return False filtered_observations = filter_observations ( this_steps_obs , do_export ) normalized_observations = mutate_observations ( filtered_observations , lambda agent_id , # type: ignore obs_name , obs : self . agent_dict [ agent_id ] . normalize_observation ( obs_name , obs ) # type: ignore ) return normalized_observations , filtered_observations def __get_observations_from_glues ( self , alive_agents : typing . Iterable [ str ]) -> OrderedDict : # pylint: disable=protected-access \"\"\" Gets the observation dict from all the glue objects for each agent Returns ------- OrderedDict: The observation dict from all the glues \"\"\" return_observation : OrderedDict = OrderedDict () p_names = [ item . name for item in self . _state . sim_platforms ] for agent_id in alive_agents : agent_class = self . agent_dict [ agent_id ] # TODO: Why is this check required here? # Why does 'alive_agents' contain agents with platforms that don't exist (or have been removed)? # This should only happpen if the user has accidentally passed in 'dead' agents if agent_class . platform_name not in p_names : self . _logger . warning ( f \" { agent_id } on { agent_class . platform_name } is not in the list of (alive) sim_platforms: { self . _state . sim_platforms } \" ) agent_class . set_removed ( True ) else : glue_obj_obs = agent_class . get_observations () if len ( glue_obj_obs ) > 0 : return_observation [ agent_id ] = glue_obj_obs return return_observation def __apply_action ( self , operable_agents , action_dict ): raw_action_dict = OrderedDict () for agent_id , agent_class in operable_agents . items (): if agent_id in action_dict : raw_action_dict [ agent_id ] = agent_class . apply_action ( action_dict [ agent_id ]) return raw_action_dict def __get_info_from_glue ( self , alive_agents : typing . Iterable [ str ]): for agent_id in alive_agents : agent_class = self . agent_dict [ agent_id ] glue_obj_info = agent_class . get_info_dict () if len ( glue_obj_info ) > 0 : self . _info [ agent_id ] = glue_obj_info def _get_observation_units_from_glues ( self ) -> OrderedDict : # pylint: disable=protected-access \"\"\" Gets the observation dict from all the glue objects for each agent Returns ------- OrderedDict: The observation dict from all the glues \"\"\" return_observation : OrderedDict = OrderedDict () p_names = [ item . name for item in self . _state . sim_platforms ] for agent_id , glue_name_obj_pair in self . _agent_glue_dict . items (): for glue_name , glue_object in glue_name_obj_pair . items (): if glue_object . _agent_id not in p_names : # pylint: disable=protected-access glue_object . set_agent_removed ( True ) try : glue_obj_obs = glue_object . observation_units () except AttributeError : glue_obj_obs = None return_observation . setdefault ( agent_id , OrderedDict ())[ glue_name ] = glue_obj_obs return return_observation def _make_glues ( self ) -> None : \"\"\" \"\"\" env_ref_stores = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] plat_to_agent : typing . Dict [ str , typing . List [ str ]] = defaultdict ( lambda : []) for agent , agent_class in self . agent_dict . items (): # get the platform for this agent plat = self . _get_platform_by_name ( agent_class . platform_name ) plat_to_agent [ plat . name ] . append ( agent ) agent_class . make_glues ( plat , agent , env_ref_stores = env_ref_stores ) if self . config . simulator . config . get ( \"disable_exclusivity_check\" , False ): return def itr_controller_glues ( glue ): if isinstance ( glue , BaseAgentControllerGlue ): yield glue if isinstance ( glue , BaseWrapperGlue ): yield from itr_controller_glues ( glue . glue ()) if isinstance ( glue , BaseDictWrapperGlue ): for g in glue . glues () . values (): yield from itr_controller_glues ( g ) if isinstance ( glue , BaseMultiWrapperGlue ): for g in glue . glues (): yield from itr_controller_glues ( g ) # validate for plat_name , agents in plat_to_agent . items (): exclusiveness : typing . Set [ str ] = set () for agent in agents : agent_class = self . agent_dict [ agent ] for glue in agent_class . agent_glue_dict . values (): for controller_glue in itr_controller_glues ( glue ): assert isinstance ( controller_glue , ControllerGlue ), ( f \"Unknown controller glue type { type ( controller_glue ) } on platform { plat_name } \" ) controller_exclusiveness = controller_glue . controller . exclusiveness assert len ( controller_exclusiveness . intersection ( exclusiveness ) ) == 0 , ( f \"Controllers not mutually exclusive on platform { plat_name } \" ) exclusiveness . update ( controller_exclusiveness ) def _make_rewards ( self ) -> None : \"\"\" \"\"\" env_ref_stores = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] for agent , agent_class in self . agent_dict . items (): agent_class . make_rewards ( agent , env_ref_stores = env_ref_stores ) def _make_dones ( self ) -> None : \"\"\" \"\"\" env_ref_stores = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] warmup_steps = self . config . sim_warmup_steps episode_length_done = Functor ( functor = EpisodeLengthDone , config = { 'horizon' : { 'value' : ( self . config . horizon + warmup_steps ) * self . sim_period , 'units' : 'second' }} ) for agent , agent_class in self . agent_dict . items (): env_dones = chain ( self . config . dones . world , self . config . dones . task [ agent_class . platform_name ], [ episode_length_done ]) env_params = [ self . local_variable_store . get ( 'world' , {}), self . local_variable_store . get ( 'task' , {}) . get ( agent_class . platform_name , {}) ] agent_class . make_dones ( agent , agent_class . platform_name , dones = env_dones , env_params = env_params , env_ref_stores = env_ref_stores ) def _make_shared_dones ( self ) -> DoneDict : # pylint: disable=no-self-use \"\"\" _get_shared_done_functors gets and initializes the shared done dict used for this iteration The shared done dictionary does not correspond to individual agents but looks sharedly at all agents. this will be called after any updates to the simulator configuration during reset Returns ------- DoneDict The DoneDict with functors used for this iteration \"\"\" done_conditions = [] ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] param_sources = [ self . local_variable_store . get ( 'shared' , {})] for done_functor in self . config . dones . shared : tmp = done_functor . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources ) done_conditions . append ( tmp ) return DoneDict ( processing_funcs = done_conditions ) @staticmethod def _create_actions ( agents , observations = {}, rewards = {}, dones = {}, info = {}): # pylint: disable=dangerous-default-value for agent_name , agent_class in agents . items (): agent_class . create_next_action ( observations . get ( agent_name ), rewards . get ( agent_name ), dones . get ( agent_name ), info . get ( agent_name ) ) def __create_space ( self , space_getter ) -> gym . spaces . Dict : \"\"\" _create_space Create spaces for all agents by calling their glue objects Parameters ---------- space_getter A function that takes a glue_obj: BaseAgentGlue and returns the space for creating this space For example space_getter=lambda glue_obj: glue_obj.observation_space() Returns ------- A space build from all the glues for each agent in this Environment \"\"\" # init our return Dict return_space = gym . spaces . dict . Dict () # loop over all the agents and their glue_name_obj_pairs list for agent_id , agent_class in self . agent_dict . items (): tmp_space = agent_class . create_space ( space_getter ) # if this agent provided anything add it to the return space if tmp_space : return_space . spaces [ agent_id ] = tmp_space return return_space @property def action_space ( self ) -> gym . spaces . Space : \"\"\" action_space The action space Returns ------- typing.Dict[str,gym.spaces.tuple.Tuple] The action space \"\"\" return self . _normalized_action_space ############ # Properties ############ @property def glue_info ( self ) -> OrderedDict : \"\"\"[summary] Returns: Union[OrderedDict, None] -- [description] \"\"\" return self . _info @property def done_info ( self ) -> OrderedDict : \"\"\"[summary] Returns ------- Union[OrderedDict, None] [description] \"\"\" return self . _done_info @property def reward_info ( self ) -> OrderedDict : \"\"\"[summary] Returns ------- Union[OrderedDict, None] [description] \"\"\" return self . _reward_info @property def observation_space ( self ) -> gym . spaces . Space : \"\"\" observation_space The observation space setup by the user Returns ------- gym.spaces.dict.Dict The observation space \"\"\" return self . _normalized_observation_space @property def state ( self ) -> StateDict : \"\"\" state of platform object. Current state. Returns ------- StateDict the dict storing the curent state of environment. \"\"\" return self . _state @property def simulator ( self ) -> BaseSimulator : \"\"\" simulator simulator instance Returns ------- BaseSimulator The simulator instance in the base \"\"\" return self . _simulator @property def observation ( self ) -> OrderedDict : \"\"\" observation get the observation for the agents in this environment Returns ------- OrderedDict the dict holding the observations for the agents \"\"\" return self . _obs_buffer . observation @property def episode_id ( self ) -> typing . Union [ int , None ]: \"\"\" get the current episode parameter provider episode id Returns ------- int or None the episode id \"\"\" return self . _episode_id def _get_platform_by_name ( self , platform_id : str ) -> BasePlatform : platform : BasePlatform = None # type: ignore for plat in self . _state . sim_platforms : if plat . name == platform_id : platform = plat if platform is None or not issubclass ( platform . __class__ , BasePlatform ): self . _logger . error ( \"-\" * 100 ) self . _logger . error ( f \" { self . _state } \" ) for i in self . _state . sim_platforms : self . _logger . error ( f \" { i . name } \" ) raise ValueError ( f \" { self . __class__ . __name__ } glue could not find a platform named { platform_id } of class BasePlatform\" ) return platform def __setup_state_history ( self ): self . _state . episode_history = defaultdict ( partial ( deque , maxlen = self . config . horizon )) self . _state . episode_state = OrderedDict () self . _state . step_state = OrderedDict () def post_process_trajectory ( self , agent_id , batch , episode , policy ): \"\"\"easy accessor for calling post process trajectory correctly Arguments: agent_id {[type]} -- agent id batch {[type]} -- post processed Batch - be careful modifying \"\"\" self . agent_dict [ agent_id ] . post_process_trajectory ( agent_id , episode . worker . env . _state , # pylint: disable=protected-access batch , episode , policy , episode . worker . env . _reward_info # pylint: disable=protected-access ) @staticmethod def __sanity_check ( space : gym . spaces . Space , space_sample : EnvSpaceUtil . sample_type ) -> None : \"\"\" Sanity checks a space_sample against a space 1. Check to ensure that the sample from the integration base Fall within the expected range of values. Note: space_sample and space expected to match up on Key level entries Parameters ---------- space: gym.spaces.Space the space to check the sample against space_sample: EnvSpaceUtil.sample_type the sample to check if it is actually in the bounds of the space Returns ------- OrderedDict: the scaled_observations \"\"\" if not space . contains ( space_sample ): space . contains ( space_sample ) EnvSpaceUtil . deep_sanity_check_space_sample ( space , space_sample ) def _save_state_pickle ( self , err : ValueError ): \"\"\"saves state for later debug Arguments: err {ValueError} -- Traceback for the error Raises: err: Customized error message to raise for the exception \"\"\" out_pickle = str ( self . config . output_path / f \"sanity_check_failure_ { self . _episode } .pkl\" ) p_dict : typing . Dict [ str , typing . Any ] = {} p_dict [ \"err\" ] = str ( err ) class NumpyArrayEncoder ( JSONEncoder ): \"\"\"Encode the numpy types for json \"\"\" def default ( self , obj ): # pylint: disable=arguments-differ val = None if isinstance ( obj , ( np . int_ , np . intc , np . intp , np . int8 , np . int16 , np . int32 , np . int64 , np . uint8 , np . uint16 , np . uint32 , np . uint64 , ), ): val = int ( obj ) elif isinstance ( obj , ( np . float_ , np . float16 , np . float32 , np . float64 )): val = float ( obj ) elif isinstance ( obj , ( np . ndarray , )): # This is the fix val = obj . tolist () elif isinstance ( obj , np . bool_ ): val = 'True' if obj is True else 'False' else : val = json . JSONEncoder . default ( self , obj ) return val def to_dict ( input_ordered_dict ): return loads ( dumps ( input_ordered_dict , cls = NumpyArrayEncoder )) p_dict [ 'action' ] = self . _actions # type: ignore p_dict [ \"observation\" ] = self . _obs_buffer . observation # type: ignore p_dict [ \"dones\" ] = to_dict ( self . _done_info ) # type: ignore # p_dict[\"env_config\"] = copy.deepcopy(self.env_config) # type: ignore p_dict [ \"step\" ] = str ( self . _episode_length ) pickle . dump ( p_dict , open ( out_pickle , \"wb\" )) raise ValueError ( f \"Error occurred: { err } \\n Saving sanity check failure output pickle to file: { out_pickle } \" ) def seed ( self , seed = None ): \"\"\"generates environment seed through rllib Keyword Arguments: seed {[int]} -- seed to set environment with (default: {None}) Returns: [int] -- [seed value] \"\"\" if not hasattr ( self , \"rng\" ): self . rng , self . config . seed = gym . utils . seeding . np_random ( seed ) return [ self . config . seed ] action_space : Space property readonly \u00a4 action_space The action space Returns \u00a4 typing.Dict[str,gym.spaces.tuple.Tuple] The action space done_info : OrderedDict property readonly \u00a4 [summary] Returns \u00a4 Union[OrderedDict, None] [description] episode_id : Optional [ int ] property readonly \u00a4 get the current episode parameter provider episode id Returns \u00a4 int or None the episode id get_validator : Type [ corl . environment . multi_agent_env . ACT3MultiAgentEnvValidator ] property readonly \u00a4 Get the validator for this class. glue_info : OrderedDict property readonly \u00a4 [summary] Returns: Type Description OrderedDict Union[OrderedDict, None] -- [description] observation : OrderedDict property readonly \u00a4 observation get the observation for the agents in this environment Returns \u00a4 OrderedDict the dict holding the observations for the agents observation_space : Space property readonly \u00a4 observation_space The observation space setup by the user Returns \u00a4 gym.spaces.dict.Dict The observation space reward_info : OrderedDict property readonly \u00a4 [summary] Returns \u00a4 Union[OrderedDict, None] [description] simulator : BaseSimulator property readonly \u00a4 simulator simulator instance Returns \u00a4 BaseSimulator The simulator instance in the base state : StateDict property readonly \u00a4 state of platform object. Current state. Returns \u00a4 StateDict the dict storing the curent state of environment. __init__ ( self , config ) special \u00a4 init initializes the rllib multi agent environment Parameters \u00a4 config : ray.rllib.env.env_context.EnvContext Passed in configuration for setting items up. Must have a 'simulator' key whose value is a BaseIntegrator type Source code in corl/environment/multi_agent_env.py def __init__ ( self , config : EnvContext ) -> None : # pylint: disable=too-many-statements, super-init-not-called \"\"\" __init__ initializes the rllib multi agent environment Parameters ---------- config : ray.rllib.env.env_context.EnvContext Passed in configuration for setting items up. Must have a 'simulator' key whose value is a BaseIntegrator type \"\"\" try : config_vars = vars ( config ) except TypeError : config_vars = {} self . config : ACT3MultiAgentEnvValidator = self . get_validator ( ** config , ** config_vars ) # Random numbers self . seed ( self . config . seed ) # setup default instance variables self . _actions : list = [] self . _obs_buffer = ObsBuffer () self . _reward : RewardDict = RewardDict () self . _done : DoneDict = DoneDict () self . _info : OrderedDict = OrderedDict () self . _episode_length : int = 0 self . _episode : int = 0 self . _episode_id : typing . Union [ int , None ] # agent glue dict is a mapping from agent id to a dict with keys for the glue names # and values of the actual glue object self . _agent_glue_dict : OrderedDict = OrderedDict () self . _agent_glue_obs_export_behavior : OrderedDict = OrderedDict () # Create the logger self . _logger = logging . getLogger ( ACT3MultiAgentEnv . __name__ ) # Extra simulation init args # assign the new output_path with the worker index back to the config for the sim/integration output_path extra_sim_init_args : typing . Dict [ str , typing . Any ] = { \"output_path\" : str ( self . config . output_path ), \"worker_index\" : self . config . worker_index , \"vector_index\" : self . config . vector_index if self . config . vector_index else 0 , } self . agent_dict , extra_sim_init_args [ \"agent_configs\" ] = env_creation . create_agent_sim_configs ( self . config . agents , self . config . agent_platforms , self . config . simulator . type , self . config . platforms , self . config . epp_registry , multiple_workers = ( self . config . num_workers > 0 ) ) def compute_lcm ( values : typing . List [ fractions . Fraction ]) -> float : assert len ( values ) > 0 lcm = values [ 0 ] . denominator for v in values : lcm = lcm // math . gcd ( lcm , v . denominator ) * v . denominator return 1.0 / lcm max_rate = self . config . max_agent_rate self . _agent_periods = { agent_id : fractions . Fraction ( 1.0 / agent . frame_rate ) . limit_denominator ( max_rate ) for agent_id , agent in self . agent_dict . items () } self . _agent_process_time : typing . Dict [ str , float ] = defaultdict ( lambda : sys . float_info . min ) self . sim_period = compute_lcm ( list ( self . _agent_periods . values ())) extra_sim_init_args [ 'frame_rate' ] = 1.0 / self . sim_period for agent_name , platform in self . config . other_platforms . items (): extra_sim_init_args [ \"agent_configs\" ][ agent_name ] = { \"platform_config\" : platform , \"parts_list\" : [], } # Debug logging self . _logger . debug ( f \"output_path : { self . config . output_path } \" ) # Sample parameter provider default_parameters = self . config . epp . config . parameters self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( self . rng ) for k , v in default_parameters . items ()}) for agent in self . agent_dict . values (): agent . fill_parameters ( rng = self . rng , default_parameters = True ) # Create the simulator for this gym environment # ---- oddity from other simulator bases HLP if not hasattr ( self , \"_simulator\" ): class SimulatorWrapper ( self . config . simulator . type ): # type: ignore \"\"\"Wrapper that injects platforms/time into state dict\"\"\" def _clear_data ( self ) -> None : if 'sim_time' in self . _state : del self . _state [ 'sim_time' ] if 'sim_platforms' in self . _state : del self . _state [ 'sim_platforms' ] def _inject_data ( self , state : StateDict ) -> StateDict : \"\"\"Ensures that time/platforms exists in state\"\"\" if 'sim_time' not in state : state [ 'sim_time' ] = self . sim_time if 'sim_platforms' not in state : state [ 'sim_platforms' ] = self . platforms return state def step ( self ) -> StateDict : \"\"\"Steps the simulation - injects data into StateDict\"\"\" self . _clear_data () return self . _inject_data ( super () . step ()) def reset ( self , * args , ** kwargs ) -> StateDict : \"\"\"Resets the simulation - injects data into StateDict\"\"\" self . _clear_data () return self . _inject_data ( super () . reset ( * args , ** kwargs )) simulator_factory = copy . deepcopy ( self . config . simulator ) simulator_factory . type = SimulatorWrapper # type: ignore self . _simulator : BaseSimulator = simulator_factory . build ( ** extra_sim_init_args ) self . _state , self . _sim_reset_args = self . _reset_simulator ( extra_sim_init_args [ \"agent_configs\" ]) # Make the glue objects from the glue mapping now that we have a simulator created self . _make_glues () # create dictionary to hold done history self . __setup_state_history () # Create the observation and action space now that we have the glue self . _observation_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . observation_space ()) self . _action_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . action_space ()) gym_space_sort ( self . _action_space ) self . _normalized_observation_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . normalized_observation_space () if glue_obj . config . training_export_behavior == TrainingExportBehavior . INCLUDE else None ) self . _normalized_action_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . normalized_action_space () ) gym_space_sort ( self . _normalized_action_space ) self . _observation_units = self . __create_space ( space_getter = lambda glue_obj : glue_obj . observation_units () if hasattr ( glue_obj , \"observation_units\" ) else None ) self . _shared_done : DoneDict = DoneDict () self . _done_info : OrderedDict = OrderedDict () self . _reward_info : OrderedDict = OrderedDict () self . _episode_init_params : dict self . done_string = \"\" self . _agent_ids = set ( self . _action_space . spaces . keys ()) self . _skip_action = False create_training_observations ( self , alive_agents , observations ) \u00a4 Filters and normalizes observations (the sample of the space) using the glue normalize functions. Parameters \u00a4 Alive_agents The agents that are still alive Observations The observations Returns \u00a4 Ordereddict the filtered/normalized observation samples Source code in corl/environment/multi_agent_env.py def create_training_observations ( self , alive_agents : typing . Iterable [ str ], observations : ObsBuffer ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" Filters and normalizes observations (the sample of the space) using the glue normalize functions. Parameters ---------- alive_agents: The agents that are still alive observations: The observations Returns ------- OrderedDict: the filtered/normalized observation samples \"\"\" this_steps_obs = OrderedDict () for agent_id in alive_agents : if agent_id in observations . observation : this_steps_obs [ agent_id ] = observations . observation [ agent_id ] elif agent_id in observations . next_observation : this_steps_obs [ agent_id ] = observations . next_observation [ agent_id ] else : raise RuntimeError ( \"ERROR: create_training_observations tried to retrieve obs for this training step\" f \" but { agent_id =} was not able to be found in either the current obs data or the \" \" obs from the previous timestep as a fallback\" ) def do_export ( agent_id , obs_name , _obs ): if agent_id in alive_agents : glue_obj = self . agent_dict [ agent_id ] . get_glue ( obs_name ) if glue_obj is not None : return glue_obj . config . training_export_behavior == TrainingExportBehavior . INCLUDE return False filtered_observations = filter_observations ( this_steps_obs , do_export ) normalized_observations = mutate_observations ( filtered_observations , lambda agent_id , # type: ignore obs_name , obs : self . agent_dict [ agent_id ] . normalize_observation ( obs_name , obs ) # type: ignore ) return normalized_observations , filtered_observations post_process_trajectory ( self , agent_id , batch , episode , policy ) \u00a4 easy accessor for calling post process trajectory correctly Source code in corl/environment/multi_agent_env.py def post_process_trajectory ( self , agent_id , batch , episode , policy ): \"\"\"easy accessor for calling post process trajectory correctly Arguments: agent_id {[type]} -- agent id batch {[type]} -- post processed Batch - be careful modifying \"\"\" self . agent_dict [ agent_id ] . post_process_trajectory ( agent_id , episode . worker . env . _state , # pylint: disable=protected-access batch , episode , policy , episode . worker . env . _reward_info # pylint: disable=protected-access ) reset ( self ) \u00a4 Resets the env and returns observations from ready agents. Returns: Type Description New observations for each ready agent. Examples: >>> from ray.rllib.env.multi_agent_env import MultiAgentEnv >>> class MyMultiAgentEnv ( MultiAgentEnv ): ... # Define your env here. ... ... >>> env = MyMultiAgentEnv () >>> obs = env . reset () >>> print ( obs ) { \"car_0\" : [ 2.4 , 1.6 ], \"car_1\" : [ 3.4 , - 3.2 ], \"traffic_light_1\" : [ 0 , 3 , 5 , 1 ], } Source code in corl/environment/multi_agent_env.py def reset ( self ): # Sample parameter provider current_parameters , self . _episode_id = self . config . epp . get_params ( self . rng ) self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( self . rng ) for k , v in current_parameters . items ()}) for agent in self . agent_dict . values (): agent . fill_parameters ( self . rng ) # 3. Reset the Done and Reward dictionaries for the next iteration self . _make_rewards () self . _make_dones () self . _shared_done : DoneDict = self . _make_shared_dones () self . _reward : RewardDict = RewardDict () self . _done : DoneDict = DoneDict () self . _done_info . clear () self . set_default_done_reward () # 4. Reset the simulation/integration self . _state , self . _sim_reset_args = self . _reset_simulator () self . _episode_length = 0 self . _actions . clear () self . _episode += 1 self . _agent_process_time . clear () ##################################################################### # Make glue sections - Given the state of the simulation we need to # update the platform interfaces. ##################################################################### self . _make_glues () ##################################################################### # get observations # For each configured agent read the observations/measurements ##################################################################### agent_list = list ( self . agent_dict . keys ()) self . _obs_buffer . next_observation = self . __get_observations_from_glues ( agent_list ) self . _obs_buffer . update_obs_pointer () # The following loop guarantees that durring training that the glue # states start with valid values for rates. The number of recommended # steps for sim is at least equal to the depth of the rate observation # tree (ex: speed - 2, acceleration - 3, jerk - 4) - recommend defaulting # to 4 as we do not go higher thank jerk # 1 step is always added for the inital obs in reset warmup = self . config . sim_warmup_steps for _ in range ( warmup ): self . _state = self . _simulator . step () self . _obs_buffer . next_observation = self . __get_observations_from_glues ( agent_list ) self . _obs_buffer . update_obs_pointer () self . __setup_state_history () for platform in self . _state . sim_platforms : self . _state . step_state [ platform . name ] = None self . _state . episode_history [ platform . name ] . clear () self . _state . episode_state [ platform . name ] = OrderedDict () # Sanity Checks and Scale # The current deep sanity check will not raise error if values are from sample are different from space during reset if self . config . deep_sanity_check : try : self . __sanity_check ( self . _observation_space , self . _obs_buffer . observation ) except ValueError as err : self . _save_state_pickle ( err ) else : if not self . _observation_space . contains ( self . _obs_buffer . observation ): raise ValueError ( 'obs not contained in obs space' ) self . _create_actions ( self . agent_dict , self . _obs_buffer . observation ) ##################################################################### # return results to RLLIB - Note that RLLIB does not do a recursive # isinstance call and as such need to make sure items are # OrderedDicts ##################################################################### trainable_observations , _ = self . create_training_observations ( agent_list , self . _obs_buffer ) return trainable_observations seed ( self , seed = None ) \u00a4 generates environment seed through rllib Keyword arguments: Name Type Description seed {[int]} -- seed to set environment with (default {None}) Returns: Type Description [int] -- [seed value] Source code in corl/environment/multi_agent_env.py def seed ( self , seed = None ): \"\"\"generates environment seed through rllib Keyword Arguments: seed {[int]} -- seed to set environment with (default: {None}) Returns: [int] -- [seed value] \"\"\" if not hasattr ( self , \"rng\" ): self . rng , self . config . seed = gym . utils . seeding . np_random ( seed ) return [ self . config . seed ] set_default_done_reward ( self ) \u00a4 Populate the done/rewards with default values Source code in corl/environment/multi_agent_env.py def set_default_done_reward ( self ): \"\"\" Populate the done/rewards with default values \"\"\" for key in self . agent_dict . keys (): # pylint: disable=C0201 self . _done [ key ] = False self . _reward [ key ] = 0 # pylint: disable=protected-access self . _shared_done [ key ] = False self . _done [ DoneFuncBase . _ALL ] = False # pylint: disable=protected-access self . _shared_done [ DoneFuncBase . _ALL ] = False # pylint: disable=protected-access step ( self , action_dict ) \u00a4 Returns observations from ready agents. The returns are dicts mapping from agent_id strings to values. The number of agents in the env can vary over time. obs ( StateDict ) : New observations for each ready agent . episode is just started , the value will be None . dones ( StateDict ) : Done values for each ready agent . The special key \"__all__\" ( required ) is used to indicate env termination . infos ( StateDict ) : Optional info values for each agent id . Source code in corl/environment/multi_agent_env.py def step ( self , action_dict : dict ) -> typing . Tuple [ OrderedDict , OrderedDict , OrderedDict , OrderedDict ]: # pylint: disable=R0912, R0914, R0915 \"\"\"Returns observations from ready agents. The returns are dicts mapping from agent_id strings to values. The number of agents in the env can vary over time. obs (StateDict): New observations for each ready agent. episode is just started, the value will be None. dones (StateDict): Done values for each ready agent. The special key \"__all__\" (required) is used to indicate env termination. infos (StateDict): Optional info values for each agent id. \"\"\" self . _episode_length += 1 operable_agents = self . _get_operable_agents () # look to add this bugsplat, but this check won't work for multi fps # if set(operable_agents.keys()) != set(action_dict.keys()): # raise RuntimeError(\"Operable_agents and action_dict keys differ!\" # f\"operable={set(operable_agents.keys())} != act={set(action_dict.keys())} \" # \"If this happens that means either your platform is not setting non operable correctly\" # \" (if extra keys are in operable set) or you do not have a done condition covering \" # \"a condition where your platform is going non operable. (if extra keys in act)\") if self . _skip_action : raw_action_dict = {} else : raw_action_dict = self . __apply_action ( operable_agents , action_dict ) # Save current action for future debugging self . _actions . append ( action_dict ) try : self . _state = self . _simulator . step () except ValueError as err : self . _save_state_pickle ( err ) # MTB - Changing to not replace operable_agents variable # We calculate observations on agents operable after sim step # - This is done because otherwise observations would be invalid # Calculate Dones/Rewards on agents operable before sim step # - This is done because if an agent \"dies\" it needs to have a final done calculated operable_agents_after_step = self . _get_operable_agents () ##################################################################### # get next observations - For each configured platform read the # observations/measurements ##################################################################### self . _obs_buffer . next_observation = self . __get_observations_from_glues ( operable_agents_after_step . keys ()) self . _info . clear () self . __get_info_from_glue ( operable_agents_after_step . keys ()) ##################################################################### # Process the done conditions # 1. Reset the rewards from the last step # 2. loops over all agents and processes the reward conditions per # agent ##################################################################### agents_done = self . __get_done_from_agents ( operable_agents . keys (), raw_action_dict = raw_action_dict ) expected_done_keys = set ( operable_agents . keys ()) expected_done_keys . add ( '__all__' ) if set ( agents_done . keys ()) != expected_done_keys : raise RuntimeError ( f 'Local dones do not match expected keys. Received \" { agents_done . keys () } \". Expected \" { expected_done_keys } \".' ) # compute if done all if not agents_done [ '__all__' ]: agent_dones = [ v for k , v in agents_done . items () if k != '__all__' ] if self . config . end_episode_on_first_agent_done : agents_done [ '__all__' ] = any ( agent_dones ) else : agents_done [ '__all__' ] = all ( agent_dones ) shared_dones , shared_done_info = self . _shared_done ( observation = self . _obs_buffer . observation , action = raw_action_dict , next_observation = self . _obs_buffer . next_observation , next_state = self . _state , observation_space = self . _observation_space , observation_units = self . _observation_units , local_dones = copy . deepcopy ( agents_done ), local_done_info = copy . deepcopy ( self . _done_info ) ) if shared_dones . keys (): if set ( shared_dones . keys ()) != expected_done_keys : raise RuntimeError ( f 'Shared dones do not match expected keys. Received \" { shared_dones . keys () } \". Expected \" { expected_done_keys } \".' ) for key in expected_done_keys : agents_done [ key ] |= shared_dones [ key ] assert shared_done_info is not None local_done_info_keys = set ( self . _done_info . keys ()) shared_done_info_keys = set ( shared_done_info ) common_keys = local_done_info_keys & shared_done_info_keys if common_keys : raise RuntimeError ( f 'Dones have common names: \" { common_keys } \"' ) for done_name , done_keys in shared_done_info . items (): for agent_name , done_status in done_keys . items (): self . _done_info [ agent_name ][ done_name ] = OrderedDict ([( self . agent_dict [ agent_name ] . platform_name , done_status )]) # compute if done all if not agents_done [ '__all__' ]: agent_dones = [ v for k , v in agents_done . items () if k != '__all__' ] if self . config . end_episode_on_first_agent_done : agents_done [ '__all__' ] = any ( agent_dones ) else : agents_done [ '__all__' ] = all ( agent_dones ) # Tell the simulator to mark the episode complete if agents_done [ '__all__' ]: self . _simulator . mark_episode_done ( self . _done_info , self . _state . episode_state ) self . _reward . reset () if agents_done [ '__all__' ]: agents_to_process_this_timestep = list ( operable_agents . keys ()) else : def do_process_agent ( self , agent_id ) -> bool : frame_rate = self . _agent_periods [ agent_id ] . numerator / self . _agent_periods [ agent_id ] . denominator return self . _state . sim_time >= self . _agent_process_time [ agent_id ] + frame_rate - self . config . timestep_epsilon agents_to_process_this_timestep = list ( filter ( partial ( do_process_agent , self ), operable_agents . keys ())) for agent_id in agents_to_process_this_timestep : self . _agent_process_time [ agent_id ] = self . _state . sim_time reward = self . __get_reward_from_agents ( agents_to_process_this_timestep , raw_action_dict = raw_action_dict ) self . _simulator . save_episode_information ( self . done_info , self . reward_info , self . _obs_buffer . observation ) # copy over observation from next to previous - There is no real reason to deep # copy here. The process of getting a new observation from the glue copies. All # we need to do is maintain the order of two buffers!!!. # Tested with: They are different and decreasing as expected # print(f\"C: {self._obs_buffer.observation['blue0']['ObserveSensor_Sensor_Fuel']}\") # print(f\"N: {self._obs_buffer.next_observation['blue0']['ObserveSensor_Sensor_Fuel']}\") self . _obs_buffer . update_obs_pointer () # Sanity checks and Scale - ensure run first time and run only every N times... # Same as RLLIB - This can add a bit of time as we are exploring complex dictionaries # default to every time if not specified... Once the limits are good we it is # recommended to increase this for training if self . config . deep_sanity_check : if self . _episode_length % self . config . sanity_check_obs == 0 : try : self . __sanity_check ( self . _observation_space , self . _obs_buffer . observation ) except ValueError as err : self . _save_state_pickle ( err ) else : if not self . _observation_space . contains ( self . _obs_buffer . observation ): raise ValueError ( 'obs not contained in obs space' ) complete_trainable_observations , complete_unnormalized_observations = self . create_training_observations ( operable_agents , self . _obs_buffer ) trainable_observations = OrderedDict () for agent_id in agents_to_process_this_timestep : trainable_observations [ agent_id ] = complete_trainable_observations [ agent_id ] trainable_rewards = get_dictionary_subset ( reward , agents_to_process_this_timestep ) trainable_dones = get_dictionary_subset ( agents_done , [ \"__all__\" ] + agents_to_process_this_timestep ) trainable_info = get_dictionary_subset ( self . _info , agents_to_process_this_timestep ) # add platform obs and env data to trainable_info (for use by custom policies) for agent_id in agents_to_process_this_timestep : if agent_id not in trainable_info : trainable_info [ agent_id ] = {} trainable_info [ agent_id ][ 'env' ] = { 'sim_period' : self . sim_period } trainable_info [ agent_id ][ 'platform_obs' ] = {} plat_name = self . agent_dict [ agent_id ] . platform_name for platform_agent in self . agent_dict : if self . agent_dict [ platform_agent ] . platform_name == plat_name : trainable_info [ agent_id ][ 'platform_obs' ][ platform_agent ] = complete_unnormalized_observations [ platform_agent ] # if not done all, delete any platforms from simulation that are done, so they don't interfere platforms_deleted = set () if not agents_done [ '__all__' ]: for agent_key , value in agents_done . items (): if agent_key != '__all__' and value : plat_name = self . agent_dict [ agent_key ] . platform_name if plat_name not in platforms_deleted : self . simulator . delete_platform ( plat_name ) platforms_deleted . add ( plat_name ) # if a platform has been deleted, we need to make sure that all agents on that platform # are also done for agent_key in trainable_dones . keys (): if agent_key == '__all__' : continue plat_name = self . agent_dict [ agent_key ] . platform_name if plat_name in platforms_deleted : trainable_dones [ agent_key ] = True ##################################################################### # return results to RLLIB - Note that RLLIB does not do a recursive # isinstance call and as such need to make sure items are # OrderedDicts ##################################################################### return trainable_observations , trainable_rewards , trainable_dones , trainable_info ACT3MultiAgentEnvEppParameters ( BaseModel ) pydantic-model \u00a4 typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name typing.Dict[str, typing.Dict[str, typing.Dict[str, typing.Any]]] = {} keys: agent name, done name, parameter name typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name typing.Dict[str, typing.Any] = {} keys: reference name typing.Dict[str, typing.Any] = {} keys: whatever the simulator wants, but it needs to be kwargs to simulator reset Source code in corl/environment/multi_agent_env.py class ACT3MultiAgentEnvEppParameters ( BaseModel ): \"\"\" world: typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name task: typing.Dict[str, typing.Dict[str, typing.Dict[str, typing.Any]]] = {} keys: agent name, done name, parameter name shared: typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name reference_store: typing.Dict[str, typing.Any] = {} keys: reference name simulator_reset: typing.Dict[str, typing.Any] = {} keys: whatever the simulator wants, but it needs to be kwargs to simulator reset \"\"\" world : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} task : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} shared : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} reference_store : typing . Dict [ str , typing . Any ] = {} simulator_reset : typing . Dict [ str , typing . Any ] = {} @staticmethod def _validate_leaves_are_parameters ( obj ): if isinstance ( obj , dict ): for _key , value in obj . items (): ACT3MultiAgentEnvEppParameters . _validate_leaves_are_parameters ( value ) elif not isinstance ( obj , Parameter ): raise TypeError ( f \"Invalid type: { type ( obj ) } (required type: { Parameter . __qualname__ } )\" ) @validator ( 'world' , 'task' , 'shared' , 'reference_store' , 'simulator_reset' ) def validate_leaves_are_parameters ( cls , v ): \"\"\" checks to make sure outer most leaf nodes of config are parameters \"\"\" ACT3MultiAgentEnvEppParameters . _validate_leaves_are_parameters ( v ) return v validate_leaves_are_parameters ( v ) classmethod \u00a4 checks to make sure outer most leaf nodes of config are parameters Source code in corl/environment/multi_agent_env.py @validator ( 'world' , 'task' , 'shared' , 'reference_store' , 'simulator_reset' ) def validate_leaves_are_parameters ( cls , v ): \"\"\" checks to make sure outer most leaf nodes of config are parameters \"\"\" ACT3MultiAgentEnvEppParameters . _validate_leaves_are_parameters ( v ) return v ACT3MultiAgentEnvValidator ( BaseModel ) pydantic-model \u00a4 Validation model for the inputs of ACT3MultiAgentEnv Source code in corl/environment/multi_agent_env.py class ACT3MultiAgentEnvValidator ( BaseModel ): \"\"\"Validation model for the inputs of ACT3MultiAgentEnv\"\"\" num_workers : NonNegativeInt = 0 worker_index : NonNegativeInt = 0 vector_index : typing . Optional [ NonNegativeInt ] = None remote : bool = False deep_sanity_check : bool = True seed : PositiveInt = 0 horizon : PositiveInt = 1000 sanity_check_obs : PositiveInt = 50 sensors_grid : typing . Optional [ typing . List ] plugin_paths : typing . List [ str ] = [] # Regex allows letters, numbers, underscore, dash, dot # Regex in output_path validator also allows forward slash # Empty string is not allowed TrialName : typing . Optional [ Annotated [ str , Field ( regex = r '^[\\w\\.-]+$' )]] = None output_date_string : typing . Optional [ Annotated [ str , Field ( regex = r '^[\\w\\.-]+$' )]] = None skip_pbs_date_update : bool = False # MyPy error ignored because it is handled by the pre-validator output_path : DirectoryPath = None # type: ignore[assignment] agent_platforms : typing . Dict agents : typing . Dict [ str , AgentParseInfo ] simulator : Factory platforms : typing . Type [ BaseAvailablePlatformTypes ] other_platforms : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} reference_store : typing . Dict [ str , ObjectStoreElem ] = {} dones : EnvironmentDoneValidator = EnvironmentDoneValidator () end_episode_on_first_agent_done : bool = False simulator_reset_parameters : typing . Dict [ str , typing . Any ] = {} episode_parameter_provider : Factory episode_parameter_provider_parameters : ACT3MultiAgentEnvEppParameters = None # type: ignore epp_registry : typing . Dict [ str , EpisodeParameterProvider ] = None # type: ignore max_agent_rate : int = 20 # the maximum rate (in Hz) that an agent may be run at timestep_epsilon : float = 1e-3 sim_warmup_steps : int = 0 # number of times to step simulator before getting initial obs @property def epp ( self ) -> EpisodeParameterProvider : \"\"\" return the current episode parameter provider \"\"\" return self . epp_registry [ ACT3MultiAgentEnv . episode_parameter_provider_name ] # pylint: disable=unsubscriptable-object class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True @validator ( 'seed' , pre = True ) def get_seed ( cls , v ): \"\"\"Compute a valid seed\"\"\" _ , seed = gym . utils . seeding . np_random ( v ) return seed @validator ( 'plugin_paths' ) def add_plugin_paths ( cls , v ): \"\"\"Use the plugin path attribute to initialize the plugin library.\"\"\" PluginLibrary . add_paths ( v ) return v @validator ( 'output_path' , pre = True , always = True ) def create_output_path ( cls , v , values ): \"\"\"Build the output path.\"\"\" v = v or 'data/act3/ray_results' v = parse_obj_as ( Annotated [ str , Field ( regex = r '^[\\w/\\.-]+$' )], v ) if values [ 'TrialName' ] is not None : if values [ \"skip_pbs_date_update\" ]: trial_prefix = '' else : trial_prefix = os . environ . get ( 'PBS_JOBID' , os . environ . get ( 'TRIAL_NAME_PREFIX' , '' )) if trial_prefix : v = os . path . join ( v , f ' { trial_prefix } - { values [ \"TrialName\" ] } ' ) else : v = os . path . join ( v , values [ 'TrialName' ]) if values [ 'output_date_string' ] is not None and not values [ \"skip_pbs_date_update\" ]: v = os . path . join ( v , values [ 'output_date_string' ]) v = os . path . abspath ( v ) v = os . path . join ( v , str ( values [ 'worker_index' ]) . zfill ( 4 )) if values [ 'vector_index' ] is not None : v = os . path . join ( v , str ( values [ 'vector_index' ]) . zfill ( 4 )) os . makedirs ( v , exist_ok = True ) return v @validator ( 'simulator' , pre = True ) def resolve_simulator_plugin ( cls , v ): \"\"\"Determine the simulator from the plugin library.\"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Let pydantic print out an error when there is no type field return v match = PluginLibrary . FindMatch ( v [ 'type' ], {}) if not issubclass ( match , BaseSimulator ): raise TypeError ( f \"Simulator must subclass BaseSimulator, but is is of type { v [ 'type' ] } \" ) return { 'type' : match , 'config' : v . get ( 'config' )} @validator ( 'platforms' , pre = True ) def resolve_platforms ( cls , v , values ): \"\"\"Determine the platforms from the plugin library.\"\"\" if not isinstance ( v , str ): return v return PluginLibrary . FindMatch ( v , { 'simulator' : values [ 'simulator' ] . type }) @validator ( 'agents' ) def agents_not_empty ( cls , v , values ): \"\"\"Ensure that at least one agent exists\"\"\" if len ( v ) == 0 : raise RuntimeError ( 'No agents exist' ) for agent_name , agent in v . items (): assert agent . platform_name in values [ 'agent_platforms' ], f \"missing platform ' { agent . platform_name } ' for agent ' { agent_name } '\" assert agent . platform_name == agent_name . split ( \"_\" )[ 0 ], f \"invalid platform name { agent . platform_name } for agent { agent_name } \" return v resolve_reference_store_factory = validator ( 'reference_store' , pre = True , each_item = True , allow_reuse = True )( Factory . resolve_factory ) @validator ( 'dones' , always = True ) def agents_match ( cls , v , values ): \"\"\"Ensure that platform in task dones match provided platforms\"\"\" # No extra agents in task dones for platform in v . task . keys (): if platform not in values [ 'agent_platforms' ]: raise RuntimeError ( f 'Platform { platform } lists a done condition but is not an allowed platform' ) # Task dones exist for all agents. Make empty ones if necessary for platform in values [ 'agent_platforms' ]: if platform not in v . task : v . task [ platform ] = {} return v @validator ( 'simulator_reset_parameters' , pre = True ) def update_units_and_parameters ( cls , v ): \"\"\"Update simulation reset parameters to meet base simulator requirements.\"\"\" return validation_helper_units_and_parameters ( v ) @validator ( 'episode_parameter_provider_parameters' , always = True , pre = True ) def build_episode_parameter_provider_parameters ( cls , _v , values ) -> ACT3MultiAgentEnvEppParameters : \"\"\"Create the episode parameter provider for this configuration\"\"\" for key in [ 'reference_store' , 'dones' , 'simulator_reset_parameters' ]: assert key in values reference_parameters : typing . Dict [ str , Parameter ] = {} for ref_name , ref_value in values [ 'reference_store' ] . items (): if isinstance ( ref_value , Parameter ): reference_parameters [ ref_name ] = ref_value world_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ] . world : functor . add_to_parameter_store ( world_parameters ) task_parameters : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Parameter ]]] = {} for agent , task_dones in values [ 'dones' ] . task . items (): agent_task_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in task_dones : functor . add_to_parameter_store ( agent_task_parameters ) task_parameters [ agent ] = agent_task_parameters shared_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ] . shared : functor . add_to_parameter_store ( shared_parameters ) sim_parameters_flat = { name : param for name , param in flatten_dict . flatten ( values [ 'simulator_reset_parameters' ]) . items () if isinstance ( param , Parameter ) } sim_parameters = flatten_dict . unflatten ( sim_parameters_flat ) return ACT3MultiAgentEnvEppParameters ( world = world_parameters , task = task_parameters , shared = shared_parameters , reference_store = reference_parameters , simulator_reset = sim_parameters ) @validator ( 'epp_registry' , always = True , pre = True ) def construct_epp_registry_if_necessary_and_validate ( cls , epp_registry , values ): \"\"\" validates the Episode Parameter provider registry \"\"\" if epp_registry is None : epp_registry = {} env_epp_parameters = dict ( values [ 'episode_parameter_provider_parameters' ]) flat_env_epp_parameters = flatten_dict . flatten ( env_epp_parameters ) env_epp = values [ 'episode_parameter_provider' ] . build ( parameters = flat_env_epp_parameters ) epp_registry [ ACT3MultiAgentEnv . episode_parameter_provider_name ] = env_epp for agent_id , agent_info in values [ 'agents' ] . items (): agent = agent_info . class_config . agent ( agent_name = agent_id , platform_name = agent_info . platform_name , ** agent_info . class_config . config ) epp_registry [ agent_id ] = agent . config . epp if ACT3MultiAgentEnv . episode_parameter_provider_name not in epp_registry : raise ValueError ( f \"Missing EPP for ' { ACT3MultiAgentEnv . episode_parameter_provider_name } '\" ) for agent_id in values [ 'agents' ]: if agent_id not in epp_registry : raise ValueError ( f \"Missing EPP for ' { agent_id } '\" ) for key , epp in epp_registry . items (): if not isinstance ( epp , EpisodeParameterProvider ): raise TypeError ( f \"Invalid type for epp_registry[' { key } ']: { type ( epp ) } , only { EpisodeParameterProvider . __qualname__ } allowed\" ) return epp_registry epp : EpisodeParameterProvider property readonly \u00a4 return the current episode parameter provider Config \u00a4 Allow arbitrary types for Parameter Source code in corl/environment/multi_agent_env.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True add_plugin_paths ( v ) classmethod \u00a4 Use the plugin path attribute to initialize the plugin library. Source code in corl/environment/multi_agent_env.py @validator ( 'plugin_paths' ) def add_plugin_paths ( cls , v ): \"\"\"Use the plugin path attribute to initialize the plugin library.\"\"\" PluginLibrary . add_paths ( v ) return v agents_match ( v , values ) classmethod \u00a4 Ensure that platform in task dones match provided platforms Source code in corl/environment/multi_agent_env.py @validator ( 'dones' , always = True ) def agents_match ( cls , v , values ): \"\"\"Ensure that platform in task dones match provided platforms\"\"\" # No extra agents in task dones for platform in v . task . keys (): if platform not in values [ 'agent_platforms' ]: raise RuntimeError ( f 'Platform { platform } lists a done condition but is not an allowed platform' ) # Task dones exist for all agents. Make empty ones if necessary for platform in values [ 'agent_platforms' ]: if platform not in v . task : v . task [ platform ] = {} return v agents_not_empty ( v , values ) classmethod \u00a4 Ensure that at least one agent exists Source code in corl/environment/multi_agent_env.py @validator ( 'agents' ) def agents_not_empty ( cls , v , values ): \"\"\"Ensure that at least one agent exists\"\"\" if len ( v ) == 0 : raise RuntimeError ( 'No agents exist' ) for agent_name , agent in v . items (): assert agent . platform_name in values [ 'agent_platforms' ], f \"missing platform ' { agent . platform_name } ' for agent ' { agent_name } '\" assert agent . platform_name == agent_name . split ( \"_\" )[ 0 ], f \"invalid platform name { agent . platform_name } for agent { agent_name } \" return v build_episode_parameter_provider_parameters ( _v , values ) classmethod \u00a4 Create the episode parameter provider for this configuration Source code in corl/environment/multi_agent_env.py @validator ( 'episode_parameter_provider_parameters' , always = True , pre = True ) def build_episode_parameter_provider_parameters ( cls , _v , values ) -> ACT3MultiAgentEnvEppParameters : \"\"\"Create the episode parameter provider for this configuration\"\"\" for key in [ 'reference_store' , 'dones' , 'simulator_reset_parameters' ]: assert key in values reference_parameters : typing . Dict [ str , Parameter ] = {} for ref_name , ref_value in values [ 'reference_store' ] . items (): if isinstance ( ref_value , Parameter ): reference_parameters [ ref_name ] = ref_value world_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ] . world : functor . add_to_parameter_store ( world_parameters ) task_parameters : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Parameter ]]] = {} for agent , task_dones in values [ 'dones' ] . task . items (): agent_task_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in task_dones : functor . add_to_parameter_store ( agent_task_parameters ) task_parameters [ agent ] = agent_task_parameters shared_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ] . shared : functor . add_to_parameter_store ( shared_parameters ) sim_parameters_flat = { name : param for name , param in flatten_dict . flatten ( values [ 'simulator_reset_parameters' ]) . items () if isinstance ( param , Parameter ) } sim_parameters = flatten_dict . unflatten ( sim_parameters_flat ) return ACT3MultiAgentEnvEppParameters ( world = world_parameters , task = task_parameters , shared = shared_parameters , reference_store = reference_parameters , simulator_reset = sim_parameters ) construct_epp_registry_if_necessary_and_validate ( epp_registry , values ) classmethod \u00a4 validates the Episode Parameter provider registry Source code in corl/environment/multi_agent_env.py @validator ( 'epp_registry' , always = True , pre = True ) def construct_epp_registry_if_necessary_and_validate ( cls , epp_registry , values ): \"\"\" validates the Episode Parameter provider registry \"\"\" if epp_registry is None : epp_registry = {} env_epp_parameters = dict ( values [ 'episode_parameter_provider_parameters' ]) flat_env_epp_parameters = flatten_dict . flatten ( env_epp_parameters ) env_epp = values [ 'episode_parameter_provider' ] . build ( parameters = flat_env_epp_parameters ) epp_registry [ ACT3MultiAgentEnv . episode_parameter_provider_name ] = env_epp for agent_id , agent_info in values [ 'agents' ] . items (): agent = agent_info . class_config . agent ( agent_name = agent_id , platform_name = agent_info . platform_name , ** agent_info . class_config . config ) epp_registry [ agent_id ] = agent . config . epp if ACT3MultiAgentEnv . episode_parameter_provider_name not in epp_registry : raise ValueError ( f \"Missing EPP for ' { ACT3MultiAgentEnv . episode_parameter_provider_name } '\" ) for agent_id in values [ 'agents' ]: if agent_id not in epp_registry : raise ValueError ( f \"Missing EPP for ' { agent_id } '\" ) for key , epp in epp_registry . items (): if not isinstance ( epp , EpisodeParameterProvider ): raise TypeError ( f \"Invalid type for epp_registry[' { key } ']: { type ( epp ) } , only { EpisodeParameterProvider . __qualname__ } allowed\" ) return epp_registry create_output_path ( v , values ) classmethod \u00a4 Build the output path. Source code in corl/environment/multi_agent_env.py @validator ( 'output_path' , pre = True , always = True ) def create_output_path ( cls , v , values ): \"\"\"Build the output path.\"\"\" v = v or 'data/act3/ray_results' v = parse_obj_as ( Annotated [ str , Field ( regex = r '^[\\w/\\.-]+$' )], v ) if values [ 'TrialName' ] is not None : if values [ \"skip_pbs_date_update\" ]: trial_prefix = '' else : trial_prefix = os . environ . get ( 'PBS_JOBID' , os . environ . get ( 'TRIAL_NAME_PREFIX' , '' )) if trial_prefix : v = os . path . join ( v , f ' { trial_prefix } - { values [ \"TrialName\" ] } ' ) else : v = os . path . join ( v , values [ 'TrialName' ]) if values [ 'output_date_string' ] is not None and not values [ \"skip_pbs_date_update\" ]: v = os . path . join ( v , values [ 'output_date_string' ]) v = os . path . abspath ( v ) v = os . path . join ( v , str ( values [ 'worker_index' ]) . zfill ( 4 )) if values [ 'vector_index' ] is not None : v = os . path . join ( v , str ( values [ 'vector_index' ]) . zfill ( 4 )) os . makedirs ( v , exist_ok = True ) return v get_seed ( v ) classmethod \u00a4 Compute a valid seed Source code in corl/environment/multi_agent_env.py @validator ( 'seed' , pre = True ) def get_seed ( cls , v ): \"\"\"Compute a valid seed\"\"\" _ , seed = gym . utils . seeding . np_random ( v ) return seed resolve_platforms ( v , values ) classmethod \u00a4 Determine the platforms from the plugin library. Source code in corl/environment/multi_agent_env.py @validator ( 'platforms' , pre = True ) def resolve_platforms ( cls , v , values ): \"\"\"Determine the platforms from the plugin library.\"\"\" if not isinstance ( v , str ): return v return PluginLibrary . FindMatch ( v , { 'simulator' : values [ 'simulator' ] . type }) resolve_reference_store_factory ( v ) classmethod \u00a4 Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) Source code in corl/environment/multi_agent_env.py @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build () resolve_simulator_plugin ( v ) classmethod \u00a4 Determine the simulator from the plugin library. Source code in corl/environment/multi_agent_env.py @validator ( 'simulator' , pre = True ) def resolve_simulator_plugin ( cls , v ): \"\"\"Determine the simulator from the plugin library.\"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Let pydantic print out an error when there is no type field return v match = PluginLibrary . FindMatch ( v [ 'type' ], {}) if not issubclass ( match , BaseSimulator ): raise TypeError ( f \"Simulator must subclass BaseSimulator, but is is of type { v [ 'type' ] } \" ) return { 'type' : match , 'config' : v . get ( 'config' )} update_units_and_parameters ( v ) classmethod \u00a4 Update simulation reset parameters to meet base simulator requirements. Source code in corl/environment/multi_agent_env.py @validator ( 'simulator_reset_parameters' , pre = True ) def update_units_and_parameters ( cls , v ): \"\"\"Update simulation reset parameters to meet base simulator requirements.\"\"\" return validation_helper_units_and_parameters ( v ) EnvironmentDoneValidator ( BaseModel ) pydantic-model \u00a4 Validation model for the dones of ACT3MultiAgentEnv Source code in corl/environment/multi_agent_env.py class EnvironmentDoneValidator ( BaseModel ): \"\"\"Validation model for the dones of ACT3MultiAgentEnv\"\"\" world : typing . List [ Functor ] = [] task : typing . Dict [ str , typing . List [ Functor ]] = {} shared : typing . List [ Functor ] = [] @validator ( 'world' , each_item = True ) def check_world ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" cls . check_done ( v ) return v @validator ( 'task' , each_item = True ) def check_task ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" for elem in v : cls . check_done ( elem ) return v @validator ( 'shared' , each_item = True ) def check_shared ( cls , v ): \"\"\"Check if dones subclass SharedDoneFuncBase\"\"\" if not issubclass ( v . functor , SharedDoneFuncBase ): raise TypeError ( f \"Shared Done functors must subclass SharedDoneFuncBase, but done { v . name } is of type { v . functor } \" ) return v @classmethod def check_done ( cls , v ) -> None : \"\"\"Check if dones subclass DoneFuncBase\"\"\" if not issubclass ( v . functor , DoneFuncBase ): raise TypeError ( f \"Done functors must subclass DoneFuncBase, but done { v . name } is of type { v . functor } \" ) if issubclass ( v . functor , EpisodeLengthDone ): raise ValueError ( \"Cannot specify EpisodeLengthDone as it is automatically added\" ) check_done ( v ) classmethod \u00a4 Check if dones subclass DoneFuncBase Source code in corl/environment/multi_agent_env.py @classmethod def check_done ( cls , v ) -> None : \"\"\"Check if dones subclass DoneFuncBase\"\"\" if not issubclass ( v . functor , DoneFuncBase ): raise TypeError ( f \"Done functors must subclass DoneFuncBase, but done { v . name } is of type { v . functor } \" ) if issubclass ( v . functor , EpisodeLengthDone ): raise ValueError ( \"Cannot specify EpisodeLengthDone as it is automatically added\" ) check_shared ( v ) classmethod \u00a4 Check if dones subclass SharedDoneFuncBase Source code in corl/environment/multi_agent_env.py @validator ( 'shared' , each_item = True ) def check_shared ( cls , v ): \"\"\"Check if dones subclass SharedDoneFuncBase\"\"\" if not issubclass ( v . functor , SharedDoneFuncBase ): raise TypeError ( f \"Shared Done functors must subclass SharedDoneFuncBase, but done { v . name } is of type { v . functor } \" ) return v check_task ( v ) classmethod \u00a4 Check if dones subclass DoneFuncBase Source code in corl/environment/multi_agent_env.py @validator ( 'task' , each_item = True ) def check_task ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" for elem in v : cls . check_done ( elem ) return v check_world ( v ) classmethod \u00a4 Check if dones subclass DoneFuncBase Source code in corl/environment/multi_agent_env.py @validator ( 'world' , each_item = True ) def check_world ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" cls . check_done ( v ) return v","title":"Multi agent env"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv","text":"ACT3MultiAgentEnv create a RLLIB MultiAgentEnv environment. The following class is intended to wrap the interactions with RLLIB and the backend simulator environment. All items here are intended to be common parts for running the RLLIB environment with ${simulator} being the unique interaction parts. Includes wrapping the creation of the simulator specific to run Includes interactions with the dones, rewards, and glues etc... Source code in corl/environment/multi_agent_env.py class ACT3MultiAgentEnv ( MultiAgentEnv ): \"\"\" ACT3MultiAgentEnv create a RLLIB MultiAgentEnv environment. The following class is intended to wrap the interactions with RLLIB and the backend simulator environment. All items here are intended to be common parts for running the RLLIB environment with ${simulator} being the unique interaction parts. 1. Includes wrapping the creation of the simulator specific to run 2. Includes interactions with the dones, rewards, and glues 3. etc... \"\"\" episode_parameter_provider_name : str = 'environment' def __init__ ( self , config : EnvContext ) -> None : # pylint: disable=too-many-statements, super-init-not-called \"\"\" __init__ initializes the rllib multi agent environment Parameters ---------- config : ray.rllib.env.env_context.EnvContext Passed in configuration for setting items up. Must have a 'simulator' key whose value is a BaseIntegrator type \"\"\" try : config_vars = vars ( config ) except TypeError : config_vars = {} self . config : ACT3MultiAgentEnvValidator = self . get_validator ( ** config , ** config_vars ) # Random numbers self . seed ( self . config . seed ) # setup default instance variables self . _actions : list = [] self . _obs_buffer = ObsBuffer () self . _reward : RewardDict = RewardDict () self . _done : DoneDict = DoneDict () self . _info : OrderedDict = OrderedDict () self . _episode_length : int = 0 self . _episode : int = 0 self . _episode_id : typing . Union [ int , None ] # agent glue dict is a mapping from agent id to a dict with keys for the glue names # and values of the actual glue object self . _agent_glue_dict : OrderedDict = OrderedDict () self . _agent_glue_obs_export_behavior : OrderedDict = OrderedDict () # Create the logger self . _logger = logging . getLogger ( ACT3MultiAgentEnv . __name__ ) # Extra simulation init args # assign the new output_path with the worker index back to the config for the sim/integration output_path extra_sim_init_args : typing . Dict [ str , typing . Any ] = { \"output_path\" : str ( self . config . output_path ), \"worker_index\" : self . config . worker_index , \"vector_index\" : self . config . vector_index if self . config . vector_index else 0 , } self . agent_dict , extra_sim_init_args [ \"agent_configs\" ] = env_creation . create_agent_sim_configs ( self . config . agents , self . config . agent_platforms , self . config . simulator . type , self . config . platforms , self . config . epp_registry , multiple_workers = ( self . config . num_workers > 0 ) ) def compute_lcm ( values : typing . List [ fractions . Fraction ]) -> float : assert len ( values ) > 0 lcm = values [ 0 ] . denominator for v in values : lcm = lcm // math . gcd ( lcm , v . denominator ) * v . denominator return 1.0 / lcm max_rate = self . config . max_agent_rate self . _agent_periods = { agent_id : fractions . Fraction ( 1.0 / agent . frame_rate ) . limit_denominator ( max_rate ) for agent_id , agent in self . agent_dict . items () } self . _agent_process_time : typing . Dict [ str , float ] = defaultdict ( lambda : sys . float_info . min ) self . sim_period = compute_lcm ( list ( self . _agent_periods . values ())) extra_sim_init_args [ 'frame_rate' ] = 1.0 / self . sim_period for agent_name , platform in self . config . other_platforms . items (): extra_sim_init_args [ \"agent_configs\" ][ agent_name ] = { \"platform_config\" : platform , \"parts_list\" : [], } # Debug logging self . _logger . debug ( f \"output_path : { self . config . output_path } \" ) # Sample parameter provider default_parameters = self . config . epp . config . parameters self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( self . rng ) for k , v in default_parameters . items ()}) for agent in self . agent_dict . values (): agent . fill_parameters ( rng = self . rng , default_parameters = True ) # Create the simulator for this gym environment # ---- oddity from other simulator bases HLP if not hasattr ( self , \"_simulator\" ): class SimulatorWrapper ( self . config . simulator . type ): # type: ignore \"\"\"Wrapper that injects platforms/time into state dict\"\"\" def _clear_data ( self ) -> None : if 'sim_time' in self . _state : del self . _state [ 'sim_time' ] if 'sim_platforms' in self . _state : del self . _state [ 'sim_platforms' ] def _inject_data ( self , state : StateDict ) -> StateDict : \"\"\"Ensures that time/platforms exists in state\"\"\" if 'sim_time' not in state : state [ 'sim_time' ] = self . sim_time if 'sim_platforms' not in state : state [ 'sim_platforms' ] = self . platforms return state def step ( self ) -> StateDict : \"\"\"Steps the simulation - injects data into StateDict\"\"\" self . _clear_data () return self . _inject_data ( super () . step ()) def reset ( self , * args , ** kwargs ) -> StateDict : \"\"\"Resets the simulation - injects data into StateDict\"\"\" self . _clear_data () return self . _inject_data ( super () . reset ( * args , ** kwargs )) simulator_factory = copy . deepcopy ( self . config . simulator ) simulator_factory . type = SimulatorWrapper # type: ignore self . _simulator : BaseSimulator = simulator_factory . build ( ** extra_sim_init_args ) self . _state , self . _sim_reset_args = self . _reset_simulator ( extra_sim_init_args [ \"agent_configs\" ]) # Make the glue objects from the glue mapping now that we have a simulator created self . _make_glues () # create dictionary to hold done history self . __setup_state_history () # Create the observation and action space now that we have the glue self . _observation_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . observation_space ()) self . _action_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . action_space ()) gym_space_sort ( self . _action_space ) self . _normalized_observation_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . normalized_observation_space () if glue_obj . config . training_export_behavior == TrainingExportBehavior . INCLUDE else None ) self . _normalized_action_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . normalized_action_space () ) gym_space_sort ( self . _normalized_action_space ) self . _observation_units = self . __create_space ( space_getter = lambda glue_obj : glue_obj . observation_units () if hasattr ( glue_obj , \"observation_units\" ) else None ) self . _shared_done : DoneDict = DoneDict () self . _done_info : OrderedDict = OrderedDict () self . _reward_info : OrderedDict = OrderedDict () self . _episode_init_params : dict self . done_string = \"\" self . _agent_ids = set ( self . _action_space . spaces . keys ()) self . _skip_action = False @property def get_validator ( self ) -> typing . Type [ ACT3MultiAgentEnvValidator ]: \"\"\"Get the validator for this class.\"\"\" return ACT3MultiAgentEnvValidator def reset ( self ): # Sample parameter provider current_parameters , self . _episode_id = self . config . epp . get_params ( self . rng ) self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( self . rng ) for k , v in current_parameters . items ()}) for agent in self . agent_dict . values (): agent . fill_parameters ( self . rng ) # 3. Reset the Done and Reward dictionaries for the next iteration self . _make_rewards () self . _make_dones () self . _shared_done : DoneDict = self . _make_shared_dones () self . _reward : RewardDict = RewardDict () self . _done : DoneDict = DoneDict () self . _done_info . clear () self . set_default_done_reward () # 4. Reset the simulation/integration self . _state , self . _sim_reset_args = self . _reset_simulator () self . _episode_length = 0 self . _actions . clear () self . _episode += 1 self . _agent_process_time . clear () ##################################################################### # Make glue sections - Given the state of the simulation we need to # update the platform interfaces. ##################################################################### self . _make_glues () ##################################################################### # get observations # For each configured agent read the observations/measurements ##################################################################### agent_list = list ( self . agent_dict . keys ()) self . _obs_buffer . next_observation = self . __get_observations_from_glues ( agent_list ) self . _obs_buffer . update_obs_pointer () # The following loop guarantees that durring training that the glue # states start with valid values for rates. The number of recommended # steps for sim is at least equal to the depth of the rate observation # tree (ex: speed - 2, acceleration - 3, jerk - 4) - recommend defaulting # to 4 as we do not go higher thank jerk # 1 step is always added for the inital obs in reset warmup = self . config . sim_warmup_steps for _ in range ( warmup ): self . _state = self . _simulator . step () self . _obs_buffer . next_observation = self . __get_observations_from_glues ( agent_list ) self . _obs_buffer . update_obs_pointer () self . __setup_state_history () for platform in self . _state . sim_platforms : self . _state . step_state [ platform . name ] = None self . _state . episode_history [ platform . name ] . clear () self . _state . episode_state [ platform . name ] = OrderedDict () # Sanity Checks and Scale # The current deep sanity check will not raise error if values are from sample are different from space during reset if self . config . deep_sanity_check : try : self . __sanity_check ( self . _observation_space , self . _obs_buffer . observation ) except ValueError as err : self . _save_state_pickle ( err ) else : if not self . _observation_space . contains ( self . _obs_buffer . observation ): raise ValueError ( 'obs not contained in obs space' ) self . _create_actions ( self . agent_dict , self . _obs_buffer . observation ) ##################################################################### # return results to RLLIB - Note that RLLIB does not do a recursive # isinstance call and as such need to make sure items are # OrderedDicts ##################################################################### trainable_observations , _ = self . create_training_observations ( agent_list , self . _obs_buffer ) return trainable_observations def _reset_simulator ( self , agent_configs = None ) -> typing . Tuple [ StateDict , typing . Dict [ str , typing . Any ]]: sim_reset_args = copy . deepcopy ( self . config . simulator_reset_parameters ) v_store = self . local_variable_store deepmerge . always_merger . merge ( sim_reset_args , v_store . get ( 'simulator_reset' , {})) self . _process_references ( sim_reset_args , v_store ) for agent_data in self . agent_dict . values (): deepmerge . always_merger . merge ( sim_reset_args , { 'platforms' : { agent_data . platform_name : agent_data . get_simulator_reset_parameters () }} ) if agent_configs is not None : sim_reset_args [ \"agent_configs_reset\" ] = agent_configs return self . _simulator . reset ( sim_reset_args ), sim_reset_args def _process_references ( self , sim_reset_args : dict , v_store : dict ) -> None : \"\"\"Process the reference store look ups for the position data Parameters ---------- sim_reset_args : dict The simulator reset parameters v_store : dict The variable store \"\"\" plat_str = \"platforms\" pos_str = \"position\" ref_str = \"reference\" if plat_str in sim_reset_args : for plat_k , plat_v in sim_reset_args [ plat_str ] . items (): if pos_str in plat_v : for position_k , position_v in plat_v [ pos_str ] . items (): if isinstance ( position_v , dict ) and ref_str in position_v : sim_reset_args [ plat_str ][ plat_k ][ pos_str ][ position_k ] = v_store [ \"reference_store\" ] . get ( position_v [ ref_str ], self . config . reference_store [ position_v [ ref_str ]] ) def _get_operable_agents ( self ): \"\"\"Determines which agents are operable in the sim, this becomes stale after the simulation is stepped\"\"\" operable_platform_names = [ item . name for item in self . state . sim_platforms if item . operable and not self . state . episode_state . get ( item . name , {}) ] operable_agents = {} for agent_name , agent in self . agent_dict . items (): if agent . platform_name in operable_platform_names : operable_agents [ agent_name ] = agent else : agent . set_removed ( True ) return operable_agents def step ( self , action_dict : dict ) -> typing . Tuple [ OrderedDict , OrderedDict , OrderedDict , OrderedDict ]: # pylint: disable=R0912, R0914, R0915 \"\"\"Returns observations from ready agents. The returns are dicts mapping from agent_id strings to values. The number of agents in the env can vary over time. obs (StateDict): New observations for each ready agent. episode is just started, the value will be None. dones (StateDict): Done values for each ready agent. The special key \"__all__\" (required) is used to indicate env termination. infos (StateDict): Optional info values for each agent id. \"\"\" self . _episode_length += 1 operable_agents = self . _get_operable_agents () # look to add this bugsplat, but this check won't work for multi fps # if set(operable_agents.keys()) != set(action_dict.keys()): # raise RuntimeError(\"Operable_agents and action_dict keys differ!\" # f\"operable={set(operable_agents.keys())} != act={set(action_dict.keys())} \" # \"If this happens that means either your platform is not setting non operable correctly\" # \" (if extra keys are in operable set) or you do not have a done condition covering \" # \"a condition where your platform is going non operable. (if extra keys in act)\") if self . _skip_action : raw_action_dict = {} else : raw_action_dict = self . __apply_action ( operable_agents , action_dict ) # Save current action for future debugging self . _actions . append ( action_dict ) try : self . _state = self . _simulator . step () except ValueError as err : self . _save_state_pickle ( err ) # MTB - Changing to not replace operable_agents variable # We calculate observations on agents operable after sim step # - This is done because otherwise observations would be invalid # Calculate Dones/Rewards on agents operable before sim step # - This is done because if an agent \"dies\" it needs to have a final done calculated operable_agents_after_step = self . _get_operable_agents () ##################################################################### # get next observations - For each configured platform read the # observations/measurements ##################################################################### self . _obs_buffer . next_observation = self . __get_observations_from_glues ( operable_agents_after_step . keys ()) self . _info . clear () self . __get_info_from_glue ( operable_agents_after_step . keys ()) ##################################################################### # Process the done conditions # 1. Reset the rewards from the last step # 2. loops over all agents and processes the reward conditions per # agent ##################################################################### agents_done = self . __get_done_from_agents ( operable_agents . keys (), raw_action_dict = raw_action_dict ) expected_done_keys = set ( operable_agents . keys ()) expected_done_keys . add ( '__all__' ) if set ( agents_done . keys ()) != expected_done_keys : raise RuntimeError ( f 'Local dones do not match expected keys. Received \" { agents_done . keys () } \". Expected \" { expected_done_keys } \".' ) # compute if done all if not agents_done [ '__all__' ]: agent_dones = [ v for k , v in agents_done . items () if k != '__all__' ] if self . config . end_episode_on_first_agent_done : agents_done [ '__all__' ] = any ( agent_dones ) else : agents_done [ '__all__' ] = all ( agent_dones ) shared_dones , shared_done_info = self . _shared_done ( observation = self . _obs_buffer . observation , action = raw_action_dict , next_observation = self . _obs_buffer . next_observation , next_state = self . _state , observation_space = self . _observation_space , observation_units = self . _observation_units , local_dones = copy . deepcopy ( agents_done ), local_done_info = copy . deepcopy ( self . _done_info ) ) if shared_dones . keys (): if set ( shared_dones . keys ()) != expected_done_keys : raise RuntimeError ( f 'Shared dones do not match expected keys. Received \" { shared_dones . keys () } \". Expected \" { expected_done_keys } \".' ) for key in expected_done_keys : agents_done [ key ] |= shared_dones [ key ] assert shared_done_info is not None local_done_info_keys = set ( self . _done_info . keys ()) shared_done_info_keys = set ( shared_done_info ) common_keys = local_done_info_keys & shared_done_info_keys if common_keys : raise RuntimeError ( f 'Dones have common names: \" { common_keys } \"' ) for done_name , done_keys in shared_done_info . items (): for agent_name , done_status in done_keys . items (): self . _done_info [ agent_name ][ done_name ] = OrderedDict ([( self . agent_dict [ agent_name ] . platform_name , done_status )]) # compute if done all if not agents_done [ '__all__' ]: agent_dones = [ v for k , v in agents_done . items () if k != '__all__' ] if self . config . end_episode_on_first_agent_done : agents_done [ '__all__' ] = any ( agent_dones ) else : agents_done [ '__all__' ] = all ( agent_dones ) # Tell the simulator to mark the episode complete if agents_done [ '__all__' ]: self . _simulator . mark_episode_done ( self . _done_info , self . _state . episode_state ) self . _reward . reset () if agents_done [ '__all__' ]: agents_to_process_this_timestep = list ( operable_agents . keys ()) else : def do_process_agent ( self , agent_id ) -> bool : frame_rate = self . _agent_periods [ agent_id ] . numerator / self . _agent_periods [ agent_id ] . denominator return self . _state . sim_time >= self . _agent_process_time [ agent_id ] + frame_rate - self . config . timestep_epsilon agents_to_process_this_timestep = list ( filter ( partial ( do_process_agent , self ), operable_agents . keys ())) for agent_id in agents_to_process_this_timestep : self . _agent_process_time [ agent_id ] = self . _state . sim_time reward = self . __get_reward_from_agents ( agents_to_process_this_timestep , raw_action_dict = raw_action_dict ) self . _simulator . save_episode_information ( self . done_info , self . reward_info , self . _obs_buffer . observation ) # copy over observation from next to previous - There is no real reason to deep # copy here. The process of getting a new observation from the glue copies. All # we need to do is maintain the order of two buffers!!!. # Tested with: They are different and decreasing as expected # print(f\"C: {self._obs_buffer.observation['blue0']['ObserveSensor_Sensor_Fuel']}\") # print(f\"N: {self._obs_buffer.next_observation['blue0']['ObserveSensor_Sensor_Fuel']}\") self . _obs_buffer . update_obs_pointer () # Sanity checks and Scale - ensure run first time and run only every N times... # Same as RLLIB - This can add a bit of time as we are exploring complex dictionaries # default to every time if not specified... Once the limits are good we it is # recommended to increase this for training if self . config . deep_sanity_check : if self . _episode_length % self . config . sanity_check_obs == 0 : try : self . __sanity_check ( self . _observation_space , self . _obs_buffer . observation ) except ValueError as err : self . _save_state_pickle ( err ) else : if not self . _observation_space . contains ( self . _obs_buffer . observation ): raise ValueError ( 'obs not contained in obs space' ) complete_trainable_observations , complete_unnormalized_observations = self . create_training_observations ( operable_agents , self . _obs_buffer ) trainable_observations = OrderedDict () for agent_id in agents_to_process_this_timestep : trainable_observations [ agent_id ] = complete_trainable_observations [ agent_id ] trainable_rewards = get_dictionary_subset ( reward , agents_to_process_this_timestep ) trainable_dones = get_dictionary_subset ( agents_done , [ \"__all__\" ] + agents_to_process_this_timestep ) trainable_info = get_dictionary_subset ( self . _info , agents_to_process_this_timestep ) # add platform obs and env data to trainable_info (for use by custom policies) for agent_id in agents_to_process_this_timestep : if agent_id not in trainable_info : trainable_info [ agent_id ] = {} trainable_info [ agent_id ][ 'env' ] = { 'sim_period' : self . sim_period } trainable_info [ agent_id ][ 'platform_obs' ] = {} plat_name = self . agent_dict [ agent_id ] . platform_name for platform_agent in self . agent_dict : if self . agent_dict [ platform_agent ] . platform_name == plat_name : trainable_info [ agent_id ][ 'platform_obs' ][ platform_agent ] = complete_unnormalized_observations [ platform_agent ] # if not done all, delete any platforms from simulation that are done, so they don't interfere platforms_deleted = set () if not agents_done [ '__all__' ]: for agent_key , value in agents_done . items (): if agent_key != '__all__' and value : plat_name = self . agent_dict [ agent_key ] . platform_name if plat_name not in platforms_deleted : self . simulator . delete_platform ( plat_name ) platforms_deleted . add ( plat_name ) # if a platform has been deleted, we need to make sure that all agents on that platform # are also done for agent_key in trainable_dones . keys (): if agent_key == '__all__' : continue plat_name = self . agent_dict [ agent_key ] . platform_name if plat_name in platforms_deleted : trainable_dones [ agent_key ] = True ##################################################################### # return results to RLLIB - Note that RLLIB does not do a recursive # isinstance call and as such need to make sure items are # OrderedDicts ##################################################################### return trainable_observations , trainable_rewards , trainable_dones , trainable_info def __get_done_from_agents ( self , alive_agents : typing . Iterable [ str ], raw_action_dict ): def or_merge ( config , path , base , nxt ): # pylint: disable=unused-argument return base or nxt merge_strategies = copy . deepcopy ( deepmerge . DEFAULT_TYPE_SPECIFIC_MERGE_STRATEGIES ) merge_strategies . append (( bool , or_merge )) or_merger = deepmerge . Merger ( merge_strategies , [], []) done = OrderedDict () done [ \"__all__\" ] = False for agent_id in alive_agents : agent_class = self . agent_dict [ agent_id ] platform_done , done_info = agent_class . get_dones ( observation = self . _obs_buffer . observation , action = raw_action_dict , next_observation = self . _obs_buffer . next_observation , next_state = self . _state , observation_space = self . _observation_space , observation_units = self . _observation_units ) done [ agent_id ] = platform_done [ agent_class . platform_name ] # get around reduction done [ \"__all__\" ] = done [ \"__all__\" ] if done [ \"__all__\" ] else platform_done . get ( \"__all__\" , False ) or_merger . merge ( self . _done_info . setdefault ( agent_id , {}), done_info ) # self._done_info[agent_id] = done_info return done def __get_reward_from_agents ( self , alive_agents : typing . Iterable [ str ], raw_action_dict ): reward = OrderedDict () for agent_id in alive_agents : agent_class = self . agent_dict [ agent_id ] agent_reward , reward_info = agent_class . get_rewards ( observation = self . _obs_buffer . observation , action = raw_action_dict , next_observation = self . _obs_buffer . next_observation , state = self . _state , next_state = self . _state , observation_space = self . _observation_space , observation_units = self . _observation_units ) # it is possible to have a HL policy that does not compute an reward # in this case just return a zero for reward value if agent_id in agent_reward : reward [ agent_id ] = agent_reward [ agent_id ] else : reward [ agent_id ] = 0 self . _reward_info [ agent_id ] = reward_info return reward def set_default_done_reward ( self ): \"\"\" Populate the done/rewards with default values \"\"\" for key in self . agent_dict . keys (): # pylint: disable=C0201 self . _done [ key ] = False self . _reward [ key ] = 0 # pylint: disable=protected-access self . _shared_done [ key ] = False self . _done [ DoneFuncBase . _ALL ] = False # pylint: disable=protected-access self . _shared_done [ DoneFuncBase . _ALL ] = False # pylint: disable=protected-access def create_training_observations ( self , alive_agents : typing . Iterable [ str ], observations : ObsBuffer ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" Filters and normalizes observations (the sample of the space) using the glue normalize functions. Parameters ---------- alive_agents: The agents that are still alive observations: The observations Returns ------- OrderedDict: the filtered/normalized observation samples \"\"\" this_steps_obs = OrderedDict () for agent_id in alive_agents : if agent_id in observations . observation : this_steps_obs [ agent_id ] = observations . observation [ agent_id ] elif agent_id in observations . next_observation : this_steps_obs [ agent_id ] = observations . next_observation [ agent_id ] else : raise RuntimeError ( \"ERROR: create_training_observations tried to retrieve obs for this training step\" f \" but { agent_id =} was not able to be found in either the current obs data or the \" \" obs from the previous timestep as a fallback\" ) def do_export ( agent_id , obs_name , _obs ): if agent_id in alive_agents : glue_obj = self . agent_dict [ agent_id ] . get_glue ( obs_name ) if glue_obj is not None : return glue_obj . config . training_export_behavior == TrainingExportBehavior . INCLUDE return False filtered_observations = filter_observations ( this_steps_obs , do_export ) normalized_observations = mutate_observations ( filtered_observations , lambda agent_id , # type: ignore obs_name , obs : self . agent_dict [ agent_id ] . normalize_observation ( obs_name , obs ) # type: ignore ) return normalized_observations , filtered_observations def __get_observations_from_glues ( self , alive_agents : typing . Iterable [ str ]) -> OrderedDict : # pylint: disable=protected-access \"\"\" Gets the observation dict from all the glue objects for each agent Returns ------- OrderedDict: The observation dict from all the glues \"\"\" return_observation : OrderedDict = OrderedDict () p_names = [ item . name for item in self . _state . sim_platforms ] for agent_id in alive_agents : agent_class = self . agent_dict [ agent_id ] # TODO: Why is this check required here? # Why does 'alive_agents' contain agents with platforms that don't exist (or have been removed)? # This should only happpen if the user has accidentally passed in 'dead' agents if agent_class . platform_name not in p_names : self . _logger . warning ( f \" { agent_id } on { agent_class . platform_name } is not in the list of (alive) sim_platforms: { self . _state . sim_platforms } \" ) agent_class . set_removed ( True ) else : glue_obj_obs = agent_class . get_observations () if len ( glue_obj_obs ) > 0 : return_observation [ agent_id ] = glue_obj_obs return return_observation def __apply_action ( self , operable_agents , action_dict ): raw_action_dict = OrderedDict () for agent_id , agent_class in operable_agents . items (): if agent_id in action_dict : raw_action_dict [ agent_id ] = agent_class . apply_action ( action_dict [ agent_id ]) return raw_action_dict def __get_info_from_glue ( self , alive_agents : typing . Iterable [ str ]): for agent_id in alive_agents : agent_class = self . agent_dict [ agent_id ] glue_obj_info = agent_class . get_info_dict () if len ( glue_obj_info ) > 0 : self . _info [ agent_id ] = glue_obj_info def _get_observation_units_from_glues ( self ) -> OrderedDict : # pylint: disable=protected-access \"\"\" Gets the observation dict from all the glue objects for each agent Returns ------- OrderedDict: The observation dict from all the glues \"\"\" return_observation : OrderedDict = OrderedDict () p_names = [ item . name for item in self . _state . sim_platforms ] for agent_id , glue_name_obj_pair in self . _agent_glue_dict . items (): for glue_name , glue_object in glue_name_obj_pair . items (): if glue_object . _agent_id not in p_names : # pylint: disable=protected-access glue_object . set_agent_removed ( True ) try : glue_obj_obs = glue_object . observation_units () except AttributeError : glue_obj_obs = None return_observation . setdefault ( agent_id , OrderedDict ())[ glue_name ] = glue_obj_obs return return_observation def _make_glues ( self ) -> None : \"\"\" \"\"\" env_ref_stores = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] plat_to_agent : typing . Dict [ str , typing . List [ str ]] = defaultdict ( lambda : []) for agent , agent_class in self . agent_dict . items (): # get the platform for this agent plat = self . _get_platform_by_name ( agent_class . platform_name ) plat_to_agent [ plat . name ] . append ( agent ) agent_class . make_glues ( plat , agent , env_ref_stores = env_ref_stores ) if self . config . simulator . config . get ( \"disable_exclusivity_check\" , False ): return def itr_controller_glues ( glue ): if isinstance ( glue , BaseAgentControllerGlue ): yield glue if isinstance ( glue , BaseWrapperGlue ): yield from itr_controller_glues ( glue . glue ()) if isinstance ( glue , BaseDictWrapperGlue ): for g in glue . glues () . values (): yield from itr_controller_glues ( g ) if isinstance ( glue , BaseMultiWrapperGlue ): for g in glue . glues (): yield from itr_controller_glues ( g ) # validate for plat_name , agents in plat_to_agent . items (): exclusiveness : typing . Set [ str ] = set () for agent in agents : agent_class = self . agent_dict [ agent ] for glue in agent_class . agent_glue_dict . values (): for controller_glue in itr_controller_glues ( glue ): assert isinstance ( controller_glue , ControllerGlue ), ( f \"Unknown controller glue type { type ( controller_glue ) } on platform { plat_name } \" ) controller_exclusiveness = controller_glue . controller . exclusiveness assert len ( controller_exclusiveness . intersection ( exclusiveness ) ) == 0 , ( f \"Controllers not mutually exclusive on platform { plat_name } \" ) exclusiveness . update ( controller_exclusiveness ) def _make_rewards ( self ) -> None : \"\"\" \"\"\" env_ref_stores = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] for agent , agent_class in self . agent_dict . items (): agent_class . make_rewards ( agent , env_ref_stores = env_ref_stores ) def _make_dones ( self ) -> None : \"\"\" \"\"\" env_ref_stores = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] warmup_steps = self . config . sim_warmup_steps episode_length_done = Functor ( functor = EpisodeLengthDone , config = { 'horizon' : { 'value' : ( self . config . horizon + warmup_steps ) * self . sim_period , 'units' : 'second' }} ) for agent , agent_class in self . agent_dict . items (): env_dones = chain ( self . config . dones . world , self . config . dones . task [ agent_class . platform_name ], [ episode_length_done ]) env_params = [ self . local_variable_store . get ( 'world' , {}), self . local_variable_store . get ( 'task' , {}) . get ( agent_class . platform_name , {}) ] agent_class . make_dones ( agent , agent_class . platform_name , dones = env_dones , env_params = env_params , env_ref_stores = env_ref_stores ) def _make_shared_dones ( self ) -> DoneDict : # pylint: disable=no-self-use \"\"\" _get_shared_done_functors gets and initializes the shared done dict used for this iteration The shared done dictionary does not correspond to individual agents but looks sharedly at all agents. this will be called after any updates to the simulator configuration during reset Returns ------- DoneDict The DoneDict with functors used for this iteration \"\"\" done_conditions = [] ref_sources = [ self . local_variable_store . get ( 'reference_store' , {}), self . config . reference_store ] param_sources = [ self . local_variable_store . get ( 'shared' , {})] for done_functor in self . config . dones . shared : tmp = done_functor . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources ) done_conditions . append ( tmp ) return DoneDict ( processing_funcs = done_conditions ) @staticmethod def _create_actions ( agents , observations = {}, rewards = {}, dones = {}, info = {}): # pylint: disable=dangerous-default-value for agent_name , agent_class in agents . items (): agent_class . create_next_action ( observations . get ( agent_name ), rewards . get ( agent_name ), dones . get ( agent_name ), info . get ( agent_name ) ) def __create_space ( self , space_getter ) -> gym . spaces . Dict : \"\"\" _create_space Create spaces for all agents by calling their glue objects Parameters ---------- space_getter A function that takes a glue_obj: BaseAgentGlue and returns the space for creating this space For example space_getter=lambda glue_obj: glue_obj.observation_space() Returns ------- A space build from all the glues for each agent in this Environment \"\"\" # init our return Dict return_space = gym . spaces . dict . Dict () # loop over all the agents and their glue_name_obj_pairs list for agent_id , agent_class in self . agent_dict . items (): tmp_space = agent_class . create_space ( space_getter ) # if this agent provided anything add it to the return space if tmp_space : return_space . spaces [ agent_id ] = tmp_space return return_space @property def action_space ( self ) -> gym . spaces . Space : \"\"\" action_space The action space Returns ------- typing.Dict[str,gym.spaces.tuple.Tuple] The action space \"\"\" return self . _normalized_action_space ############ # Properties ############ @property def glue_info ( self ) -> OrderedDict : \"\"\"[summary] Returns: Union[OrderedDict, None] -- [description] \"\"\" return self . _info @property def done_info ( self ) -> OrderedDict : \"\"\"[summary] Returns ------- Union[OrderedDict, None] [description] \"\"\" return self . _done_info @property def reward_info ( self ) -> OrderedDict : \"\"\"[summary] Returns ------- Union[OrderedDict, None] [description] \"\"\" return self . _reward_info @property def observation_space ( self ) -> gym . spaces . Space : \"\"\" observation_space The observation space setup by the user Returns ------- gym.spaces.dict.Dict The observation space \"\"\" return self . _normalized_observation_space @property def state ( self ) -> StateDict : \"\"\" state of platform object. Current state. Returns ------- StateDict the dict storing the curent state of environment. \"\"\" return self . _state @property def simulator ( self ) -> BaseSimulator : \"\"\" simulator simulator instance Returns ------- BaseSimulator The simulator instance in the base \"\"\" return self . _simulator @property def observation ( self ) -> OrderedDict : \"\"\" observation get the observation for the agents in this environment Returns ------- OrderedDict the dict holding the observations for the agents \"\"\" return self . _obs_buffer . observation @property def episode_id ( self ) -> typing . Union [ int , None ]: \"\"\" get the current episode parameter provider episode id Returns ------- int or None the episode id \"\"\" return self . _episode_id def _get_platform_by_name ( self , platform_id : str ) -> BasePlatform : platform : BasePlatform = None # type: ignore for plat in self . _state . sim_platforms : if plat . name == platform_id : platform = plat if platform is None or not issubclass ( platform . __class__ , BasePlatform ): self . _logger . error ( \"-\" * 100 ) self . _logger . error ( f \" { self . _state } \" ) for i in self . _state . sim_platforms : self . _logger . error ( f \" { i . name } \" ) raise ValueError ( f \" { self . __class__ . __name__ } glue could not find a platform named { platform_id } of class BasePlatform\" ) return platform def __setup_state_history ( self ): self . _state . episode_history = defaultdict ( partial ( deque , maxlen = self . config . horizon )) self . _state . episode_state = OrderedDict () self . _state . step_state = OrderedDict () def post_process_trajectory ( self , agent_id , batch , episode , policy ): \"\"\"easy accessor for calling post process trajectory correctly Arguments: agent_id {[type]} -- agent id batch {[type]} -- post processed Batch - be careful modifying \"\"\" self . agent_dict [ agent_id ] . post_process_trajectory ( agent_id , episode . worker . env . _state , # pylint: disable=protected-access batch , episode , policy , episode . worker . env . _reward_info # pylint: disable=protected-access ) @staticmethod def __sanity_check ( space : gym . spaces . Space , space_sample : EnvSpaceUtil . sample_type ) -> None : \"\"\" Sanity checks a space_sample against a space 1. Check to ensure that the sample from the integration base Fall within the expected range of values. Note: space_sample and space expected to match up on Key level entries Parameters ---------- space: gym.spaces.Space the space to check the sample against space_sample: EnvSpaceUtil.sample_type the sample to check if it is actually in the bounds of the space Returns ------- OrderedDict: the scaled_observations \"\"\" if not space . contains ( space_sample ): space . contains ( space_sample ) EnvSpaceUtil . deep_sanity_check_space_sample ( space , space_sample ) def _save_state_pickle ( self , err : ValueError ): \"\"\"saves state for later debug Arguments: err {ValueError} -- Traceback for the error Raises: err: Customized error message to raise for the exception \"\"\" out_pickle = str ( self . config . output_path / f \"sanity_check_failure_ { self . _episode } .pkl\" ) p_dict : typing . Dict [ str , typing . Any ] = {} p_dict [ \"err\" ] = str ( err ) class NumpyArrayEncoder ( JSONEncoder ): \"\"\"Encode the numpy types for json \"\"\" def default ( self , obj ): # pylint: disable=arguments-differ val = None if isinstance ( obj , ( np . int_ , np . intc , np . intp , np . int8 , np . int16 , np . int32 , np . int64 , np . uint8 , np . uint16 , np . uint32 , np . uint64 , ), ): val = int ( obj ) elif isinstance ( obj , ( np . float_ , np . float16 , np . float32 , np . float64 )): val = float ( obj ) elif isinstance ( obj , ( np . ndarray , )): # This is the fix val = obj . tolist () elif isinstance ( obj , np . bool_ ): val = 'True' if obj is True else 'False' else : val = json . JSONEncoder . default ( self , obj ) return val def to_dict ( input_ordered_dict ): return loads ( dumps ( input_ordered_dict , cls = NumpyArrayEncoder )) p_dict [ 'action' ] = self . _actions # type: ignore p_dict [ \"observation\" ] = self . _obs_buffer . observation # type: ignore p_dict [ \"dones\" ] = to_dict ( self . _done_info ) # type: ignore # p_dict[\"env_config\"] = copy.deepcopy(self.env_config) # type: ignore p_dict [ \"step\" ] = str ( self . _episode_length ) pickle . dump ( p_dict , open ( out_pickle , \"wb\" )) raise ValueError ( f \"Error occurred: { err } \\n Saving sanity check failure output pickle to file: { out_pickle } \" ) def seed ( self , seed = None ): \"\"\"generates environment seed through rllib Keyword Arguments: seed {[int]} -- seed to set environment with (default: {None}) Returns: [int] -- [seed value] \"\"\" if not hasattr ( self , \"rng\" ): self . rng , self . config . seed = gym . utils . seeding . np_random ( seed ) return [ self . config . seed ]","title":"ACT3MultiAgentEnv"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.action_space","text":"action_space The action space","title":"action_space"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.action_space--returns","text":"typing.Dict[str,gym.spaces.tuple.Tuple] The action space","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.done_info","text":"[summary]","title":"done_info"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.done_info--returns","text":"Union[OrderedDict, None] [description]","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.episode_id","text":"get the current episode parameter provider episode id","title":"episode_id"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.episode_id--returns","text":"int or None the episode id","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.get_validator","text":"Get the validator for this class.","title":"get_validator"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.glue_info","text":"[summary] Returns: Type Description OrderedDict Union[OrderedDict, None] -- [description]","title":"glue_info"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.observation","text":"observation get the observation for the agents in this environment","title":"observation"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.observation--returns","text":"OrderedDict the dict holding the observations for the agents","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.observation_space","text":"observation_space The observation space setup by the user","title":"observation_space"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.observation_space--returns","text":"gym.spaces.dict.Dict The observation space","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.reward_info","text":"[summary]","title":"reward_info"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.reward_info--returns","text":"Union[OrderedDict, None] [description]","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.simulator","text":"simulator simulator instance","title":"simulator"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.simulator--returns","text":"BaseSimulator The simulator instance in the base","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.state","text":"state of platform object. Current state.","title":"state"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.state--returns","text":"StateDict the dict storing the curent state of environment.","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.__init__","text":"init initializes the rllib multi agent environment","title":"__init__()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.__init__--parameters","text":"config : ray.rllib.env.env_context.EnvContext Passed in configuration for setting items up. Must have a 'simulator' key whose value is a BaseIntegrator type Source code in corl/environment/multi_agent_env.py def __init__ ( self , config : EnvContext ) -> None : # pylint: disable=too-many-statements, super-init-not-called \"\"\" __init__ initializes the rllib multi agent environment Parameters ---------- config : ray.rllib.env.env_context.EnvContext Passed in configuration for setting items up. Must have a 'simulator' key whose value is a BaseIntegrator type \"\"\" try : config_vars = vars ( config ) except TypeError : config_vars = {} self . config : ACT3MultiAgentEnvValidator = self . get_validator ( ** config , ** config_vars ) # Random numbers self . seed ( self . config . seed ) # setup default instance variables self . _actions : list = [] self . _obs_buffer = ObsBuffer () self . _reward : RewardDict = RewardDict () self . _done : DoneDict = DoneDict () self . _info : OrderedDict = OrderedDict () self . _episode_length : int = 0 self . _episode : int = 0 self . _episode_id : typing . Union [ int , None ] # agent glue dict is a mapping from agent id to a dict with keys for the glue names # and values of the actual glue object self . _agent_glue_dict : OrderedDict = OrderedDict () self . _agent_glue_obs_export_behavior : OrderedDict = OrderedDict () # Create the logger self . _logger = logging . getLogger ( ACT3MultiAgentEnv . __name__ ) # Extra simulation init args # assign the new output_path with the worker index back to the config for the sim/integration output_path extra_sim_init_args : typing . Dict [ str , typing . Any ] = { \"output_path\" : str ( self . config . output_path ), \"worker_index\" : self . config . worker_index , \"vector_index\" : self . config . vector_index if self . config . vector_index else 0 , } self . agent_dict , extra_sim_init_args [ \"agent_configs\" ] = env_creation . create_agent_sim_configs ( self . config . agents , self . config . agent_platforms , self . config . simulator . type , self . config . platforms , self . config . epp_registry , multiple_workers = ( self . config . num_workers > 0 ) ) def compute_lcm ( values : typing . List [ fractions . Fraction ]) -> float : assert len ( values ) > 0 lcm = values [ 0 ] . denominator for v in values : lcm = lcm // math . gcd ( lcm , v . denominator ) * v . denominator return 1.0 / lcm max_rate = self . config . max_agent_rate self . _agent_periods = { agent_id : fractions . Fraction ( 1.0 / agent . frame_rate ) . limit_denominator ( max_rate ) for agent_id , agent in self . agent_dict . items () } self . _agent_process_time : typing . Dict [ str , float ] = defaultdict ( lambda : sys . float_info . min ) self . sim_period = compute_lcm ( list ( self . _agent_periods . values ())) extra_sim_init_args [ 'frame_rate' ] = 1.0 / self . sim_period for agent_name , platform in self . config . other_platforms . items (): extra_sim_init_args [ \"agent_configs\" ][ agent_name ] = { \"platform_config\" : platform , \"parts_list\" : [], } # Debug logging self . _logger . debug ( f \"output_path : { self . config . output_path } \" ) # Sample parameter provider default_parameters = self . config . epp . config . parameters self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( self . rng ) for k , v in default_parameters . items ()}) for agent in self . agent_dict . values (): agent . fill_parameters ( rng = self . rng , default_parameters = True ) # Create the simulator for this gym environment # ---- oddity from other simulator bases HLP if not hasattr ( self , \"_simulator\" ): class SimulatorWrapper ( self . config . simulator . type ): # type: ignore \"\"\"Wrapper that injects platforms/time into state dict\"\"\" def _clear_data ( self ) -> None : if 'sim_time' in self . _state : del self . _state [ 'sim_time' ] if 'sim_platforms' in self . _state : del self . _state [ 'sim_platforms' ] def _inject_data ( self , state : StateDict ) -> StateDict : \"\"\"Ensures that time/platforms exists in state\"\"\" if 'sim_time' not in state : state [ 'sim_time' ] = self . sim_time if 'sim_platforms' not in state : state [ 'sim_platforms' ] = self . platforms return state def step ( self ) -> StateDict : \"\"\"Steps the simulation - injects data into StateDict\"\"\" self . _clear_data () return self . _inject_data ( super () . step ()) def reset ( self , * args , ** kwargs ) -> StateDict : \"\"\"Resets the simulation - injects data into StateDict\"\"\" self . _clear_data () return self . _inject_data ( super () . reset ( * args , ** kwargs )) simulator_factory = copy . deepcopy ( self . config . simulator ) simulator_factory . type = SimulatorWrapper # type: ignore self . _simulator : BaseSimulator = simulator_factory . build ( ** extra_sim_init_args ) self . _state , self . _sim_reset_args = self . _reset_simulator ( extra_sim_init_args [ \"agent_configs\" ]) # Make the glue objects from the glue mapping now that we have a simulator created self . _make_glues () # create dictionary to hold done history self . __setup_state_history () # Create the observation and action space now that we have the glue self . _observation_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . observation_space ()) self . _action_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . action_space ()) gym_space_sort ( self . _action_space ) self . _normalized_observation_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . normalized_observation_space () if glue_obj . config . training_export_behavior == TrainingExportBehavior . INCLUDE else None ) self . _normalized_action_space : gym . spaces . Dict = self . __create_space ( space_getter = lambda glue_obj : glue_obj . normalized_action_space () ) gym_space_sort ( self . _normalized_action_space ) self . _observation_units = self . __create_space ( space_getter = lambda glue_obj : glue_obj . observation_units () if hasattr ( glue_obj , \"observation_units\" ) else None ) self . _shared_done : DoneDict = DoneDict () self . _done_info : OrderedDict = OrderedDict () self . _reward_info : OrderedDict = OrderedDict () self . _episode_init_params : dict self . done_string = \"\" self . _agent_ids = set ( self . _action_space . spaces . keys ()) self . _skip_action = False","title":"Parameters"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.create_training_observations","text":"Filters and normalizes observations (the sample of the space) using the glue normalize functions.","title":"create_training_observations()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.create_training_observations--parameters","text":"Alive_agents The agents that are still alive Observations The observations","title":"Parameters"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.create_training_observations--returns","text":"Ordereddict the filtered/normalized observation samples Source code in corl/environment/multi_agent_env.py def create_training_observations ( self , alive_agents : typing . Iterable [ str ], observations : ObsBuffer ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" Filters and normalizes observations (the sample of the space) using the glue normalize functions. Parameters ---------- alive_agents: The agents that are still alive observations: The observations Returns ------- OrderedDict: the filtered/normalized observation samples \"\"\" this_steps_obs = OrderedDict () for agent_id in alive_agents : if agent_id in observations . observation : this_steps_obs [ agent_id ] = observations . observation [ agent_id ] elif agent_id in observations . next_observation : this_steps_obs [ agent_id ] = observations . next_observation [ agent_id ] else : raise RuntimeError ( \"ERROR: create_training_observations tried to retrieve obs for this training step\" f \" but { agent_id =} was not able to be found in either the current obs data or the \" \" obs from the previous timestep as a fallback\" ) def do_export ( agent_id , obs_name , _obs ): if agent_id in alive_agents : glue_obj = self . agent_dict [ agent_id ] . get_glue ( obs_name ) if glue_obj is not None : return glue_obj . config . training_export_behavior == TrainingExportBehavior . INCLUDE return False filtered_observations = filter_observations ( this_steps_obs , do_export ) normalized_observations = mutate_observations ( filtered_observations , lambda agent_id , # type: ignore obs_name , obs : self . agent_dict [ agent_id ] . normalize_observation ( obs_name , obs ) # type: ignore ) return normalized_observations , filtered_observations","title":"Returns"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.post_process_trajectory","text":"easy accessor for calling post process trajectory correctly Source code in corl/environment/multi_agent_env.py def post_process_trajectory ( self , agent_id , batch , episode , policy ): \"\"\"easy accessor for calling post process trajectory correctly Arguments: agent_id {[type]} -- agent id batch {[type]} -- post processed Batch - be careful modifying \"\"\" self . agent_dict [ agent_id ] . post_process_trajectory ( agent_id , episode . worker . env . _state , # pylint: disable=protected-access batch , episode , policy , episode . worker . env . _reward_info # pylint: disable=protected-access )","title":"post_process_trajectory()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.reset","text":"Resets the env and returns observations from ready agents. Returns: Type Description New observations for each ready agent. Examples: >>> from ray.rllib.env.multi_agent_env import MultiAgentEnv >>> class MyMultiAgentEnv ( MultiAgentEnv ): ... # Define your env here. ... ... >>> env = MyMultiAgentEnv () >>> obs = env . reset () >>> print ( obs ) { \"car_0\" : [ 2.4 , 1.6 ], \"car_1\" : [ 3.4 , - 3.2 ], \"traffic_light_1\" : [ 0 , 3 , 5 , 1 ], } Source code in corl/environment/multi_agent_env.py def reset ( self ): # Sample parameter provider current_parameters , self . _episode_id = self . config . epp . get_params ( self . rng ) self . local_variable_store = flatten_dict . unflatten ({ k : v . get_value ( self . rng ) for k , v in current_parameters . items ()}) for agent in self . agent_dict . values (): agent . fill_parameters ( self . rng ) # 3. Reset the Done and Reward dictionaries for the next iteration self . _make_rewards () self . _make_dones () self . _shared_done : DoneDict = self . _make_shared_dones () self . _reward : RewardDict = RewardDict () self . _done : DoneDict = DoneDict () self . _done_info . clear () self . set_default_done_reward () # 4. Reset the simulation/integration self . _state , self . _sim_reset_args = self . _reset_simulator () self . _episode_length = 0 self . _actions . clear () self . _episode += 1 self . _agent_process_time . clear () ##################################################################### # Make glue sections - Given the state of the simulation we need to # update the platform interfaces. ##################################################################### self . _make_glues () ##################################################################### # get observations # For each configured agent read the observations/measurements ##################################################################### agent_list = list ( self . agent_dict . keys ()) self . _obs_buffer . next_observation = self . __get_observations_from_glues ( agent_list ) self . _obs_buffer . update_obs_pointer () # The following loop guarantees that durring training that the glue # states start with valid values for rates. The number of recommended # steps for sim is at least equal to the depth of the rate observation # tree (ex: speed - 2, acceleration - 3, jerk - 4) - recommend defaulting # to 4 as we do not go higher thank jerk # 1 step is always added for the inital obs in reset warmup = self . config . sim_warmup_steps for _ in range ( warmup ): self . _state = self . _simulator . step () self . _obs_buffer . next_observation = self . __get_observations_from_glues ( agent_list ) self . _obs_buffer . update_obs_pointer () self . __setup_state_history () for platform in self . _state . sim_platforms : self . _state . step_state [ platform . name ] = None self . _state . episode_history [ platform . name ] . clear () self . _state . episode_state [ platform . name ] = OrderedDict () # Sanity Checks and Scale # The current deep sanity check will not raise error if values are from sample are different from space during reset if self . config . deep_sanity_check : try : self . __sanity_check ( self . _observation_space , self . _obs_buffer . observation ) except ValueError as err : self . _save_state_pickle ( err ) else : if not self . _observation_space . contains ( self . _obs_buffer . observation ): raise ValueError ( 'obs not contained in obs space' ) self . _create_actions ( self . agent_dict , self . _obs_buffer . observation ) ##################################################################### # return results to RLLIB - Note that RLLIB does not do a recursive # isinstance call and as such need to make sure items are # OrderedDicts ##################################################################### trainable_observations , _ = self . create_training_observations ( agent_list , self . _obs_buffer ) return trainable_observations","title":"reset()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.seed","text":"generates environment seed through rllib Keyword arguments: Name Type Description seed {[int]} -- seed to set environment with (default {None}) Returns: Type Description [int] -- [seed value] Source code in corl/environment/multi_agent_env.py def seed ( self , seed = None ): \"\"\"generates environment seed through rllib Keyword Arguments: seed {[int]} -- seed to set environment with (default: {None}) Returns: [int] -- [seed value] \"\"\" if not hasattr ( self , \"rng\" ): self . rng , self . config . seed = gym . utils . seeding . np_random ( seed ) return [ self . config . seed ]","title":"seed()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.set_default_done_reward","text":"Populate the done/rewards with default values Source code in corl/environment/multi_agent_env.py def set_default_done_reward ( self ): \"\"\" Populate the done/rewards with default values \"\"\" for key in self . agent_dict . keys (): # pylint: disable=C0201 self . _done [ key ] = False self . _reward [ key ] = 0 # pylint: disable=protected-access self . _shared_done [ key ] = False self . _done [ DoneFuncBase . _ALL ] = False # pylint: disable=protected-access self . _shared_done [ DoneFuncBase . _ALL ] = False # pylint: disable=protected-access","title":"set_default_done_reward()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnv.step","text":"Returns observations from ready agents. The returns are dicts mapping from agent_id strings to values. The number of agents in the env can vary over time. obs ( StateDict ) : New observations for each ready agent . episode is just started , the value will be None . dones ( StateDict ) : Done values for each ready agent . The special key \"__all__\" ( required ) is used to indicate env termination . infos ( StateDict ) : Optional info values for each agent id . Source code in corl/environment/multi_agent_env.py def step ( self , action_dict : dict ) -> typing . Tuple [ OrderedDict , OrderedDict , OrderedDict , OrderedDict ]: # pylint: disable=R0912, R0914, R0915 \"\"\"Returns observations from ready agents. The returns are dicts mapping from agent_id strings to values. The number of agents in the env can vary over time. obs (StateDict): New observations for each ready agent. episode is just started, the value will be None. dones (StateDict): Done values for each ready agent. The special key \"__all__\" (required) is used to indicate env termination. infos (StateDict): Optional info values for each agent id. \"\"\" self . _episode_length += 1 operable_agents = self . _get_operable_agents () # look to add this bugsplat, but this check won't work for multi fps # if set(operable_agents.keys()) != set(action_dict.keys()): # raise RuntimeError(\"Operable_agents and action_dict keys differ!\" # f\"operable={set(operable_agents.keys())} != act={set(action_dict.keys())} \" # \"If this happens that means either your platform is not setting non operable correctly\" # \" (if extra keys are in operable set) or you do not have a done condition covering \" # \"a condition where your platform is going non operable. (if extra keys in act)\") if self . _skip_action : raw_action_dict = {} else : raw_action_dict = self . __apply_action ( operable_agents , action_dict ) # Save current action for future debugging self . _actions . append ( action_dict ) try : self . _state = self . _simulator . step () except ValueError as err : self . _save_state_pickle ( err ) # MTB - Changing to not replace operable_agents variable # We calculate observations on agents operable after sim step # - This is done because otherwise observations would be invalid # Calculate Dones/Rewards on agents operable before sim step # - This is done because if an agent \"dies\" it needs to have a final done calculated operable_agents_after_step = self . _get_operable_agents () ##################################################################### # get next observations - For each configured platform read the # observations/measurements ##################################################################### self . _obs_buffer . next_observation = self . __get_observations_from_glues ( operable_agents_after_step . keys ()) self . _info . clear () self . __get_info_from_glue ( operable_agents_after_step . keys ()) ##################################################################### # Process the done conditions # 1. Reset the rewards from the last step # 2. loops over all agents and processes the reward conditions per # agent ##################################################################### agents_done = self . __get_done_from_agents ( operable_agents . keys (), raw_action_dict = raw_action_dict ) expected_done_keys = set ( operable_agents . keys ()) expected_done_keys . add ( '__all__' ) if set ( agents_done . keys ()) != expected_done_keys : raise RuntimeError ( f 'Local dones do not match expected keys. Received \" { agents_done . keys () } \". Expected \" { expected_done_keys } \".' ) # compute if done all if not agents_done [ '__all__' ]: agent_dones = [ v for k , v in agents_done . items () if k != '__all__' ] if self . config . end_episode_on_first_agent_done : agents_done [ '__all__' ] = any ( agent_dones ) else : agents_done [ '__all__' ] = all ( agent_dones ) shared_dones , shared_done_info = self . _shared_done ( observation = self . _obs_buffer . observation , action = raw_action_dict , next_observation = self . _obs_buffer . next_observation , next_state = self . _state , observation_space = self . _observation_space , observation_units = self . _observation_units , local_dones = copy . deepcopy ( agents_done ), local_done_info = copy . deepcopy ( self . _done_info ) ) if shared_dones . keys (): if set ( shared_dones . keys ()) != expected_done_keys : raise RuntimeError ( f 'Shared dones do not match expected keys. Received \" { shared_dones . keys () } \". Expected \" { expected_done_keys } \".' ) for key in expected_done_keys : agents_done [ key ] |= shared_dones [ key ] assert shared_done_info is not None local_done_info_keys = set ( self . _done_info . keys ()) shared_done_info_keys = set ( shared_done_info ) common_keys = local_done_info_keys & shared_done_info_keys if common_keys : raise RuntimeError ( f 'Dones have common names: \" { common_keys } \"' ) for done_name , done_keys in shared_done_info . items (): for agent_name , done_status in done_keys . items (): self . _done_info [ agent_name ][ done_name ] = OrderedDict ([( self . agent_dict [ agent_name ] . platform_name , done_status )]) # compute if done all if not agents_done [ '__all__' ]: agent_dones = [ v for k , v in agents_done . items () if k != '__all__' ] if self . config . end_episode_on_first_agent_done : agents_done [ '__all__' ] = any ( agent_dones ) else : agents_done [ '__all__' ] = all ( agent_dones ) # Tell the simulator to mark the episode complete if agents_done [ '__all__' ]: self . _simulator . mark_episode_done ( self . _done_info , self . _state . episode_state ) self . _reward . reset () if agents_done [ '__all__' ]: agents_to_process_this_timestep = list ( operable_agents . keys ()) else : def do_process_agent ( self , agent_id ) -> bool : frame_rate = self . _agent_periods [ agent_id ] . numerator / self . _agent_periods [ agent_id ] . denominator return self . _state . sim_time >= self . _agent_process_time [ agent_id ] + frame_rate - self . config . timestep_epsilon agents_to_process_this_timestep = list ( filter ( partial ( do_process_agent , self ), operable_agents . keys ())) for agent_id in agents_to_process_this_timestep : self . _agent_process_time [ agent_id ] = self . _state . sim_time reward = self . __get_reward_from_agents ( agents_to_process_this_timestep , raw_action_dict = raw_action_dict ) self . _simulator . save_episode_information ( self . done_info , self . reward_info , self . _obs_buffer . observation ) # copy over observation from next to previous - There is no real reason to deep # copy here. The process of getting a new observation from the glue copies. All # we need to do is maintain the order of two buffers!!!. # Tested with: They are different and decreasing as expected # print(f\"C: {self._obs_buffer.observation['blue0']['ObserveSensor_Sensor_Fuel']}\") # print(f\"N: {self._obs_buffer.next_observation['blue0']['ObserveSensor_Sensor_Fuel']}\") self . _obs_buffer . update_obs_pointer () # Sanity checks and Scale - ensure run first time and run only every N times... # Same as RLLIB - This can add a bit of time as we are exploring complex dictionaries # default to every time if not specified... Once the limits are good we it is # recommended to increase this for training if self . config . deep_sanity_check : if self . _episode_length % self . config . sanity_check_obs == 0 : try : self . __sanity_check ( self . _observation_space , self . _obs_buffer . observation ) except ValueError as err : self . _save_state_pickle ( err ) else : if not self . _observation_space . contains ( self . _obs_buffer . observation ): raise ValueError ( 'obs not contained in obs space' ) complete_trainable_observations , complete_unnormalized_observations = self . create_training_observations ( operable_agents , self . _obs_buffer ) trainable_observations = OrderedDict () for agent_id in agents_to_process_this_timestep : trainable_observations [ agent_id ] = complete_trainable_observations [ agent_id ] trainable_rewards = get_dictionary_subset ( reward , agents_to_process_this_timestep ) trainable_dones = get_dictionary_subset ( agents_done , [ \"__all__\" ] + agents_to_process_this_timestep ) trainable_info = get_dictionary_subset ( self . _info , agents_to_process_this_timestep ) # add platform obs and env data to trainable_info (for use by custom policies) for agent_id in agents_to_process_this_timestep : if agent_id not in trainable_info : trainable_info [ agent_id ] = {} trainable_info [ agent_id ][ 'env' ] = { 'sim_period' : self . sim_period } trainable_info [ agent_id ][ 'platform_obs' ] = {} plat_name = self . agent_dict [ agent_id ] . platform_name for platform_agent in self . agent_dict : if self . agent_dict [ platform_agent ] . platform_name == plat_name : trainable_info [ agent_id ][ 'platform_obs' ][ platform_agent ] = complete_unnormalized_observations [ platform_agent ] # if not done all, delete any platforms from simulation that are done, so they don't interfere platforms_deleted = set () if not agents_done [ '__all__' ]: for agent_key , value in agents_done . items (): if agent_key != '__all__' and value : plat_name = self . agent_dict [ agent_key ] . platform_name if plat_name not in platforms_deleted : self . simulator . delete_platform ( plat_name ) platforms_deleted . add ( plat_name ) # if a platform has been deleted, we need to make sure that all agents on that platform # are also done for agent_key in trainable_dones . keys (): if agent_key == '__all__' : continue plat_name = self . agent_dict [ agent_key ] . platform_name if plat_name in platforms_deleted : trainable_dones [ agent_key ] = True ##################################################################### # return results to RLLIB - Note that RLLIB does not do a recursive # isinstance call and as such need to make sure items are # OrderedDicts ##################################################################### return trainable_observations , trainable_rewards , trainable_dones , trainable_info","title":"step()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvEppParameters","text":"typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name typing.Dict[str, typing.Dict[str, typing.Dict[str, typing.Any]]] = {} keys: agent name, done name, parameter name typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name typing.Dict[str, typing.Any] = {} keys: reference name typing.Dict[str, typing.Any] = {} keys: whatever the simulator wants, but it needs to be kwargs to simulator reset Source code in corl/environment/multi_agent_env.py class ACT3MultiAgentEnvEppParameters ( BaseModel ): \"\"\" world: typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name task: typing.Dict[str, typing.Dict[str, typing.Dict[str, typing.Any]]] = {} keys: agent name, done name, parameter name shared: typing.Dict[str, typing.Dict[str, typing.Any]] = {} keys: done name, parameter name reference_store: typing.Dict[str, typing.Any] = {} keys: reference name simulator_reset: typing.Dict[str, typing.Any] = {} keys: whatever the simulator wants, but it needs to be kwargs to simulator reset \"\"\" world : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} task : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , typing . Any ]]] = {} shared : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} reference_store : typing . Dict [ str , typing . Any ] = {} simulator_reset : typing . Dict [ str , typing . Any ] = {} @staticmethod def _validate_leaves_are_parameters ( obj ): if isinstance ( obj , dict ): for _key , value in obj . items (): ACT3MultiAgentEnvEppParameters . _validate_leaves_are_parameters ( value ) elif not isinstance ( obj , Parameter ): raise TypeError ( f \"Invalid type: { type ( obj ) } (required type: { Parameter . __qualname__ } )\" ) @validator ( 'world' , 'task' , 'shared' , 'reference_store' , 'simulator_reset' ) def validate_leaves_are_parameters ( cls , v ): \"\"\" checks to make sure outer most leaf nodes of config are parameters \"\"\" ACT3MultiAgentEnvEppParameters . _validate_leaves_are_parameters ( v ) return v","title":"ACT3MultiAgentEnvEppParameters"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvEppParameters.validate_leaves_are_parameters","text":"checks to make sure outer most leaf nodes of config are parameters Source code in corl/environment/multi_agent_env.py @validator ( 'world' , 'task' , 'shared' , 'reference_store' , 'simulator_reset' ) def validate_leaves_are_parameters ( cls , v ): \"\"\" checks to make sure outer most leaf nodes of config are parameters \"\"\" ACT3MultiAgentEnvEppParameters . _validate_leaves_are_parameters ( v ) return v","title":"validate_leaves_are_parameters()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator","text":"Validation model for the inputs of ACT3MultiAgentEnv Source code in corl/environment/multi_agent_env.py class ACT3MultiAgentEnvValidator ( BaseModel ): \"\"\"Validation model for the inputs of ACT3MultiAgentEnv\"\"\" num_workers : NonNegativeInt = 0 worker_index : NonNegativeInt = 0 vector_index : typing . Optional [ NonNegativeInt ] = None remote : bool = False deep_sanity_check : bool = True seed : PositiveInt = 0 horizon : PositiveInt = 1000 sanity_check_obs : PositiveInt = 50 sensors_grid : typing . Optional [ typing . List ] plugin_paths : typing . List [ str ] = [] # Regex allows letters, numbers, underscore, dash, dot # Regex in output_path validator also allows forward slash # Empty string is not allowed TrialName : typing . Optional [ Annotated [ str , Field ( regex = r '^[\\w\\.-]+$' )]] = None output_date_string : typing . Optional [ Annotated [ str , Field ( regex = r '^[\\w\\.-]+$' )]] = None skip_pbs_date_update : bool = False # MyPy error ignored because it is handled by the pre-validator output_path : DirectoryPath = None # type: ignore[assignment] agent_platforms : typing . Dict agents : typing . Dict [ str , AgentParseInfo ] simulator : Factory platforms : typing . Type [ BaseAvailablePlatformTypes ] other_platforms : typing . Dict [ str , typing . Dict [ str , typing . Any ]] = {} reference_store : typing . Dict [ str , ObjectStoreElem ] = {} dones : EnvironmentDoneValidator = EnvironmentDoneValidator () end_episode_on_first_agent_done : bool = False simulator_reset_parameters : typing . Dict [ str , typing . Any ] = {} episode_parameter_provider : Factory episode_parameter_provider_parameters : ACT3MultiAgentEnvEppParameters = None # type: ignore epp_registry : typing . Dict [ str , EpisodeParameterProvider ] = None # type: ignore max_agent_rate : int = 20 # the maximum rate (in Hz) that an agent may be run at timestep_epsilon : float = 1e-3 sim_warmup_steps : int = 0 # number of times to step simulator before getting initial obs @property def epp ( self ) -> EpisodeParameterProvider : \"\"\" return the current episode parameter provider \"\"\" return self . epp_registry [ ACT3MultiAgentEnv . episode_parameter_provider_name ] # pylint: disable=unsubscriptable-object class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True @validator ( 'seed' , pre = True ) def get_seed ( cls , v ): \"\"\"Compute a valid seed\"\"\" _ , seed = gym . utils . seeding . np_random ( v ) return seed @validator ( 'plugin_paths' ) def add_plugin_paths ( cls , v ): \"\"\"Use the plugin path attribute to initialize the plugin library.\"\"\" PluginLibrary . add_paths ( v ) return v @validator ( 'output_path' , pre = True , always = True ) def create_output_path ( cls , v , values ): \"\"\"Build the output path.\"\"\" v = v or 'data/act3/ray_results' v = parse_obj_as ( Annotated [ str , Field ( regex = r '^[\\w/\\.-]+$' )], v ) if values [ 'TrialName' ] is not None : if values [ \"skip_pbs_date_update\" ]: trial_prefix = '' else : trial_prefix = os . environ . get ( 'PBS_JOBID' , os . environ . get ( 'TRIAL_NAME_PREFIX' , '' )) if trial_prefix : v = os . path . join ( v , f ' { trial_prefix } - { values [ \"TrialName\" ] } ' ) else : v = os . path . join ( v , values [ 'TrialName' ]) if values [ 'output_date_string' ] is not None and not values [ \"skip_pbs_date_update\" ]: v = os . path . join ( v , values [ 'output_date_string' ]) v = os . path . abspath ( v ) v = os . path . join ( v , str ( values [ 'worker_index' ]) . zfill ( 4 )) if values [ 'vector_index' ] is not None : v = os . path . join ( v , str ( values [ 'vector_index' ]) . zfill ( 4 )) os . makedirs ( v , exist_ok = True ) return v @validator ( 'simulator' , pre = True ) def resolve_simulator_plugin ( cls , v ): \"\"\"Determine the simulator from the plugin library.\"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Let pydantic print out an error when there is no type field return v match = PluginLibrary . FindMatch ( v [ 'type' ], {}) if not issubclass ( match , BaseSimulator ): raise TypeError ( f \"Simulator must subclass BaseSimulator, but is is of type { v [ 'type' ] } \" ) return { 'type' : match , 'config' : v . get ( 'config' )} @validator ( 'platforms' , pre = True ) def resolve_platforms ( cls , v , values ): \"\"\"Determine the platforms from the plugin library.\"\"\" if not isinstance ( v , str ): return v return PluginLibrary . FindMatch ( v , { 'simulator' : values [ 'simulator' ] . type }) @validator ( 'agents' ) def agents_not_empty ( cls , v , values ): \"\"\"Ensure that at least one agent exists\"\"\" if len ( v ) == 0 : raise RuntimeError ( 'No agents exist' ) for agent_name , agent in v . items (): assert agent . platform_name in values [ 'agent_platforms' ], f \"missing platform ' { agent . platform_name } ' for agent ' { agent_name } '\" assert agent . platform_name == agent_name . split ( \"_\" )[ 0 ], f \"invalid platform name { agent . platform_name } for agent { agent_name } \" return v resolve_reference_store_factory = validator ( 'reference_store' , pre = True , each_item = True , allow_reuse = True )( Factory . resolve_factory ) @validator ( 'dones' , always = True ) def agents_match ( cls , v , values ): \"\"\"Ensure that platform in task dones match provided platforms\"\"\" # No extra agents in task dones for platform in v . task . keys (): if platform not in values [ 'agent_platforms' ]: raise RuntimeError ( f 'Platform { platform } lists a done condition but is not an allowed platform' ) # Task dones exist for all agents. Make empty ones if necessary for platform in values [ 'agent_platforms' ]: if platform not in v . task : v . task [ platform ] = {} return v @validator ( 'simulator_reset_parameters' , pre = True ) def update_units_and_parameters ( cls , v ): \"\"\"Update simulation reset parameters to meet base simulator requirements.\"\"\" return validation_helper_units_and_parameters ( v ) @validator ( 'episode_parameter_provider_parameters' , always = True , pre = True ) def build_episode_parameter_provider_parameters ( cls , _v , values ) -> ACT3MultiAgentEnvEppParameters : \"\"\"Create the episode parameter provider for this configuration\"\"\" for key in [ 'reference_store' , 'dones' , 'simulator_reset_parameters' ]: assert key in values reference_parameters : typing . Dict [ str , Parameter ] = {} for ref_name , ref_value in values [ 'reference_store' ] . items (): if isinstance ( ref_value , Parameter ): reference_parameters [ ref_name ] = ref_value world_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ] . world : functor . add_to_parameter_store ( world_parameters ) task_parameters : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Parameter ]]] = {} for agent , task_dones in values [ 'dones' ] . task . items (): agent_task_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in task_dones : functor . add_to_parameter_store ( agent_task_parameters ) task_parameters [ agent ] = agent_task_parameters shared_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ] . shared : functor . add_to_parameter_store ( shared_parameters ) sim_parameters_flat = { name : param for name , param in flatten_dict . flatten ( values [ 'simulator_reset_parameters' ]) . items () if isinstance ( param , Parameter ) } sim_parameters = flatten_dict . unflatten ( sim_parameters_flat ) return ACT3MultiAgentEnvEppParameters ( world = world_parameters , task = task_parameters , shared = shared_parameters , reference_store = reference_parameters , simulator_reset = sim_parameters ) @validator ( 'epp_registry' , always = True , pre = True ) def construct_epp_registry_if_necessary_and_validate ( cls , epp_registry , values ): \"\"\" validates the Episode Parameter provider registry \"\"\" if epp_registry is None : epp_registry = {} env_epp_parameters = dict ( values [ 'episode_parameter_provider_parameters' ]) flat_env_epp_parameters = flatten_dict . flatten ( env_epp_parameters ) env_epp = values [ 'episode_parameter_provider' ] . build ( parameters = flat_env_epp_parameters ) epp_registry [ ACT3MultiAgentEnv . episode_parameter_provider_name ] = env_epp for agent_id , agent_info in values [ 'agents' ] . items (): agent = agent_info . class_config . agent ( agent_name = agent_id , platform_name = agent_info . platform_name , ** agent_info . class_config . config ) epp_registry [ agent_id ] = agent . config . epp if ACT3MultiAgentEnv . episode_parameter_provider_name not in epp_registry : raise ValueError ( f \"Missing EPP for ' { ACT3MultiAgentEnv . episode_parameter_provider_name } '\" ) for agent_id in values [ 'agents' ]: if agent_id not in epp_registry : raise ValueError ( f \"Missing EPP for ' { agent_id } '\" ) for key , epp in epp_registry . items (): if not isinstance ( epp , EpisodeParameterProvider ): raise TypeError ( f \"Invalid type for epp_registry[' { key } ']: { type ( epp ) } , only { EpisodeParameterProvider . __qualname__ } allowed\" ) return epp_registry","title":"ACT3MultiAgentEnvValidator"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.epp","text":"return the current episode parameter provider","title":"epp"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.Config","text":"Allow arbitrary types for Parameter Source code in corl/environment/multi_agent_env.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.add_plugin_paths","text":"Use the plugin path attribute to initialize the plugin library. Source code in corl/environment/multi_agent_env.py @validator ( 'plugin_paths' ) def add_plugin_paths ( cls , v ): \"\"\"Use the plugin path attribute to initialize the plugin library.\"\"\" PluginLibrary . add_paths ( v ) return v","title":"add_plugin_paths()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.agents_match","text":"Ensure that platform in task dones match provided platforms Source code in corl/environment/multi_agent_env.py @validator ( 'dones' , always = True ) def agents_match ( cls , v , values ): \"\"\"Ensure that platform in task dones match provided platforms\"\"\" # No extra agents in task dones for platform in v . task . keys (): if platform not in values [ 'agent_platforms' ]: raise RuntimeError ( f 'Platform { platform } lists a done condition but is not an allowed platform' ) # Task dones exist for all agents. Make empty ones if necessary for platform in values [ 'agent_platforms' ]: if platform not in v . task : v . task [ platform ] = {} return v","title":"agents_match()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.agents_not_empty","text":"Ensure that at least one agent exists Source code in corl/environment/multi_agent_env.py @validator ( 'agents' ) def agents_not_empty ( cls , v , values ): \"\"\"Ensure that at least one agent exists\"\"\" if len ( v ) == 0 : raise RuntimeError ( 'No agents exist' ) for agent_name , agent in v . items (): assert agent . platform_name in values [ 'agent_platforms' ], f \"missing platform ' { agent . platform_name } ' for agent ' { agent_name } '\" assert agent . platform_name == agent_name . split ( \"_\" )[ 0 ], f \"invalid platform name { agent . platform_name } for agent { agent_name } \" return v","title":"agents_not_empty()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.build_episode_parameter_provider_parameters","text":"Create the episode parameter provider for this configuration Source code in corl/environment/multi_agent_env.py @validator ( 'episode_parameter_provider_parameters' , always = True , pre = True ) def build_episode_parameter_provider_parameters ( cls , _v , values ) -> ACT3MultiAgentEnvEppParameters : \"\"\"Create the episode parameter provider for this configuration\"\"\" for key in [ 'reference_store' , 'dones' , 'simulator_reset_parameters' ]: assert key in values reference_parameters : typing . Dict [ str , Parameter ] = {} for ref_name , ref_value in values [ 'reference_store' ] . items (): if isinstance ( ref_value , Parameter ): reference_parameters [ ref_name ] = ref_value world_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ] . world : functor . add_to_parameter_store ( world_parameters ) task_parameters : typing . Dict [ str , typing . Dict [ str , typing . Dict [ str , Parameter ]]] = {} for agent , task_dones in values [ 'dones' ] . task . items (): agent_task_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in task_dones : functor . add_to_parameter_store ( agent_task_parameters ) task_parameters [ agent ] = agent_task_parameters shared_parameters : typing . Dict [ str , typing . Dict [ str , Parameter ]] = {} for functor in values [ 'dones' ] . shared : functor . add_to_parameter_store ( shared_parameters ) sim_parameters_flat = { name : param for name , param in flatten_dict . flatten ( values [ 'simulator_reset_parameters' ]) . items () if isinstance ( param , Parameter ) } sim_parameters = flatten_dict . unflatten ( sim_parameters_flat ) return ACT3MultiAgentEnvEppParameters ( world = world_parameters , task = task_parameters , shared = shared_parameters , reference_store = reference_parameters , simulator_reset = sim_parameters )","title":"build_episode_parameter_provider_parameters()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.construct_epp_registry_if_necessary_and_validate","text":"validates the Episode Parameter provider registry Source code in corl/environment/multi_agent_env.py @validator ( 'epp_registry' , always = True , pre = True ) def construct_epp_registry_if_necessary_and_validate ( cls , epp_registry , values ): \"\"\" validates the Episode Parameter provider registry \"\"\" if epp_registry is None : epp_registry = {} env_epp_parameters = dict ( values [ 'episode_parameter_provider_parameters' ]) flat_env_epp_parameters = flatten_dict . flatten ( env_epp_parameters ) env_epp = values [ 'episode_parameter_provider' ] . build ( parameters = flat_env_epp_parameters ) epp_registry [ ACT3MultiAgentEnv . episode_parameter_provider_name ] = env_epp for agent_id , agent_info in values [ 'agents' ] . items (): agent = agent_info . class_config . agent ( agent_name = agent_id , platform_name = agent_info . platform_name , ** agent_info . class_config . config ) epp_registry [ agent_id ] = agent . config . epp if ACT3MultiAgentEnv . episode_parameter_provider_name not in epp_registry : raise ValueError ( f \"Missing EPP for ' { ACT3MultiAgentEnv . episode_parameter_provider_name } '\" ) for agent_id in values [ 'agents' ]: if agent_id not in epp_registry : raise ValueError ( f \"Missing EPP for ' { agent_id } '\" ) for key , epp in epp_registry . items (): if not isinstance ( epp , EpisodeParameterProvider ): raise TypeError ( f \"Invalid type for epp_registry[' { key } ']: { type ( epp ) } , only { EpisodeParameterProvider . __qualname__ } allowed\" ) return epp_registry","title":"construct_epp_registry_if_necessary_and_validate()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.create_output_path","text":"Build the output path. Source code in corl/environment/multi_agent_env.py @validator ( 'output_path' , pre = True , always = True ) def create_output_path ( cls , v , values ): \"\"\"Build the output path.\"\"\" v = v or 'data/act3/ray_results' v = parse_obj_as ( Annotated [ str , Field ( regex = r '^[\\w/\\.-]+$' )], v ) if values [ 'TrialName' ] is not None : if values [ \"skip_pbs_date_update\" ]: trial_prefix = '' else : trial_prefix = os . environ . get ( 'PBS_JOBID' , os . environ . get ( 'TRIAL_NAME_PREFIX' , '' )) if trial_prefix : v = os . path . join ( v , f ' { trial_prefix } - { values [ \"TrialName\" ] } ' ) else : v = os . path . join ( v , values [ 'TrialName' ]) if values [ 'output_date_string' ] is not None and not values [ \"skip_pbs_date_update\" ]: v = os . path . join ( v , values [ 'output_date_string' ]) v = os . path . abspath ( v ) v = os . path . join ( v , str ( values [ 'worker_index' ]) . zfill ( 4 )) if values [ 'vector_index' ] is not None : v = os . path . join ( v , str ( values [ 'vector_index' ]) . zfill ( 4 )) os . makedirs ( v , exist_ok = True ) return v","title":"create_output_path()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.get_seed","text":"Compute a valid seed Source code in corl/environment/multi_agent_env.py @validator ( 'seed' , pre = True ) def get_seed ( cls , v ): \"\"\"Compute a valid seed\"\"\" _ , seed = gym . utils . seeding . np_random ( v ) return seed","title":"get_seed()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.resolve_platforms","text":"Determine the platforms from the plugin library. Source code in corl/environment/multi_agent_env.py @validator ( 'platforms' , pre = True ) def resolve_platforms ( cls , v , values ): \"\"\"Determine the platforms from the plugin library.\"\"\" if not isinstance ( v , str ): return v return PluginLibrary . FindMatch ( v , { 'simulator' : values [ 'simulator' ] . type })","title":"resolve_platforms()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.resolve_reference_store_factory","text":"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) Source code in corl/environment/multi_agent_env.py @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build ()","title":"resolve_reference_store_factory()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.resolve_simulator_plugin","text":"Determine the simulator from the plugin library. Source code in corl/environment/multi_agent_env.py @validator ( 'simulator' , pre = True ) def resolve_simulator_plugin ( cls , v ): \"\"\"Determine the simulator from the plugin library.\"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Let pydantic print out an error when there is no type field return v match = PluginLibrary . FindMatch ( v [ 'type' ], {}) if not issubclass ( match , BaseSimulator ): raise TypeError ( f \"Simulator must subclass BaseSimulator, but is is of type { v [ 'type' ] } \" ) return { 'type' : match , 'config' : v . get ( 'config' )}","title":"resolve_simulator_plugin()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.ACT3MultiAgentEnvValidator.update_units_and_parameters","text":"Update simulation reset parameters to meet base simulator requirements. Source code in corl/environment/multi_agent_env.py @validator ( 'simulator_reset_parameters' , pre = True ) def update_units_and_parameters ( cls , v ): \"\"\"Update simulation reset parameters to meet base simulator requirements.\"\"\" return validation_helper_units_and_parameters ( v )","title":"update_units_and_parameters()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.EnvironmentDoneValidator","text":"Validation model for the dones of ACT3MultiAgentEnv Source code in corl/environment/multi_agent_env.py class EnvironmentDoneValidator ( BaseModel ): \"\"\"Validation model for the dones of ACT3MultiAgentEnv\"\"\" world : typing . List [ Functor ] = [] task : typing . Dict [ str , typing . List [ Functor ]] = {} shared : typing . List [ Functor ] = [] @validator ( 'world' , each_item = True ) def check_world ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" cls . check_done ( v ) return v @validator ( 'task' , each_item = True ) def check_task ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" for elem in v : cls . check_done ( elem ) return v @validator ( 'shared' , each_item = True ) def check_shared ( cls , v ): \"\"\"Check if dones subclass SharedDoneFuncBase\"\"\" if not issubclass ( v . functor , SharedDoneFuncBase ): raise TypeError ( f \"Shared Done functors must subclass SharedDoneFuncBase, but done { v . name } is of type { v . functor } \" ) return v @classmethod def check_done ( cls , v ) -> None : \"\"\"Check if dones subclass DoneFuncBase\"\"\" if not issubclass ( v . functor , DoneFuncBase ): raise TypeError ( f \"Done functors must subclass DoneFuncBase, but done { v . name } is of type { v . functor } \" ) if issubclass ( v . functor , EpisodeLengthDone ): raise ValueError ( \"Cannot specify EpisodeLengthDone as it is automatically added\" )","title":"EnvironmentDoneValidator"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.EnvironmentDoneValidator.check_done","text":"Check if dones subclass DoneFuncBase Source code in corl/environment/multi_agent_env.py @classmethod def check_done ( cls , v ) -> None : \"\"\"Check if dones subclass DoneFuncBase\"\"\" if not issubclass ( v . functor , DoneFuncBase ): raise TypeError ( f \"Done functors must subclass DoneFuncBase, but done { v . name } is of type { v . functor } \" ) if issubclass ( v . functor , EpisodeLengthDone ): raise ValueError ( \"Cannot specify EpisodeLengthDone as it is automatically added\" )","title":"check_done()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.EnvironmentDoneValidator.check_shared","text":"Check if dones subclass SharedDoneFuncBase Source code in corl/environment/multi_agent_env.py @validator ( 'shared' , each_item = True ) def check_shared ( cls , v ): \"\"\"Check if dones subclass SharedDoneFuncBase\"\"\" if not issubclass ( v . functor , SharedDoneFuncBase ): raise TypeError ( f \"Shared Done functors must subclass SharedDoneFuncBase, but done { v . name } is of type { v . functor } \" ) return v","title":"check_shared()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.EnvironmentDoneValidator.check_task","text":"Check if dones subclass DoneFuncBase Source code in corl/environment/multi_agent_env.py @validator ( 'task' , each_item = True ) def check_task ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" for elem in v : cls . check_done ( elem ) return v","title":"check_task()"},{"location":"reference/environment/multi_agent_env/#corl.environment.multi_agent_env.EnvironmentDoneValidator.check_world","text":"Check if dones subclass DoneFuncBase Source code in corl/environment/multi_agent_env.py @validator ( 'world' , each_item = True ) def check_world ( cls , v ): \"\"\"Check if dones subclass DoneFuncBase\"\"\" cls . check_done ( v ) return v","title":"check_world()"},{"location":"reference/environment/utils/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"  init  "},{"location":"reference/environment/utils/env_creation/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. create_agent_sim_configs ( agents , agent_platforms , sim_class , avail_platforms , epp_registry , * , multiple_workers = False ) \u00a4 Creates dictionary of agent configs used byt the simmulator Parameters \u00a4 agents: dictionary of agent names and configs sim_class: simulator class used by environment avail_platforms: available platforms in the simulator Returns \u00a4 None Source code in corl/environment/utils/env_creation.py def create_agent_sim_configs ( agents : typing . Dict [ str , AgentParseInfo ], agent_platforms : typing . Dict [ str , PlatformParseInfo ], sim_class : typing . Callable , avail_platforms : typing . Callable , epp_registry : typing . Dict [ str , EpisodeParameterProvider ], * , multiple_workers : bool = False ) -> typing . Tuple [ typing . Dict [ str , BaseAgent ], dict ]: \"\"\" Creates dictionary of agent configs used byt the simmulator Parameters ---------- agents: dictionary of agent names and configs sim_class: simulator class used by environment avail_platforms: available platforms in the simulator Returns ------- None \"\"\" sim_agent_configs = {} agent_dict = {} for platform_name , platform_config in agent_platforms . items (): agent_platform = avail_platforms . ParseFromNameModel ( platform_config ) # type: ignore agent_part_cls : typing . List = [] agent_part_cfg : typing . List = [] for agent_name , agent_configs in agents . items (): if agent_configs . platform_name != platform_name : continue agent_class = agent_configs . class_config . agent ( ** agent_configs . class_config . config , epp = epp_registry [ agent_name ], agent_name = agent_name , platform_name = platform_name , multiple_workers = multiple_workers ) agent_dict [ agent_name ] = agent_class partial_agent_part_list = agent_class . get_platform_parts ( sim_class , agent_platform ) for ( cls , cfg ) in partial_agent_part_list : unique = True for i , part_cls in enumerate ( agent_part_cls ): if cls == part_cls and agent_part_cfg [ i ] == cfg : unique = False break if unique : agent_part_cls . append ( cls ) agent_part_cfg . append ( cfg ) agent_part_list = list ( zip ( agent_part_cls , agent_part_cfg )) sim_agent_configs [ platform_name ] = { \"platform_config\" : platform_config , \"parts_list\" : agent_part_list , } return agent_dict , sim_agent_configs","title":"Env creation"},{"location":"reference/environment/utils/env_creation/#corl.environment.utils.env_creation.create_agent_sim_configs","text":"Creates dictionary of agent configs used byt the simmulator","title":"create_agent_sim_configs()"},{"location":"reference/environment/utils/env_creation/#corl.environment.utils.env_creation.create_agent_sim_configs--parameters","text":"agents: dictionary of agent names and configs sim_class: simulator class used by environment avail_platforms: available platforms in the simulator","title":"Parameters"},{"location":"reference/environment/utils/env_creation/#corl.environment.utils.env_creation.create_agent_sim_configs--returns","text":"None Source code in corl/environment/utils/env_creation.py def create_agent_sim_configs ( agents : typing . Dict [ str , AgentParseInfo ], agent_platforms : typing . Dict [ str , PlatformParseInfo ], sim_class : typing . Callable , avail_platforms : typing . Callable , epp_registry : typing . Dict [ str , EpisodeParameterProvider ], * , multiple_workers : bool = False ) -> typing . Tuple [ typing . Dict [ str , BaseAgent ], dict ]: \"\"\" Creates dictionary of agent configs used byt the simmulator Parameters ---------- agents: dictionary of agent names and configs sim_class: simulator class used by environment avail_platforms: available platforms in the simulator Returns ------- None \"\"\" sim_agent_configs = {} agent_dict = {} for platform_name , platform_config in agent_platforms . items (): agent_platform = avail_platforms . ParseFromNameModel ( platform_config ) # type: ignore agent_part_cls : typing . List = [] agent_part_cfg : typing . List = [] for agent_name , agent_configs in agents . items (): if agent_configs . platform_name != platform_name : continue agent_class = agent_configs . class_config . agent ( ** agent_configs . class_config . config , epp = epp_registry [ agent_name ], agent_name = agent_name , platform_name = platform_name , multiple_workers = multiple_workers ) agent_dict [ agent_name ] = agent_class partial_agent_part_list = agent_class . get_platform_parts ( sim_class , agent_platform ) for ( cls , cfg ) in partial_agent_part_list : unique = True for i , part_cls in enumerate ( agent_part_cls ): if cls == part_cls and agent_part_cfg [ i ] == cfg : unique = False break if unique : agent_part_cls . append ( cls ) agent_part_cfg . append ( cfg ) agent_part_list = list ( zip ( agent_part_cls , agent_part_cfg )) sim_agent_configs [ platform_name ] = { \"platform_config\" : platform_config , \"parts_list\" : agent_part_list , } return agent_dict , sim_agent_configs","title":"Returns"},{"location":"reference/environment/utils/obs_buffer/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. ObsBuffer \u00a4 Wrapper for OBS functions Source code in corl/environment/utils/obs_buffer.py class ObsBuffer : \"\"\"Wrapper for OBS functions \"\"\" def __init__ ( self ) -> None : \"\"\"Constructor \"\"\" self . _buffer1 : OrderedDict = OrderedDict () self . _buffer2 : OrderedDict = OrderedDict () self . _index = 0 def update_obs_pointer ( self ): \"\"\"get the next observation pointer for next and current \"\"\" self . _index += 1 @property def observation ( self ) -> OrderedDict : \"\"\"Every other call we update the pointer to the correct obs buffer Returns ------- OrderedDict The observation dictionary \"\"\" if self . _index % 2 == 0 : return self . _buffer1 return self . _buffer2 @observation . setter def observation ( self , data : OrderedDict ) -> None : \"\"\"sets the observation Parameters ---------- data : OrderedDict The update data \"\"\" if self . _index % 2 == 0 : # Environment.ObsBuffer.update_dict(self._buffer1, data) self . _buffer1 = data else : # Environment.ObsBuffer.update_dict(self._buffer2, data) self . _buffer2 = data @property def next_observation ( self ) -> OrderedDict : \"\"\"Every other call we update the pointer to the correct next obs buffer Returns ------- OrderedDict The next observation dictionary \"\"\" if self . _index % 2 == 0 : return self . _buffer2 return self . _buffer1 @next_observation . setter def next_observation ( self , data : OrderedDict ) -> None : \"\"\"sets the next observation Parameters ---------- data : OrderedDict The update data \"\"\" if self . _index % 2 == 0 : # Environment.ObsBuffer.update_dict(self._buffer2, data) self . _buffer2 = data else : # Environment.ObsBuffer.update_dict(self._buffer1, data) self . _buffer1 = data next_observation : OrderedDict property writable \u00a4 Every other call we update the pointer to the correct next obs buffer Returns OrderedDict The next observation dictionary observation : OrderedDict property writable \u00a4 Every other call we update the pointer to the correct obs buffer Returns OrderedDict The observation dictionary __init__ ( self ) special \u00a4 Constructor Source code in corl/environment/utils/obs_buffer.py def __init__ ( self ) -> None : \"\"\"Constructor \"\"\" self . _buffer1 : OrderedDict = OrderedDict () self . _buffer2 : OrderedDict = OrderedDict () self . _index = 0 update_obs_pointer ( self ) \u00a4 get the next observation pointer for next and current Source code in corl/environment/utils/obs_buffer.py def update_obs_pointer ( self ): \"\"\"get the next observation pointer for next and current \"\"\" self . _index += 1","title":"Obs buffer"},{"location":"reference/environment/utils/obs_buffer/#corl.environment.utils.obs_buffer.ObsBuffer","text":"Wrapper for OBS functions Source code in corl/environment/utils/obs_buffer.py class ObsBuffer : \"\"\"Wrapper for OBS functions \"\"\" def __init__ ( self ) -> None : \"\"\"Constructor \"\"\" self . _buffer1 : OrderedDict = OrderedDict () self . _buffer2 : OrderedDict = OrderedDict () self . _index = 0 def update_obs_pointer ( self ): \"\"\"get the next observation pointer for next and current \"\"\" self . _index += 1 @property def observation ( self ) -> OrderedDict : \"\"\"Every other call we update the pointer to the correct obs buffer Returns ------- OrderedDict The observation dictionary \"\"\" if self . _index % 2 == 0 : return self . _buffer1 return self . _buffer2 @observation . setter def observation ( self , data : OrderedDict ) -> None : \"\"\"sets the observation Parameters ---------- data : OrderedDict The update data \"\"\" if self . _index % 2 == 0 : # Environment.ObsBuffer.update_dict(self._buffer1, data) self . _buffer1 = data else : # Environment.ObsBuffer.update_dict(self._buffer2, data) self . _buffer2 = data @property def next_observation ( self ) -> OrderedDict : \"\"\"Every other call we update the pointer to the correct next obs buffer Returns ------- OrderedDict The next observation dictionary \"\"\" if self . _index % 2 == 0 : return self . _buffer2 return self . _buffer1 @next_observation . setter def next_observation ( self , data : OrderedDict ) -> None : \"\"\"sets the next observation Parameters ---------- data : OrderedDict The update data \"\"\" if self . _index % 2 == 0 : # Environment.ObsBuffer.update_dict(self._buffer2, data) self . _buffer2 = data else : # Environment.ObsBuffer.update_dict(self._buffer1, data) self . _buffer1 = data","title":"ObsBuffer"},{"location":"reference/environment/utils/obs_buffer/#corl.environment.utils.obs_buffer.ObsBuffer.next_observation","text":"Every other call we update the pointer to the correct next obs buffer Returns OrderedDict The next observation dictionary","title":"next_observation"},{"location":"reference/environment/utils/obs_buffer/#corl.environment.utils.obs_buffer.ObsBuffer.observation","text":"Every other call we update the pointer to the correct obs buffer Returns OrderedDict The observation dictionary","title":"observation"},{"location":"reference/environment/utils/obs_buffer/#corl.environment.utils.obs_buffer.ObsBuffer.__init__","text":"Constructor Source code in corl/environment/utils/obs_buffer.py def __init__ ( self ) -> None : \"\"\"Constructor \"\"\" self . _buffer1 : OrderedDict = OrderedDict () self . _buffer2 : OrderedDict = OrderedDict () self . _index = 0","title":"__init__()"},{"location":"reference/environment/utils/obs_buffer/#corl.environment.utils.obs_buffer.ObsBuffer.update_obs_pointer","text":"get the next observation pointer for next and current Source code in corl/environment/utils/obs_buffer.py def update_obs_pointer ( self ): \"\"\"get the next observation pointer for next and current \"\"\" self . _index += 1","title":"update_obs_pointer()"},{"location":"reference/environment/utils/space_sort/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. gym_space_sort ( space ) \u00a4 Space Any gym.spaces.Space (or nested hierarchy of spaces) to be sorted. This sorts any gym.space.Dict in the hierarchy in KEY order. This is necessary as a workaround to a bug in ray/rllib which causes the action space that is generated from a dictionary to be in KEY order (the result of this line: https://github.com/ray-project/ray/blob/9c26d6c6de3016eb7409db8eca51f8cac4b2944e/rllib/utils/exploration/random.py#L119 which uses https://tree.readthedocs.io/en/latest/api.html#tree.map_structure , which internally uses https://tree.readthedocs.io/en/latest/api.html#tree.flatten which, per the docs, causes the returned map to be in KEY order). Source code in corl/environment/utils/space_sort.py def gym_space_sort ( space : gym . spaces . Space ): \"\"\" space: Any gym.spaces.Space (or nested hierarchy of spaces) to be sorted. This sorts any gym.space.Dict in the hierarchy in KEY order. This is necessary as a workaround to a bug in ray/rllib which causes the action space that is generated from a dictionary to be in KEY order (the result of this line: https://github.com/ray-project/ray/blob/9c26d6c6de3016eb7409db8eca51f8cac4b2944e/rllib/utils/exploration/random.py#L119 which uses https://tree.readthedocs.io/en/latest/api.html#tree.map_structure, which internally uses https://tree.readthedocs.io/en/latest/api.html#tree.flatten which, per the docs, causes the returned map to be in KEY order). \"\"\" if isinstance ( space , gym . spaces . Dict ): space_dict : gym . spaces . Dict = typing . cast ( gym . spaces . Dict , space ) for key in space_dict . spaces : sub_space = space_dict . spaces [ key ] gym_space_sort ( sub_space ) space . spaces = OrderedDict ( sorted ( space_dict . spaces . items ())) if isinstance ( space , gym . spaces . Tuple ): space_tuple : gym . spaces . Tuple = typing . cast ( gym . spaces . Tuple , space ) for sub_space in space_tuple . spaces : gym_space_sort ( sub_space )","title":"Space sort"},{"location":"reference/environment/utils/space_sort/#corl.environment.utils.space_sort.gym_space_sort","text":"Space Any gym.spaces.Space (or nested hierarchy of spaces) to be sorted. This sorts any gym.space.Dict in the hierarchy in KEY order. This is necessary as a workaround to a bug in ray/rllib which causes the action space that is generated from a dictionary to be in KEY order (the result of this line: https://github.com/ray-project/ray/blob/9c26d6c6de3016eb7409db8eca51f8cac4b2944e/rllib/utils/exploration/random.py#L119 which uses https://tree.readthedocs.io/en/latest/api.html#tree.map_structure , which internally uses https://tree.readthedocs.io/en/latest/api.html#tree.flatten which, per the docs, causes the returned map to be in KEY order). Source code in corl/environment/utils/space_sort.py def gym_space_sort ( space : gym . spaces . Space ): \"\"\" space: Any gym.spaces.Space (or nested hierarchy of spaces) to be sorted. This sorts any gym.space.Dict in the hierarchy in KEY order. This is necessary as a workaround to a bug in ray/rllib which causes the action space that is generated from a dictionary to be in KEY order (the result of this line: https://github.com/ray-project/ray/blob/9c26d6c6de3016eb7409db8eca51f8cac4b2944e/rllib/utils/exploration/random.py#L119 which uses https://tree.readthedocs.io/en/latest/api.html#tree.map_structure, which internally uses https://tree.readthedocs.io/en/latest/api.html#tree.flatten which, per the docs, causes the returned map to be in KEY order). \"\"\" if isinstance ( space , gym . spaces . Dict ): space_dict : gym . spaces . Dict = typing . cast ( gym . spaces . Dict , space ) for key in space_dict . spaces : sub_space = space_dict . spaces [ key ] gym_space_sort ( sub_space ) space . spaces = OrderedDict ( sorted ( space_dict . spaces . items ())) if isinstance ( space , gym . spaces . Tuple ): space_tuple : gym . spaces . Tuple = typing . cast ( gym . spaces . Tuple , space ) for sub_space in space_tuple . spaces : gym_space_sort ( sub_space )","title":"gym_space_sort()"},{"location":"reference/episode_parameter_providers/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Episode Parameter Provider"},{"location":"reference/episode_parameter_providers/core/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. EpisodeParameterProvider ( ABC ) \u00a4 Interface definition for episode parameter providers. Source code in corl/episode_parameter_providers/core.py class EpisodeParameterProvider ( abc . ABC ): \"\"\"Interface definition for episode parameter providers. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeParameterProviderValidator = self . get_validator ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ EpisodeParameterProviderValidator ]: \"\"\"Get the validator for this class.\"\"\" return EpisodeParameterProviderValidator def get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: \"\"\"Get the next instance of episode parameters from this provider Subclasses: Override _do_get_params. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. Returns ------- ParameterModel The parameters for this episode episode_id The episode index number for this set of parameters \"\"\" output , episode_id = self . _do_get_params ( rng ) extra_keys = output . keys () - self . config . parameters . keys () if extra_keys : raise KeyError ( f 'Extra keys provided: { extra_keys } ' ) missing_keys = self . config . parameters . keys () - output . keys () if missing_keys : raise KeyError ( f 'Missing keys: { missing_keys } ' ) bad_types : typing . Dict [ str , str ] = {} for key , value in output . items (): if not isinstance ( value , Parameter ): bad_types [ '.' . join ( key )] = type ( value ) . __name__ if bad_types : raise TypeError ( f 'Unsupported types: { bad_types } ' ) return output , episode_id @abc . abstractmethod def _do_get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: \"\"\"Get the next instance of episode parameters from this provider This is an abstract method that must be overridden by each subclass. DO NOT CALL DIRECTLY. USE get_params. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. Returns ------- ParameterCollection The parameters for this episode episode_id The episode index number for this set of parameters \"\"\" raise NotImplementedError def compute_metrics ( self ) -> typing . Dict [ str , typing . Any ]: # pylint: disable=no-self-use \"\"\"Get metrics on the operation of this provider. Often used in `on_episode_end` training callbacks. \"\"\" return {} def update ( self , results : dict , rng : Randomness ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Update the operation of this provider. Often used in `on_train_result` training callbacks. Parameters ---------- results : dict As described by ray.rllib.agents.callbacks.DefaultCallbacks.on_train_result. See https://docs.ray.io/en/master/_modules/ray/rllib/agents/callbacks.html#DefaultCallbacks.on_train_result rng : Union[Generator, RandomState] Random number generator from which to draw random values. \"\"\" ... def save_checkpoint ( self , checkpoint_path : PathLike ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Save the internal state of the parameter provider. Parameters ---------- checkpoint_path : PathLike Filesystem path at which to save the checkpoint \"\"\" ... def load_checkpoint ( self , checkpoint_path : PathLike ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Load the internal state from a checkpoint. Parameters ---------- checkpoint_path : PathLike Filesystem path from which to restore the checkpoint \"\"\" ... get_validator : Type [ corl . episode_parameter_providers . core . EpisodeParameterProviderValidator ] property readonly \u00a4 Get the validator for this class. compute_metrics ( self ) \u00a4 Get metrics on the operation of this provider. Often used in on_episode_end training callbacks. Source code in corl/episode_parameter_providers/core.py def compute_metrics ( self ) -> typing . Dict [ str , typing . Any ]: # pylint: disable=no-self-use \"\"\"Get metrics on the operation of this provider. Often used in `on_episode_end` training callbacks. \"\"\" return {} get_params ( self , rng ) \u00a4 Get the next instance of episode parameters from this provider Subclasses: Override _do_get_params. Parameters \u00a4 rng : Union[Generator, RandomState] Random number generator from which to draw random values. Returns \u00a4 ParameterModel The parameters for this episode episode_id The episode index number for this set of parameters Source code in corl/episode_parameter_providers/core.py def get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: \"\"\"Get the next instance of episode parameters from this provider Subclasses: Override _do_get_params. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. Returns ------- ParameterModel The parameters for this episode episode_id The episode index number for this set of parameters \"\"\" output , episode_id = self . _do_get_params ( rng ) extra_keys = output . keys () - self . config . parameters . keys () if extra_keys : raise KeyError ( f 'Extra keys provided: { extra_keys } ' ) missing_keys = self . config . parameters . keys () - output . keys () if missing_keys : raise KeyError ( f 'Missing keys: { missing_keys } ' ) bad_types : typing . Dict [ str , str ] = {} for key , value in output . items (): if not isinstance ( value , Parameter ): bad_types [ '.' . join ( key )] = type ( value ) . __name__ if bad_types : raise TypeError ( f 'Unsupported types: { bad_types } ' ) return output , episode_id load_checkpoint ( self , checkpoint_path ) \u00a4 Load the internal state from a checkpoint. Parameters \u00a4 checkpoint_path : PathLike Filesystem path from which to restore the checkpoint Source code in corl/episode_parameter_providers/core.py def load_checkpoint ( self , checkpoint_path : PathLike ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Load the internal state from a checkpoint. Parameters ---------- checkpoint_path : PathLike Filesystem path from which to restore the checkpoint \"\"\" ... save_checkpoint ( self , checkpoint_path ) \u00a4 Save the internal state of the parameter provider. Parameters \u00a4 checkpoint_path : PathLike Filesystem path at which to save the checkpoint Source code in corl/episode_parameter_providers/core.py def save_checkpoint ( self , checkpoint_path : PathLike ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Save the internal state of the parameter provider. Parameters ---------- checkpoint_path : PathLike Filesystem path at which to save the checkpoint \"\"\" ... update ( self , results , rng ) \u00a4 Update the operation of this provider. Often used in on_train_result training callbacks. Parameters \u00a4 results : dict As described by ray.rllib.agents.callbacks.DefaultCallbacks.on_train_result. See https://docs.ray.io/en/master/_modules/ray/rllib/agents/callbacks.html#DefaultCallbacks.on_train_result rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/episode_parameter_providers/core.py def update ( self , results : dict , rng : Randomness ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Update the operation of this provider. Often used in `on_train_result` training callbacks. Parameters ---------- results : dict As described by ray.rllib.agents.callbacks.DefaultCallbacks.on_train_result. See https://docs.ray.io/en/master/_modules/ray/rllib/agents/callbacks.html#DefaultCallbacks.on_train_result rng : Union[Generator, RandomState] Random number generator from which to draw random values. \"\"\" ... EpisodeParameterProviderValidator ( BaseModel ) pydantic-model \u00a4 Validation model for the inputs of EpisodeParameterProvider Source code in corl/episode_parameter_providers/core.py class EpisodeParameterProviderValidator ( BaseModel ): \"\"\"Validation model for the inputs of EpisodeParameterProvider\"\"\" parameters : ParameterModel = {} class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True allow_mutation = False Config \u00a4 Allow arbitrary types for Parameter Source code in corl/episode_parameter_providers/core.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True allow_mutation = False","title":"Core"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider","text":"Interface definition for episode parameter providers. Source code in corl/episode_parameter_providers/core.py class EpisodeParameterProvider ( abc . ABC ): \"\"\"Interface definition for episode parameter providers. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeParameterProviderValidator = self . get_validator ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ EpisodeParameterProviderValidator ]: \"\"\"Get the validator for this class.\"\"\" return EpisodeParameterProviderValidator def get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: \"\"\"Get the next instance of episode parameters from this provider Subclasses: Override _do_get_params. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. Returns ------- ParameterModel The parameters for this episode episode_id The episode index number for this set of parameters \"\"\" output , episode_id = self . _do_get_params ( rng ) extra_keys = output . keys () - self . config . parameters . keys () if extra_keys : raise KeyError ( f 'Extra keys provided: { extra_keys } ' ) missing_keys = self . config . parameters . keys () - output . keys () if missing_keys : raise KeyError ( f 'Missing keys: { missing_keys } ' ) bad_types : typing . Dict [ str , str ] = {} for key , value in output . items (): if not isinstance ( value , Parameter ): bad_types [ '.' . join ( key )] = type ( value ) . __name__ if bad_types : raise TypeError ( f 'Unsupported types: { bad_types } ' ) return output , episode_id @abc . abstractmethod def _do_get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: \"\"\"Get the next instance of episode parameters from this provider This is an abstract method that must be overridden by each subclass. DO NOT CALL DIRECTLY. USE get_params. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. Returns ------- ParameterCollection The parameters for this episode episode_id The episode index number for this set of parameters \"\"\" raise NotImplementedError def compute_metrics ( self ) -> typing . Dict [ str , typing . Any ]: # pylint: disable=no-self-use \"\"\"Get metrics on the operation of this provider. Often used in `on_episode_end` training callbacks. \"\"\" return {} def update ( self , results : dict , rng : Randomness ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Update the operation of this provider. Often used in `on_train_result` training callbacks. Parameters ---------- results : dict As described by ray.rllib.agents.callbacks.DefaultCallbacks.on_train_result. See https://docs.ray.io/en/master/_modules/ray/rllib/agents/callbacks.html#DefaultCallbacks.on_train_result rng : Union[Generator, RandomState] Random number generator from which to draw random values. \"\"\" ... def save_checkpoint ( self , checkpoint_path : PathLike ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Save the internal state of the parameter provider. Parameters ---------- checkpoint_path : PathLike Filesystem path at which to save the checkpoint \"\"\" ... def load_checkpoint ( self , checkpoint_path : PathLike ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Load the internal state from a checkpoint. Parameters ---------- checkpoint_path : PathLike Filesystem path from which to restore the checkpoint \"\"\" ...","title":"EpisodeParameterProvider"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.get_validator","text":"Get the validator for this class.","title":"get_validator"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.compute_metrics","text":"Get metrics on the operation of this provider. Often used in on_episode_end training callbacks. Source code in corl/episode_parameter_providers/core.py def compute_metrics ( self ) -> typing . Dict [ str , typing . Any ]: # pylint: disable=no-self-use \"\"\"Get metrics on the operation of this provider. Often used in `on_episode_end` training callbacks. \"\"\" return {}","title":"compute_metrics()"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.get_params","text":"Get the next instance of episode parameters from this provider Subclasses: Override _do_get_params.","title":"get_params()"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.get_params--parameters","text":"rng : Union[Generator, RandomState] Random number generator from which to draw random values.","title":"Parameters"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.get_params--returns","text":"ParameterModel The parameters for this episode episode_id The episode index number for this set of parameters Source code in corl/episode_parameter_providers/core.py def get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: \"\"\"Get the next instance of episode parameters from this provider Subclasses: Override _do_get_params. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. Returns ------- ParameterModel The parameters for this episode episode_id The episode index number for this set of parameters \"\"\" output , episode_id = self . _do_get_params ( rng ) extra_keys = output . keys () - self . config . parameters . keys () if extra_keys : raise KeyError ( f 'Extra keys provided: { extra_keys } ' ) missing_keys = self . config . parameters . keys () - output . keys () if missing_keys : raise KeyError ( f 'Missing keys: { missing_keys } ' ) bad_types : typing . Dict [ str , str ] = {} for key , value in output . items (): if not isinstance ( value , Parameter ): bad_types [ '.' . join ( key )] = type ( value ) . __name__ if bad_types : raise TypeError ( f 'Unsupported types: { bad_types } ' ) return output , episode_id","title":"Returns"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.load_checkpoint","text":"Load the internal state from a checkpoint.","title":"load_checkpoint()"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.load_checkpoint--parameters","text":"checkpoint_path : PathLike Filesystem path from which to restore the checkpoint Source code in corl/episode_parameter_providers/core.py def load_checkpoint ( self , checkpoint_path : PathLike ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Load the internal state from a checkpoint. Parameters ---------- checkpoint_path : PathLike Filesystem path from which to restore the checkpoint \"\"\" ...","title":"Parameters"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.save_checkpoint","text":"Save the internal state of the parameter provider.","title":"save_checkpoint()"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.save_checkpoint--parameters","text":"checkpoint_path : PathLike Filesystem path at which to save the checkpoint Source code in corl/episode_parameter_providers/core.py def save_checkpoint ( self , checkpoint_path : PathLike ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Save the internal state of the parameter provider. Parameters ---------- checkpoint_path : PathLike Filesystem path at which to save the checkpoint \"\"\" ...","title":"Parameters"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.update","text":"Update the operation of this provider. Often used in on_train_result training callbacks.","title":"update()"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProvider.update--parameters","text":"results : dict As described by ray.rllib.agents.callbacks.DefaultCallbacks.on_train_result. See https://docs.ray.io/en/master/_modules/ray/rllib/agents/callbacks.html#DefaultCallbacks.on_train_result rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/episode_parameter_providers/core.py def update ( self , results : dict , rng : Randomness ) -> None : # pylint: disable=no-self-use, unused-argument \"\"\"Update the operation of this provider. Often used in `on_train_result` training callbacks. Parameters ---------- results : dict As described by ray.rllib.agents.callbacks.DefaultCallbacks.on_train_result. See https://docs.ray.io/en/master/_modules/ray/rllib/agents/callbacks.html#DefaultCallbacks.on_train_result rng : Union[Generator, RandomState] Random number generator from which to draw random values. \"\"\" ...","title":"Parameters"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProviderValidator","text":"Validation model for the inputs of EpisodeParameterProvider Source code in corl/episode_parameter_providers/core.py class EpisodeParameterProviderValidator ( BaseModel ): \"\"\"Validation model for the inputs of EpisodeParameterProvider\"\"\" parameters : ParameterModel = {} class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True allow_mutation = False","title":"EpisodeParameterProviderValidator"},{"location":"reference/episode_parameter_providers/core/#corl.episode_parameter_providers.core.EpisodeParameterProviderValidator.Config","text":"Allow arbitrary types for Parameter Source code in corl/episode_parameter_providers/core.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True allow_mutation = False","title":"Config"},{"location":"reference/episode_parameter_providers/remote/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. RayActorProxy \u00a4 Creates a 'proxy' to a named ray actor Source code in corl/episode_parameter_providers/remote.py class RayActorProxy : \"\"\"Creates a 'proxy' to a named ray actor\"\"\" def __init__ ( self , actor_id : str , namespace : typing . Optional [ str ] = None ): self . _actor_id = actor_id self . _namespace = namespace @property # type: ignore @lru_cache ( maxsize = 1 ) def actor ( self ) -> ray . actor . ActorHandle : \"\"\"Retrieves the named ray actor\"\"\" if not ray . is_initialized (): raise RuntimeError ( 'Cannot get named actor without ray' ) return ray . get_actor ( self . _actor_id , self . _namespace ) actor : ActorHandle property readonly \u00a4 Retrieves the named ray actor RemoteEpisodeParameterProvider ( EpisodeParameterProvider ) \u00a4 Wrap EpisodeParameterProvider as a ray remote actor and manage data passing between ray processes. Source code in corl/episode_parameter_providers/remote.py class RemoteEpisodeParameterProvider ( EpisodeParameterProvider ): \"\"\"Wrap EpisodeParameterProvider as a ray remote actor and manage data passing between ray processes.\"\"\" def __init__ ( self , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) self . config = typing . cast ( RemoteEpisodeParameterProviderValidator , self . config ) ray . remote ( self . config . internal_class ) . options ( # type: ignore name = self . config . actor_name , namespace = self . config . namespace , lifetime = 'detached' , ) . remote ( ** self . config . internal_config , ** kwargs ) # noqa: E126 I could not get this to format self . _rap = RayActorProxy ( self . config . actor_name , namespace = self . config . namespace ) @property def get_validator ( self ) -> typing . Type [ RemoteEpisodeParameterProviderValidator ]: return RemoteEpisodeParameterProviderValidator @staticmethod def wrap_epp_factory ( epp_factory : Factory , actor_name : str , namespace : str = None ) -> Factory : \"\"\"Wraps an existing EpisodeParameterProvider Factory as a RemoteEpisodeParameterProvider\"\"\" if not issubclass ( epp_factory . type , EpisodeParameterProvider ): # type: ignore raise TypeError ( f \"Invalid Factory.type: { epp_factory . type } , { EpisodeParameterProvider . __qualname__ } required\" ) remote_config = { 'namespace' : namespace , 'actor_name' : actor_name , 'internal_class' : epp_factory . type , 'internal_config' : epp_factory . config } return Factory ( type = RemoteEpisodeParameterProvider , config = remote_config ) def kill_actor ( self ) -> None : \"\"\"Kill the underlying actor used by this provider.\"\"\" try : ray . kill ( self . _rap . actor ) except ray . exceptions . RaySystemError : pass def _do_get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: if isinstance ( rng , Generator ): seed = rng . integers ( low = 0 , high = 1000000 ) new_rng = np . random . default_rng ( seed = seed ) elif isinstance ( rng , RandomState ): seed = rng . randint ( low = 0 , high = 1000000 ) new_rng , _ = seeding . np_random ( seed ) else : raise RuntimeError ( f \"rng type provided to function was { rng } , but this class only knows numpy Generator or RandomState\" ) return ray . get ( self . _rap . actor . get_params . remote ( new_rng )) # type: ignore def compute_metrics ( self ) -> typing . Dict [ str , typing . Any ]: return ray . get ( self . _rap . actor . compute_metrics . remote ()) # type: ignore def update ( self , results : dict , rng : Randomness ) -> None : if isinstance ( rng , Generator ): seed = rng . integers ( low = 0 , high = 1000000 ) new_rng = np . random . default_rng ( seed = seed ) elif isinstance ( rng , RandomState ): seed = rng . randint ( low = 0 , high = 1000000 ) new_rng , _ = seeding . np_random ( seed ) else : raise RuntimeError ( f \"rng type provided to function was { rng } , but this class only knows numpy Generator or RandomState\" ) ray . get ( self . _rap . actor . update . remote ( results , new_rng )) # type: ignore def save_checkpoint ( self , checkpoint_path ) -> None : ray . get ( self . _rap . actor . save_checkpoint . remote ( checkpoint_path )) # type: ignore def load_checkpoint ( self , checkpoint_path ) -> None : ray . get ( self . _rap . actor . load_checkpoint . remote ( checkpoint_path )) # type: ignore get_validator : Type [ corl . episode_parameter_providers . remote . RemoteEpisodeParameterProviderValidator ] property readonly \u00a4 Get the validator for this class. compute_metrics ( self ) \u00a4 Get metrics on the operation of this provider. Often used in on_episode_end training callbacks. Source code in corl/episode_parameter_providers/remote.py def compute_metrics ( self ) -> typing . Dict [ str , typing . Any ]: return ray . get ( self . _rap . actor . compute_metrics . remote ()) # type: ignore kill_actor ( self ) \u00a4 Kill the underlying actor used by this provider. Source code in corl/episode_parameter_providers/remote.py def kill_actor ( self ) -> None : \"\"\"Kill the underlying actor used by this provider.\"\"\" try : ray . kill ( self . _rap . actor ) except ray . exceptions . RaySystemError : pass load_checkpoint ( self , checkpoint_path ) \u00a4 Load the internal state from a checkpoint. Parameters \u00a4 checkpoint_path : PathLike Filesystem path from which to restore the checkpoint Source code in corl/episode_parameter_providers/remote.py def load_checkpoint ( self , checkpoint_path ) -> None : ray . get ( self . _rap . actor . load_checkpoint . remote ( checkpoint_path )) # type: ignore save_checkpoint ( self , checkpoint_path ) \u00a4 Save the internal state of the parameter provider. Parameters \u00a4 checkpoint_path : PathLike Filesystem path at which to save the checkpoint Source code in corl/episode_parameter_providers/remote.py def save_checkpoint ( self , checkpoint_path ) -> None : ray . get ( self . _rap . actor . save_checkpoint . remote ( checkpoint_path )) # type: ignore update ( self , results , rng ) \u00a4 Update the operation of this provider. Often used in on_train_result training callbacks. Parameters \u00a4 results : dict As described by ray.rllib.agents.callbacks.DefaultCallbacks.on_train_result. See https://docs.ray.io/en/master/_modules/ray/rllib/agents/callbacks.html#DefaultCallbacks.on_train_result rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/episode_parameter_providers/remote.py def update ( self , results : dict , rng : Randomness ) -> None : if isinstance ( rng , Generator ): seed = rng . integers ( low = 0 , high = 1000000 ) new_rng = np . random . default_rng ( seed = seed ) elif isinstance ( rng , RandomState ): seed = rng . randint ( low = 0 , high = 1000000 ) new_rng , _ = seeding . np_random ( seed ) else : raise RuntimeError ( f \"rng type provided to function was { rng } , but this class only knows numpy Generator or RandomState\" ) ray . get ( self . _rap . actor . update . remote ( results , new_rng )) # type: ignore wrap_epp_factory ( epp_factory , actor_name , namespace = None ) staticmethod \u00a4 Wraps an existing EpisodeParameterProvider Factory as a RemoteEpisodeParameterProvider Source code in corl/episode_parameter_providers/remote.py @staticmethod def wrap_epp_factory ( epp_factory : Factory , actor_name : str , namespace : str = None ) -> Factory : \"\"\"Wraps an existing EpisodeParameterProvider Factory as a RemoteEpisodeParameterProvider\"\"\" if not issubclass ( epp_factory . type , EpisodeParameterProvider ): # type: ignore raise TypeError ( f \"Invalid Factory.type: { epp_factory . type } , { EpisodeParameterProvider . __qualname__ } required\" ) remote_config = { 'namespace' : namespace , 'actor_name' : actor_name , 'internal_class' : epp_factory . type , 'internal_config' : epp_factory . config } return Factory ( type = RemoteEpisodeParameterProvider , config = remote_config ) RemoteEpisodeParameterProviderValidator ( EpisodeParameterProviderValidator ) pydantic-model \u00a4 Validation model for the inputs of RemoteEpisodeParameterProvider Source code in corl/episode_parameter_providers/remote.py class RemoteEpisodeParameterProviderValidator ( EpisodeParameterProviderValidator ): \"\"\"Validation model for the inputs of RemoteEpisodeParameterProvider\"\"\" internal_class : typing . Type [ EpisodeParameterProvider ] internal_config : typing . Dict [ str , typing . Any ] = {} actor_name : str namespace : typing . Optional [ str ] = None @validator ( 'internal_class' ) def internal_not_remote ( cls , v ): \"\"\"Confirm that internal class is not also remote\"\"\" assert v != RemoteEpisodeParameterProvider return v internal_not_remote ( v ) classmethod \u00a4 Confirm that internal class is not also remote Source code in corl/episode_parameter_providers/remote.py @validator ( 'internal_class' ) def internal_not_remote ( cls , v ): \"\"\"Confirm that internal class is not also remote\"\"\" assert v != RemoteEpisodeParameterProvider return v","title":"Remote"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RayActorProxy","text":"Creates a 'proxy' to a named ray actor Source code in corl/episode_parameter_providers/remote.py class RayActorProxy : \"\"\"Creates a 'proxy' to a named ray actor\"\"\" def __init__ ( self , actor_id : str , namespace : typing . Optional [ str ] = None ): self . _actor_id = actor_id self . _namespace = namespace @property # type: ignore @lru_cache ( maxsize = 1 ) def actor ( self ) -> ray . actor . ActorHandle : \"\"\"Retrieves the named ray actor\"\"\" if not ray . is_initialized (): raise RuntimeError ( 'Cannot get named actor without ray' ) return ray . get_actor ( self . _actor_id , self . _namespace )","title":"RayActorProxy"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RayActorProxy.actor","text":"Retrieves the named ray actor","title":"actor"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider","text":"Wrap EpisodeParameterProvider as a ray remote actor and manage data passing between ray processes. Source code in corl/episode_parameter_providers/remote.py class RemoteEpisodeParameterProvider ( EpisodeParameterProvider ): \"\"\"Wrap EpisodeParameterProvider as a ray remote actor and manage data passing between ray processes.\"\"\" def __init__ ( self , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) self . config = typing . cast ( RemoteEpisodeParameterProviderValidator , self . config ) ray . remote ( self . config . internal_class ) . options ( # type: ignore name = self . config . actor_name , namespace = self . config . namespace , lifetime = 'detached' , ) . remote ( ** self . config . internal_config , ** kwargs ) # noqa: E126 I could not get this to format self . _rap = RayActorProxy ( self . config . actor_name , namespace = self . config . namespace ) @property def get_validator ( self ) -> typing . Type [ RemoteEpisodeParameterProviderValidator ]: return RemoteEpisodeParameterProviderValidator @staticmethod def wrap_epp_factory ( epp_factory : Factory , actor_name : str , namespace : str = None ) -> Factory : \"\"\"Wraps an existing EpisodeParameterProvider Factory as a RemoteEpisodeParameterProvider\"\"\" if not issubclass ( epp_factory . type , EpisodeParameterProvider ): # type: ignore raise TypeError ( f \"Invalid Factory.type: { epp_factory . type } , { EpisodeParameterProvider . __qualname__ } required\" ) remote_config = { 'namespace' : namespace , 'actor_name' : actor_name , 'internal_class' : epp_factory . type , 'internal_config' : epp_factory . config } return Factory ( type = RemoteEpisodeParameterProvider , config = remote_config ) def kill_actor ( self ) -> None : \"\"\"Kill the underlying actor used by this provider.\"\"\" try : ray . kill ( self . _rap . actor ) except ray . exceptions . RaySystemError : pass def _do_get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: if isinstance ( rng , Generator ): seed = rng . integers ( low = 0 , high = 1000000 ) new_rng = np . random . default_rng ( seed = seed ) elif isinstance ( rng , RandomState ): seed = rng . randint ( low = 0 , high = 1000000 ) new_rng , _ = seeding . np_random ( seed ) else : raise RuntimeError ( f \"rng type provided to function was { rng } , but this class only knows numpy Generator or RandomState\" ) return ray . get ( self . _rap . actor . get_params . remote ( new_rng )) # type: ignore def compute_metrics ( self ) -> typing . Dict [ str , typing . Any ]: return ray . get ( self . _rap . actor . compute_metrics . remote ()) # type: ignore def update ( self , results : dict , rng : Randomness ) -> None : if isinstance ( rng , Generator ): seed = rng . integers ( low = 0 , high = 1000000 ) new_rng = np . random . default_rng ( seed = seed ) elif isinstance ( rng , RandomState ): seed = rng . randint ( low = 0 , high = 1000000 ) new_rng , _ = seeding . np_random ( seed ) else : raise RuntimeError ( f \"rng type provided to function was { rng } , but this class only knows numpy Generator or RandomState\" ) ray . get ( self . _rap . actor . update . remote ( results , new_rng )) # type: ignore def save_checkpoint ( self , checkpoint_path ) -> None : ray . get ( self . _rap . actor . save_checkpoint . remote ( checkpoint_path )) # type: ignore def load_checkpoint ( self , checkpoint_path ) -> None : ray . get ( self . _rap . actor . load_checkpoint . remote ( checkpoint_path )) # type: ignore","title":"RemoteEpisodeParameterProvider"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.get_validator","text":"Get the validator for this class.","title":"get_validator"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.compute_metrics","text":"Get metrics on the operation of this provider. Often used in on_episode_end training callbacks. Source code in corl/episode_parameter_providers/remote.py def compute_metrics ( self ) -> typing . Dict [ str , typing . Any ]: return ray . get ( self . _rap . actor . compute_metrics . remote ()) # type: ignore","title":"compute_metrics()"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.kill_actor","text":"Kill the underlying actor used by this provider. Source code in corl/episode_parameter_providers/remote.py def kill_actor ( self ) -> None : \"\"\"Kill the underlying actor used by this provider.\"\"\" try : ray . kill ( self . _rap . actor ) except ray . exceptions . RaySystemError : pass","title":"kill_actor()"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.load_checkpoint","text":"Load the internal state from a checkpoint.","title":"load_checkpoint()"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.load_checkpoint--parameters","text":"checkpoint_path : PathLike Filesystem path from which to restore the checkpoint Source code in corl/episode_parameter_providers/remote.py def load_checkpoint ( self , checkpoint_path ) -> None : ray . get ( self . _rap . actor . load_checkpoint . remote ( checkpoint_path )) # type: ignore","title":"Parameters"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.save_checkpoint","text":"Save the internal state of the parameter provider.","title":"save_checkpoint()"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.save_checkpoint--parameters","text":"checkpoint_path : PathLike Filesystem path at which to save the checkpoint Source code in corl/episode_parameter_providers/remote.py def save_checkpoint ( self , checkpoint_path ) -> None : ray . get ( self . _rap . actor . save_checkpoint . remote ( checkpoint_path )) # type: ignore","title":"Parameters"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.update","text":"Update the operation of this provider. Often used in on_train_result training callbacks.","title":"update()"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.update--parameters","text":"results : dict As described by ray.rllib.agents.callbacks.DefaultCallbacks.on_train_result. See https://docs.ray.io/en/master/_modules/ray/rllib/agents/callbacks.html#DefaultCallbacks.on_train_result rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/episode_parameter_providers/remote.py def update ( self , results : dict , rng : Randomness ) -> None : if isinstance ( rng , Generator ): seed = rng . integers ( low = 0 , high = 1000000 ) new_rng = np . random . default_rng ( seed = seed ) elif isinstance ( rng , RandomState ): seed = rng . randint ( low = 0 , high = 1000000 ) new_rng , _ = seeding . np_random ( seed ) else : raise RuntimeError ( f \"rng type provided to function was { rng } , but this class only knows numpy Generator or RandomState\" ) ray . get ( self . _rap . actor . update . remote ( results , new_rng )) # type: ignore","title":"Parameters"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProvider.wrap_epp_factory","text":"Wraps an existing EpisodeParameterProvider Factory as a RemoteEpisodeParameterProvider Source code in corl/episode_parameter_providers/remote.py @staticmethod def wrap_epp_factory ( epp_factory : Factory , actor_name : str , namespace : str = None ) -> Factory : \"\"\"Wraps an existing EpisodeParameterProvider Factory as a RemoteEpisodeParameterProvider\"\"\" if not issubclass ( epp_factory . type , EpisodeParameterProvider ): # type: ignore raise TypeError ( f \"Invalid Factory.type: { epp_factory . type } , { EpisodeParameterProvider . __qualname__ } required\" ) remote_config = { 'namespace' : namespace , 'actor_name' : actor_name , 'internal_class' : epp_factory . type , 'internal_config' : epp_factory . config } return Factory ( type = RemoteEpisodeParameterProvider , config = remote_config )","title":"wrap_epp_factory()"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProviderValidator","text":"Validation model for the inputs of RemoteEpisodeParameterProvider Source code in corl/episode_parameter_providers/remote.py class RemoteEpisodeParameterProviderValidator ( EpisodeParameterProviderValidator ): \"\"\"Validation model for the inputs of RemoteEpisodeParameterProvider\"\"\" internal_class : typing . Type [ EpisodeParameterProvider ] internal_config : typing . Dict [ str , typing . Any ] = {} actor_name : str namespace : typing . Optional [ str ] = None @validator ( 'internal_class' ) def internal_not_remote ( cls , v ): \"\"\"Confirm that internal class is not also remote\"\"\" assert v != RemoteEpisodeParameterProvider return v","title":"RemoteEpisodeParameterProviderValidator"},{"location":"reference/episode_parameter_providers/remote/#corl.episode_parameter_providers.remote.RemoteEpisodeParameterProviderValidator.internal_not_remote","text":"Confirm that internal class is not also remote Source code in corl/episode_parameter_providers/remote.py @validator ( 'internal_class' ) def internal_not_remote ( cls , v ): \"\"\"Confirm that internal class is not also remote\"\"\" assert v != RemoteEpisodeParameterProvider return v","title":"internal_not_remote()"},{"location":"reference/episode_parameter_providers/simple/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. SimpleParameterProvider ( EpisodeParameterProvider ) \u00a4 EpisodeParameterProvider that does nothing but return the default. Source code in corl/episode_parameter_providers/simple.py class SimpleParameterProvider ( EpisodeParameterProvider ): \"\"\"EpisodeParameterProvider that does nothing but return the default.\"\"\" def _do_get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: return self . config . parameters , None","title":"Simple"},{"location":"reference/episode_parameter_providers/simple/#corl.episode_parameter_providers.simple.SimpleParameterProvider","text":"EpisodeParameterProvider that does nothing but return the default. Source code in corl/episode_parameter_providers/simple.py class SimpleParameterProvider ( EpisodeParameterProvider ): \"\"\"EpisodeParameterProvider that does nothing but return the default.\"\"\" def _do_get_params ( self , rng : Randomness ) -> typing . Tuple [ ParameterModel , typing . Union [ int , None ]]: return self . config . parameters , None","title":"SimpleParameterProvider"},{"location":"reference/experiments/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Experiments"},{"location":"reference/experiments/base_experiment/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseAutoDetect \u00a4 Base class interface for setting rllib config if in auto mode Source code in corl/experiments/base_experiment.py class BaseAutoDetect : \"\"\"Base class interface for setting rllib config if in auto mode \"\"\" def autodetect_system ( self ) -> str : \"\"\"gets the default system based on user defined function Returns ------- str the base system to use \"\"\" return \"local\" autodetect_system ( self ) \u00a4 gets the default system based on user defined function Returns \u00a4 str the base system to use Source code in corl/experiments/base_experiment.py def autodetect_system ( self ) -> str : \"\"\"gets the default system based on user defined function Returns ------- str the base system to use \"\"\" return \"local\" BaseExperiment ( ABC ) \u00a4 Experiment provides an anstract class to run specific types of experiments this allows users to do specific setup steps or to run some sort of custom training loop Source code in corl/experiments/base_experiment.py class BaseExperiment ( abc . ABC ): \"\"\" Experiment provides an anstract class to run specific types of experiments this allows users to do specific setup steps or to run some sort of custom training loop \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseExperimentValidator = self . get_validator ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseExperimentValidator ]: \"\"\" Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class \"\"\" return BaseExperimentValidator @property def get_policy_validator ( self ) -> typing . Type [ BasePolicyValidator ]: \"\"\" Get the policy validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the policy config \"\"\" return BasePolicyValidator @abc . abstractmethod def run_experiment ( self , args : argparse . Namespace ): \"\"\" Runs the experiment associated with this experiment class Arguments: args {argparse.Namespace} -- The args provided by the argparse in corl.train_rl \"\"\" ... def create_agents ( self , platform_configs : typing . Sequence [ typing . Tuple [ str , str ]], agent_configs : typing . Sequence [ typing . Tuple [ str , str , str , str ]] ) -> typing . Tuple [ dict , dict ]: \"\"\"Create the requested agents and add them to the environment configuration. Parameters ---------- agent_configs : typing.Sequence[typing.Tuple[str, str, str, str]] A sequence of agents. Each agent consists of a name, configuration filename, platform filename and policy configuration filename. \"\"\" platforms = {} for platform_name , platform_file in platform_configs : assert platform_name not in platforms , 'duplicate platforms not allowed' platform_config = load_file ( platform_file ) platforms [ platform_name ] = platform_config agents = {} for policy_name , platform_name , agent_file , policy_file in agent_configs : assert platform_name in platforms , f \"invalid platform ' { platform_name } ' not in { platforms } \" config = load_file ( agent_file ) parsed_agent = AgentParseBase ( ** config ) policy_config = load_file ( policy_file ) parsed_policy = self . get_policy_validator ( ** policy_config ) agents [ policy_name ] = AgentParseInfo ( class_config = parsed_agent , platform_name = platform_name , policy_config = parsed_policy ) return agents , platforms @staticmethod def create_other_platforms ( other_platforms_config : typing . Sequence [ typing . Tuple [ str , str ]]) -> dict : \"\"\"Create the requested other platforms and add them to the environment configuration. Parameters ---------- other_platforms_config : typing.Sequence[typing.Tuple[str, str]] A sequence of platforms. Each platform consists of a name and platform filename. \"\"\" other_platforms = dict () if other_platforms_config : for platform_name , platform_file in other_platforms_config : platform_config = load_file ( platform_file ) other_platforms [ platform_name ] = platform_config return other_platforms get_policy_validator : Type [ corl . policies . base_policy . BasePolicyValidator ] property readonly \u00a4 Get the policy validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the policy config get_validator : Type [ corl . experiments . base_experiment . BaseExperimentValidator ] property readonly \u00a4 Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class create_agents ( self , platform_configs , agent_configs ) \u00a4 Create the requested agents and add them to the environment configuration. Parameters \u00a4 agent_configs : typing.Sequence[typing.Tuple[str, str, str, str]] A sequence of agents. Each agent consists of a name, configuration filename, platform filename and policy configuration filename. Source code in corl/experiments/base_experiment.py def create_agents ( self , platform_configs : typing . Sequence [ typing . Tuple [ str , str ]], agent_configs : typing . Sequence [ typing . Tuple [ str , str , str , str ]] ) -> typing . Tuple [ dict , dict ]: \"\"\"Create the requested agents and add them to the environment configuration. Parameters ---------- agent_configs : typing.Sequence[typing.Tuple[str, str, str, str]] A sequence of agents. Each agent consists of a name, configuration filename, platform filename and policy configuration filename. \"\"\" platforms = {} for platform_name , platform_file in platform_configs : assert platform_name not in platforms , 'duplicate platforms not allowed' platform_config = load_file ( platform_file ) platforms [ platform_name ] = platform_config agents = {} for policy_name , platform_name , agent_file , policy_file in agent_configs : assert platform_name in platforms , f \"invalid platform ' { platform_name } ' not in { platforms } \" config = load_file ( agent_file ) parsed_agent = AgentParseBase ( ** config ) policy_config = load_file ( policy_file ) parsed_policy = self . get_policy_validator ( ** policy_config ) agents [ policy_name ] = AgentParseInfo ( class_config = parsed_agent , platform_name = platform_name , policy_config = parsed_policy ) return agents , platforms create_other_platforms ( other_platforms_config ) staticmethod \u00a4 Create the requested other platforms and add them to the environment configuration. Parameters \u00a4 other_platforms_config : typing.Sequence[typing.Tuple[str, str]] A sequence of platforms. Each platform consists of a name and platform filename. Source code in corl/experiments/base_experiment.py @staticmethod def create_other_platforms ( other_platforms_config : typing . Sequence [ typing . Tuple [ str , str ]]) -> dict : \"\"\"Create the requested other platforms and add them to the environment configuration. Parameters ---------- other_platforms_config : typing.Sequence[typing.Tuple[str, str]] A sequence of platforms. Each platform consists of a name and platform filename. \"\"\" other_platforms = dict () if other_platforms_config : for platform_name , platform_file in other_platforms_config : platform_config = load_file ( platform_file ) other_platforms [ platform_name ] = platform_config return other_platforms run_experiment ( self , args ) \u00a4 Runs the experiment associated with this experiment class Source code in corl/experiments/base_experiment.py @abc . abstractmethod def run_experiment ( self , args : argparse . Namespace ): \"\"\" Runs the experiment associated with this experiment class Arguments: args {argparse.Namespace} -- The args provided by the argparse in corl.train_rl \"\"\" ... BaseExperimentValidator ( BaseModel ) pydantic-model \u00a4 Base Validator to subclass for Experiments subclassing BaseExperiment Source code in corl/experiments/base_experiment.py class BaseExperimentValidator ( BaseModel ): \"\"\" Base Validator to subclass for Experiments subclassing BaseExperiment \"\"\" ... ExperimentParse ( BaseModel ) pydantic-model \u00a4 [summary] experiment_class: The experiment class to run config: the configuration to pass to that experiment Source code in corl/experiments/base_experiment.py class ExperimentParse ( BaseModel ): \"\"\"[summary] experiment_class: The experiment class to run config: the configuration to pass to that experiment \"\"\" experiment_class : PyObject auto_system_detect_class : typing . Optional [ PyObject ] = None config : typing . Dict [ str , typing . Any ] @validator ( 'experiment_class' ) def check_experiment_class ( cls , v ): \"\"\" Validates the experiment class actually subclasses BaseExperiment Class \"\"\" if not issubclass ( v , BaseExperiment ): raise ValueError ( f \"Experiment functors must subclass BaseExperiment, but experiment { v } \" ) return v @validator ( 'auto_system_detect_class' ) def check_auto_system_detect_class ( cls , v ): \"\"\" Validates the auto system detect class actually subclasses BaseAutoDetect Class \"\"\" if v is not None : if not issubclass ( v , BaseAutoDetect ): raise ValueError ( f \"Experiment functors must subclass BaseAutoDetect, but experiment { v } \" ) return v check_auto_system_detect_class ( v ) classmethod \u00a4 Validates the auto system detect class actually subclasses BaseAutoDetect Class Source code in corl/experiments/base_experiment.py @validator ( 'auto_system_detect_class' ) def check_auto_system_detect_class ( cls , v ): \"\"\" Validates the auto system detect class actually subclasses BaseAutoDetect Class \"\"\" if v is not None : if not issubclass ( v , BaseAutoDetect ): raise ValueError ( f \"Experiment functors must subclass BaseAutoDetect, but experiment { v } \" ) return v check_experiment_class ( v ) classmethod \u00a4 Validates the experiment class actually subclasses BaseExperiment Class Source code in corl/experiments/base_experiment.py @validator ( 'experiment_class' ) def check_experiment_class ( cls , v ): \"\"\" Validates the experiment class actually subclasses BaseExperiment Class \"\"\" if not issubclass ( v , BaseExperiment ): raise ValueError ( f \"Experiment functors must subclass BaseExperiment, but experiment { v } \" ) return v","title":"Base experiment"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseAutoDetect","text":"Base class interface for setting rllib config if in auto mode Source code in corl/experiments/base_experiment.py class BaseAutoDetect : \"\"\"Base class interface for setting rllib config if in auto mode \"\"\" def autodetect_system ( self ) -> str : \"\"\"gets the default system based on user defined function Returns ------- str the base system to use \"\"\" return \"local\"","title":"BaseAutoDetect"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseAutoDetect.autodetect_system","text":"gets the default system based on user defined function","title":"autodetect_system()"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseAutoDetect.autodetect_system--returns","text":"str the base system to use Source code in corl/experiments/base_experiment.py def autodetect_system ( self ) -> str : \"\"\"gets the default system based on user defined function Returns ------- str the base system to use \"\"\" return \"local\"","title":"Returns"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperiment","text":"Experiment provides an anstract class to run specific types of experiments this allows users to do specific setup steps or to run some sort of custom training loop Source code in corl/experiments/base_experiment.py class BaseExperiment ( abc . ABC ): \"\"\" Experiment provides an anstract class to run specific types of experiments this allows users to do specific setup steps or to run some sort of custom training loop \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseExperimentValidator = self . get_validator ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseExperimentValidator ]: \"\"\" Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class \"\"\" return BaseExperimentValidator @property def get_policy_validator ( self ) -> typing . Type [ BasePolicyValidator ]: \"\"\" Get the policy validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the policy config \"\"\" return BasePolicyValidator @abc . abstractmethod def run_experiment ( self , args : argparse . Namespace ): \"\"\" Runs the experiment associated with this experiment class Arguments: args {argparse.Namespace} -- The args provided by the argparse in corl.train_rl \"\"\" ... def create_agents ( self , platform_configs : typing . Sequence [ typing . Tuple [ str , str ]], agent_configs : typing . Sequence [ typing . Tuple [ str , str , str , str ]] ) -> typing . Tuple [ dict , dict ]: \"\"\"Create the requested agents and add them to the environment configuration. Parameters ---------- agent_configs : typing.Sequence[typing.Tuple[str, str, str, str]] A sequence of agents. Each agent consists of a name, configuration filename, platform filename and policy configuration filename. \"\"\" platforms = {} for platform_name , platform_file in platform_configs : assert platform_name not in platforms , 'duplicate platforms not allowed' platform_config = load_file ( platform_file ) platforms [ platform_name ] = platform_config agents = {} for policy_name , platform_name , agent_file , policy_file in agent_configs : assert platform_name in platforms , f \"invalid platform ' { platform_name } ' not in { platforms } \" config = load_file ( agent_file ) parsed_agent = AgentParseBase ( ** config ) policy_config = load_file ( policy_file ) parsed_policy = self . get_policy_validator ( ** policy_config ) agents [ policy_name ] = AgentParseInfo ( class_config = parsed_agent , platform_name = platform_name , policy_config = parsed_policy ) return agents , platforms @staticmethod def create_other_platforms ( other_platforms_config : typing . Sequence [ typing . Tuple [ str , str ]]) -> dict : \"\"\"Create the requested other platforms and add them to the environment configuration. Parameters ---------- other_platforms_config : typing.Sequence[typing.Tuple[str, str]] A sequence of platforms. Each platform consists of a name and platform filename. \"\"\" other_platforms = dict () if other_platforms_config : for platform_name , platform_file in other_platforms_config : platform_config = load_file ( platform_file ) other_platforms [ platform_name ] = platform_config return other_platforms","title":"BaseExperiment"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperiment.get_policy_validator","text":"Get the policy validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the policy config","title":"get_policy_validator"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperiment.get_validator","text":"Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class","title":"get_validator"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperiment.create_agents","text":"Create the requested agents and add them to the environment configuration.","title":"create_agents()"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperiment.create_agents--parameters","text":"agent_configs : typing.Sequence[typing.Tuple[str, str, str, str]] A sequence of agents. Each agent consists of a name, configuration filename, platform filename and policy configuration filename. Source code in corl/experiments/base_experiment.py def create_agents ( self , platform_configs : typing . Sequence [ typing . Tuple [ str , str ]], agent_configs : typing . Sequence [ typing . Tuple [ str , str , str , str ]] ) -> typing . Tuple [ dict , dict ]: \"\"\"Create the requested agents and add them to the environment configuration. Parameters ---------- agent_configs : typing.Sequence[typing.Tuple[str, str, str, str]] A sequence of agents. Each agent consists of a name, configuration filename, platform filename and policy configuration filename. \"\"\" platforms = {} for platform_name , platform_file in platform_configs : assert platform_name not in platforms , 'duplicate platforms not allowed' platform_config = load_file ( platform_file ) platforms [ platform_name ] = platform_config agents = {} for policy_name , platform_name , agent_file , policy_file in agent_configs : assert platform_name in platforms , f \"invalid platform ' { platform_name } ' not in { platforms } \" config = load_file ( agent_file ) parsed_agent = AgentParseBase ( ** config ) policy_config = load_file ( policy_file ) parsed_policy = self . get_policy_validator ( ** policy_config ) agents [ policy_name ] = AgentParseInfo ( class_config = parsed_agent , platform_name = platform_name , policy_config = parsed_policy ) return agents , platforms","title":"Parameters"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperiment.create_other_platforms","text":"Create the requested other platforms and add them to the environment configuration.","title":"create_other_platforms()"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperiment.create_other_platforms--parameters","text":"other_platforms_config : typing.Sequence[typing.Tuple[str, str]] A sequence of platforms. Each platform consists of a name and platform filename. Source code in corl/experiments/base_experiment.py @staticmethod def create_other_platforms ( other_platforms_config : typing . Sequence [ typing . Tuple [ str , str ]]) -> dict : \"\"\"Create the requested other platforms and add them to the environment configuration. Parameters ---------- other_platforms_config : typing.Sequence[typing.Tuple[str, str]] A sequence of platforms. Each platform consists of a name and platform filename. \"\"\" other_platforms = dict () if other_platforms_config : for platform_name , platform_file in other_platforms_config : platform_config = load_file ( platform_file ) other_platforms [ platform_name ] = platform_config return other_platforms","title":"Parameters"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperiment.run_experiment","text":"Runs the experiment associated with this experiment class Source code in corl/experiments/base_experiment.py @abc . abstractmethod def run_experiment ( self , args : argparse . Namespace ): \"\"\" Runs the experiment associated with this experiment class Arguments: args {argparse.Namespace} -- The args provided by the argparse in corl.train_rl \"\"\" ...","title":"run_experiment()"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.BaseExperimentValidator","text":"Base Validator to subclass for Experiments subclassing BaseExperiment Source code in corl/experiments/base_experiment.py class BaseExperimentValidator ( BaseModel ): \"\"\" Base Validator to subclass for Experiments subclassing BaseExperiment \"\"\" ...","title":"BaseExperimentValidator"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.ExperimentParse","text":"[summary] experiment_class: The experiment class to run config: the configuration to pass to that experiment Source code in corl/experiments/base_experiment.py class ExperimentParse ( BaseModel ): \"\"\"[summary] experiment_class: The experiment class to run config: the configuration to pass to that experiment \"\"\" experiment_class : PyObject auto_system_detect_class : typing . Optional [ PyObject ] = None config : typing . Dict [ str , typing . Any ] @validator ( 'experiment_class' ) def check_experiment_class ( cls , v ): \"\"\" Validates the experiment class actually subclasses BaseExperiment Class \"\"\" if not issubclass ( v , BaseExperiment ): raise ValueError ( f \"Experiment functors must subclass BaseExperiment, but experiment { v } \" ) return v @validator ( 'auto_system_detect_class' ) def check_auto_system_detect_class ( cls , v ): \"\"\" Validates the auto system detect class actually subclasses BaseAutoDetect Class \"\"\" if v is not None : if not issubclass ( v , BaseAutoDetect ): raise ValueError ( f \"Experiment functors must subclass BaseAutoDetect, but experiment { v } \" ) return v","title":"ExperimentParse"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.ExperimentParse.check_auto_system_detect_class","text":"Validates the auto system detect class actually subclasses BaseAutoDetect Class Source code in corl/experiments/base_experiment.py @validator ( 'auto_system_detect_class' ) def check_auto_system_detect_class ( cls , v ): \"\"\" Validates the auto system detect class actually subclasses BaseAutoDetect Class \"\"\" if v is not None : if not issubclass ( v , BaseAutoDetect ): raise ValueError ( f \"Experiment functors must subclass BaseAutoDetect, but experiment { v } \" ) return v","title":"check_auto_system_detect_class()"},{"location":"reference/experiments/base_experiment/#corl.experiments.base_experiment.ExperimentParse.check_experiment_class","text":"Validates the experiment class actually subclasses BaseExperiment Class Source code in corl/experiments/base_experiment.py @validator ( 'experiment_class' ) def check_experiment_class ( cls , v ): \"\"\" Validates the experiment class actually subclasses BaseExperiment Class \"\"\" if not issubclass ( v , BaseExperiment ): raise ValueError ( f \"Experiment functors must subclass BaseExperiment, but experiment { v } \" ) return v","title":"check_experiment_class()"},{"location":"reference/experiments/benchmark_experiment/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BenchmarkExperiment ( BaseExperiment ) \u00a4 The Rllib Experiment is an experiment for running multiagent configurable environments with patchable settings Source code in corl/experiments/benchmark_experiment.py class BenchmarkExperiment ( BaseExperiment ): \"\"\" The Rllib Experiment is an experiment for running multiagent configurable environments with patchable settings \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BenchmarkExperimentValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BenchmarkExperimentValidator ]: return BenchmarkExperimentValidator @property def get_policy_validator ( self ) -> typing . Type [ RllibPolicyValidator ]: \"\"\"Return validator\"\"\" return RllibPolicyValidator def run_experiment ( self , args : argparse . Namespace ) -> None : rllib_config = self . _select_rllib_config ( args . compute_platform ) if args . compute_platform in [ 'ray' ]: self . _update_ray_config_for_ray_platform () if args . debug : rllib_config [ 'num_workers' ] = 0 self . config . ray_config [ 'local_mode' ] = True self . _add_trial_creator () ray . init ( ** self . config . ray_config ) self . config . env_config [ \"agents\" ], self . config . env_config [ \"agent_platforms\" ] = self . create_agents ( args . platform_config , args . agent_config ) self . config . env_config [ \"horizon\" ] = rllib_config [ \"horizon\" ] if args . other_platform : self . config . env_config [ \"other_platforms\" ] = self . create_other_platforms ( args . other_platform ) if not self . config . ray_config [ 'local_mode' ]: self . config . env_config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** self . config . env_config [ 'episode_parameter_provider' ]), actor_name = ACT3MultiAgentEnv . episode_parameter_provider_name ) for agent_name , agent_configs in self . config . env_config [ 'agents' ] . items (): agent_configs . class_config . config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** agent_configs . class_config . config [ 'episode_parameter_provider' ]), agent_name ) self . config . env_config [ 'epp_registry' ] = ACT3MultiAgentEnvValidator ( ** self . config . env_config ) . epp_registry tmp = ACT3MultiAgentEnv ( self . config . env_config ) act_space = tmp . action_space env = tmp profiler = Profiler () profiler . start () # temp = {} # retrieve action # if sanity_check_state_dict: # multi_actions_list = sanity_check_state_dict[\"action\"] # else: # multi_actions_list = None total_timesteps = 0 for ep in range ( 10 ): st = time . time () # obs = env.reset() env . reset () # if debug_print: # print(f\"First obs: {obs}\") done = False step = 0 # temp[ep] = {} # temp[ep][\"rew\"] = [] # temp[ep][\"obs\"] = [] # temp[ep][\"multi_done\"] = [] # temp[ep][\"info\"] = [] # temp[ep][\"step_data\"] = [] while not done : # Start keep track the states of platforms # temp_step_data = {} # if multi_actions_list: # if (not done) and (step >= len(multi_actions_list)): # print(\"Reached the end of recorded actions but still not done\") # break # multi_actions = multi_actions_list[step] # else: # generate a random action multi_actions = self . generate_action ( act_space ) # try: # if skip_actions: # obs, rew, multi_done, info = env.step({}) # else: # obs, rew, multi_done, info = env.step(multi_actions) _ , _ , multi_done , _ = env . step ( multi_actions ) # except Exception as e: # pylint: disable=broad-except # print(f'Failed at episode {ep} step {step} with error: {e} \\n Simulator outputs are saved at {env.output_path}') # break # Extract platform state data for further analysis # if export_step_data: # temp_step_data = platforms_data_extractor(env.state.sim_platforms, temp_step_data) # temp[ep][\"rew\"].append(rew) # temp[ep][\"obs\"].append(obs) # temp[ep][\"multi_done\"].append(str(multi_done)) # temp[ep][\"info\"].append(info) # temp[ep][\"step_data\"].append(temp_step_data) # debug_func(debug_print, env, step, obs, rew, multi_done, info) done = multi_done [ \"__all__\" ] step += 1 total_timesteps += step et = time . time () print ( f \" { ep } :SPS: { step / ( et - st ) } , { step } \" ) profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) def generate_action ( self , act_space ): \"\"\" randomly select an action to take \"\"\" # generate a random action multi_actions = { a_k : { s_k : s . sample () for s_k , s in a_s . spaces . items ()} for a_k , a_s in act_space . spaces . items ()} return multi_actions def get_callbacks ( self ) -> typing . Type [ EnvironmentDefaultCallbacks ]: \"\"\"Get the environment callbacks\"\"\" return EnvironmentDefaultCallbacks def _select_rllib_config ( self , platform : typing . Optional [ str ]) -> typing . Dict [ str , typing . Any ]: \"\"\"Extract the rllib config for the proper computational platform Parameters ---------- platform : typing.Optional[str] Specification of the computational platform to use, such as \"local\", \"hpc\", etc. This must be present in the rllib_configs. If None, the rllib_configs must only have a single entry. Returns ------- typing.Dict[str, typing.Any] Rllib configuration for the desired computational platform. Raises ------ RuntimeError The requested computational platform does not exist or None was used when multiple platforms were defined. \"\"\" if platform is not None : return self . config . rllib_configs [ platform ] if len ( self . config . rllib_configs ) == 1 : return self . config . rllib_configs [ next ( iter ( self . config . rllib_configs ))] raise RuntimeError ( f 'Invalid rllib_config for platform \" { platform } \"' ) def _update_ray_config_for_ray_platform ( self ) -> None : \"\"\"Update the ray configuration for ray platforms \"\"\" self . config . ray_config [ 'address' ] = 'auto' self . config . ray_config [ 'log_to_driver' ] = False def _enable_episode_parameter_provider_checkpointing ( self ) -> None : base_trainer = self . config . tune_config [ \"run_or_experiment\" ] trainer_class = get_trainable_cls ( base_trainer ) class EpisodeParameterProviderSavingTrainer ( trainer_class ): # type: ignore[valid-type, misc] \"\"\" Tune Trainer that adds capability to restore progress of the EpisodeParameterProvider on restoring training progress \"\"\" def save_checkpoint ( self , checkpoint_path ): \"\"\" adds additional checkpoint saving functionality by also saving any episode parameter providers currently running \"\"\" tmp = super () . save_checkpoint ( checkpoint_path ) checkpoint_folder = pathlib . Path ( checkpoint_path ) # Environment epp_name = ACT3MultiAgentEnv . episode_parameter_provider_name env = self . workers . local_worker () . env epp : EpisodeParameterProvider = env . config . epp epp . save_checkpoint ( checkpoint_folder / epp_name ) # Agents for agent_name , agent_configs in env . agent_dict . items (): epp = agent_configs . config . epp epp . save_checkpoint ( checkpoint_folder / agent_name ) return tmp def load_checkpoint ( self , checkpoint_path ): \"\"\" adds additional checkpoint loading functionality by also loading any episode parameter providers with the checkpoint \"\"\" super () . load_checkpoint ( checkpoint_path ) checkpoint_folder = pathlib . Path ( checkpoint_path ) . parent # Environment epp_name = ACT3MultiAgentEnv . episode_parameter_provider_name env = self . workers . local_worker () . env epp : EpisodeParameterProvider = env . config . epp epp . load_checkpoint ( checkpoint_folder / epp_name ) # Agents for agent_name , agent_configs in env . agent_dict . items (): epp = agent_configs . config . epp epp . load_checkpoint ( checkpoint_folder / agent_name ) self . config . tune_config [ \"run_or_experiment\" ] = EpisodeParameterProviderSavingTrainer def _add_trial_creator ( self ): \"\"\"Updates the trial name based on the HPC Job Number and the trial name in the configuration \"\"\" if \"trial_name_creator\" not in self . config . tune_config : def trial_name_prefix ( trial ): \"\"\" Args: trial (Trial): A generated trial object. Returns: trial_name (str): String representation of Trial prefixed by the contents of the environment variable: TRIAL_NAME_PREFIX Or the prefix 'RUN' if none is set. \"\"\" trial_prefix = os . environ . get ( 'PBS_JOBID' , os . environ . get ( 'TRIAL_NAME_PREFIX' , \"\" )) trial_name = \"\" if \"TrialName\" in self . config . env_config . keys (): if trial_prefix : trial_name = \"-\" + self . config . env_config [ \"TrialName\" ] else : trial_name = self . config . env_config [ \"TrialName\" ] return f \" { trial_prefix }{ trial_name } - { trial } \" self . config . tune_config [ \"trial_name_creator\" ] = trial_name_prefix get_policy_validator : Type [ corl . experiments . benchmark_experiment . RllibPolicyValidator ] property readonly \u00a4 Return validator get_validator : Type [ corl . experiments . benchmark_experiment . BenchmarkExperimentValidator ] property readonly \u00a4 Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class generate_action ( self , act_space ) \u00a4 randomly select an action to take Source code in corl/experiments/benchmark_experiment.py def generate_action ( self , act_space ): \"\"\" randomly select an action to take \"\"\" # generate a random action multi_actions = { a_k : { s_k : s . sample () for s_k , s in a_s . spaces . items ()} for a_k , a_s in act_space . spaces . items ()} return multi_actions get_callbacks ( self ) \u00a4 Get the environment callbacks Source code in corl/experiments/benchmark_experiment.py def get_callbacks ( self ) -> typing . Type [ EnvironmentDefaultCallbacks ]: \"\"\"Get the environment callbacks\"\"\" return EnvironmentDefaultCallbacks run_experiment ( self , args ) \u00a4 Runs the experiment associated with this experiment class Source code in corl/experiments/benchmark_experiment.py def run_experiment ( self , args : argparse . Namespace ) -> None : rllib_config = self . _select_rllib_config ( args . compute_platform ) if args . compute_platform in [ 'ray' ]: self . _update_ray_config_for_ray_platform () if args . debug : rllib_config [ 'num_workers' ] = 0 self . config . ray_config [ 'local_mode' ] = True self . _add_trial_creator () ray . init ( ** self . config . ray_config ) self . config . env_config [ \"agents\" ], self . config . env_config [ \"agent_platforms\" ] = self . create_agents ( args . platform_config , args . agent_config ) self . config . env_config [ \"horizon\" ] = rllib_config [ \"horizon\" ] if args . other_platform : self . config . env_config [ \"other_platforms\" ] = self . create_other_platforms ( args . other_platform ) if not self . config . ray_config [ 'local_mode' ]: self . config . env_config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** self . config . env_config [ 'episode_parameter_provider' ]), actor_name = ACT3MultiAgentEnv . episode_parameter_provider_name ) for agent_name , agent_configs in self . config . env_config [ 'agents' ] . items (): agent_configs . class_config . config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** agent_configs . class_config . config [ 'episode_parameter_provider' ]), agent_name ) self . config . env_config [ 'epp_registry' ] = ACT3MultiAgentEnvValidator ( ** self . config . env_config ) . epp_registry tmp = ACT3MultiAgentEnv ( self . config . env_config ) act_space = tmp . action_space env = tmp profiler = Profiler () profiler . start () # temp = {} # retrieve action # if sanity_check_state_dict: # multi_actions_list = sanity_check_state_dict[\"action\"] # else: # multi_actions_list = None total_timesteps = 0 for ep in range ( 10 ): st = time . time () # obs = env.reset() env . reset () # if debug_print: # print(f\"First obs: {obs}\") done = False step = 0 # temp[ep] = {} # temp[ep][\"rew\"] = [] # temp[ep][\"obs\"] = [] # temp[ep][\"multi_done\"] = [] # temp[ep][\"info\"] = [] # temp[ep][\"step_data\"] = [] while not done : # Start keep track the states of platforms # temp_step_data = {} # if multi_actions_list: # if (not done) and (step >= len(multi_actions_list)): # print(\"Reached the end of recorded actions but still not done\") # break # multi_actions = multi_actions_list[step] # else: # generate a random action multi_actions = self . generate_action ( act_space ) # try: # if skip_actions: # obs, rew, multi_done, info = env.step({}) # else: # obs, rew, multi_done, info = env.step(multi_actions) _ , _ , multi_done , _ = env . step ( multi_actions ) # except Exception as e: # pylint: disable=broad-except # print(f'Failed at episode {ep} step {step} with error: {e} \\n Simulator outputs are saved at {env.output_path}') # break # Extract platform state data for further analysis # if export_step_data: # temp_step_data = platforms_data_extractor(env.state.sim_platforms, temp_step_data) # temp[ep][\"rew\"].append(rew) # temp[ep][\"obs\"].append(obs) # temp[ep][\"multi_done\"].append(str(multi_done)) # temp[ep][\"info\"].append(info) # temp[ep][\"step_data\"].append(temp_step_data) # debug_func(debug_print, env, step, obs, rew, multi_done, info) done = multi_done [ \"__all__\" ] step += 1 total_timesteps += step et = time . time () print ( f \" { ep } :SPS: { step / ( et - st ) } , { step } \" ) profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) BenchmarkExperimentValidator ( BaseExperimentValidator ) pydantic-model \u00a4 ray_config: dictionary to be fed into ray init, validated by ray init call env_config: environment configuration, validated by environment class rllib_configs: a dictionary Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/benchmark_experiment.py class BenchmarkExperimentValidator ( BaseExperimentValidator ): \"\"\" ray_config: dictionary to be fed into ray init, validated by ray init call env_config: environment configuration, validated by environment class rllib_configs: a dictionary Arguments: BaseModel {[type]} -- [description] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" ray_config : typing . Dict [ str , typing . Any ] env_config : EnvContext rllib_configs : typing . Dict [ str , typing . Dict [ str , typing . Any ]] tune_config : typing . Dict [ str , typing . Any ] trainable_config : typing . Optional [ typing . Dict [ str , typing . Any ]] @validator ( 'rllib_configs' , pre = True ) def apply_patches_rllib_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" if not isinstance ( v , dict ): raise RuntimeError ( \"rllib_configs are expected to be a dict of keys to different compute configs\" ) rllib_configs = {} for key , value in v . items (): if isinstance ( value , list ): rllib_configs [ key ] = apply_patches ( value ) elif isinstance ( value , dict ): rllib_configs [ key ] = value return rllib_configs @validator ( 'ray_config' , 'tune_config' , 'trainable_config' , 'env_config' , pre = True ) def apply_patches_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: [type] -- [description] \"\"\" if isinstance ( v , list ): v = apply_patches ( v ) return v @validator ( 'env_config' ) def no_horizon ( cls , v ): \"\"\"Ensure that the horizon is not specified in the env_config.\"\"\" if 'horizon' in v : raise ValueError ( 'Cannot specify the horizon in the env_config' ) return v apply_patches_configs ( v ) classmethod \u00a4 reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: Type Description [type] -- [description] Source code in corl/experiments/benchmark_experiment.py @validator ( 'ray_config' , 'tune_config' , 'trainable_config' , 'env_config' , pre = True ) def apply_patches_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: [type] -- [description] \"\"\" if isinstance ( v , list ): v = apply_patches ( v ) return v apply_patches_rllib_configs ( v ) classmethod \u00a4 The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/benchmark_experiment.py @validator ( 'rllib_configs' , pre = True ) def apply_patches_rllib_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" if not isinstance ( v , dict ): raise RuntimeError ( \"rllib_configs are expected to be a dict of keys to different compute configs\" ) rllib_configs = {} for key , value in v . items (): if isinstance ( value , list ): rllib_configs [ key ] = apply_patches ( value ) elif isinstance ( value , dict ): rllib_configs [ key ] = value return rllib_configs no_horizon ( v ) classmethod \u00a4 Ensure that the horizon is not specified in the env_config. Source code in corl/experiments/benchmark_experiment.py @validator ( 'env_config' ) def no_horizon ( cls , v ): \"\"\"Ensure that the horizon is not specified in the env_config.\"\"\" if 'horizon' in v : raise ValueError ( 'Cannot specify the horizon in the env_config' ) return v RllibPolicyValidator ( BasePolicyValidator ) pydantic-model \u00a4 policy_class: callable policy class None will use default from trainer train: should this policy be trained Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/benchmark_experiment.py class RllibPolicyValidator ( BasePolicyValidator ): \"\"\" policy_class: callable policy class None will use default from trainer train: should this policy be trained Arguments: BaseModel {[type]} -- [description] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" config : typing . Dict [ str , typing . Any ] = {} policy_class : typing . Union [ PyObject , None ] = None train : bool = True","title":"Benchmark experiment"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperiment","text":"The Rllib Experiment is an experiment for running multiagent configurable environments with patchable settings Source code in corl/experiments/benchmark_experiment.py class BenchmarkExperiment ( BaseExperiment ): \"\"\" The Rllib Experiment is an experiment for running multiagent configurable environments with patchable settings \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BenchmarkExperimentValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BenchmarkExperimentValidator ]: return BenchmarkExperimentValidator @property def get_policy_validator ( self ) -> typing . Type [ RllibPolicyValidator ]: \"\"\"Return validator\"\"\" return RllibPolicyValidator def run_experiment ( self , args : argparse . Namespace ) -> None : rllib_config = self . _select_rllib_config ( args . compute_platform ) if args . compute_platform in [ 'ray' ]: self . _update_ray_config_for_ray_platform () if args . debug : rllib_config [ 'num_workers' ] = 0 self . config . ray_config [ 'local_mode' ] = True self . _add_trial_creator () ray . init ( ** self . config . ray_config ) self . config . env_config [ \"agents\" ], self . config . env_config [ \"agent_platforms\" ] = self . create_agents ( args . platform_config , args . agent_config ) self . config . env_config [ \"horizon\" ] = rllib_config [ \"horizon\" ] if args . other_platform : self . config . env_config [ \"other_platforms\" ] = self . create_other_platforms ( args . other_platform ) if not self . config . ray_config [ 'local_mode' ]: self . config . env_config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** self . config . env_config [ 'episode_parameter_provider' ]), actor_name = ACT3MultiAgentEnv . episode_parameter_provider_name ) for agent_name , agent_configs in self . config . env_config [ 'agents' ] . items (): agent_configs . class_config . config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** agent_configs . class_config . config [ 'episode_parameter_provider' ]), agent_name ) self . config . env_config [ 'epp_registry' ] = ACT3MultiAgentEnvValidator ( ** self . config . env_config ) . epp_registry tmp = ACT3MultiAgentEnv ( self . config . env_config ) act_space = tmp . action_space env = tmp profiler = Profiler () profiler . start () # temp = {} # retrieve action # if sanity_check_state_dict: # multi_actions_list = sanity_check_state_dict[\"action\"] # else: # multi_actions_list = None total_timesteps = 0 for ep in range ( 10 ): st = time . time () # obs = env.reset() env . reset () # if debug_print: # print(f\"First obs: {obs}\") done = False step = 0 # temp[ep] = {} # temp[ep][\"rew\"] = [] # temp[ep][\"obs\"] = [] # temp[ep][\"multi_done\"] = [] # temp[ep][\"info\"] = [] # temp[ep][\"step_data\"] = [] while not done : # Start keep track the states of platforms # temp_step_data = {} # if multi_actions_list: # if (not done) and (step >= len(multi_actions_list)): # print(\"Reached the end of recorded actions but still not done\") # break # multi_actions = multi_actions_list[step] # else: # generate a random action multi_actions = self . generate_action ( act_space ) # try: # if skip_actions: # obs, rew, multi_done, info = env.step({}) # else: # obs, rew, multi_done, info = env.step(multi_actions) _ , _ , multi_done , _ = env . step ( multi_actions ) # except Exception as e: # pylint: disable=broad-except # print(f'Failed at episode {ep} step {step} with error: {e} \\n Simulator outputs are saved at {env.output_path}') # break # Extract platform state data for further analysis # if export_step_data: # temp_step_data = platforms_data_extractor(env.state.sim_platforms, temp_step_data) # temp[ep][\"rew\"].append(rew) # temp[ep][\"obs\"].append(obs) # temp[ep][\"multi_done\"].append(str(multi_done)) # temp[ep][\"info\"].append(info) # temp[ep][\"step_data\"].append(temp_step_data) # debug_func(debug_print, env, step, obs, rew, multi_done, info) done = multi_done [ \"__all__\" ] step += 1 total_timesteps += step et = time . time () print ( f \" { ep } :SPS: { step / ( et - st ) } , { step } \" ) profiler . stop () print ( profiler . output_text ( unicode = True , color = True )) def generate_action ( self , act_space ): \"\"\" randomly select an action to take \"\"\" # generate a random action multi_actions = { a_k : { s_k : s . sample () for s_k , s in a_s . spaces . items ()} for a_k , a_s in act_space . spaces . items ()} return multi_actions def get_callbacks ( self ) -> typing . Type [ EnvironmentDefaultCallbacks ]: \"\"\"Get the environment callbacks\"\"\" return EnvironmentDefaultCallbacks def _select_rllib_config ( self , platform : typing . Optional [ str ]) -> typing . Dict [ str , typing . Any ]: \"\"\"Extract the rllib config for the proper computational platform Parameters ---------- platform : typing.Optional[str] Specification of the computational platform to use, such as \"local\", \"hpc\", etc. This must be present in the rllib_configs. If None, the rllib_configs must only have a single entry. Returns ------- typing.Dict[str, typing.Any] Rllib configuration for the desired computational platform. Raises ------ RuntimeError The requested computational platform does not exist or None was used when multiple platforms were defined. \"\"\" if platform is not None : return self . config . rllib_configs [ platform ] if len ( self . config . rllib_configs ) == 1 : return self . config . rllib_configs [ next ( iter ( self . config . rllib_configs ))] raise RuntimeError ( f 'Invalid rllib_config for platform \" { platform } \"' ) def _update_ray_config_for_ray_platform ( self ) -> None : \"\"\"Update the ray configuration for ray platforms \"\"\" self . config . ray_config [ 'address' ] = 'auto' self . config . ray_config [ 'log_to_driver' ] = False def _enable_episode_parameter_provider_checkpointing ( self ) -> None : base_trainer = self . config . tune_config [ \"run_or_experiment\" ] trainer_class = get_trainable_cls ( base_trainer ) class EpisodeParameterProviderSavingTrainer ( trainer_class ): # type: ignore[valid-type, misc] \"\"\" Tune Trainer that adds capability to restore progress of the EpisodeParameterProvider on restoring training progress \"\"\" def save_checkpoint ( self , checkpoint_path ): \"\"\" adds additional checkpoint saving functionality by also saving any episode parameter providers currently running \"\"\" tmp = super () . save_checkpoint ( checkpoint_path ) checkpoint_folder = pathlib . Path ( checkpoint_path ) # Environment epp_name = ACT3MultiAgentEnv . episode_parameter_provider_name env = self . workers . local_worker () . env epp : EpisodeParameterProvider = env . config . epp epp . save_checkpoint ( checkpoint_folder / epp_name ) # Agents for agent_name , agent_configs in env . agent_dict . items (): epp = agent_configs . config . epp epp . save_checkpoint ( checkpoint_folder / agent_name ) return tmp def load_checkpoint ( self , checkpoint_path ): \"\"\" adds additional checkpoint loading functionality by also loading any episode parameter providers with the checkpoint \"\"\" super () . load_checkpoint ( checkpoint_path ) checkpoint_folder = pathlib . Path ( checkpoint_path ) . parent # Environment epp_name = ACT3MultiAgentEnv . episode_parameter_provider_name env = self . workers . local_worker () . env epp : EpisodeParameterProvider = env . config . epp epp . load_checkpoint ( checkpoint_folder / epp_name ) # Agents for agent_name , agent_configs in env . agent_dict . items (): epp = agent_configs . config . epp epp . load_checkpoint ( checkpoint_folder / agent_name ) self . config . tune_config [ \"run_or_experiment\" ] = EpisodeParameterProviderSavingTrainer def _add_trial_creator ( self ): \"\"\"Updates the trial name based on the HPC Job Number and the trial name in the configuration \"\"\" if \"trial_name_creator\" not in self . config . tune_config : def trial_name_prefix ( trial ): \"\"\" Args: trial (Trial): A generated trial object. Returns: trial_name (str): String representation of Trial prefixed by the contents of the environment variable: TRIAL_NAME_PREFIX Or the prefix 'RUN' if none is set. \"\"\" trial_prefix = os . environ . get ( 'PBS_JOBID' , os . environ . get ( 'TRIAL_NAME_PREFIX' , \"\" )) trial_name = \"\" if \"TrialName\" in self . config . env_config . keys (): if trial_prefix : trial_name = \"-\" + self . config . env_config [ \"TrialName\" ] else : trial_name = self . config . env_config [ \"TrialName\" ] return f \" { trial_prefix }{ trial_name } - { trial } \" self . config . tune_config [ \"trial_name_creator\" ] = trial_name_prefix","title":"BenchmarkExperiment"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperiment.get_policy_validator","text":"Return validator","title":"get_policy_validator"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperiment.get_validator","text":"Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class","title":"get_validator"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperiment.generate_action","text":"randomly select an action to take Source code in corl/experiments/benchmark_experiment.py def generate_action ( self , act_space ): \"\"\" randomly select an action to take \"\"\" # generate a random action multi_actions = { a_k : { s_k : s . sample () for s_k , s in a_s . spaces . items ()} for a_k , a_s in act_space . spaces . items ()} return multi_actions","title":"generate_action()"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperiment.get_callbacks","text":"Get the environment callbacks Source code in corl/experiments/benchmark_experiment.py def get_callbacks ( self ) -> typing . Type [ EnvironmentDefaultCallbacks ]: \"\"\"Get the environment callbacks\"\"\" return EnvironmentDefaultCallbacks","title":"get_callbacks()"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperiment.run_experiment","text":"Runs the experiment associated with this experiment class Source code in corl/experiments/benchmark_experiment.py def run_experiment ( self , args : argparse . Namespace ) -> None : rllib_config = self . _select_rllib_config ( args . compute_platform ) if args . compute_platform in [ 'ray' ]: self . _update_ray_config_for_ray_platform () if args . debug : rllib_config [ 'num_workers' ] = 0 self . config . ray_config [ 'local_mode' ] = True self . _add_trial_creator () ray . init ( ** self . config . ray_config ) self . config . env_config [ \"agents\" ], self . config . env_config [ \"agent_platforms\" ] = self . create_agents ( args . platform_config , args . agent_config ) self . config . env_config [ \"horizon\" ] = rllib_config [ \"horizon\" ] if args . other_platform : self . config . env_config [ \"other_platforms\" ] = self . create_other_platforms ( args . other_platform ) if not self . config . ray_config [ 'local_mode' ]: self . config . env_config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** self . config . env_config [ 'episode_parameter_provider' ]), actor_name = ACT3MultiAgentEnv . episode_parameter_provider_name ) for agent_name , agent_configs in self . config . env_config [ 'agents' ] . items (): agent_configs . class_config . config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** agent_configs . class_config . config [ 'episode_parameter_provider' ]), agent_name ) self . config . env_config [ 'epp_registry' ] = ACT3MultiAgentEnvValidator ( ** self . config . env_config ) . epp_registry tmp = ACT3MultiAgentEnv ( self . config . env_config ) act_space = tmp . action_space env = tmp profiler = Profiler () profiler . start () # temp = {} # retrieve action # if sanity_check_state_dict: # multi_actions_list = sanity_check_state_dict[\"action\"] # else: # multi_actions_list = None total_timesteps = 0 for ep in range ( 10 ): st = time . time () # obs = env.reset() env . reset () # if debug_print: # print(f\"First obs: {obs}\") done = False step = 0 # temp[ep] = {} # temp[ep][\"rew\"] = [] # temp[ep][\"obs\"] = [] # temp[ep][\"multi_done\"] = [] # temp[ep][\"info\"] = [] # temp[ep][\"step_data\"] = [] while not done : # Start keep track the states of platforms # temp_step_data = {} # if multi_actions_list: # if (not done) and (step >= len(multi_actions_list)): # print(\"Reached the end of recorded actions but still not done\") # break # multi_actions = multi_actions_list[step] # else: # generate a random action multi_actions = self . generate_action ( act_space ) # try: # if skip_actions: # obs, rew, multi_done, info = env.step({}) # else: # obs, rew, multi_done, info = env.step(multi_actions) _ , _ , multi_done , _ = env . step ( multi_actions ) # except Exception as e: # pylint: disable=broad-except # print(f'Failed at episode {ep} step {step} with error: {e} \\n Simulator outputs are saved at {env.output_path}') # break # Extract platform state data for further analysis # if export_step_data: # temp_step_data = platforms_data_extractor(env.state.sim_platforms, temp_step_data) # temp[ep][\"rew\"].append(rew) # temp[ep][\"obs\"].append(obs) # temp[ep][\"multi_done\"].append(str(multi_done)) # temp[ep][\"info\"].append(info) # temp[ep][\"step_data\"].append(temp_step_data) # debug_func(debug_print, env, step, obs, rew, multi_done, info) done = multi_done [ \"__all__\" ] step += 1 total_timesteps += step et = time . time () print ( f \" { ep } :SPS: { step / ( et - st ) } , { step } \" ) profiler . stop () print ( profiler . output_text ( unicode = True , color = True ))","title":"run_experiment()"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperimentValidator","text":"ray_config: dictionary to be fed into ray init, validated by ray init call env_config: environment configuration, validated by environment class rllib_configs: a dictionary Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/benchmark_experiment.py class BenchmarkExperimentValidator ( BaseExperimentValidator ): \"\"\" ray_config: dictionary to be fed into ray init, validated by ray init call env_config: environment configuration, validated by environment class rllib_configs: a dictionary Arguments: BaseModel {[type]} -- [description] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" ray_config : typing . Dict [ str , typing . Any ] env_config : EnvContext rllib_configs : typing . Dict [ str , typing . Dict [ str , typing . Any ]] tune_config : typing . Dict [ str , typing . Any ] trainable_config : typing . Optional [ typing . Dict [ str , typing . Any ]] @validator ( 'rllib_configs' , pre = True ) def apply_patches_rllib_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" if not isinstance ( v , dict ): raise RuntimeError ( \"rllib_configs are expected to be a dict of keys to different compute configs\" ) rllib_configs = {} for key , value in v . items (): if isinstance ( value , list ): rllib_configs [ key ] = apply_patches ( value ) elif isinstance ( value , dict ): rllib_configs [ key ] = value return rllib_configs @validator ( 'ray_config' , 'tune_config' , 'trainable_config' , 'env_config' , pre = True ) def apply_patches_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: [type] -- [description] \"\"\" if isinstance ( v , list ): v = apply_patches ( v ) return v @validator ( 'env_config' ) def no_horizon ( cls , v ): \"\"\"Ensure that the horizon is not specified in the env_config.\"\"\" if 'horizon' in v : raise ValueError ( 'Cannot specify the horizon in the env_config' ) return v","title":"BenchmarkExperimentValidator"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperimentValidator.apply_patches_configs","text":"reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: Type Description [type] -- [description] Source code in corl/experiments/benchmark_experiment.py @validator ( 'ray_config' , 'tune_config' , 'trainable_config' , 'env_config' , pre = True ) def apply_patches_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: [type] -- [description] \"\"\" if isinstance ( v , list ): v = apply_patches ( v ) return v","title":"apply_patches_configs()"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperimentValidator.apply_patches_rllib_configs","text":"The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/benchmark_experiment.py @validator ( 'rllib_configs' , pre = True ) def apply_patches_rllib_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" if not isinstance ( v , dict ): raise RuntimeError ( \"rllib_configs are expected to be a dict of keys to different compute configs\" ) rllib_configs = {} for key , value in v . items (): if isinstance ( value , list ): rllib_configs [ key ] = apply_patches ( value ) elif isinstance ( value , dict ): rllib_configs [ key ] = value return rllib_configs","title":"apply_patches_rllib_configs()"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.BenchmarkExperimentValidator.no_horizon","text":"Ensure that the horizon is not specified in the env_config. Source code in corl/experiments/benchmark_experiment.py @validator ( 'env_config' ) def no_horizon ( cls , v ): \"\"\"Ensure that the horizon is not specified in the env_config.\"\"\" if 'horizon' in v : raise ValueError ( 'Cannot specify the horizon in the env_config' ) return v","title":"no_horizon()"},{"location":"reference/experiments/benchmark_experiment/#corl.experiments.benchmark_experiment.RllibPolicyValidator","text":"policy_class: callable policy class None will use default from trainer train: should this policy be trained Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/benchmark_experiment.py class RllibPolicyValidator ( BasePolicyValidator ): \"\"\" policy_class: callable policy class None will use default from trainer train: should this policy be trained Arguments: BaseModel {[type]} -- [description] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" config : typing . Dict [ str , typing . Any ] = {} policy_class : typing . Union [ PyObject , None ] = None train : bool = True","title":"RllibPolicyValidator"},{"location":"reference/experiments/rllib_experiment/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. RllibExperiment ( BaseExperiment ) \u00a4 The Rllib Experiment is an experiment for running multiagent configurable environments with patchable settings Source code in corl/experiments/rllib_experiment.py class RllibExperiment ( BaseExperiment ): \"\"\" The Rllib Experiment is an experiment for running multiagent configurable environments with patchable settings \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : RllibExperimentValidator self . _logger = logging . getLogger ( RllibExperiment . __name__ ) super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ RllibExperimentValidator ]: return RllibExperimentValidator @property def get_policy_validator ( self ) -> typing . Type [ RllibPolicyValidator ]: \"\"\"Return validator\"\"\" return RllibPolicyValidator def run_experiment ( self , args : argparse . Namespace ) -> None : rllib_config = self . _select_rllib_config ( args . compute_platform ) if args . compute_platform in [ 'ray' ]: self . _update_ray_config_for_ray_platform () self . _add_trial_creator () # This needs to be before the ray cluster is initialized if args . debug : self . config . ray_config [ 'local_mode' ] = True ray . init ( ** self . config . ray_config ) ray_resources = ray . available_resources () auto_configure_rllib_config ( rllib_config , self . config . auto_rllib_config_setup , ray_resources ) self . config . env_config [ \"agents\" ], self . config . env_config [ \"agent_platforms\" ] = self . create_agents ( args . platform_config , args . agent_config ) self . config . env_config [ \"horizon\" ] = rllib_config [ \"horizon\" ] if args . output : self . config . env_config [ \"output_path\" ] = args . output self . config . tune_config [ \"local_dir\" ] = args . output if args . name : self . config . env_config [ \"TrialName\" ] = args . name if args . other_platform : self . config . env_config [ \"other_platforms\" ] = self . create_other_platforms ( args . other_platform ) if not self . config . ray_config [ 'local_mode' ]: self . config . env_config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** self . config . env_config [ 'episode_parameter_provider' ]), actor_name = ACT3MultiAgentEnv . episode_parameter_provider_name ) for agent_name , agent_configs in self . config . env_config [ 'agents' ] . items (): agent_configs . class_config . config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** agent_configs . class_config . config [ 'episode_parameter_provider' ]), agent_name ) self . config . env_config [ 'epp_registry' ] = ACT3MultiAgentEnvValidator ( ** self . config . env_config ) . epp_registry tmp = ACT3MultiAgentEnv ( self . config . env_config ) tmp_as = tmp . action_space tmp_os = tmp . observation_space tmp_ac = self . config . env_config [ 'agents' ] policies = { policy_name : ( tmp_ac [ policy_name ] . policy_config [ \"policy_class\" ], policy_obs , tmp_as [ policy_name ], tmp_ac [ policy_name ] . policy_config [ \"config\" ] ) for policy_name , policy_obs in tmp_os . spaces . items () if tmp_ac [ policy_name ] } train_policies = [ policy_name for policy_name in policies . keys () if tmp_ac [ policy_name ] . policy_config [ \"train\" ]] self . _update_rllib_config ( rllib_config , train_policies , policies , args ) self . _enable_episode_parameter_provider_checkpointing () if args . profile : if \"stop\" not in self . config . tune_config : self . config . tune_config [ \"stop\" ] = {} self . config . tune_config [ \"stop\" ][ \"training_iteration\" ] = args . profile_iterations search_class = None if self . config . hparam_search_class is not None : if self . config . hparam_search_config is not None : search_class = self . config . hparam_search_class ( ** self . config . hparam_search_config ) else : search_class = self . config . hparam_search_class () search_class . add_algorithm_hparams ( rllib_config , self . config . tune_config ) tune . run ( config = rllib_config , ** self . config . tune_config , ) def _update_rllib_config ( self , rllib_config , train_policies , policies , args : argparse . Namespace ) -> None : \"\"\" Update several rllib config fields \"\"\" rllib_config [ \"multiagent\" ] = { \"policies\" : policies , \"policy_mapping_fn\" : lambda agent_id : agent_id , \"policies_to_train\" : train_policies } rllib_config [ \"env\" ] = ACT3MultiAgentEnv callback_list = [ self . get_callbacks ()] if self . config . extra_callbacks : callback_list . extend ( self . config . extra_callbacks ) # type: ignore[arg-type] rllib_config [ \"callbacks\" ] = MultiCallbacks ( callback_list ) rllib_config [ \"env_config\" ] = self . config . env_config now = datetime . now () rllib_config [ \"env_config\" ][ \"output_date_string\" ] = f \" { now . strftime ( '%Y%m %d _%H%M%S' ) } _ { socket . gethostname () } \" rllib_config [ \"create_env_on_driver\" ] = True rllib_config [ \"batch_mode\" ] = \"complete_episodes\" self . _add_git_hashes_to_config ( rllib_config ) if args . debug : rllib_config [ 'num_workers' ] = 0 def _add_git_hashes_to_config ( self , rllib_config ) -> None : \"\"\"adds git hashes (or package version information if git information is unavailable) of key modules to rllib_config[\"env_config\"][\"git_hash\"]. Key modules are the following: - corl, - whatever cwd is set to at the time of the function call (notionally /opt/project /) - any other modules listed in rllib_config[\"env_config\"][\"plugin_paths\"] This information is not actually used by ACT3MultiAgentEnv; however, putting it in the env_config means that this information is saved to the params.pkl and thus is available for later inspection while seeking to understand the performance of a trained model. \"\"\" try : # pattern used below to find root repository paths repo_pattern = r \"(?P<repopath>.*)\\/__init__.py\" rp = re . compile ( repo_pattern ) corl_pattern = r \"corl.*\" cp0 = re . compile ( corl_pattern ) rllib_config [ \"env_config\" ][ \"git_hash\" ] = dict () # store hash on cwd cwd = os . getcwd () try : githash = git . Repo ( cwd , search_parent_directories = True ) . head . object . hexsha rllib_config [ \"env_config\" ][ \"git_hash\" ][ \"cwd\" ] = githash self . _logger . info ( f \"cwd hash: { githash } \" ) except git . InvalidGitRepositoryError : self . _logger . warning ( \"cwd is not a git repo \\n \" ) # go ahead and strip out corl related things from plugin_path plugpath = [] for item in rllib_config [ 'env_config' ][ 'plugin_paths' ]: match0 = cp0 . match ( item ) if match0 is None : plugpath . append ( item ) plugpath . append ( 'corl' ) # add git hashes to env_config dictionary for module0 in plugpath : env_hash_key = module0 module1 = importlib . import_module ( module0 ) modulefile = module1 . __file__ if modulefile is not None : match0 = rp . match ( modulefile ) if match0 is not None : repo_path = match0 . group ( 'repopath' ) try : githash = git . Repo ( repo_path , search_parent_directories = True ) . head . object . hexsha rllib_config [ \"env_config\" ][ \"git_hash\" ][ env_hash_key ] = githash self . _logger . info ( f \" { module0 } hash: { githash } \" ) except git . InvalidGitRepositoryError : # possibly installed in image but not a git repo # look for version number if hasattr ( module1 , 'version' ) and hasattr ( module1 . version , '__version__' ): githash = module1 . version . __version__ rllib_config [ \"env_config\" ][ \"git_hash\" ][ env_hash_key ] = githash self . _logger . info ( f \" { module0 } hash: { githash } \" ) else : self . _logger . warning (( f \"module: { module0 } , repopath: { repo_path } \" \"is invalid git repo \\n \" )) sys . stderr . write (( f \"module: { module0 } , repopath: { repo_path } \" \"is invalid git repo \\n \" )) except ValueError : warnings . warn ( \"Unable to add the gitlab hash to experiment!!!\" ) def get_callbacks ( self ) -> typing . Type [ EnvironmentDefaultCallbacks ]: \"\"\"Get the environment callbacks\"\"\" return EnvironmentDefaultCallbacks def _select_rllib_config ( self , platform : typing . Optional [ str ]) -> typing . Dict [ str , typing . Any ]: \"\"\"Extract the rllib config for the proper computational platform Parameters ---------- platform : typing.Optional[str] Specification of the computational platform to use, such as \"local\", \"hpc\", etc. This must be present in the rllib_configs. If None, the rllib_configs must only have a single entry. Returns ------- typing.Dict[str, typing.Any] Rllib configuration for the desired computational platform. Raises ------ RuntimeError The requested computational platform does not exist or None was used when multiple platforms were defined. \"\"\" if platform is not None : return self . config . rllib_configs [ platform ] if len ( self . config . rllib_configs ) == 1 : return self . config . rllib_configs [ next ( iter ( self . config . rllib_configs ))] raise RuntimeError ( f 'Invalid rllib_config for platform \" { platform } \"' ) def _update_ray_config_for_ray_platform ( self ) -> None : \"\"\"Update the ray configuration for ray platforms \"\"\" self . config . ray_config [ 'address' ] = 'auto' self . config . ray_config [ 'log_to_driver' ] = False def _enable_episode_parameter_provider_checkpointing ( self ) -> None : base_trainer = self . config . tune_config [ \"run_or_experiment\" ] trainer_class = get_trainable_cls ( base_trainer ) class EpisodeParameterProviderSavingTrainer ( trainer_class ): # type: ignore[valid-type, misc] \"\"\" Tune Trainer that adds capability to restore progress of the EpisodeParameterProvider on restoring training progress \"\"\" def save_checkpoint ( self , checkpoint_path ): \"\"\" adds additional checkpoint saving functionality by also saving any episode parameter providers currently running \"\"\" tmp = super () . save_checkpoint ( checkpoint_path ) checkpoint_folder = pathlib . Path ( checkpoint_path ) # Environment epp_name = ACT3MultiAgentEnv . episode_parameter_provider_name env = self . workers . local_worker () . env epp : EpisodeParameterProvider = env . config . epp epp . save_checkpoint ( checkpoint_folder / epp_name ) # Agents for agent_name , agent_configs in env . agent_dict . items (): epp = agent_configs . config . epp epp . save_checkpoint ( checkpoint_folder / agent_name ) return tmp def load_checkpoint ( self , checkpoint_path ): \"\"\" adds additional checkpoint loading functionality by also loading any episode parameter providers with the checkpoint \"\"\" super () . load_checkpoint ( checkpoint_path ) checkpoint_folder = pathlib . Path ( checkpoint_path ) . parent # Environment epp_name = ACT3MultiAgentEnv . episode_parameter_provider_name env = self . workers . local_worker () . env epp : EpisodeParameterProvider = env . config . epp epp . load_checkpoint ( checkpoint_folder / epp_name ) # Agents for agent_name , agent_configs in env . agent_dict . items (): epp = agent_configs . config . epp epp . load_checkpoint ( checkpoint_folder / agent_name ) self . config . tune_config [ \"run_or_experiment\" ] = EpisodeParameterProviderSavingTrainer def _add_trial_creator ( self ): \"\"\"Updates the trial name based on the HPC Job Number and the trial name in the configuration \"\"\" if \"trial_name_creator\" not in self . config . tune_config : if self . config . trial_creator_function is None : def trial_name_prefix ( trial ): \"\"\" Args: trial (Trial): A generated trial object. Returns: trial_name (str): String representation of Trial prefixed by the contents of the environment variable: TRIAL_NAME_PREFIX Or the prefix 'RUN' if none is set. \"\"\" trial_prefix = os . environ . get ( 'PBS_JOBID' , os . environ . get ( 'TRIAL_NAME_PREFIX' , \"\" )) trial_name = \"\" if \"TrialName\" in self . config . env_config . keys (): if trial_prefix : trial_name = \"-\" + self . config . env_config [ \"TrialName\" ] else : trial_name = self . config . env_config [ \"TrialName\" ] return f \" { trial_prefix }{ trial_name } - { trial } \" self . config . tune_config [ \"trial_name_creator\" ] = trial_name_prefix else : self . config . tune_config [ \"trial_name_creator\" ] = self . config . trial_creator_function get_policy_validator : Type [ corl . experiments . rllib_experiment . RllibPolicyValidator ] property readonly \u00a4 Return validator get_validator : Type [ corl . experiments . rllib_experiment . RllibExperimentValidator ] property readonly \u00a4 Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class get_callbacks ( self ) \u00a4 Get the environment callbacks Source code in corl/experiments/rllib_experiment.py def get_callbacks ( self ) -> typing . Type [ EnvironmentDefaultCallbacks ]: \"\"\"Get the environment callbacks\"\"\" return EnvironmentDefaultCallbacks run_experiment ( self , args ) \u00a4 Runs the experiment associated with this experiment class Source code in corl/experiments/rllib_experiment.py def run_experiment ( self , args : argparse . Namespace ) -> None : rllib_config = self . _select_rllib_config ( args . compute_platform ) if args . compute_platform in [ 'ray' ]: self . _update_ray_config_for_ray_platform () self . _add_trial_creator () # This needs to be before the ray cluster is initialized if args . debug : self . config . ray_config [ 'local_mode' ] = True ray . init ( ** self . config . ray_config ) ray_resources = ray . available_resources () auto_configure_rllib_config ( rllib_config , self . config . auto_rllib_config_setup , ray_resources ) self . config . env_config [ \"agents\" ], self . config . env_config [ \"agent_platforms\" ] = self . create_agents ( args . platform_config , args . agent_config ) self . config . env_config [ \"horizon\" ] = rllib_config [ \"horizon\" ] if args . output : self . config . env_config [ \"output_path\" ] = args . output self . config . tune_config [ \"local_dir\" ] = args . output if args . name : self . config . env_config [ \"TrialName\" ] = args . name if args . other_platform : self . config . env_config [ \"other_platforms\" ] = self . create_other_platforms ( args . other_platform ) if not self . config . ray_config [ 'local_mode' ]: self . config . env_config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** self . config . env_config [ 'episode_parameter_provider' ]), actor_name = ACT3MultiAgentEnv . episode_parameter_provider_name ) for agent_name , agent_configs in self . config . env_config [ 'agents' ] . items (): agent_configs . class_config . config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** agent_configs . class_config . config [ 'episode_parameter_provider' ]), agent_name ) self . config . env_config [ 'epp_registry' ] = ACT3MultiAgentEnvValidator ( ** self . config . env_config ) . epp_registry tmp = ACT3MultiAgentEnv ( self . config . env_config ) tmp_as = tmp . action_space tmp_os = tmp . observation_space tmp_ac = self . config . env_config [ 'agents' ] policies = { policy_name : ( tmp_ac [ policy_name ] . policy_config [ \"policy_class\" ], policy_obs , tmp_as [ policy_name ], tmp_ac [ policy_name ] . policy_config [ \"config\" ] ) for policy_name , policy_obs in tmp_os . spaces . items () if tmp_ac [ policy_name ] } train_policies = [ policy_name for policy_name in policies . keys () if tmp_ac [ policy_name ] . policy_config [ \"train\" ]] self . _update_rllib_config ( rllib_config , train_policies , policies , args ) self . _enable_episode_parameter_provider_checkpointing () if args . profile : if \"stop\" not in self . config . tune_config : self . config . tune_config [ \"stop\" ] = {} self . config . tune_config [ \"stop\" ][ \"training_iteration\" ] = args . profile_iterations search_class = None if self . config . hparam_search_class is not None : if self . config . hparam_search_config is not None : search_class = self . config . hparam_search_class ( ** self . config . hparam_search_config ) else : search_class = self . config . hparam_search_class () search_class . add_algorithm_hparams ( rllib_config , self . config . tune_config ) tune . run ( config = rllib_config , ** self . config . tune_config , ) RllibExperimentValidator ( BaseExperimentValidator ) pydantic-model \u00a4 ray_config: dictionary to be fed into ray init, validated by ray init call env_config: environment configuration, validated by environment class a mapping of compute platforms to rllib configs, see apply_patches_rllib_configs for information on the typing tune_config: kwarg arguments to be sent to tune for this experiment extra_callbacks: extra rllib callbacks that will be added to the callback list this function will overwrite the default trial string creator and allow more fine tune trial name creators Source code in corl/experiments/rllib_experiment.py class RllibExperimentValidator ( BaseExperimentValidator ): \"\"\" ray_config: dictionary to be fed into ray init, validated by ray init call env_config: environment configuration, validated by environment class rllib_configs: a mapping of compute platforms to rllib configs, see apply_patches_rllib_configs for information on the typing tune_config: kwarg arguments to be sent to tune for this experiment extra_callbacks: extra rllib callbacks that will be added to the callback list trial_creator_function: this function will overwrite the default trial string creator and allow more fine tune trial name creators \"\"\" ray_config : typing . Dict [ str , typing . Any ] env_config : EnvContext rllib_configs : typing . Dict [ str , typing . Dict [ str , typing . Any ]] tune_config : typing . Dict [ str , typing . Any ] trainable_config : typing . Optional [ typing . Dict [ str , typing . Any ]] auto_rllib_config_setup : AutoRllibConfigSetup = AutoRllibConfigSetup () hparam_search_class : typing . Optional [ PyObject ] hparam_search_config : typing . Optional [ typing . Dict [ str , typing . Any ]] extra_callbacks : typing . Optional [ typing . List [ PyObject ]] trial_creator_function : typing . Optional [ PyObject ] @validator ( 'rllib_configs' , pre = True ) def apply_patches_rllib_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" if not isinstance ( v , dict ): raise RuntimeError ( \"rllib_configs are expected to be a dict of keys to different compute configs\" ) rllib_configs = {} for key , value in v . items (): if isinstance ( value , list ): rllib_configs [ key ] = apply_patches ( value ) elif isinstance ( value , dict ): rllib_configs [ key ] = value return rllib_configs @validator ( 'ray_config' , 'tune_config' , 'trainable_config' , 'env_config' , pre = True ) def apply_patches_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: [type] -- [description] \"\"\" if isinstance ( v , list ): v = apply_patches ( v ) return v @validator ( 'env_config' ) def no_horizon ( cls , v ): \"\"\"Ensure that the horizon is not specified in the env_config.\"\"\" if 'horizon' in v : raise ValueError ( 'Cannot specify the horizon in the env_config' ) return v apply_patches_configs ( v ) classmethod \u00a4 reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: Type Description [type] -- [description] Source code in corl/experiments/rllib_experiment.py @validator ( 'ray_config' , 'tune_config' , 'trainable_config' , 'env_config' , pre = True ) def apply_patches_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: [type] -- [description] \"\"\" if isinstance ( v , list ): v = apply_patches ( v ) return v apply_patches_rllib_configs ( v ) classmethod \u00a4 The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/rllib_experiment.py @validator ( 'rllib_configs' , pre = True ) def apply_patches_rllib_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" if not isinstance ( v , dict ): raise RuntimeError ( \"rllib_configs are expected to be a dict of keys to different compute configs\" ) rllib_configs = {} for key , value in v . items (): if isinstance ( value , list ): rllib_configs [ key ] = apply_patches ( value ) elif isinstance ( value , dict ): rllib_configs [ key ] = value return rllib_configs no_horizon ( v ) classmethod \u00a4 Ensure that the horizon is not specified in the env_config. Source code in corl/experiments/rllib_experiment.py @validator ( 'env_config' ) def no_horizon ( cls , v ): \"\"\"Ensure that the horizon is not specified in the env_config.\"\"\" if 'horizon' in v : raise ValueError ( 'Cannot specify the horizon in the env_config' ) return v RllibPolicyValidator ( BasePolicyValidator ) pydantic-model \u00a4 policy_class: callable policy class None will use default from trainer train: should this policy be trained Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/rllib_experiment.py class RllibPolicyValidator ( BasePolicyValidator ): \"\"\" policy_class: callable policy class None will use default from trainer train: should this policy be trained Arguments: BaseModel {[type]} -- [description] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" config : typing . Dict [ str , typing . Any ] = {} policy_class : typing . Union [ PyObject , None ] = None train : bool = True","title":"Rllib experiment"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperiment","text":"The Rllib Experiment is an experiment for running multiagent configurable environments with patchable settings Source code in corl/experiments/rllib_experiment.py class RllibExperiment ( BaseExperiment ): \"\"\" The Rllib Experiment is an experiment for running multiagent configurable environments with patchable settings \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : RllibExperimentValidator self . _logger = logging . getLogger ( RllibExperiment . __name__ ) super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ RllibExperimentValidator ]: return RllibExperimentValidator @property def get_policy_validator ( self ) -> typing . Type [ RllibPolicyValidator ]: \"\"\"Return validator\"\"\" return RllibPolicyValidator def run_experiment ( self , args : argparse . Namespace ) -> None : rllib_config = self . _select_rllib_config ( args . compute_platform ) if args . compute_platform in [ 'ray' ]: self . _update_ray_config_for_ray_platform () self . _add_trial_creator () # This needs to be before the ray cluster is initialized if args . debug : self . config . ray_config [ 'local_mode' ] = True ray . init ( ** self . config . ray_config ) ray_resources = ray . available_resources () auto_configure_rllib_config ( rllib_config , self . config . auto_rllib_config_setup , ray_resources ) self . config . env_config [ \"agents\" ], self . config . env_config [ \"agent_platforms\" ] = self . create_agents ( args . platform_config , args . agent_config ) self . config . env_config [ \"horizon\" ] = rllib_config [ \"horizon\" ] if args . output : self . config . env_config [ \"output_path\" ] = args . output self . config . tune_config [ \"local_dir\" ] = args . output if args . name : self . config . env_config [ \"TrialName\" ] = args . name if args . other_platform : self . config . env_config [ \"other_platforms\" ] = self . create_other_platforms ( args . other_platform ) if not self . config . ray_config [ 'local_mode' ]: self . config . env_config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** self . config . env_config [ 'episode_parameter_provider' ]), actor_name = ACT3MultiAgentEnv . episode_parameter_provider_name ) for agent_name , agent_configs in self . config . env_config [ 'agents' ] . items (): agent_configs . class_config . config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** agent_configs . class_config . config [ 'episode_parameter_provider' ]), agent_name ) self . config . env_config [ 'epp_registry' ] = ACT3MultiAgentEnvValidator ( ** self . config . env_config ) . epp_registry tmp = ACT3MultiAgentEnv ( self . config . env_config ) tmp_as = tmp . action_space tmp_os = tmp . observation_space tmp_ac = self . config . env_config [ 'agents' ] policies = { policy_name : ( tmp_ac [ policy_name ] . policy_config [ \"policy_class\" ], policy_obs , tmp_as [ policy_name ], tmp_ac [ policy_name ] . policy_config [ \"config\" ] ) for policy_name , policy_obs in tmp_os . spaces . items () if tmp_ac [ policy_name ] } train_policies = [ policy_name for policy_name in policies . keys () if tmp_ac [ policy_name ] . policy_config [ \"train\" ]] self . _update_rllib_config ( rllib_config , train_policies , policies , args ) self . _enable_episode_parameter_provider_checkpointing () if args . profile : if \"stop\" not in self . config . tune_config : self . config . tune_config [ \"stop\" ] = {} self . config . tune_config [ \"stop\" ][ \"training_iteration\" ] = args . profile_iterations search_class = None if self . config . hparam_search_class is not None : if self . config . hparam_search_config is not None : search_class = self . config . hparam_search_class ( ** self . config . hparam_search_config ) else : search_class = self . config . hparam_search_class () search_class . add_algorithm_hparams ( rllib_config , self . config . tune_config ) tune . run ( config = rllib_config , ** self . config . tune_config , ) def _update_rllib_config ( self , rllib_config , train_policies , policies , args : argparse . Namespace ) -> None : \"\"\" Update several rllib config fields \"\"\" rllib_config [ \"multiagent\" ] = { \"policies\" : policies , \"policy_mapping_fn\" : lambda agent_id : agent_id , \"policies_to_train\" : train_policies } rllib_config [ \"env\" ] = ACT3MultiAgentEnv callback_list = [ self . get_callbacks ()] if self . config . extra_callbacks : callback_list . extend ( self . config . extra_callbacks ) # type: ignore[arg-type] rllib_config [ \"callbacks\" ] = MultiCallbacks ( callback_list ) rllib_config [ \"env_config\" ] = self . config . env_config now = datetime . now () rllib_config [ \"env_config\" ][ \"output_date_string\" ] = f \" { now . strftime ( '%Y%m %d _%H%M%S' ) } _ { socket . gethostname () } \" rllib_config [ \"create_env_on_driver\" ] = True rllib_config [ \"batch_mode\" ] = \"complete_episodes\" self . _add_git_hashes_to_config ( rllib_config ) if args . debug : rllib_config [ 'num_workers' ] = 0 def _add_git_hashes_to_config ( self , rllib_config ) -> None : \"\"\"adds git hashes (or package version information if git information is unavailable) of key modules to rllib_config[\"env_config\"][\"git_hash\"]. Key modules are the following: - corl, - whatever cwd is set to at the time of the function call (notionally /opt/project /) - any other modules listed in rllib_config[\"env_config\"][\"plugin_paths\"] This information is not actually used by ACT3MultiAgentEnv; however, putting it in the env_config means that this information is saved to the params.pkl and thus is available for later inspection while seeking to understand the performance of a trained model. \"\"\" try : # pattern used below to find root repository paths repo_pattern = r \"(?P<repopath>.*)\\/__init__.py\" rp = re . compile ( repo_pattern ) corl_pattern = r \"corl.*\" cp0 = re . compile ( corl_pattern ) rllib_config [ \"env_config\" ][ \"git_hash\" ] = dict () # store hash on cwd cwd = os . getcwd () try : githash = git . Repo ( cwd , search_parent_directories = True ) . head . object . hexsha rllib_config [ \"env_config\" ][ \"git_hash\" ][ \"cwd\" ] = githash self . _logger . info ( f \"cwd hash: { githash } \" ) except git . InvalidGitRepositoryError : self . _logger . warning ( \"cwd is not a git repo \\n \" ) # go ahead and strip out corl related things from plugin_path plugpath = [] for item in rllib_config [ 'env_config' ][ 'plugin_paths' ]: match0 = cp0 . match ( item ) if match0 is None : plugpath . append ( item ) plugpath . append ( 'corl' ) # add git hashes to env_config dictionary for module0 in plugpath : env_hash_key = module0 module1 = importlib . import_module ( module0 ) modulefile = module1 . __file__ if modulefile is not None : match0 = rp . match ( modulefile ) if match0 is not None : repo_path = match0 . group ( 'repopath' ) try : githash = git . Repo ( repo_path , search_parent_directories = True ) . head . object . hexsha rllib_config [ \"env_config\" ][ \"git_hash\" ][ env_hash_key ] = githash self . _logger . info ( f \" { module0 } hash: { githash } \" ) except git . InvalidGitRepositoryError : # possibly installed in image but not a git repo # look for version number if hasattr ( module1 , 'version' ) and hasattr ( module1 . version , '__version__' ): githash = module1 . version . __version__ rllib_config [ \"env_config\" ][ \"git_hash\" ][ env_hash_key ] = githash self . _logger . info ( f \" { module0 } hash: { githash } \" ) else : self . _logger . warning (( f \"module: { module0 } , repopath: { repo_path } \" \"is invalid git repo \\n \" )) sys . stderr . write (( f \"module: { module0 } , repopath: { repo_path } \" \"is invalid git repo \\n \" )) except ValueError : warnings . warn ( \"Unable to add the gitlab hash to experiment!!!\" ) def get_callbacks ( self ) -> typing . Type [ EnvironmentDefaultCallbacks ]: \"\"\"Get the environment callbacks\"\"\" return EnvironmentDefaultCallbacks def _select_rllib_config ( self , platform : typing . Optional [ str ]) -> typing . Dict [ str , typing . Any ]: \"\"\"Extract the rllib config for the proper computational platform Parameters ---------- platform : typing.Optional[str] Specification of the computational platform to use, such as \"local\", \"hpc\", etc. This must be present in the rllib_configs. If None, the rllib_configs must only have a single entry. Returns ------- typing.Dict[str, typing.Any] Rllib configuration for the desired computational platform. Raises ------ RuntimeError The requested computational platform does not exist or None was used when multiple platforms were defined. \"\"\" if platform is not None : return self . config . rllib_configs [ platform ] if len ( self . config . rllib_configs ) == 1 : return self . config . rllib_configs [ next ( iter ( self . config . rllib_configs ))] raise RuntimeError ( f 'Invalid rllib_config for platform \" { platform } \"' ) def _update_ray_config_for_ray_platform ( self ) -> None : \"\"\"Update the ray configuration for ray platforms \"\"\" self . config . ray_config [ 'address' ] = 'auto' self . config . ray_config [ 'log_to_driver' ] = False def _enable_episode_parameter_provider_checkpointing ( self ) -> None : base_trainer = self . config . tune_config [ \"run_or_experiment\" ] trainer_class = get_trainable_cls ( base_trainer ) class EpisodeParameterProviderSavingTrainer ( trainer_class ): # type: ignore[valid-type, misc] \"\"\" Tune Trainer that adds capability to restore progress of the EpisodeParameterProvider on restoring training progress \"\"\" def save_checkpoint ( self , checkpoint_path ): \"\"\" adds additional checkpoint saving functionality by also saving any episode parameter providers currently running \"\"\" tmp = super () . save_checkpoint ( checkpoint_path ) checkpoint_folder = pathlib . Path ( checkpoint_path ) # Environment epp_name = ACT3MultiAgentEnv . episode_parameter_provider_name env = self . workers . local_worker () . env epp : EpisodeParameterProvider = env . config . epp epp . save_checkpoint ( checkpoint_folder / epp_name ) # Agents for agent_name , agent_configs in env . agent_dict . items (): epp = agent_configs . config . epp epp . save_checkpoint ( checkpoint_folder / agent_name ) return tmp def load_checkpoint ( self , checkpoint_path ): \"\"\" adds additional checkpoint loading functionality by also loading any episode parameter providers with the checkpoint \"\"\" super () . load_checkpoint ( checkpoint_path ) checkpoint_folder = pathlib . Path ( checkpoint_path ) . parent # Environment epp_name = ACT3MultiAgentEnv . episode_parameter_provider_name env = self . workers . local_worker () . env epp : EpisodeParameterProvider = env . config . epp epp . load_checkpoint ( checkpoint_folder / epp_name ) # Agents for agent_name , agent_configs in env . agent_dict . items (): epp = agent_configs . config . epp epp . load_checkpoint ( checkpoint_folder / agent_name ) self . config . tune_config [ \"run_or_experiment\" ] = EpisodeParameterProviderSavingTrainer def _add_trial_creator ( self ): \"\"\"Updates the trial name based on the HPC Job Number and the trial name in the configuration \"\"\" if \"trial_name_creator\" not in self . config . tune_config : if self . config . trial_creator_function is None : def trial_name_prefix ( trial ): \"\"\" Args: trial (Trial): A generated trial object. Returns: trial_name (str): String representation of Trial prefixed by the contents of the environment variable: TRIAL_NAME_PREFIX Or the prefix 'RUN' if none is set. \"\"\" trial_prefix = os . environ . get ( 'PBS_JOBID' , os . environ . get ( 'TRIAL_NAME_PREFIX' , \"\" )) trial_name = \"\" if \"TrialName\" in self . config . env_config . keys (): if trial_prefix : trial_name = \"-\" + self . config . env_config [ \"TrialName\" ] else : trial_name = self . config . env_config [ \"TrialName\" ] return f \" { trial_prefix }{ trial_name } - { trial } \" self . config . tune_config [ \"trial_name_creator\" ] = trial_name_prefix else : self . config . tune_config [ \"trial_name_creator\" ] = self . config . trial_creator_function","title":"RllibExperiment"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperiment.get_policy_validator","text":"Return validator","title":"get_policy_validator"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperiment.get_validator","text":"Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class","title":"get_validator"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperiment.get_callbacks","text":"Get the environment callbacks Source code in corl/experiments/rllib_experiment.py def get_callbacks ( self ) -> typing . Type [ EnvironmentDefaultCallbacks ]: \"\"\"Get the environment callbacks\"\"\" return EnvironmentDefaultCallbacks","title":"get_callbacks()"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperiment.run_experiment","text":"Runs the experiment associated with this experiment class Source code in corl/experiments/rllib_experiment.py def run_experiment ( self , args : argparse . Namespace ) -> None : rllib_config = self . _select_rllib_config ( args . compute_platform ) if args . compute_platform in [ 'ray' ]: self . _update_ray_config_for_ray_platform () self . _add_trial_creator () # This needs to be before the ray cluster is initialized if args . debug : self . config . ray_config [ 'local_mode' ] = True ray . init ( ** self . config . ray_config ) ray_resources = ray . available_resources () auto_configure_rllib_config ( rllib_config , self . config . auto_rllib_config_setup , ray_resources ) self . config . env_config [ \"agents\" ], self . config . env_config [ \"agent_platforms\" ] = self . create_agents ( args . platform_config , args . agent_config ) self . config . env_config [ \"horizon\" ] = rllib_config [ \"horizon\" ] if args . output : self . config . env_config [ \"output_path\" ] = args . output self . config . tune_config [ \"local_dir\" ] = args . output if args . name : self . config . env_config [ \"TrialName\" ] = args . name if args . other_platform : self . config . env_config [ \"other_platforms\" ] = self . create_other_platforms ( args . other_platform ) if not self . config . ray_config [ 'local_mode' ]: self . config . env_config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** self . config . env_config [ 'episode_parameter_provider' ]), actor_name = ACT3MultiAgentEnv . episode_parameter_provider_name ) for agent_name , agent_configs in self . config . env_config [ 'agents' ] . items (): agent_configs . class_config . config [ 'episode_parameter_provider' ] = RemoteEpisodeParameterProvider . wrap_epp_factory ( Factory ( ** agent_configs . class_config . config [ 'episode_parameter_provider' ]), agent_name ) self . config . env_config [ 'epp_registry' ] = ACT3MultiAgentEnvValidator ( ** self . config . env_config ) . epp_registry tmp = ACT3MultiAgentEnv ( self . config . env_config ) tmp_as = tmp . action_space tmp_os = tmp . observation_space tmp_ac = self . config . env_config [ 'agents' ] policies = { policy_name : ( tmp_ac [ policy_name ] . policy_config [ \"policy_class\" ], policy_obs , tmp_as [ policy_name ], tmp_ac [ policy_name ] . policy_config [ \"config\" ] ) for policy_name , policy_obs in tmp_os . spaces . items () if tmp_ac [ policy_name ] } train_policies = [ policy_name for policy_name in policies . keys () if tmp_ac [ policy_name ] . policy_config [ \"train\" ]] self . _update_rllib_config ( rllib_config , train_policies , policies , args ) self . _enable_episode_parameter_provider_checkpointing () if args . profile : if \"stop\" not in self . config . tune_config : self . config . tune_config [ \"stop\" ] = {} self . config . tune_config [ \"stop\" ][ \"training_iteration\" ] = args . profile_iterations search_class = None if self . config . hparam_search_class is not None : if self . config . hparam_search_config is not None : search_class = self . config . hparam_search_class ( ** self . config . hparam_search_config ) else : search_class = self . config . hparam_search_class () search_class . add_algorithm_hparams ( rllib_config , self . config . tune_config ) tune . run ( config = rllib_config , ** self . config . tune_config , )","title":"run_experiment()"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperimentValidator","text":"ray_config: dictionary to be fed into ray init, validated by ray init call env_config: environment configuration, validated by environment class a mapping of compute platforms to rllib configs, see apply_patches_rllib_configs for information on the typing tune_config: kwarg arguments to be sent to tune for this experiment extra_callbacks: extra rllib callbacks that will be added to the callback list this function will overwrite the default trial string creator and allow more fine tune trial name creators Source code in corl/experiments/rllib_experiment.py class RllibExperimentValidator ( BaseExperimentValidator ): \"\"\" ray_config: dictionary to be fed into ray init, validated by ray init call env_config: environment configuration, validated by environment class rllib_configs: a mapping of compute platforms to rllib configs, see apply_patches_rllib_configs for information on the typing tune_config: kwarg arguments to be sent to tune for this experiment extra_callbacks: extra rllib callbacks that will be added to the callback list trial_creator_function: this function will overwrite the default trial string creator and allow more fine tune trial name creators \"\"\" ray_config : typing . Dict [ str , typing . Any ] env_config : EnvContext rllib_configs : typing . Dict [ str , typing . Dict [ str , typing . Any ]] tune_config : typing . Dict [ str , typing . Any ] trainable_config : typing . Optional [ typing . Dict [ str , typing . Any ]] auto_rllib_config_setup : AutoRllibConfigSetup = AutoRllibConfigSetup () hparam_search_class : typing . Optional [ PyObject ] hparam_search_config : typing . Optional [ typing . Dict [ str , typing . Any ]] extra_callbacks : typing . Optional [ typing . List [ PyObject ]] trial_creator_function : typing . Optional [ PyObject ] @validator ( 'rllib_configs' , pre = True ) def apply_patches_rllib_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" if not isinstance ( v , dict ): raise RuntimeError ( \"rllib_configs are expected to be a dict of keys to different compute configs\" ) rllib_configs = {} for key , value in v . items (): if isinstance ( value , list ): rllib_configs [ key ] = apply_patches ( value ) elif isinstance ( value , dict ): rllib_configs [ key ] = value return rllib_configs @validator ( 'ray_config' , 'tune_config' , 'trainable_config' , 'env_config' , pre = True ) def apply_patches_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: [type] -- [description] \"\"\" if isinstance ( v , list ): v = apply_patches ( v ) return v @validator ( 'env_config' ) def no_horizon ( cls , v ): \"\"\"Ensure that the horizon is not specified in the env_config.\"\"\" if 'horizon' in v : raise ValueError ( 'Cannot specify the horizon in the env_config' ) return v","title":"RllibExperimentValidator"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperimentValidator.apply_patches_configs","text":"reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: Type Description [type] -- [description] Source code in corl/experiments/rllib_experiment.py @validator ( 'ray_config' , 'tune_config' , 'trainable_config' , 'env_config' , pre = True ) def apply_patches_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" reduces a field from typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] to typing.Dict[str, typing.Any] by patching the first dictionary in the list with each patch afterwards Returns: [type] -- [description] \"\"\" if isinstance ( v , list ): v = apply_patches ( v ) return v","title":"apply_patches_configs()"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperimentValidator.apply_patches_rllib_configs","text":"The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/rllib_experiment.py @validator ( 'rllib_configs' , pre = True ) def apply_patches_rllib_configs ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\" The dictionary of rllib configs may come in as a dictionary of lists of dictionaries, this function is responsible for collapsing the list down to a typing.Dict[str, typing.Dict[str, typing.Any]] instead of typing.Dict[str, typing.Union[typing.List[typing.Dict[str, typing.Any]], typing.Dict[str, typing.Any]]] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" if not isinstance ( v , dict ): raise RuntimeError ( \"rllib_configs are expected to be a dict of keys to different compute configs\" ) rllib_configs = {} for key , value in v . items (): if isinstance ( value , list ): rllib_configs [ key ] = apply_patches ( value ) elif isinstance ( value , dict ): rllib_configs [ key ] = value return rllib_configs","title":"apply_patches_rllib_configs()"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibExperimentValidator.no_horizon","text":"Ensure that the horizon is not specified in the env_config. Source code in corl/experiments/rllib_experiment.py @validator ( 'env_config' ) def no_horizon ( cls , v ): \"\"\"Ensure that the horizon is not specified in the env_config.\"\"\" if 'horizon' in v : raise ValueError ( 'Cannot specify the horizon in the env_config' ) return v","title":"no_horizon()"},{"location":"reference/experiments/rllib_experiment/#corl.experiments.rllib_experiment.RllibPolicyValidator","text":"policy_class: callable policy class None will use default from trainer train: should this policy be trained Exceptions: Type Description RuntimeError [description] Returns: Type Description [type] -- [description] Source code in corl/experiments/rllib_experiment.py class RllibPolicyValidator ( BasePolicyValidator ): \"\"\" policy_class: callable policy class None will use default from trainer train: should this policy be trained Arguments: BaseModel {[type]} -- [description] Raises: RuntimeError: [description] Returns: [type] -- [description] \"\"\" config : typing . Dict [ str , typing . Any ] = {} policy_class : typing . Union [ PyObject , None ] = None train : bool = True","title":"RllibPolicyValidator"},{"location":"reference/glues/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Glues"},{"location":"reference/glues/base_dict_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseDictWrapperGlue ( BaseAgentGlue ) \u00a4 A base object that glues can inherit in order to \"wrap\" multiple glue instances, addressed by keys Source code in corl/glues/base_dict_wrapper.py class BaseDictWrapperGlue ( BaseAgentGlue ): \"\"\"A base object that glues can inherit in order to \"wrap\" multiple glue instances, addressed by keys \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseDictWrapperGlueValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> Type [ BaseDictWrapperGlueValidator ]: return BaseDictWrapperGlueValidator def glues ( self ) -> Dict [ str , BaseAgentGlue ]: \"\"\"Get the wrapped glue instances dict \"\"\" return self . config . wrapped def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) for glue in self . glues () . values (): glue . set_agent_removed () get_validator : Type [ corl . glues . base_dict_wrapper . BaseDictWrapperGlueValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.base_dict_wrapper.BaseDictWrapperGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs glues ( self ) \u00a4 Get the wrapped glue instances dict Source code in corl/glues/base_dict_wrapper.py def glues ( self ) -> Dict [ str , BaseAgentGlue ]: \"\"\"Get the wrapped glue instances dict \"\"\" return self . config . wrapped set_agent_removed ( self , agent_removed = True ) \u00a4 Notify the glue that the agent it is 'attached' to has been removed by the simulation Source code in corl/glues/base_dict_wrapper.py def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) for glue in self . glues () . values (): glue . set_agent_removed () BaseDictWrapperGlueValidator ( BaseAgentGlueValidator ) pydantic-model \u00a4 wrap_dict - A dict of the wrapped glues Source code in corl/glues/base_dict_wrapper.py class BaseDictWrapperGlueValidator ( BaseAgentGlueValidator ): \"\"\" wrap_dict - A dict of the wrapped glues \"\"\" wrapped : Dict [ str , BaseAgentGlue ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Base dict wrapper"},{"location":"reference/glues/base_dict_wrapper/#corl.glues.base_dict_wrapper.BaseDictWrapperGlue","text":"A base object that glues can inherit in order to \"wrap\" multiple glue instances, addressed by keys Source code in corl/glues/base_dict_wrapper.py class BaseDictWrapperGlue ( BaseAgentGlue ): \"\"\"A base object that glues can inherit in order to \"wrap\" multiple glue instances, addressed by keys \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseDictWrapperGlueValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> Type [ BaseDictWrapperGlueValidator ]: return BaseDictWrapperGlueValidator def glues ( self ) -> Dict [ str , BaseAgentGlue ]: \"\"\"Get the wrapped glue instances dict \"\"\" return self . config . wrapped def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) for glue in self . glues () . values (): glue . set_agent_removed ()","title":"BaseDictWrapperGlue"},{"location":"reference/glues/base_dict_wrapper/#corl.glues.base_dict_wrapper.BaseDictWrapperGlue.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.base_dict_wrapper.BaseDictWrapperGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/base_dict_wrapper/#corl.glues.base_dict_wrapper.BaseDictWrapperGlue.glues","text":"Get the wrapped glue instances dict Source code in corl/glues/base_dict_wrapper.py def glues ( self ) -> Dict [ str , BaseAgentGlue ]: \"\"\"Get the wrapped glue instances dict \"\"\" return self . config . wrapped","title":"glues()"},{"location":"reference/glues/base_dict_wrapper/#corl.glues.base_dict_wrapper.BaseDictWrapperGlue.set_agent_removed","text":"Notify the glue that the agent it is 'attached' to has been removed by the simulation Source code in corl/glues/base_dict_wrapper.py def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) for glue in self . glues () . values (): glue . set_agent_removed ()","title":"set_agent_removed()"},{"location":"reference/glues/base_dict_wrapper/#corl.glues.base_dict_wrapper.BaseDictWrapperGlueValidator","text":"wrap_dict - A dict of the wrapped glues Source code in corl/glues/base_dict_wrapper.py class BaseDictWrapperGlueValidator ( BaseAgentGlueValidator ): \"\"\" wrap_dict - A dict of the wrapped glues \"\"\" wrapped : Dict [ str , BaseAgentGlue ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseDictWrapperGlueValidator"},{"location":"reference/glues/base_glue/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. This base class (BasePlatformGlue) is the \"glue\" that sticks the base integration object(s) to the environment.This class and its derived classes are expected to be agnostic to the specific integration. The idea is that you can then use the same \"glue\" for training and inference on any integration. The main purpose of the code in this class is to expose the specific actions and observations for your given task. For example you may not want the full controls from a BaseController for a given problem. This class is where you decide to modify the controls exposed to the policy to only include what you want actually controlled. Deciding on how to combine actions is much more important when there are multiple actions like with movement. These connections also apply to observations for instance with multiple observations like platform state and sensor observations. How the actions and observations are \"glued\" together is specific to the task and thus require an implementation of the base class. The glue you use must match the actual agent in the environment or be robust enough to handle that agent. For example if there are different controllers than expected do you throw an error or just reduce the action space. A good practice is to assert the control, or sensor is a type or a subclass of a type to guarantee it has the functionality you expect. The glue classes are modular in that you can use multiple glue classes per agent. This allows stacking a glue to a stick controller glue to get a and stick controlled agent. However, some control schemes may not be compatible with each other. For example if you try to add a pitch control with the stick and a pitch rate controller, the resulting behavior is not straightforward and may cause your backend to throw an error. BaseAgentControllerGlue ( BaseAgentPlatformGlue , ABC ) \u00a4 BaseAgentControllerGlue assumes that this glue is for a controller and that an associated 'get_applied_control' method is available Source code in corl/glues/base_glue.py class BaseAgentControllerGlue ( BaseAgentPlatformGlue , abc . ABC ): \"\"\" BaseAgentControllerGlue assumes that this glue is for a controller and that an associated 'get_applied_control' method is available \"\"\" @abc . abstractmethod def get_applied_control ( self ) -> OrderedDict : \"\"\" Get the currently applied controls Returns ------- OrderedDict The currently applied controls \"\"\" ... get_applied_control ( self ) \u00a4 Get the currently applied controls Returns \u00a4 OrderedDict The currently applied controls Source code in corl/glues/base_glue.py @abc . abstractmethod def get_applied_control ( self ) -> OrderedDict : \"\"\" Get the currently applied controls Returns ------- OrderedDict The currently applied controls \"\"\" ... BaseAgentGlue ( ABC ) \u00a4 BasePlatformGlue abstract class that provides the action space, observation space and how to apply actions and get observations for a platform Source code in corl/glues/base_glue.py class BaseAgentGlue ( abc . ABC ): \"\"\" BasePlatformGlue abstract class that provides the action space, observation space and how to apply actions and get observations for a platform \"\"\" def __init__ ( self , ** kwargs ) -> None : \"\"\" The init function for an Agent Glue class Parameters ---------- state: typing.Tuple[BasePlatform] The initial state of the environment so the glue can stick together platform(s) or platform parts agent_id: str The name of the agent this glue is stuck to config: dict The configuration parameters of this glue class \"\"\" self . config : BaseAgentGlueValidator = self . get_validator ( ** kwargs ) self . _agent_removed = False @property def get_validator ( self ) -> typing . Type [ BaseAgentGlueValidator ]: \"\"\"returns the validator for this class Returns: BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs \"\"\" return BaseAgentGlueValidator @lru_cache ( maxsize = 1 ) @abc . abstractmethod def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differiciate it from other glues. \"\"\" ... @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. that defines the action given to apply_action Returns ------- gym.spaces.Space The gym Space that defines the action given to the apply_action function \"\"\" ... @lru_cache ( maxsize = 1 ) def normalized_action_space ( self ) -> typing . Optional [ gym . spaces . Space ]: \"\"\" Normalizes an action space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will output The default implementation scales all Box spaces to a low=-1. and high =1. This function should be a dual function to the unnormalize_action Returns ------- gym.spaces.Space: The scaled action space \"\"\" action_space = self . action_space () if action_space and self . config . normalization . enabled : return EnvSpaceUtil . normalize_space ( space = action_space , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) if action_space : return action_space return None def apply_action ( self , action : EnvSpaceUtil . sample_type , # pylint: disable=unused-argument observation : EnvSpaceUtil . sample_type # pylint: disable=unused-argument ) -> None : \"\"\" Apply the action for the controller, etc. Parameters ---------- action The action for the class to apply to the platform observation The current observations before appling the action \"\"\" ... def unnormalize_action ( self , action : EnvSpaceUtil . sample_type ) -> EnvSpaceUtil . sample_type : \"\"\" Un-Normalizes an action space from this glue's normalized_action_space to some raw bounds The default implementation assumes the normalized scale was low=-1. and high =1. This function should be a dual function to the normalize_action_space Parameters ---------- action: EnvSpaceUtil.sample_type The action to unnormalize Returns ------- EnvSpaceUtil.sample_type: the unnormalized action \"\"\" if self . config . normalization . enabled : ret = EnvSpaceUtil . unscale_sample_from_space ( space = self . action_space (), space_sample = action , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) else : ret = action return ret @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\" Build the observation space for the platform using the state of the platform, controller, sensors, etc. Returns ------- gym.spaces.Space The gym space that defines the returned space from the get_observation function \"\"\" ... @lru_cache ( maxsize = 1 ) def normalized_observation_space ( self ) -> typing . Optional [ gym . spaces . Space ]: \"\"\" Normalizes an observation space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will take as input for observations The default implementation scales all Box spaces to a low=-1. and high =1. This function should be the same as the normalize_observation function but return a Space and not the sample Returns ------- gym.spaces.Space: The scaled observation space \"\"\" observation_space = self . observation_space () if observation_space and self . config . normalization . enabled : return EnvSpaceUtil . normalize_space ( space = observation_space , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) if observation_space : return observation_space return None def get_observation ( self ) -> EnvSpaceUtil . sample_type : \"\"\" Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns ------- EnvSpaceUtil.sample_type The actual observation for this platform from this glue class \"\"\" ... def normalize_observation ( self , observation : EnvSpaceUtil . sample_type ) -> EnvSpaceUtil . sample_type : \"\"\" Normalizes an observation from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will use as observations Parameters ---------- observation: EnvSpaceUtil.sample_type The observation we want to scale Returns ------- EnvSpaceUtil.sample_type: The scaled observation \"\"\" if not self . config . normalization . enabled : ret = observation else : ret = EnvSpaceUtil . scale_sample_from_space ( space = self . observation_space (), space_sample = observation , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum , ) return ret def get_info_dict ( self ) -> EnvSpaceUtil . sample_type : \"\"\" Get the user specified metadata/metrics/etc. Returns ------- EnvSpaceUtil.sample_type The actual info dict object for this platform from this glue class \"\"\" ... def agent_removed ( self ) -> bool : \"\"\" Returns true if the agent has been removed, false otherwise \"\"\" return self . _agent_removed def set_agent_removed ( self , agent_removed : bool = True ) -> None : \"\"\" Notify the glue that the agent it is 'attached' to has been removed by the simulation \"\"\" self . _agent_removed = agent_removed get_validator : Type [ corl . glues . base_glue . BaseAgentGlueValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.base_glue.BaseAgentGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs __init__ ( self , ** kwargs ) special \u00a4 The init function for an Agent Glue class Parameters \u00a4 typing.Tuple[BasePlatform] The initial state of the environment so the glue can stick together platform(s) or platform parts str The name of the agent this glue is stuck to dict The configuration parameters of this glue class Source code in corl/glues/base_glue.py def __init__ ( self , ** kwargs ) -> None : \"\"\" The init function for an Agent Glue class Parameters ---------- state: typing.Tuple[BasePlatform] The initial state of the environment so the glue can stick together platform(s) or platform parts agent_id: str The name of the agent this glue is stuck to config: dict The configuration parameters of this glue class \"\"\" self . config : BaseAgentGlueValidator = self . get_validator ( ** kwargs ) self . _agent_removed = False action_space ( self ) \u00a4 Build the action space for the controller, etc. that defines the action given to apply_action Returns \u00a4 gym.spaces.Space The gym Space that defines the action given to the apply_action function Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. that defines the action given to apply_action Returns ------- gym.spaces.Space The gym Space that defines the action given to the apply_action function \"\"\" ... agent_removed ( self ) \u00a4 Returns true if the agent has been removed, false otherwise Source code in corl/glues/base_glue.py def agent_removed ( self ) -> bool : \"\"\" Returns true if the agent has been removed, false otherwise \"\"\" return self . _agent_removed apply_action ( self , action , observation ) \u00a4 Apply the action for the controller, etc. Parameters \u00a4 action The action for the class to apply to the platform observation The current observations before appling the action Source code in corl/glues/base_glue.py def apply_action ( self , action : EnvSpaceUtil . sample_type , # pylint: disable=unused-argument observation : EnvSpaceUtil . sample_type # pylint: disable=unused-argument ) -> None : \"\"\" Apply the action for the controller, etc. Parameters ---------- action The action for the class to apply to the platform observation The current observations before appling the action \"\"\" ... get_info_dict ( self ) \u00a4 Get the user specified metadata/metrics/etc. Returns \u00a4 EnvSpaceUtil.sample_type The actual info dict object for this platform from this glue class Source code in corl/glues/base_glue.py def get_info_dict ( self ) -> EnvSpaceUtil . sample_type : \"\"\" Get the user specified metadata/metrics/etc. Returns ------- EnvSpaceUtil.sample_type The actual info dict object for this platform from this glue class \"\"\" ... get_observation ( self ) \u00a4 Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns \u00a4 EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/base_glue.py def get_observation ( self ) -> EnvSpaceUtil . sample_type : \"\"\" Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns ------- EnvSpaceUtil.sample_type The actual observation for this platform from this glue class \"\"\" ... get_unique_name ( self ) \u00a4 Provies a unique name of the glue to differiciate it from other glues. Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) @abc . abstractmethod def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differiciate it from other glues. \"\"\" ... normalize_observation ( self , observation ) \u00a4 Normalizes an observation from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will use as observations Parameters \u00a4 EnvSpaceUtil.sample_type The observation we want to scale Returns \u00a4 EnvSpaceUtil.sample_type: The scaled observation Source code in corl/glues/base_glue.py def normalize_observation ( self , observation : EnvSpaceUtil . sample_type ) -> EnvSpaceUtil . sample_type : \"\"\" Normalizes an observation from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will use as observations Parameters ---------- observation: EnvSpaceUtil.sample_type The observation we want to scale Returns ------- EnvSpaceUtil.sample_type: The scaled observation \"\"\" if not self . config . normalization . enabled : ret = observation else : ret = EnvSpaceUtil . scale_sample_from_space ( space = self . observation_space (), space_sample = observation , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum , ) return ret normalized_action_space ( self ) \u00a4 Normalizes an action space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will output The default implementation scales all Box spaces to a low=-1. and high =1. This function should be a dual function to the unnormalize_action Returns \u00a4 gym.spaces.Space: The scaled action space Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) def normalized_action_space ( self ) -> typing . Optional [ gym . spaces . Space ]: \"\"\" Normalizes an action space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will output The default implementation scales all Box spaces to a low=-1. and high =1. This function should be a dual function to the unnormalize_action Returns ------- gym.spaces.Space: The scaled action space \"\"\" action_space = self . action_space () if action_space and self . config . normalization . enabled : return EnvSpaceUtil . normalize_space ( space = action_space , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) if action_space : return action_space return None normalized_observation_space ( self ) \u00a4 Normalizes an observation space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will take as input for observations The default implementation scales all Box spaces to a low=-1. and high =1. This function should be the same as the normalize_observation function but return a Space and not the sample Returns \u00a4 gym.spaces.Space: The scaled observation space Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) def normalized_observation_space ( self ) -> typing . Optional [ gym . spaces . Space ]: \"\"\" Normalizes an observation space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will take as input for observations The default implementation scales all Box spaces to a low=-1. and high =1. This function should be the same as the normalize_observation function but return a Space and not the sample Returns ------- gym.spaces.Space: The scaled observation space \"\"\" observation_space = self . observation_space () if observation_space and self . config . normalization . enabled : return EnvSpaceUtil . normalize_space ( space = observation_space , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) if observation_space : return observation_space return None observation_space ( self ) \u00a4 Build the observation space for the platform using the state of the platform, controller, sensors, etc. Returns \u00a4 gym.spaces.Space The gym space that defines the returned space from the get_observation function Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\" Build the observation space for the platform using the state of the platform, controller, sensors, etc. Returns ------- gym.spaces.Space The gym space that defines the returned space from the get_observation function \"\"\" ... set_agent_removed ( self , agent_removed = True ) \u00a4 Notify the glue that the agent it is 'attached' to has been removed by the simulation Source code in corl/glues/base_glue.py def set_agent_removed ( self , agent_removed : bool = True ) -> None : \"\"\" Notify the glue that the agent it is 'attached' to has been removed by the simulation \"\"\" self . _agent_removed = agent_removed unnormalize_action ( self , action ) \u00a4 Un-Normalizes an action space from this glue's normalized_action_space to some raw bounds The default implementation assumes the normalized scale was low=-1. and high =1. This function should be a dual function to the normalize_action_space Parameters \u00a4 EnvSpaceUtil.sample_type The action to unnormalize Returns \u00a4 EnvSpaceUtil.sample_type: the unnormalized action Source code in corl/glues/base_glue.py def unnormalize_action ( self , action : EnvSpaceUtil . sample_type ) -> EnvSpaceUtil . sample_type : \"\"\" Un-Normalizes an action space from this glue's normalized_action_space to some raw bounds The default implementation assumes the normalized scale was low=-1. and high =1. This function should be a dual function to the normalize_action_space Parameters ---------- action: EnvSpaceUtil.sample_type The action to unnormalize Returns ------- EnvSpaceUtil.sample_type: the unnormalized action \"\"\" if self . config . normalization . enabled : ret = EnvSpaceUtil . unscale_sample_from_space ( space = self . action_space (), space_sample = action , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) else : ret = action return ret BaseAgentGlueNormalizationValidator ( BaseModel ) pydantic-model \u00a4 enabled: if normalization is enabled for this glue minimum: the minimum value this glue's output will be normalized to maximum: the maximum value this glue's output will be normalized to Source code in corl/glues/base_glue.py class BaseAgentGlueNormalizationValidator ( BaseModel ): \"\"\" enabled: if normalization is enabled for this glue minimum: the minimum value this glue's output will be normalized to maximum: the maximum value this glue's output will be normalized to \"\"\" enabled : bool = True minimum : float = - 1.0 maximum : float = 1.0 BaseAgentGlueValidator ( BaseModel ) pydantic-model \u00a4 name: the custom name of this glue agent_name: the name of the agent who owns this glue seed: temporary - assume this will be removed normalization: how to handle obs normalization, see BaseAgentGlueNormalizationValidator Source code in corl/glues/base_glue.py class BaseAgentGlueValidator ( BaseModel ): \"\"\" name: the custom name of this glue agent_name: the name of the agent who owns this glue seed: temporary - assume this will be removed normalization: how to handle obs normalization, see BaseAgentGlueNormalizationValidator \"\"\" name : typing . Optional [ str ] agent_name : str seed : typing . Optional [ int ] normalization : BaseAgentGlueNormalizationValidator = BaseAgentGlueNormalizationValidator () training_export_behavior : TrainingExportBehavior = TrainingExportBehavior . INCLUDE BaseAgentPlatformGlue ( BaseAgentGlue , ABC ) \u00a4 BaseAgentPlatformGlue assumes that this glue corresponds to exactly one platform with the same platform name as the agent_id this glue is attached to This will then set the self._platform to this platform Source code in corl/glues/base_glue.py class BaseAgentPlatformGlue ( BaseAgentGlue , abc . ABC ): \"\"\" BaseAgentPlatformGlue assumes that this glue corresponds to exactly one platform with the same platform name as the agent_id this glue is attached to This will then set the self._platform to this platform \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseAgentPlatformGlueValidator super () . __init__ ( ** kwargs ) self . _platform : BasePlatform = self . config . platform self . _logger = logging . getLogger ( BaseAgentPlatformGlue . __name__ ) @property def get_validator ( self ) -> typing . Type [ BaseAgentPlatformGlueValidator ]: return BaseAgentPlatformGlueValidator get_validator : Type [ corl . glues . base_glue . BaseAgentPlatformGlueValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.base_glue.BaseAgentPlatformGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs BaseAgentPlatformGlueValidator ( BaseAgentGlueValidator ) pydantic-model \u00a4 platform: The platform object that this glue will read from/apply actions to Source code in corl/glues/base_glue.py class BaseAgentPlatformGlueValidator ( BaseAgentGlueValidator ): \"\"\" platform: The platform object that this glue will read from/apply actions to \"\"\" platform : BasePlatform class Config : \"\"\" This allows pydantic to validate that platform is a BasePlatform \"\"\" arbitrary_types_allowed = True Config \u00a4 This allows pydantic to validate that platform is a BasePlatform Source code in corl/glues/base_glue.py class Config : \"\"\" This allows pydantic to validate that platform is a BasePlatform \"\"\" arbitrary_types_allowed = True TrainingExportBehavior ( str , Enum ) \u00a4 Enumeration of training behaviors Source code in corl/glues/base_glue.py class TrainingExportBehavior ( str , enum . Enum ): \"\"\"Enumeration of training behaviors \"\"\" INCLUDE = \"INCLUDE\" EXCLUDE = \"EXCLUDE\"","title":"Base glue"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentControllerGlue","text":"BaseAgentControllerGlue assumes that this glue is for a controller and that an associated 'get_applied_control' method is available Source code in corl/glues/base_glue.py class BaseAgentControllerGlue ( BaseAgentPlatformGlue , abc . ABC ): \"\"\" BaseAgentControllerGlue assumes that this glue is for a controller and that an associated 'get_applied_control' method is available \"\"\" @abc . abstractmethod def get_applied_control ( self ) -> OrderedDict : \"\"\" Get the currently applied controls Returns ------- OrderedDict The currently applied controls \"\"\" ...","title":"BaseAgentControllerGlue"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentControllerGlue.get_applied_control","text":"Get the currently applied controls","title":"get_applied_control()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentControllerGlue.get_applied_control--returns","text":"OrderedDict The currently applied controls Source code in corl/glues/base_glue.py @abc . abstractmethod def get_applied_control ( self ) -> OrderedDict : \"\"\" Get the currently applied controls Returns ------- OrderedDict The currently applied controls \"\"\" ...","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue","text":"BasePlatformGlue abstract class that provides the action space, observation space and how to apply actions and get observations for a platform Source code in corl/glues/base_glue.py class BaseAgentGlue ( abc . ABC ): \"\"\" BasePlatformGlue abstract class that provides the action space, observation space and how to apply actions and get observations for a platform \"\"\" def __init__ ( self , ** kwargs ) -> None : \"\"\" The init function for an Agent Glue class Parameters ---------- state: typing.Tuple[BasePlatform] The initial state of the environment so the glue can stick together platform(s) or platform parts agent_id: str The name of the agent this glue is stuck to config: dict The configuration parameters of this glue class \"\"\" self . config : BaseAgentGlueValidator = self . get_validator ( ** kwargs ) self . _agent_removed = False @property def get_validator ( self ) -> typing . Type [ BaseAgentGlueValidator ]: \"\"\"returns the validator for this class Returns: BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs \"\"\" return BaseAgentGlueValidator @lru_cache ( maxsize = 1 ) @abc . abstractmethod def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differiciate it from other glues. \"\"\" ... @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. that defines the action given to apply_action Returns ------- gym.spaces.Space The gym Space that defines the action given to the apply_action function \"\"\" ... @lru_cache ( maxsize = 1 ) def normalized_action_space ( self ) -> typing . Optional [ gym . spaces . Space ]: \"\"\" Normalizes an action space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will output The default implementation scales all Box spaces to a low=-1. and high =1. This function should be a dual function to the unnormalize_action Returns ------- gym.spaces.Space: The scaled action space \"\"\" action_space = self . action_space () if action_space and self . config . normalization . enabled : return EnvSpaceUtil . normalize_space ( space = action_space , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) if action_space : return action_space return None def apply_action ( self , action : EnvSpaceUtil . sample_type , # pylint: disable=unused-argument observation : EnvSpaceUtil . sample_type # pylint: disable=unused-argument ) -> None : \"\"\" Apply the action for the controller, etc. Parameters ---------- action The action for the class to apply to the platform observation The current observations before appling the action \"\"\" ... def unnormalize_action ( self , action : EnvSpaceUtil . sample_type ) -> EnvSpaceUtil . sample_type : \"\"\" Un-Normalizes an action space from this glue's normalized_action_space to some raw bounds The default implementation assumes the normalized scale was low=-1. and high =1. This function should be a dual function to the normalize_action_space Parameters ---------- action: EnvSpaceUtil.sample_type The action to unnormalize Returns ------- EnvSpaceUtil.sample_type: the unnormalized action \"\"\" if self . config . normalization . enabled : ret = EnvSpaceUtil . unscale_sample_from_space ( space = self . action_space (), space_sample = action , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) else : ret = action return ret @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\" Build the observation space for the platform using the state of the platform, controller, sensors, etc. Returns ------- gym.spaces.Space The gym space that defines the returned space from the get_observation function \"\"\" ... @lru_cache ( maxsize = 1 ) def normalized_observation_space ( self ) -> typing . Optional [ gym . spaces . Space ]: \"\"\" Normalizes an observation space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will take as input for observations The default implementation scales all Box spaces to a low=-1. and high =1. This function should be the same as the normalize_observation function but return a Space and not the sample Returns ------- gym.spaces.Space: The scaled observation space \"\"\" observation_space = self . observation_space () if observation_space and self . config . normalization . enabled : return EnvSpaceUtil . normalize_space ( space = observation_space , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) if observation_space : return observation_space return None def get_observation ( self ) -> EnvSpaceUtil . sample_type : \"\"\" Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns ------- EnvSpaceUtil.sample_type The actual observation for this platform from this glue class \"\"\" ... def normalize_observation ( self , observation : EnvSpaceUtil . sample_type ) -> EnvSpaceUtil . sample_type : \"\"\" Normalizes an observation from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will use as observations Parameters ---------- observation: EnvSpaceUtil.sample_type The observation we want to scale Returns ------- EnvSpaceUtil.sample_type: The scaled observation \"\"\" if not self . config . normalization . enabled : ret = observation else : ret = EnvSpaceUtil . scale_sample_from_space ( space = self . observation_space (), space_sample = observation , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum , ) return ret def get_info_dict ( self ) -> EnvSpaceUtil . sample_type : \"\"\" Get the user specified metadata/metrics/etc. Returns ------- EnvSpaceUtil.sample_type The actual info dict object for this platform from this glue class \"\"\" ... def agent_removed ( self ) -> bool : \"\"\" Returns true if the agent has been removed, false otherwise \"\"\" return self . _agent_removed def set_agent_removed ( self , agent_removed : bool = True ) -> None : \"\"\" Notify the glue that the agent it is 'attached' to has been removed by the simulation \"\"\" self . _agent_removed = agent_removed","title":"BaseAgentGlue"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.base_glue.BaseAgentGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.__init__","text":"The init function for an Agent Glue class","title":"__init__()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.__init__--parameters","text":"typing.Tuple[BasePlatform] The initial state of the environment so the glue can stick together platform(s) or platform parts str The name of the agent this glue is stuck to dict The configuration parameters of this glue class Source code in corl/glues/base_glue.py def __init__ ( self , ** kwargs ) -> None : \"\"\" The init function for an Agent Glue class Parameters ---------- state: typing.Tuple[BasePlatform] The initial state of the environment so the glue can stick together platform(s) or platform parts agent_id: str The name of the agent this glue is stuck to config: dict The configuration parameters of this glue class \"\"\" self . config : BaseAgentGlueValidator = self . get_validator ( ** kwargs ) self . _agent_removed = False","title":"Parameters"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.action_space","text":"Build the action space for the controller, etc. that defines the action given to apply_action","title":"action_space()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.action_space--returns","text":"gym.spaces.Space The gym Space that defines the action given to the apply_action function Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. that defines the action given to apply_action Returns ------- gym.spaces.Space The gym Space that defines the action given to the apply_action function \"\"\" ...","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.agent_removed","text":"Returns true if the agent has been removed, false otherwise Source code in corl/glues/base_glue.py def agent_removed ( self ) -> bool : \"\"\" Returns true if the agent has been removed, false otherwise \"\"\" return self . _agent_removed","title":"agent_removed()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.apply_action","text":"Apply the action for the controller, etc.","title":"apply_action()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.apply_action--parameters","text":"action The action for the class to apply to the platform observation The current observations before appling the action Source code in corl/glues/base_glue.py def apply_action ( self , action : EnvSpaceUtil . sample_type , # pylint: disable=unused-argument observation : EnvSpaceUtil . sample_type # pylint: disable=unused-argument ) -> None : \"\"\" Apply the action for the controller, etc. Parameters ---------- action The action for the class to apply to the platform observation The current observations before appling the action \"\"\" ...","title":"Parameters"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.get_info_dict","text":"Get the user specified metadata/metrics/etc.","title":"get_info_dict()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.get_info_dict--returns","text":"EnvSpaceUtil.sample_type The actual info dict object for this platform from this glue class Source code in corl/glues/base_glue.py def get_info_dict ( self ) -> EnvSpaceUtil . sample_type : \"\"\" Get the user specified metadata/metrics/etc. Returns ------- EnvSpaceUtil.sample_type The actual info dict object for this platform from this glue class \"\"\" ...","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.get_observation","text":"Get the actual observation for the platform using the state of the platform, controller, sensors, etc.","title":"get_observation()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.get_observation--returns","text":"EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/base_glue.py def get_observation ( self ) -> EnvSpaceUtil . sample_type : \"\"\" Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns ------- EnvSpaceUtil.sample_type The actual observation for this platform from this glue class \"\"\" ...","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.get_unique_name","text":"Provies a unique name of the glue to differiciate it from other glues. Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) @abc . abstractmethod def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differiciate it from other glues. \"\"\" ...","title":"get_unique_name()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.normalize_observation","text":"Normalizes an observation from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will use as observations","title":"normalize_observation()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.normalize_observation--parameters","text":"EnvSpaceUtil.sample_type The observation we want to scale","title":"Parameters"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.normalize_observation--returns","text":"EnvSpaceUtil.sample_type: The scaled observation Source code in corl/glues/base_glue.py def normalize_observation ( self , observation : EnvSpaceUtil . sample_type ) -> EnvSpaceUtil . sample_type : \"\"\" Normalizes an observation from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will use as observations Parameters ---------- observation: EnvSpaceUtil.sample_type The observation we want to scale Returns ------- EnvSpaceUtil.sample_type: The scaled observation \"\"\" if not self . config . normalization . enabled : ret = observation else : ret = EnvSpaceUtil . scale_sample_from_space ( space = self . observation_space (), space_sample = observation , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum , ) return ret","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.normalized_action_space","text":"Normalizes an action space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will output The default implementation scales all Box spaces to a low=-1. and high =1. This function should be a dual function to the unnormalize_action","title":"normalized_action_space()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.normalized_action_space--returns","text":"gym.spaces.Space: The scaled action space Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) def normalized_action_space ( self ) -> typing . Optional [ gym . spaces . Space ]: \"\"\" Normalizes an action space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will output The default implementation scales all Box spaces to a low=-1. and high =1. This function should be a dual function to the unnormalize_action Returns ------- gym.spaces.Space: The scaled action space \"\"\" action_space = self . action_space () if action_space and self . config . normalization . enabled : return EnvSpaceUtil . normalize_space ( space = action_space , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) if action_space : return action_space return None","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.normalized_observation_space","text":"Normalizes an observation space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will take as input for observations The default implementation scales all Box spaces to a low=-1. and high =1. This function should be the same as the normalize_observation function but return a Space and not the sample","title":"normalized_observation_space()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.normalized_observation_space--returns","text":"gym.spaces.Space: The scaled observation space Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) def normalized_observation_space ( self ) -> typing . Optional [ gym . spaces . Space ]: \"\"\" Normalizes an observation space from this glue to some normalized bounds. There is not rules on what \"normal\" is. The only idea is that the \"normal\" range is what the Policy will take as input for observations The default implementation scales all Box spaces to a low=-1. and high =1. This function should be the same as the normalize_observation function but return a Space and not the sample Returns ------- gym.spaces.Space: The scaled observation space \"\"\" observation_space = self . observation_space () if observation_space and self . config . normalization . enabled : return EnvSpaceUtil . normalize_space ( space = observation_space , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) if observation_space : return observation_space return None","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.observation_space","text":"Build the observation space for the platform using the state of the platform, controller, sensors, etc.","title":"observation_space()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.observation_space--returns","text":"gym.spaces.Space The gym space that defines the returned space from the get_observation function Source code in corl/glues/base_glue.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\" Build the observation space for the platform using the state of the platform, controller, sensors, etc. Returns ------- gym.spaces.Space The gym space that defines the returned space from the get_observation function \"\"\" ...","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.set_agent_removed","text":"Notify the glue that the agent it is 'attached' to has been removed by the simulation Source code in corl/glues/base_glue.py def set_agent_removed ( self , agent_removed : bool = True ) -> None : \"\"\" Notify the glue that the agent it is 'attached' to has been removed by the simulation \"\"\" self . _agent_removed = agent_removed","title":"set_agent_removed()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.unnormalize_action","text":"Un-Normalizes an action space from this glue's normalized_action_space to some raw bounds The default implementation assumes the normalized scale was low=-1. and high =1. This function should be a dual function to the normalize_action_space","title":"unnormalize_action()"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.unnormalize_action--parameters","text":"EnvSpaceUtil.sample_type The action to unnormalize","title":"Parameters"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlue.unnormalize_action--returns","text":"EnvSpaceUtil.sample_type: the unnormalized action Source code in corl/glues/base_glue.py def unnormalize_action ( self , action : EnvSpaceUtil . sample_type ) -> EnvSpaceUtil . sample_type : \"\"\" Un-Normalizes an action space from this glue's normalized_action_space to some raw bounds The default implementation assumes the normalized scale was low=-1. and high =1. This function should be a dual function to the normalize_action_space Parameters ---------- action: EnvSpaceUtil.sample_type The action to unnormalize Returns ------- EnvSpaceUtil.sample_type: the unnormalized action \"\"\" if self . config . normalization . enabled : ret = EnvSpaceUtil . unscale_sample_from_space ( space = self . action_space (), space_sample = action , out_min = self . config . normalization . minimum , out_max = self . config . normalization . maximum ) else : ret = action return ret","title":"Returns"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlueNormalizationValidator","text":"enabled: if normalization is enabled for this glue minimum: the minimum value this glue's output will be normalized to maximum: the maximum value this glue's output will be normalized to Source code in corl/glues/base_glue.py class BaseAgentGlueNormalizationValidator ( BaseModel ): \"\"\" enabled: if normalization is enabled for this glue minimum: the minimum value this glue's output will be normalized to maximum: the maximum value this glue's output will be normalized to \"\"\" enabled : bool = True minimum : float = - 1.0 maximum : float = 1.0","title":"BaseAgentGlueNormalizationValidator"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentGlueValidator","text":"name: the custom name of this glue agent_name: the name of the agent who owns this glue seed: temporary - assume this will be removed normalization: how to handle obs normalization, see BaseAgentGlueNormalizationValidator Source code in corl/glues/base_glue.py class BaseAgentGlueValidator ( BaseModel ): \"\"\" name: the custom name of this glue agent_name: the name of the agent who owns this glue seed: temporary - assume this will be removed normalization: how to handle obs normalization, see BaseAgentGlueNormalizationValidator \"\"\" name : typing . Optional [ str ] agent_name : str seed : typing . Optional [ int ] normalization : BaseAgentGlueNormalizationValidator = BaseAgentGlueNormalizationValidator () training_export_behavior : TrainingExportBehavior = TrainingExportBehavior . INCLUDE","title":"BaseAgentGlueValidator"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentPlatformGlue","text":"BaseAgentPlatformGlue assumes that this glue corresponds to exactly one platform with the same platform name as the agent_id this glue is attached to This will then set the self._platform to this platform Source code in corl/glues/base_glue.py class BaseAgentPlatformGlue ( BaseAgentGlue , abc . ABC ): \"\"\" BaseAgentPlatformGlue assumes that this glue corresponds to exactly one platform with the same platform name as the agent_id this glue is attached to This will then set the self._platform to this platform \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseAgentPlatformGlueValidator super () . __init__ ( ** kwargs ) self . _platform : BasePlatform = self . config . platform self . _logger = logging . getLogger ( BaseAgentPlatformGlue . __name__ ) @property def get_validator ( self ) -> typing . Type [ BaseAgentPlatformGlueValidator ]: return BaseAgentPlatformGlueValidator","title":"BaseAgentPlatformGlue"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentPlatformGlue.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.base_glue.BaseAgentPlatformGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentPlatformGlueValidator","text":"platform: The platform object that this glue will read from/apply actions to Source code in corl/glues/base_glue.py class BaseAgentPlatformGlueValidator ( BaseAgentGlueValidator ): \"\"\" platform: The platform object that this glue will read from/apply actions to \"\"\" platform : BasePlatform class Config : \"\"\" This allows pydantic to validate that platform is a BasePlatform \"\"\" arbitrary_types_allowed = True","title":"BaseAgentPlatformGlueValidator"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.BaseAgentPlatformGlueValidator.Config","text":"This allows pydantic to validate that platform is a BasePlatform Source code in corl/glues/base_glue.py class Config : \"\"\" This allows pydantic to validate that platform is a BasePlatform \"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"reference/glues/base_glue/#corl.glues.base_glue.TrainingExportBehavior","text":"Enumeration of training behaviors Source code in corl/glues/base_glue.py class TrainingExportBehavior ( str , enum . Enum ): \"\"\"Enumeration of training behaviors \"\"\" INCLUDE = \"INCLUDE\" EXCLUDE = \"EXCLUDE\"","title":"TrainingExportBehavior"},{"location":"reference/glues/base_multi_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseMultiWrapperGlue ( BaseAgentGlue ) \u00a4 A base object that glues can inherit in order to \"wrap\" multiple glue instances Source code in corl/glues/base_multi_wrapper.py class BaseMultiWrapperGlue ( BaseAgentGlue ): \"\"\"A base object that glues can inherit in order to \"wrap\" multiple glue instances \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseMultiWrapperGlueValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> Type [ BaseMultiWrapperGlueValidator ]: return BaseMultiWrapperGlueValidator def glues ( self ) -> List [ BaseAgentGlue ]: \"\"\"Get the wrapped glue instances \"\"\" return self . config . wrapped def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) for glue in self . glues (): glue . set_agent_removed () get_validator : Type [ corl . glues . base_multi_wrapper . BaseMultiWrapperGlueValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.base_multi_wrapper.BaseMultiWrapperGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs glues ( self ) \u00a4 Get the wrapped glue instances Source code in corl/glues/base_multi_wrapper.py def glues ( self ) -> List [ BaseAgentGlue ]: \"\"\"Get the wrapped glue instances \"\"\" return self . config . wrapped set_agent_removed ( self , agent_removed = True ) \u00a4 Notify the glue that the agent it is 'attached' to has been removed by the simulation Source code in corl/glues/base_multi_wrapper.py def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) for glue in self . glues (): glue . set_agent_removed () BaseMultiWrapperGlueValidator ( BaseAgentGlueValidator ) pydantic-model \u00a4 wrapped - the wrapped glue instances Source code in corl/glues/base_multi_wrapper.py class BaseMultiWrapperGlueValidator ( BaseAgentGlueValidator ): \"\"\" wrapped - the wrapped glue instances \"\"\" wrapped : List [ BaseAgentGlue ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Base multi wrapper"},{"location":"reference/glues/base_multi_wrapper/#corl.glues.base_multi_wrapper.BaseMultiWrapperGlue","text":"A base object that glues can inherit in order to \"wrap\" multiple glue instances Source code in corl/glues/base_multi_wrapper.py class BaseMultiWrapperGlue ( BaseAgentGlue ): \"\"\"A base object that glues can inherit in order to \"wrap\" multiple glue instances \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseMultiWrapperGlueValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> Type [ BaseMultiWrapperGlueValidator ]: return BaseMultiWrapperGlueValidator def glues ( self ) -> List [ BaseAgentGlue ]: \"\"\"Get the wrapped glue instances \"\"\" return self . config . wrapped def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) for glue in self . glues (): glue . set_agent_removed ()","title":"BaseMultiWrapperGlue"},{"location":"reference/glues/base_multi_wrapper/#corl.glues.base_multi_wrapper.BaseMultiWrapperGlue.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.base_multi_wrapper.BaseMultiWrapperGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/base_multi_wrapper/#corl.glues.base_multi_wrapper.BaseMultiWrapperGlue.glues","text":"Get the wrapped glue instances Source code in corl/glues/base_multi_wrapper.py def glues ( self ) -> List [ BaseAgentGlue ]: \"\"\"Get the wrapped glue instances \"\"\" return self . config . wrapped","title":"glues()"},{"location":"reference/glues/base_multi_wrapper/#corl.glues.base_multi_wrapper.BaseMultiWrapperGlue.set_agent_removed","text":"Notify the glue that the agent it is 'attached' to has been removed by the simulation Source code in corl/glues/base_multi_wrapper.py def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) for glue in self . glues (): glue . set_agent_removed ()","title":"set_agent_removed()"},{"location":"reference/glues/base_multi_wrapper/#corl.glues.base_multi_wrapper.BaseMultiWrapperGlueValidator","text":"wrapped - the wrapped glue instances Source code in corl/glues/base_multi_wrapper.py class BaseMultiWrapperGlueValidator ( BaseAgentGlueValidator ): \"\"\" wrapped - the wrapped glue instances \"\"\" wrapped : List [ BaseAgentGlue ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseMultiWrapperGlueValidator"},{"location":"reference/glues/base_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseWrapperGlue ( BaseAgentGlue ) \u00a4 A base object that glues can inherit in order to \"wrap\" a single glue instance Source code in corl/glues/base_wrapper.py class BaseWrapperGlue ( BaseAgentGlue ): \"\"\"A base object that glues can inherit in order to \"wrap\" a single glue instance \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseWrapperGlueValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseWrapperGlueValidator ]: return BaseWrapperGlueValidator def glue ( self ) -> BaseAgentGlue : \"\"\"Get the wrapped glue instance \"\"\" return self . config . wrapped def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) self . glue () . set_agent_removed ( agent_removed ) get_validator : Type [ corl . glues . base_wrapper . BaseWrapperGlueValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.base_wrapper.BaseWrapperGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs glue ( self ) \u00a4 Get the wrapped glue instance Source code in corl/glues/base_wrapper.py def glue ( self ) -> BaseAgentGlue : \"\"\"Get the wrapped glue instance \"\"\" return self . config . wrapped set_agent_removed ( self , agent_removed = True ) \u00a4 Notify the glue that the agent it is 'attached' to has been removed by the simulation Source code in corl/glues/base_wrapper.py def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) self . glue () . set_agent_removed ( agent_removed ) BaseWrapperGlueValidator ( BaseAgentGlueValidator ) pydantic-model \u00a4 wrapped - the wrapped glue instance Source code in corl/glues/base_wrapper.py class BaseWrapperGlueValidator ( BaseAgentGlueValidator ): \"\"\" wrapped - the wrapped glue instance \"\"\" wrapped : BaseAgentGlue class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Base wrapper"},{"location":"reference/glues/base_wrapper/#corl.glues.base_wrapper.BaseWrapperGlue","text":"A base object that glues can inherit in order to \"wrap\" a single glue instance Source code in corl/glues/base_wrapper.py class BaseWrapperGlue ( BaseAgentGlue ): \"\"\"A base object that glues can inherit in order to \"wrap\" a single glue instance \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseWrapperGlueValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseWrapperGlueValidator ]: return BaseWrapperGlueValidator def glue ( self ) -> BaseAgentGlue : \"\"\"Get the wrapped glue instance \"\"\" return self . config . wrapped def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) self . glue () . set_agent_removed ( agent_removed )","title":"BaseWrapperGlue"},{"location":"reference/glues/base_wrapper/#corl.glues.base_wrapper.BaseWrapperGlue.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.base_wrapper.BaseWrapperGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/base_wrapper/#corl.glues.base_wrapper.BaseWrapperGlue.glue","text":"Get the wrapped glue instance Source code in corl/glues/base_wrapper.py def glue ( self ) -> BaseAgentGlue : \"\"\"Get the wrapped glue instance \"\"\" return self . config . wrapped","title":"glue()"},{"location":"reference/glues/base_wrapper/#corl.glues.base_wrapper.BaseWrapperGlue.set_agent_removed","text":"Notify the glue that the agent it is 'attached' to has been removed by the simulation Source code in corl/glues/base_wrapper.py def set_agent_removed ( self , agent_removed : bool = True ) -> None : super () . set_agent_removed ( agent_removed ) self . glue () . set_agent_removed ( agent_removed )","title":"set_agent_removed()"},{"location":"reference/glues/base_wrapper/#corl.glues.base_wrapper.BaseWrapperGlueValidator","text":"wrapped - the wrapped glue instance Source code in corl/glues/base_wrapper.py class BaseWrapperGlueValidator ( BaseAgentGlueValidator ): \"\"\" wrapped - the wrapped glue instance \"\"\" wrapped : BaseAgentGlue class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseWrapperGlueValidator"},{"location":"reference/glues/common/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Common"},{"location":"reference/glues/common/arithmetic_multi_glue/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. ArithmeticMultiGlue implementation ArithmeticMultiGlue ( BaseMultiWrapperGlue ) \u00a4 ArithmeticMultiGlue takes in a list of wrapped glues and performs some arithmetic operation on their output Source code in corl/glues/common/arithmetic_multi_glue.py class ArithmeticMultiGlue ( BaseMultiWrapperGlue ): \"\"\" ArithmeticMultiGlue takes in a list of wrapped glues and performs some arithmetic operation on their output \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ArithmeticMultiGlueValidator super () . __init__ ( ** kwargs ) self . operator = np . sum # if self.config.mode == \"sub\": # self.operator = np.subtract # elif self.config.mode == \"mult\": # self.operator = np.multiply # elif self.config.mode == \"div\": # self.operator = np.divide self . field_names = [] for glue in self . glues (): space = glue . observation_space () if len ( space . spaces ) > 1 : raise RuntimeError ( \"ArithmeticMultiGlue can only wrap a glue with one output\" ) self . field_names . append ( list ( space . spaces . keys ())[ 0 ]) class Fields : \"\"\" Field data \"\"\" RESULT = \"result\" @property def get_validator ( self ) -> typing . Type [ ArithmeticMultiGlueValidator ]: return ArithmeticMultiGlueValidator @lru_cache ( maxsize = 1 ) def get_unique_name ( self ): \"\"\"Class method that retreives the unique name for the glue instance \"\"\" tmp = [ glue . get_unique_name () for glue in self . glues ()] if any ( tmp_str is None for tmp_str in tmp ): return None wrapped_glue_names = \"\" . join ( tmp ) return wrapped_glue_names + self . config . mode def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () d [ f \" { self . Fields . RESULT } \" ] = np . asarray ( [( self . config . limit . maximum + self . config . limit . minimum ) / 2 ], dtype = np . float32 ) # type: ignore return d @lru_cache ( maxsize = 1 ) def observation_space ( self ): d = gym . spaces . dict . Dict () d . spaces [ f \" { self . Fields . RESULT } \" ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) return d def get_observation ( self ): d = OrderedDict () tmp_output = [ glue . get_observation ()[ field_name ] for glue , field_name in zip ( self . glues (), self . field_names )] d [ self . Fields . RESULT ] = np . array ([ self . operator ( tmp_output )], dtype = np . float32 ) return d @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : return None def apply_action ( self , action , observation ): return None get_validator : Type [ corl . glues . common . arithmetic_multi_glue . ArithmeticMultiGlueValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs Fields \u00a4 Field data Source code in corl/glues/common/arithmetic_multi_glue.py class Fields : \"\"\" Field data \"\"\" RESULT = \"result\" apply_action ( self , action , observation ) \u00a4 Apply the action for the controller, etc. Parameters \u00a4 action The action for the class to apply to the platform observation The current observations before appling the action Source code in corl/glues/common/arithmetic_multi_glue.py def apply_action ( self , action , observation ): return None get_observation ( self ) \u00a4 Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns \u00a4 EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/common/arithmetic_multi_glue.py def get_observation ( self ): d = OrderedDict () tmp_output = [ glue . get_observation ()[ field_name ] for glue , field_name in zip ( self . glues (), self . field_names )] d [ self . Fields . RESULT ] = np . array ([ self . operator ( tmp_output )], dtype = np . float32 ) return d get_unique_name ( self ) \u00a4 Class method that retreives the unique name for the glue instance Source code in corl/glues/common/arithmetic_multi_glue.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ): \"\"\"Class method that retreives the unique name for the glue instance \"\"\" tmp = [ glue . get_unique_name () for glue in self . glues ()] if any ( tmp_str is None for tmp_str in tmp ): return None wrapped_glue_names = \"\" . join ( tmp ) return wrapped_glue_names + self . config . mode invalid_value ( self ) \u00a4 When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: Type Description OrderedDict OrderedDict -- Dictionary with entry containing 1D array Source code in corl/glues/common/arithmetic_multi_glue.py def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () d [ f \" { self . Fields . RESULT } \" ] = np . asarray ( [( self . config . limit . maximum + self . config . limit . minimum ) / 2 ], dtype = np . float32 ) # type: ignore return d ArithmeticMultiGlueValidator ( BaseMultiWrapperGlueValidator ) pydantic-model \u00a4 mode: what arithmetic operation to run on the output of the wrapped glues limit: the expected limit for this glue Source code in corl/glues/common/arithmetic_multi_glue.py class ArithmeticMultiGlueValidator ( BaseMultiWrapperGlueValidator ): \"\"\" mode: what arithmetic operation to run on the output of the wrapped glues limit: the expected limit for this glue \"\"\" mode : typing . Literal [ \"sum\" , \"sub\" , \"mult\" , \"div\" ] = \"sum\" limit : LimitConfigValidator","title":"Arithmetic multi glue"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue","text":"ArithmeticMultiGlue takes in a list of wrapped glues and performs some arithmetic operation on their output Source code in corl/glues/common/arithmetic_multi_glue.py class ArithmeticMultiGlue ( BaseMultiWrapperGlue ): \"\"\" ArithmeticMultiGlue takes in a list of wrapped glues and performs some arithmetic operation on their output \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ArithmeticMultiGlueValidator super () . __init__ ( ** kwargs ) self . operator = np . sum # if self.config.mode == \"sub\": # self.operator = np.subtract # elif self.config.mode == \"mult\": # self.operator = np.multiply # elif self.config.mode == \"div\": # self.operator = np.divide self . field_names = [] for glue in self . glues (): space = glue . observation_space () if len ( space . spaces ) > 1 : raise RuntimeError ( \"ArithmeticMultiGlue can only wrap a glue with one output\" ) self . field_names . append ( list ( space . spaces . keys ())[ 0 ]) class Fields : \"\"\" Field data \"\"\" RESULT = \"result\" @property def get_validator ( self ) -> typing . Type [ ArithmeticMultiGlueValidator ]: return ArithmeticMultiGlueValidator @lru_cache ( maxsize = 1 ) def get_unique_name ( self ): \"\"\"Class method that retreives the unique name for the glue instance \"\"\" tmp = [ glue . get_unique_name () for glue in self . glues ()] if any ( tmp_str is None for tmp_str in tmp ): return None wrapped_glue_names = \"\" . join ( tmp ) return wrapped_glue_names + self . config . mode def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () d [ f \" { self . Fields . RESULT } \" ] = np . asarray ( [( self . config . limit . maximum + self . config . limit . minimum ) / 2 ], dtype = np . float32 ) # type: ignore return d @lru_cache ( maxsize = 1 ) def observation_space ( self ): d = gym . spaces . dict . Dict () d . spaces [ f \" { self . Fields . RESULT } \" ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) return d def get_observation ( self ): d = OrderedDict () tmp_output = [ glue . get_observation ()[ field_name ] for glue , field_name in zip ( self . glues (), self . field_names )] d [ self . Fields . RESULT ] = np . array ([ self . operator ( tmp_output )], dtype = np . float32 ) return d @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : return None def apply_action ( self , action , observation ): return None","title":"ArithmeticMultiGlue"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue.Fields","text":"Field data Source code in corl/glues/common/arithmetic_multi_glue.py class Fields : \"\"\" Field data \"\"\" RESULT = \"result\"","title":"Fields"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue.apply_action","text":"Apply the action for the controller, etc.","title":"apply_action()"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue.apply_action--parameters","text":"action The action for the class to apply to the platform observation The current observations before appling the action Source code in corl/glues/common/arithmetic_multi_glue.py def apply_action ( self , action , observation ): return None","title":"Parameters"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue.get_observation","text":"Get the actual observation for the platform using the state of the platform, controller, sensors, etc.","title":"get_observation()"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue.get_observation--returns","text":"EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/common/arithmetic_multi_glue.py def get_observation ( self ): d = OrderedDict () tmp_output = [ glue . get_observation ()[ field_name ] for glue , field_name in zip ( self . glues (), self . field_names )] d [ self . Fields . RESULT ] = np . array ([ self . operator ( tmp_output )], dtype = np . float32 ) return d","title":"Returns"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue.get_unique_name","text":"Class method that retreives the unique name for the glue instance Source code in corl/glues/common/arithmetic_multi_glue.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ): \"\"\"Class method that retreives the unique name for the glue instance \"\"\" tmp = [ glue . get_unique_name () for glue in self . glues ()] if any ( tmp_str is None for tmp_str in tmp ): return None wrapped_glue_names = \"\" . join ( tmp ) return wrapped_glue_names + self . config . mode","title":"get_unique_name()"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlue.invalid_value","text":"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: Type Description OrderedDict OrderedDict -- Dictionary with entry containing 1D array Source code in corl/glues/common/arithmetic_multi_glue.py def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () d [ f \" { self . Fields . RESULT } \" ] = np . asarray ( [( self . config . limit . maximum + self . config . limit . minimum ) / 2 ], dtype = np . float32 ) # type: ignore return d","title":"invalid_value()"},{"location":"reference/glues/common/arithmetic_multi_glue/#corl.glues.common.arithmetic_multi_glue.ArithmeticMultiGlueValidator","text":"mode: what arithmetic operation to run on the output of the wrapped glues limit: the expected limit for this glue Source code in corl/glues/common/arithmetic_multi_glue.py class ArithmeticMultiGlueValidator ( BaseMultiWrapperGlueValidator ): \"\"\" mode: what arithmetic operation to run on the output of the wrapped glues limit: the expected limit for this glue \"\"\" mode : typing . Literal [ \"sum\" , \"sub\" , \"mult\" , \"div\" ] = \"sum\" limit : LimitConfigValidator","title":"ArithmeticMultiGlueValidator"},{"location":"reference/glues/common/controller_glue/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Controller Glue ControllerGlue ( BaseAgentControllerGlue ) \u00a4 This simple glue class wraps a controller and creates an action space based on the controller exclusiveness This class has no observation space or observations Source code in corl/glues/common/controller_glue.py class ControllerGlue ( BaseAgentControllerGlue ): \"\"\" This simple glue class wraps a controller and creates an action space based on the controller exclusiveness This class has no observation space or observations \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ControllerGlueValidator super () . __init__ ( ** kwargs ) self . _controller = get_controller_by_name ( self . _platform , self . config . controller ) self . _controller_name : str = self . config . controller self . _key = self . _controller . control_properties . name self . _control_properties = self . _controller . control_properties @property def controller ( self ) -> BaseController : \"\"\"Returns controller Returns ------- BaseController The controller for this glue \"\"\" return self . _controller @property def get_validator ( self ) -> typing . Type [ ControllerGlueValidator ]: return ControllerGlueValidator @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" action_space_dict = {} if isinstance ( self . _control_properties , list ): action_spaces = [ control_prop . create_space () for control_prop in self . _control_properties ] action_space_dict [ self . _key ] = gym . spaces . tuple . Tuple ( tuple ( action_spaces )) else : action_space_dict [ self . _key ] = self . _control_properties . create_space () return gym . spaces . Dict ( action_space_dict ) def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ) -> None : \"\"\"Apply the action for the controller, etc. Parameters ---------- action : EnvSpaceUtil.sample_type The action that is to be applied at the controller level observation : EnvSpaceUtil.sample_type The current observable state by the agent (integration focus) \"\"\" if isinstance ( action , ( tuple , list )): raise ValueError ( \"Unexpected action of type tuple or list\" ) control = action [ self . _key ] self . _controller . apply_control ( control = control ) def get_applied_control ( self ) -> OrderedDict : control_dict = OrderedDict () if not self . _agent_removed : control_dict [ self . _key ] = self . _controller . get_validated_applied_control () else : control_dict [ self . _key ] = self . action_space ()[ self . _key ] . sample () return control_dict @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : obs_space = gym . spaces . dict . Dict () obs_space . spaces [ 'invalid' ] = gym . spaces . Discrete ( 2 ) obs_space . spaces [ 'control' ] = self . action_space ()[ self . _key ] return obs_space def get_observation ( self ) -> OrderedDict : obs_dict = OrderedDict () obs_dict [ 'invalid' ] = 1 if self . _agent_removed else 0 obs_dict [ 'invalid' ] = 1 if obs_dict [ 'invalid' ] or not self . controller . valid else 0 obs_dict [ 'control' ] = self . get_applied_control ()[ self . _key ] return obs_dict @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differentiate it from other glues. \"\"\" control_properties = self . _control_properties if isinstance ( control_properties , list ): controller_names : typing . List [ str ] = [] for controller in control_properties : controller_names . append ( controller . name ) unique_name = '' . join ([ controller_name . capitalize () for controller_name in controller_names ]) else : unique_name = control_properties . name . capitalize () return self . config . controller + \"_\" + unique_name @property def resolved_controller_class_name ( self ) -> str : \"\"\"Class name of the internal controller.\"\"\" return type ( self . _controller ) . __name__ controller : BaseController property readonly \u00a4 Returns controller Returns \u00a4 BaseController The controller for this glue get_validator : Type [ corl . glues . common . controller_glue . ControllerGlueValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.common.controller_glue.ControllerGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs resolved_controller_class_name : str property readonly \u00a4 Class name of the internal controller. action_space ( self ) \u00a4 Build the action space for the controller, etc. Source code in corl/glues/common/controller_glue.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" action_space_dict = {} if isinstance ( self . _control_properties , list ): action_spaces = [ control_prop . create_space () for control_prop in self . _control_properties ] action_space_dict [ self . _key ] = gym . spaces . tuple . Tuple ( tuple ( action_spaces )) else : action_space_dict [ self . _key ] = self . _control_properties . create_space () return gym . spaces . Dict ( action_space_dict ) apply_action ( self , action , observation ) \u00a4 Apply the action for the controller, etc. Parameters \u00a4 action : EnvSpaceUtil.sample_type The action that is to be applied at the controller level observation : EnvSpaceUtil.sample_type The current observable state by the agent (integration focus) Source code in corl/glues/common/controller_glue.py def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ) -> None : \"\"\"Apply the action for the controller, etc. Parameters ---------- action : EnvSpaceUtil.sample_type The action that is to be applied at the controller level observation : EnvSpaceUtil.sample_type The current observable state by the agent (integration focus) \"\"\" if isinstance ( action , ( tuple , list )): raise ValueError ( \"Unexpected action of type tuple or list\" ) control = action [ self . _key ] self . _controller . apply_control ( control = control ) get_applied_control ( self ) \u00a4 Get the currently applied controls Returns \u00a4 OrderedDict The currently applied controls Source code in corl/glues/common/controller_glue.py def get_applied_control ( self ) -> OrderedDict : control_dict = OrderedDict () if not self . _agent_removed : control_dict [ self . _key ] = self . _controller . get_validated_applied_control () else : control_dict [ self . _key ] = self . action_space ()[ self . _key ] . sample () return control_dict get_observation ( self ) \u00a4 Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns \u00a4 EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/common/controller_glue.py def get_observation ( self ) -> OrderedDict : obs_dict = OrderedDict () obs_dict [ 'invalid' ] = 1 if self . _agent_removed else 0 obs_dict [ 'invalid' ] = 1 if obs_dict [ 'invalid' ] or not self . controller . valid else 0 obs_dict [ 'control' ] = self . get_applied_control ()[ self . _key ] return obs_dict get_unique_name ( self ) \u00a4 Provies a unique name of the glue to differentiate it from other glues. Source code in corl/glues/common/controller_glue.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differentiate it from other glues. \"\"\" control_properties = self . _control_properties if isinstance ( control_properties , list ): controller_names : typing . List [ str ] = [] for controller in control_properties : controller_names . append ( controller . name ) unique_name = '' . join ([ controller_name . capitalize () for controller_name in controller_names ]) else : unique_name = control_properties . name . capitalize () return self . config . controller + \"_\" + unique_name ControllerGlueValidator ( BaseAgentPlatformGlueValidator ) pydantic-model \u00a4 controller: Which controller to get from parent platform minimum: temporary value to override the minumum for this glue maximum: temporary value to override the minimum for this glue if you are using more than one of the same controller, use this to access which index to use Source code in corl/glues/common/controller_glue.py class ControllerGlueValidator ( BaseAgentPlatformGlueValidator ): \"\"\" controller: Which controller to get from parent platform minimum: temporary value to override the minumum for this glue maximum: temporary value to override the minimum for this glue index: if you are using more than one of the same controller, use this to access which index to use \"\"\" controller : str","title":"Controller glue"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue","text":"This simple glue class wraps a controller and creates an action space based on the controller exclusiveness This class has no observation space or observations Source code in corl/glues/common/controller_glue.py class ControllerGlue ( BaseAgentControllerGlue ): \"\"\" This simple glue class wraps a controller and creates an action space based on the controller exclusiveness This class has no observation space or observations \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ControllerGlueValidator super () . __init__ ( ** kwargs ) self . _controller = get_controller_by_name ( self . _platform , self . config . controller ) self . _controller_name : str = self . config . controller self . _key = self . _controller . control_properties . name self . _control_properties = self . _controller . control_properties @property def controller ( self ) -> BaseController : \"\"\"Returns controller Returns ------- BaseController The controller for this glue \"\"\" return self . _controller @property def get_validator ( self ) -> typing . Type [ ControllerGlueValidator ]: return ControllerGlueValidator @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" action_space_dict = {} if isinstance ( self . _control_properties , list ): action_spaces = [ control_prop . create_space () for control_prop in self . _control_properties ] action_space_dict [ self . _key ] = gym . spaces . tuple . Tuple ( tuple ( action_spaces )) else : action_space_dict [ self . _key ] = self . _control_properties . create_space () return gym . spaces . Dict ( action_space_dict ) def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ) -> None : \"\"\"Apply the action for the controller, etc. Parameters ---------- action : EnvSpaceUtil.sample_type The action that is to be applied at the controller level observation : EnvSpaceUtil.sample_type The current observable state by the agent (integration focus) \"\"\" if isinstance ( action , ( tuple , list )): raise ValueError ( \"Unexpected action of type tuple or list\" ) control = action [ self . _key ] self . _controller . apply_control ( control = control ) def get_applied_control ( self ) -> OrderedDict : control_dict = OrderedDict () if not self . _agent_removed : control_dict [ self . _key ] = self . _controller . get_validated_applied_control () else : control_dict [ self . _key ] = self . action_space ()[ self . _key ] . sample () return control_dict @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : obs_space = gym . spaces . dict . Dict () obs_space . spaces [ 'invalid' ] = gym . spaces . Discrete ( 2 ) obs_space . spaces [ 'control' ] = self . action_space ()[ self . _key ] return obs_space def get_observation ( self ) -> OrderedDict : obs_dict = OrderedDict () obs_dict [ 'invalid' ] = 1 if self . _agent_removed else 0 obs_dict [ 'invalid' ] = 1 if obs_dict [ 'invalid' ] or not self . controller . valid else 0 obs_dict [ 'control' ] = self . get_applied_control ()[ self . _key ] return obs_dict @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differentiate it from other glues. \"\"\" control_properties = self . _control_properties if isinstance ( control_properties , list ): controller_names : typing . List [ str ] = [] for controller in control_properties : controller_names . append ( controller . name ) unique_name = '' . join ([ controller_name . capitalize () for controller_name in controller_names ]) else : unique_name = control_properties . name . capitalize () return self . config . controller + \"_\" + unique_name @property def resolved_controller_class_name ( self ) -> str : \"\"\"Class name of the internal controller.\"\"\" return type ( self . _controller ) . __name__","title":"ControllerGlue"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.controller","text":"Returns controller","title":"controller"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.controller--returns","text":"BaseController The controller for this glue","title":"Returns"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.common.controller_glue.ControllerGlueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.resolved_controller_class_name","text":"Class name of the internal controller.","title":"resolved_controller_class_name"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.action_space","text":"Build the action space for the controller, etc. Source code in corl/glues/common/controller_glue.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" action_space_dict = {} if isinstance ( self . _control_properties , list ): action_spaces = [ control_prop . create_space () for control_prop in self . _control_properties ] action_space_dict [ self . _key ] = gym . spaces . tuple . Tuple ( tuple ( action_spaces )) else : action_space_dict [ self . _key ] = self . _control_properties . create_space () return gym . spaces . Dict ( action_space_dict )","title":"action_space()"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.apply_action","text":"Apply the action for the controller, etc.","title":"apply_action()"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.apply_action--parameters","text":"action : EnvSpaceUtil.sample_type The action that is to be applied at the controller level observation : EnvSpaceUtil.sample_type The current observable state by the agent (integration focus) Source code in corl/glues/common/controller_glue.py def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ) -> None : \"\"\"Apply the action for the controller, etc. Parameters ---------- action : EnvSpaceUtil.sample_type The action that is to be applied at the controller level observation : EnvSpaceUtil.sample_type The current observable state by the agent (integration focus) \"\"\" if isinstance ( action , ( tuple , list )): raise ValueError ( \"Unexpected action of type tuple or list\" ) control = action [ self . _key ] self . _controller . apply_control ( control = control )","title":"Parameters"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.get_applied_control","text":"Get the currently applied controls","title":"get_applied_control()"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.get_applied_control--returns","text":"OrderedDict The currently applied controls Source code in corl/glues/common/controller_glue.py def get_applied_control ( self ) -> OrderedDict : control_dict = OrderedDict () if not self . _agent_removed : control_dict [ self . _key ] = self . _controller . get_validated_applied_control () else : control_dict [ self . _key ] = self . action_space ()[ self . _key ] . sample () return control_dict","title":"Returns"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.get_observation","text":"Get the actual observation for the platform using the state of the platform, controller, sensors, etc.","title":"get_observation()"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.get_observation--returns","text":"EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/common/controller_glue.py def get_observation ( self ) -> OrderedDict : obs_dict = OrderedDict () obs_dict [ 'invalid' ] = 1 if self . _agent_removed else 0 obs_dict [ 'invalid' ] = 1 if obs_dict [ 'invalid' ] or not self . controller . valid else 0 obs_dict [ 'control' ] = self . get_applied_control ()[ self . _key ] return obs_dict","title":"Returns"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlue.get_unique_name","text":"Provies a unique name of the glue to differentiate it from other glues. Source code in corl/glues/common/controller_glue.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differentiate it from other glues. \"\"\" control_properties = self . _control_properties if isinstance ( control_properties , list ): controller_names : typing . List [ str ] = [] for controller in control_properties : controller_names . append ( controller . name ) unique_name = '' . join ([ controller_name . capitalize () for controller_name in controller_names ]) else : unique_name = control_properties . name . capitalize () return self . config . controller + \"_\" + unique_name","title":"get_unique_name()"},{"location":"reference/glues/common/controller_glue/#corl.glues.common.controller_glue.ControllerGlueValidator","text":"controller: Which controller to get from parent platform minimum: temporary value to override the minumum for this glue maximum: temporary value to override the minimum for this glue if you are using more than one of the same controller, use this to access which index to use Source code in corl/glues/common/controller_glue.py class ControllerGlueValidator ( BaseAgentPlatformGlueValidator ): \"\"\" controller: Which controller to get from parent platform minimum: temporary value to override the minumum for this glue maximum: temporary value to override the minimum for this glue index: if you are using more than one of the same controller, use this to access which index to use \"\"\" controller : str","title":"ControllerGlueValidator"},{"location":"reference/glues/common/observe_part_validity/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. ObservePartValidity ( BaseAgentPlatformGlue ) \u00a4 Glue to observe a part's validity flag Source code in corl/glues/common/observe_part_validity.py class ObservePartValidity ( BaseAgentPlatformGlue ): \"\"\"Glue to observe a part's validity flag \"\"\" # pylint: disable=too-few-public-methods class Fields : \"\"\" Fields in this glue \"\"\" VALIDITY_OBSERVATION = \"validity_observation\" @property def get_validator ( self ) -> typing . Type [ ObservePartValidityValidator ]: return ObservePartValidityValidator def __init__ ( self , ** kwargs ) -> None : self . config : ObservePartValidityValidator super () . __init__ ( ** kwargs ) self . _part : BasePlatformPart = get_part_by_name ( self . _platform , self . config . part ) self . _part_name : str = self . config . part @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retrieves the unique name for the glue instance \"\"\" return \"ObserveValidity_\" + self . _part_name @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . VALIDITY_OBSERVATION ] = NoneUnitType return d @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . VALIDITY_OBSERVATION ] = gym . spaces . Discrete ( 2 ) return d def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" d = OrderedDict () d [ self . Fields . VALIDITY_OBSERVATION ] = int ( self . _part . valid ) return d get_validator : Type [ corl . glues . common . observe_part_validity . ObservePartValidityValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.common.observe_part_validity.ObservePartValidityValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs Fields \u00a4 Fields in this glue Source code in corl/glues/common/observe_part_validity.py class Fields : \"\"\" Fields in this glue \"\"\" VALIDITY_OBSERVATION = \"validity_observation\" get_observation ( self ) \u00a4 Observation Values Source code in corl/glues/common/observe_part_validity.py def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" d = OrderedDict () d [ self . Fields . VALIDITY_OBSERVATION ] = int ( self . _part . valid ) return d get_unique_name ( self ) \u00a4 Class method that retrieves the unique name for the glue instance Source code in corl/glues/common/observe_part_validity.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retrieves the unique name for the glue instance \"\"\" return \"ObserveValidity_\" + self . _part_name observation_space ( self ) \u00a4 Observation Space Source code in corl/glues/common/observe_part_validity.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . VALIDITY_OBSERVATION ] = gym . spaces . Discrete ( 2 ) return d observation_units ( self ) \u00a4 Units of the sensors in this glue Source code in corl/glues/common/observe_part_validity.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . VALIDITY_OBSERVATION ] = NoneUnitType return d ObservePartValidityValidator ( BaseAgentPlatformGlueValidator ) pydantic-model \u00a4 part: which part to find on the platform Source code in corl/glues/common/observe_part_validity.py class ObservePartValidityValidator ( BaseAgentPlatformGlueValidator ): \"\"\" part: which part to find on the platform \"\"\" part : str","title":"Observe part validity"},{"location":"reference/glues/common/observe_part_validity/#corl.glues.common.observe_part_validity.ObservePartValidity","text":"Glue to observe a part's validity flag Source code in corl/glues/common/observe_part_validity.py class ObservePartValidity ( BaseAgentPlatformGlue ): \"\"\"Glue to observe a part's validity flag \"\"\" # pylint: disable=too-few-public-methods class Fields : \"\"\" Fields in this glue \"\"\" VALIDITY_OBSERVATION = \"validity_observation\" @property def get_validator ( self ) -> typing . Type [ ObservePartValidityValidator ]: return ObservePartValidityValidator def __init__ ( self , ** kwargs ) -> None : self . config : ObservePartValidityValidator super () . __init__ ( ** kwargs ) self . _part : BasePlatformPart = get_part_by_name ( self . _platform , self . config . part ) self . _part_name : str = self . config . part @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retrieves the unique name for the glue instance \"\"\" return \"ObserveValidity_\" + self . _part_name @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . VALIDITY_OBSERVATION ] = NoneUnitType return d @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . VALIDITY_OBSERVATION ] = gym . spaces . Discrete ( 2 ) return d def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" d = OrderedDict () d [ self . Fields . VALIDITY_OBSERVATION ] = int ( self . _part . valid ) return d","title":"ObservePartValidity"},{"location":"reference/glues/common/observe_part_validity/#corl.glues.common.observe_part_validity.ObservePartValidity.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.common.observe_part_validity.ObservePartValidityValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/common/observe_part_validity/#corl.glues.common.observe_part_validity.ObservePartValidity.Fields","text":"Fields in this glue Source code in corl/glues/common/observe_part_validity.py class Fields : \"\"\" Fields in this glue \"\"\" VALIDITY_OBSERVATION = \"validity_observation\"","title":"Fields"},{"location":"reference/glues/common/observe_part_validity/#corl.glues.common.observe_part_validity.ObservePartValidity.get_observation","text":"Observation Values Source code in corl/glues/common/observe_part_validity.py def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" d = OrderedDict () d [ self . Fields . VALIDITY_OBSERVATION ] = int ( self . _part . valid ) return d","title":"get_observation()"},{"location":"reference/glues/common/observe_part_validity/#corl.glues.common.observe_part_validity.ObservePartValidity.get_unique_name","text":"Class method that retrieves the unique name for the glue instance Source code in corl/glues/common/observe_part_validity.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retrieves the unique name for the glue instance \"\"\" return \"ObserveValidity_\" + self . _part_name","title":"get_unique_name()"},{"location":"reference/glues/common/observe_part_validity/#corl.glues.common.observe_part_validity.ObservePartValidity.observation_space","text":"Observation Space Source code in corl/glues/common/observe_part_validity.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . VALIDITY_OBSERVATION ] = gym . spaces . Discrete ( 2 ) return d","title":"observation_space()"},{"location":"reference/glues/common/observe_part_validity/#corl.glues.common.observe_part_validity.ObservePartValidity.observation_units","text":"Units of the sensors in this glue Source code in corl/glues/common/observe_part_validity.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . VALIDITY_OBSERVATION ] = NoneUnitType return d","title":"observation_units()"},{"location":"reference/glues/common/observe_part_validity/#corl.glues.common.observe_part_validity.ObservePartValidityValidator","text":"part: which part to find on the platform Source code in corl/glues/common/observe_part_validity.py class ObservePartValidityValidator ( BaseAgentPlatformGlueValidator ): \"\"\" part: which part to find on the platform \"\"\" part : str","title":"ObservePartValidityValidator"},{"location":"reference/glues/common/observe_sensor/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. ObserveSensor ( BaseAgentPlatformGlue ) \u00a4 Glue to observe a sensor Configuration string String of sensor class, sensor base name, or the PluginLibrary group name For example consider a platform which has \"SimOrientationRateSensor\" attached . The following will all be equivalent : - SimOrientationRateSensor - BaseOrientationRateSensor - Sensor_OrientationRate Important : If multiple sensors are found an array will be returned ( ex . Gload Learjet ) This will also be the name attached to the glue , which can then be used Source code in corl/glues/common/observe_sensor.py class ObserveSensor ( BaseAgentPlatformGlue ): \"\"\"Glue to observe a sensor Configuration: sensor: string String of sensor class, sensor base name, or the PluginLibrary group name For example consider a platform which has \"SimOrientationRateSensor\" attached. The following will all be equivalent: - SimOrientationRateSensor - BaseOrientationRateSensor - Sensor_OrientationRate Important: If multiple sensors are found an array will be returned (ex. Gload Learjet) This will also be the name attached to the glue, which can then be used \"\"\" # pylint: disable=too-few-public-methods class Fields : \"\"\" Fields in this glue \"\"\" DIRECT_OBSERVATION = \"direct_observation\" @property def get_validator ( self ) -> typing . Type [ ObserveSensorValidator ]: return ObserveSensorValidator def __init__ ( self , ** kwargs ) -> None : self . config : ObserveSensorValidator super () . __init__ ( ** kwargs ) self . _sensor = get_sensor_by_name ( self . _platform , self . config . sensor ) self . _sensor_name : str = self . config . sensor self . out_units = self . config . output_units @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" # TODO: This may result in ObserveSensor_Sensor_X, which is kinda ugly return \"ObserveSensor_\" + self . _sensor_name # TODO: broken def invalid_value ( self ) -> OrderedDict : \"\"\"Return zeros when invalid \"\"\" arr : typing . List [ float ] = [] if isinstance ( self . _sensor . measurement_properties . low , abc . Sequence ): # type: ignore for _ in enumerate ( self . _sensor . measurement_properties . low ): # type: ignore arr . append ( 0.0 ) elif np . isscalar ( self . _sensor . measurement_properties . low ): # type: ignore arr . append ( 0.0 ) else : raise RuntimeError ( \"Expecting either array or scalar\" ) d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = np . asarray ( arr ) return d @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" out_units = [] for value in self . out_units : if isinstance ( value , abc . Sequence ): # list of lists 2D case sub_units = [] for val in value : sub_units . append ( GetStrFromUnit ( val )) out_units . append ( sub_units ) else : # 1D case out_units . append ( GetStrFromUnit ( value )) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = out_units return d @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () if isinstance ( self . _sensor . measurement_properties , BoxProp ): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_converted_space ( self . out_units ) elif isinstance ( self . _sensor . measurement_properties , ( DiscreteProp )): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_space () elif isinstance ( self . _sensor . measurement_properties , ( MultiBinary )): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_space () else : raise TypeError ( \"Only supports {BoxProp.__name__} , {MultiBinary.__name__} and {DiscreteProp.__name__} \" ) return d def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" d = OrderedDict () if isinstance ( self . _sensor . measurement_properties , BoxProp ): sensed_value = self . _sensor . get_measurement () if isinstance ( sensed_value , np . ndarray ): sensed_value = sensed_value . tolist () else : sensed_value = list ( sensed_value ) for indx , value in enumerate ( sensed_value ): if isinstance ( value , abc . Sequence ): # list of lists 2D case for indx1 , value1 in enumerate ( value ): in_unit2d = self . _sensor . measurement_properties . unit [ indx ][ indx1 ] assert isinstance ( in_unit2d , str ) assert isinstance ( self . out_units , abc . Sequence ) out_unit1 = self . out_units [ indx ] assert isinstance ( out_unit1 , abc . Sequence ) out_unit2d = out_unit1 [ indx1 ] assert isinstance ( out_unit2d , enum . Enum ) assert isinstance ( sensed_value , abc . Sequence ) sensed_value1 = sensed_value [ indx ] assert isinstance ( sensed_value1 , list ) sensed_value1 [ indx1 ] = Convert ( value1 , in_unit2d , out_unit2d ) else : # 1D case in_unit = self . _sensor . measurement_properties . unit [ indx ] assert isinstance ( in_unit , str ) out_unit = self . out_units [ indx ] assert isinstance ( out_unit , enum . Enum ) assert isinstance ( sensed_value , list ) sensed_value [ indx ] = Convert ( value , in_unit , out_unit ) d [ self . Fields . DIRECT_OBSERVATION ] = np . array ( sensed_value , dtype = np . float32 ) elif isinstance ( self . _sensor . measurement_properties , ( DiscreteProp )): sensed_value_discrete : float = self . _sensor . get_measurement ()[ 0 ] d [ self . Fields . DIRECT_OBSERVATION ] = np . array ( sensed_value_discrete , dtype = np . int32 ) elif isinstance ( self . _sensor . measurement_properties , ( MultiBinary )): sensed_value_multi_binary = self . _sensor . get_measurement () d [ self . Fields . DIRECT_OBSERVATION ] = sensed_value_multi_binary # type: ignore else : raise TypeError ( \"Only supports {BoxProp.__name__} , {MultiBinary.__name__} and {DiscreteProp.__name__} \" ) return d @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"No Actions \"\"\" return None def apply_action ( self , action , observation ): \"\"\"No Actions \"\"\" return None get_validator : Type [ corl . glues . common . observe_sensor . ObserveSensorValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.common.observe_sensor.ObserveSensorValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs Fields \u00a4 Fields in this glue Source code in corl/glues/common/observe_sensor.py class Fields : \"\"\" Fields in this glue \"\"\" DIRECT_OBSERVATION = \"direct_observation\" action_space ( self ) \u00a4 No Actions Source code in corl/glues/common/observe_sensor.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"No Actions \"\"\" return None apply_action ( self , action , observation ) \u00a4 No Actions Source code in corl/glues/common/observe_sensor.py def apply_action ( self , action , observation ): \"\"\"No Actions \"\"\" return None get_observation ( self ) \u00a4 Observation Values Source code in corl/glues/common/observe_sensor.py def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" d = OrderedDict () if isinstance ( self . _sensor . measurement_properties , BoxProp ): sensed_value = self . _sensor . get_measurement () if isinstance ( sensed_value , np . ndarray ): sensed_value = sensed_value . tolist () else : sensed_value = list ( sensed_value ) for indx , value in enumerate ( sensed_value ): if isinstance ( value , abc . Sequence ): # list of lists 2D case for indx1 , value1 in enumerate ( value ): in_unit2d = self . _sensor . measurement_properties . unit [ indx ][ indx1 ] assert isinstance ( in_unit2d , str ) assert isinstance ( self . out_units , abc . Sequence ) out_unit1 = self . out_units [ indx ] assert isinstance ( out_unit1 , abc . Sequence ) out_unit2d = out_unit1 [ indx1 ] assert isinstance ( out_unit2d , enum . Enum ) assert isinstance ( sensed_value , abc . Sequence ) sensed_value1 = sensed_value [ indx ] assert isinstance ( sensed_value1 , list ) sensed_value1 [ indx1 ] = Convert ( value1 , in_unit2d , out_unit2d ) else : # 1D case in_unit = self . _sensor . measurement_properties . unit [ indx ] assert isinstance ( in_unit , str ) out_unit = self . out_units [ indx ] assert isinstance ( out_unit , enum . Enum ) assert isinstance ( sensed_value , list ) sensed_value [ indx ] = Convert ( value , in_unit , out_unit ) d [ self . Fields . DIRECT_OBSERVATION ] = np . array ( sensed_value , dtype = np . float32 ) elif isinstance ( self . _sensor . measurement_properties , ( DiscreteProp )): sensed_value_discrete : float = self . _sensor . get_measurement ()[ 0 ] d [ self . Fields . DIRECT_OBSERVATION ] = np . array ( sensed_value_discrete , dtype = np . int32 ) elif isinstance ( self . _sensor . measurement_properties , ( MultiBinary )): sensed_value_multi_binary = self . _sensor . get_measurement () d [ self . Fields . DIRECT_OBSERVATION ] = sensed_value_multi_binary # type: ignore else : raise TypeError ( \"Only supports {BoxProp.__name__} , {MultiBinary.__name__} and {DiscreteProp.__name__} \" ) return d get_unique_name ( self ) \u00a4 Class method that retreives the unique name for the glue instance Source code in corl/glues/common/observe_sensor.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" # TODO: This may result in ObserveSensor_Sensor_X, which is kinda ugly return \"ObserveSensor_\" + self . _sensor_name invalid_value ( self ) \u00a4 Return zeros when invalid Source code in corl/glues/common/observe_sensor.py def invalid_value ( self ) -> OrderedDict : \"\"\"Return zeros when invalid \"\"\" arr : typing . List [ float ] = [] if isinstance ( self . _sensor . measurement_properties . low , abc . Sequence ): # type: ignore for _ in enumerate ( self . _sensor . measurement_properties . low ): # type: ignore arr . append ( 0.0 ) elif np . isscalar ( self . _sensor . measurement_properties . low ): # type: ignore arr . append ( 0.0 ) else : raise RuntimeError ( \"Expecting either array or scalar\" ) d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = np . asarray ( arr ) return d observation_space ( self ) \u00a4 Observation Space Source code in corl/glues/common/observe_sensor.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () if isinstance ( self . _sensor . measurement_properties , BoxProp ): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_converted_space ( self . out_units ) elif isinstance ( self . _sensor . measurement_properties , ( DiscreteProp )): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_space () elif isinstance ( self . _sensor . measurement_properties , ( MultiBinary )): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_space () else : raise TypeError ( \"Only supports {BoxProp.__name__} , {MultiBinary.__name__} and {DiscreteProp.__name__} \" ) return d observation_units ( self ) \u00a4 Units of the sensors in this glue Source code in corl/glues/common/observe_sensor.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" out_units = [] for value in self . out_units : if isinstance ( value , abc . Sequence ): # list of lists 2D case sub_units = [] for val in value : sub_units . append ( GetStrFromUnit ( val )) out_units . append ( sub_units ) else : # 1D case out_units . append ( GetStrFromUnit ( value )) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = out_units return d ObserveSensorValidator ( BaseAgentPlatformGlueValidator ) pydantic-model \u00a4 output_units: unit to convert the output data to sensor: which sensor to find on the platform Source code in corl/glues/common/observe_sensor.py class ObserveSensorValidator ( BaseAgentPlatformGlueValidator ): \"\"\" output_units: unit to convert the output data to sensor: which sensor to find on the platform \"\"\" sensor : str output_units : typing . Union [ typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] = [] @validator ( 'output_units' , always = True , pre = True ) def validate_output_units ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"output_units validator\"\"\" units = [] sensor_obj = get_sensor_by_name ( values [ 'platform' ], values [ 'sensor' ]) if not v : # Configuration does not provide output units if hasattr ( sensor_obj . measurement_properties , 'unit' ): # Sensor measurement properties has unit attribute, get the default unit for value in sensor_obj . measurement_properties . unit : if not isinstance ( value , str ) and isinstance ( value , abc . Sequence ): # list of lists 2D case units . append ([ GetUnitFromStr ( val ) . DEFAULT for val in value ]) else : # 1D case units . append ( GetUnitFromStr ( value ) . DEFAULT ) else : # Sensor measurement properties does not have unit attribute, use NoneUnitType.DEFAULT units . append ( NoneUnitType . DEFAULT ) else : # Configuration provides output units, convert string units to enum.Enum units # Assert that sensor has units assert hasattr ( sensor_obj . measurement_properties , 'unit' ), \"Unexpected output_units when sensor does not have unit attribute\" if isinstance ( v , str ): # Configuration units are a single string assert isinstance ( v , str ), \"Unexpected input output_units type\" assert len ( sensor_obj . measurement_properties . unit ) == 1 , \"Unexpected length of output_units\" units . append ( GetUnitFromStr ( v )) elif isinstance ( v , abc . Sequence ): # Configuration units are a list assert len ( v ) == len ( sensor_obj . measurement_properties . unit ), \"Unexpected length of output_units\" for i , value in enumerate ( v ): if not isinstance ( value , str ) and isinstance ( value , abc . Sequence ): # list of lists 2D case assert len ( value ) == len ( sensor_obj . measurement_properties . unit [ i ]), \"Unexpected length of output_units\" sub_units = [] for j , val in enumerate ( value ): assert isinstance ( val , str ), \"Unexpected input output_units type\" assert isinstance ( GetUnitFromStr ( sensor_obj . measurement_properties . unit [ i ][ j ]), type ( GetUnitFromStr ( val ))), \"Unexpected unit dimension\" sub_units . append ( GetUnitFromStr ( val )) units . append ( sub_units ) else : # 1D case assert isinstance ( value , str ), \"Unexpected input output_units type\" assert isinstance ( GetUnitFromStr ( sensor_obj . measurement_properties . unit [ i ]), type ( GetUnitFromStr ( value ))), \"Unexpected unit dimension\" units . append ( GetUnitFromStr ( value )) else : raise TypeError ( f 'Unexpected_input output_units type: { type ( v ) . __name__ } ' ) return units validate_output_units ( v , values ) classmethod \u00a4 output_units validator Source code in corl/glues/common/observe_sensor.py @validator ( 'output_units' , always = True , pre = True ) def validate_output_units ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"output_units validator\"\"\" units = [] sensor_obj = get_sensor_by_name ( values [ 'platform' ], values [ 'sensor' ]) if not v : # Configuration does not provide output units if hasattr ( sensor_obj . measurement_properties , 'unit' ): # Sensor measurement properties has unit attribute, get the default unit for value in sensor_obj . measurement_properties . unit : if not isinstance ( value , str ) and isinstance ( value , abc . Sequence ): # list of lists 2D case units . append ([ GetUnitFromStr ( val ) . DEFAULT for val in value ]) else : # 1D case units . append ( GetUnitFromStr ( value ) . DEFAULT ) else : # Sensor measurement properties does not have unit attribute, use NoneUnitType.DEFAULT units . append ( NoneUnitType . DEFAULT ) else : # Configuration provides output units, convert string units to enum.Enum units # Assert that sensor has units assert hasattr ( sensor_obj . measurement_properties , 'unit' ), \"Unexpected output_units when sensor does not have unit attribute\" if isinstance ( v , str ): # Configuration units are a single string assert isinstance ( v , str ), \"Unexpected input output_units type\" assert len ( sensor_obj . measurement_properties . unit ) == 1 , \"Unexpected length of output_units\" units . append ( GetUnitFromStr ( v )) elif isinstance ( v , abc . Sequence ): # Configuration units are a list assert len ( v ) == len ( sensor_obj . measurement_properties . unit ), \"Unexpected length of output_units\" for i , value in enumerate ( v ): if not isinstance ( value , str ) and isinstance ( value , abc . Sequence ): # list of lists 2D case assert len ( value ) == len ( sensor_obj . measurement_properties . unit [ i ]), \"Unexpected length of output_units\" sub_units = [] for j , val in enumerate ( value ): assert isinstance ( val , str ), \"Unexpected input output_units type\" assert isinstance ( GetUnitFromStr ( sensor_obj . measurement_properties . unit [ i ][ j ]), type ( GetUnitFromStr ( val ))), \"Unexpected unit dimension\" sub_units . append ( GetUnitFromStr ( val )) units . append ( sub_units ) else : # 1D case assert isinstance ( value , str ), \"Unexpected input output_units type\" assert isinstance ( GetUnitFromStr ( sensor_obj . measurement_properties . unit [ i ]), type ( GetUnitFromStr ( value ))), \"Unexpected unit dimension\" units . append ( GetUnitFromStr ( value )) else : raise TypeError ( f 'Unexpected_input output_units type: { type ( v ) . __name__ } ' ) return units","title":"Observe sensor"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor","text":"Glue to observe a sensor Configuration string String of sensor class, sensor base name, or the PluginLibrary group name For example consider a platform which has \"SimOrientationRateSensor\" attached . The following will all be equivalent : - SimOrientationRateSensor - BaseOrientationRateSensor - Sensor_OrientationRate Important : If multiple sensors are found an array will be returned ( ex . Gload Learjet ) This will also be the name attached to the glue , which can then be used Source code in corl/glues/common/observe_sensor.py class ObserveSensor ( BaseAgentPlatformGlue ): \"\"\"Glue to observe a sensor Configuration: sensor: string String of sensor class, sensor base name, or the PluginLibrary group name For example consider a platform which has \"SimOrientationRateSensor\" attached. The following will all be equivalent: - SimOrientationRateSensor - BaseOrientationRateSensor - Sensor_OrientationRate Important: If multiple sensors are found an array will be returned (ex. Gload Learjet) This will also be the name attached to the glue, which can then be used \"\"\" # pylint: disable=too-few-public-methods class Fields : \"\"\" Fields in this glue \"\"\" DIRECT_OBSERVATION = \"direct_observation\" @property def get_validator ( self ) -> typing . Type [ ObserveSensorValidator ]: return ObserveSensorValidator def __init__ ( self , ** kwargs ) -> None : self . config : ObserveSensorValidator super () . __init__ ( ** kwargs ) self . _sensor = get_sensor_by_name ( self . _platform , self . config . sensor ) self . _sensor_name : str = self . config . sensor self . out_units = self . config . output_units @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" # TODO: This may result in ObserveSensor_Sensor_X, which is kinda ugly return \"ObserveSensor_\" + self . _sensor_name # TODO: broken def invalid_value ( self ) -> OrderedDict : \"\"\"Return zeros when invalid \"\"\" arr : typing . List [ float ] = [] if isinstance ( self . _sensor . measurement_properties . low , abc . Sequence ): # type: ignore for _ in enumerate ( self . _sensor . measurement_properties . low ): # type: ignore arr . append ( 0.0 ) elif np . isscalar ( self . _sensor . measurement_properties . low ): # type: ignore arr . append ( 0.0 ) else : raise RuntimeError ( \"Expecting either array or scalar\" ) d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = np . asarray ( arr ) return d @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" out_units = [] for value in self . out_units : if isinstance ( value , abc . Sequence ): # list of lists 2D case sub_units = [] for val in value : sub_units . append ( GetStrFromUnit ( val )) out_units . append ( sub_units ) else : # 1D case out_units . append ( GetStrFromUnit ( value )) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = out_units return d @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () if isinstance ( self . _sensor . measurement_properties , BoxProp ): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_converted_space ( self . out_units ) elif isinstance ( self . _sensor . measurement_properties , ( DiscreteProp )): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_space () elif isinstance ( self . _sensor . measurement_properties , ( MultiBinary )): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_space () else : raise TypeError ( \"Only supports {BoxProp.__name__} , {MultiBinary.__name__} and {DiscreteProp.__name__} \" ) return d def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" d = OrderedDict () if isinstance ( self . _sensor . measurement_properties , BoxProp ): sensed_value = self . _sensor . get_measurement () if isinstance ( sensed_value , np . ndarray ): sensed_value = sensed_value . tolist () else : sensed_value = list ( sensed_value ) for indx , value in enumerate ( sensed_value ): if isinstance ( value , abc . Sequence ): # list of lists 2D case for indx1 , value1 in enumerate ( value ): in_unit2d = self . _sensor . measurement_properties . unit [ indx ][ indx1 ] assert isinstance ( in_unit2d , str ) assert isinstance ( self . out_units , abc . Sequence ) out_unit1 = self . out_units [ indx ] assert isinstance ( out_unit1 , abc . Sequence ) out_unit2d = out_unit1 [ indx1 ] assert isinstance ( out_unit2d , enum . Enum ) assert isinstance ( sensed_value , abc . Sequence ) sensed_value1 = sensed_value [ indx ] assert isinstance ( sensed_value1 , list ) sensed_value1 [ indx1 ] = Convert ( value1 , in_unit2d , out_unit2d ) else : # 1D case in_unit = self . _sensor . measurement_properties . unit [ indx ] assert isinstance ( in_unit , str ) out_unit = self . out_units [ indx ] assert isinstance ( out_unit , enum . Enum ) assert isinstance ( sensed_value , list ) sensed_value [ indx ] = Convert ( value , in_unit , out_unit ) d [ self . Fields . DIRECT_OBSERVATION ] = np . array ( sensed_value , dtype = np . float32 ) elif isinstance ( self . _sensor . measurement_properties , ( DiscreteProp )): sensed_value_discrete : float = self . _sensor . get_measurement ()[ 0 ] d [ self . Fields . DIRECT_OBSERVATION ] = np . array ( sensed_value_discrete , dtype = np . int32 ) elif isinstance ( self . _sensor . measurement_properties , ( MultiBinary )): sensed_value_multi_binary = self . _sensor . get_measurement () d [ self . Fields . DIRECT_OBSERVATION ] = sensed_value_multi_binary # type: ignore else : raise TypeError ( \"Only supports {BoxProp.__name__} , {MultiBinary.__name__} and {DiscreteProp.__name__} \" ) return d @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"No Actions \"\"\" return None def apply_action ( self , action , observation ): \"\"\"No Actions \"\"\" return None","title":"ObserveSensor"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.common.observe_sensor.ObserveSensorValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.Fields","text":"Fields in this glue Source code in corl/glues/common/observe_sensor.py class Fields : \"\"\" Fields in this glue \"\"\" DIRECT_OBSERVATION = \"direct_observation\"","title":"Fields"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.action_space","text":"No Actions Source code in corl/glues/common/observe_sensor.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"No Actions \"\"\" return None","title":"action_space()"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.apply_action","text":"No Actions Source code in corl/glues/common/observe_sensor.py def apply_action ( self , action , observation ): \"\"\"No Actions \"\"\" return None","title":"apply_action()"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.get_observation","text":"Observation Values Source code in corl/glues/common/observe_sensor.py def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" d = OrderedDict () if isinstance ( self . _sensor . measurement_properties , BoxProp ): sensed_value = self . _sensor . get_measurement () if isinstance ( sensed_value , np . ndarray ): sensed_value = sensed_value . tolist () else : sensed_value = list ( sensed_value ) for indx , value in enumerate ( sensed_value ): if isinstance ( value , abc . Sequence ): # list of lists 2D case for indx1 , value1 in enumerate ( value ): in_unit2d = self . _sensor . measurement_properties . unit [ indx ][ indx1 ] assert isinstance ( in_unit2d , str ) assert isinstance ( self . out_units , abc . Sequence ) out_unit1 = self . out_units [ indx ] assert isinstance ( out_unit1 , abc . Sequence ) out_unit2d = out_unit1 [ indx1 ] assert isinstance ( out_unit2d , enum . Enum ) assert isinstance ( sensed_value , abc . Sequence ) sensed_value1 = sensed_value [ indx ] assert isinstance ( sensed_value1 , list ) sensed_value1 [ indx1 ] = Convert ( value1 , in_unit2d , out_unit2d ) else : # 1D case in_unit = self . _sensor . measurement_properties . unit [ indx ] assert isinstance ( in_unit , str ) out_unit = self . out_units [ indx ] assert isinstance ( out_unit , enum . Enum ) assert isinstance ( sensed_value , list ) sensed_value [ indx ] = Convert ( value , in_unit , out_unit ) d [ self . Fields . DIRECT_OBSERVATION ] = np . array ( sensed_value , dtype = np . float32 ) elif isinstance ( self . _sensor . measurement_properties , ( DiscreteProp )): sensed_value_discrete : float = self . _sensor . get_measurement ()[ 0 ] d [ self . Fields . DIRECT_OBSERVATION ] = np . array ( sensed_value_discrete , dtype = np . int32 ) elif isinstance ( self . _sensor . measurement_properties , ( MultiBinary )): sensed_value_multi_binary = self . _sensor . get_measurement () d [ self . Fields . DIRECT_OBSERVATION ] = sensed_value_multi_binary # type: ignore else : raise TypeError ( \"Only supports {BoxProp.__name__} , {MultiBinary.__name__} and {DiscreteProp.__name__} \" ) return d","title":"get_observation()"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.get_unique_name","text":"Class method that retreives the unique name for the glue instance Source code in corl/glues/common/observe_sensor.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" # TODO: This may result in ObserveSensor_Sensor_X, which is kinda ugly return \"ObserveSensor_\" + self . _sensor_name","title":"get_unique_name()"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.invalid_value","text":"Return zeros when invalid Source code in corl/glues/common/observe_sensor.py def invalid_value ( self ) -> OrderedDict : \"\"\"Return zeros when invalid \"\"\" arr : typing . List [ float ] = [] if isinstance ( self . _sensor . measurement_properties . low , abc . Sequence ): # type: ignore for _ in enumerate ( self . _sensor . measurement_properties . low ): # type: ignore arr . append ( 0.0 ) elif np . isscalar ( self . _sensor . measurement_properties . low ): # type: ignore arr . append ( 0.0 ) else : raise RuntimeError ( \"Expecting either array or scalar\" ) d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = np . asarray ( arr ) return d","title":"invalid_value()"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.observation_space","text":"Observation Space Source code in corl/glues/common/observe_sensor.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () if isinstance ( self . _sensor . measurement_properties , BoxProp ): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_converted_space ( self . out_units ) elif isinstance ( self . _sensor . measurement_properties , ( DiscreteProp )): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_space () elif isinstance ( self . _sensor . measurement_properties , ( MultiBinary )): d . spaces [ self . Fields . DIRECT_OBSERVATION ] = self . _sensor . measurement_properties . create_space () else : raise TypeError ( \"Only supports {BoxProp.__name__} , {MultiBinary.__name__} and {DiscreteProp.__name__} \" ) return d","title":"observation_space()"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensor.observation_units","text":"Units of the sensors in this glue Source code in corl/glues/common/observe_sensor.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" out_units = [] for value in self . out_units : if isinstance ( value , abc . Sequence ): # list of lists 2D case sub_units = [] for val in value : sub_units . append ( GetStrFromUnit ( val )) out_units . append ( sub_units ) else : # 1D case out_units . append ( GetStrFromUnit ( value )) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = out_units return d","title":"observation_units()"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensorValidator","text":"output_units: unit to convert the output data to sensor: which sensor to find on the platform Source code in corl/glues/common/observe_sensor.py class ObserveSensorValidator ( BaseAgentPlatformGlueValidator ): \"\"\" output_units: unit to convert the output data to sensor: which sensor to find on the platform \"\"\" sensor : str output_units : typing . Union [ typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] = [] @validator ( 'output_units' , always = True , pre = True ) def validate_output_units ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"output_units validator\"\"\" units = [] sensor_obj = get_sensor_by_name ( values [ 'platform' ], values [ 'sensor' ]) if not v : # Configuration does not provide output units if hasattr ( sensor_obj . measurement_properties , 'unit' ): # Sensor measurement properties has unit attribute, get the default unit for value in sensor_obj . measurement_properties . unit : if not isinstance ( value , str ) and isinstance ( value , abc . Sequence ): # list of lists 2D case units . append ([ GetUnitFromStr ( val ) . DEFAULT for val in value ]) else : # 1D case units . append ( GetUnitFromStr ( value ) . DEFAULT ) else : # Sensor measurement properties does not have unit attribute, use NoneUnitType.DEFAULT units . append ( NoneUnitType . DEFAULT ) else : # Configuration provides output units, convert string units to enum.Enum units # Assert that sensor has units assert hasattr ( sensor_obj . measurement_properties , 'unit' ), \"Unexpected output_units when sensor does not have unit attribute\" if isinstance ( v , str ): # Configuration units are a single string assert isinstance ( v , str ), \"Unexpected input output_units type\" assert len ( sensor_obj . measurement_properties . unit ) == 1 , \"Unexpected length of output_units\" units . append ( GetUnitFromStr ( v )) elif isinstance ( v , abc . Sequence ): # Configuration units are a list assert len ( v ) == len ( sensor_obj . measurement_properties . unit ), \"Unexpected length of output_units\" for i , value in enumerate ( v ): if not isinstance ( value , str ) and isinstance ( value , abc . Sequence ): # list of lists 2D case assert len ( value ) == len ( sensor_obj . measurement_properties . unit [ i ]), \"Unexpected length of output_units\" sub_units = [] for j , val in enumerate ( value ): assert isinstance ( val , str ), \"Unexpected input output_units type\" assert isinstance ( GetUnitFromStr ( sensor_obj . measurement_properties . unit [ i ][ j ]), type ( GetUnitFromStr ( val ))), \"Unexpected unit dimension\" sub_units . append ( GetUnitFromStr ( val )) units . append ( sub_units ) else : # 1D case assert isinstance ( value , str ), \"Unexpected input output_units type\" assert isinstance ( GetUnitFromStr ( sensor_obj . measurement_properties . unit [ i ]), type ( GetUnitFromStr ( value ))), \"Unexpected unit dimension\" units . append ( GetUnitFromStr ( value )) else : raise TypeError ( f 'Unexpected_input output_units type: { type ( v ) . __name__ } ' ) return units","title":"ObserveSensorValidator"},{"location":"reference/glues/common/observe_sensor/#corl.glues.common.observe_sensor.ObserveSensorValidator.validate_output_units","text":"output_units validator Source code in corl/glues/common/observe_sensor.py @validator ( 'output_units' , always = True , pre = True ) def validate_output_units ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"output_units validator\"\"\" units = [] sensor_obj = get_sensor_by_name ( values [ 'platform' ], values [ 'sensor' ]) if not v : # Configuration does not provide output units if hasattr ( sensor_obj . measurement_properties , 'unit' ): # Sensor measurement properties has unit attribute, get the default unit for value in sensor_obj . measurement_properties . unit : if not isinstance ( value , str ) and isinstance ( value , abc . Sequence ): # list of lists 2D case units . append ([ GetUnitFromStr ( val ) . DEFAULT for val in value ]) else : # 1D case units . append ( GetUnitFromStr ( value ) . DEFAULT ) else : # Sensor measurement properties does not have unit attribute, use NoneUnitType.DEFAULT units . append ( NoneUnitType . DEFAULT ) else : # Configuration provides output units, convert string units to enum.Enum units # Assert that sensor has units assert hasattr ( sensor_obj . measurement_properties , 'unit' ), \"Unexpected output_units when sensor does not have unit attribute\" if isinstance ( v , str ): # Configuration units are a single string assert isinstance ( v , str ), \"Unexpected input output_units type\" assert len ( sensor_obj . measurement_properties . unit ) == 1 , \"Unexpected length of output_units\" units . append ( GetUnitFromStr ( v )) elif isinstance ( v , abc . Sequence ): # Configuration units are a list assert len ( v ) == len ( sensor_obj . measurement_properties . unit ), \"Unexpected length of output_units\" for i , value in enumerate ( v ): if not isinstance ( value , str ) and isinstance ( value , abc . Sequence ): # list of lists 2D case assert len ( value ) == len ( sensor_obj . measurement_properties . unit [ i ]), \"Unexpected length of output_units\" sub_units = [] for j , val in enumerate ( value ): assert isinstance ( val , str ), \"Unexpected input output_units type\" assert isinstance ( GetUnitFromStr ( sensor_obj . measurement_properties . unit [ i ][ j ]), type ( GetUnitFromStr ( val ))), \"Unexpected unit dimension\" sub_units . append ( GetUnitFromStr ( val )) units . append ( sub_units ) else : # 1D case assert isinstance ( value , str ), \"Unexpected input output_units type\" assert isinstance ( GetUnitFromStr ( sensor_obj . measurement_properties . unit [ i ]), type ( GetUnitFromStr ( value ))), \"Unexpected unit dimension\" units . append ( GetUnitFromStr ( value )) else : raise TypeError ( f 'Unexpected_input output_units type: { type ( v ) . __name__ } ' ) return units","title":"validate_output_units()"},{"location":"reference/glues/common/observe_sensor_repeated/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. ObserveSensorRepeated Glue ObserveSensorRepeated ( BaseAgentPlatformGlue ) \u00a4 Glue to observe a sensor that can offer a variable length output Configuration string String of sensor class, sensor base name, or the PluginLibrary group name For example consider a platform which has \"SimOrientationRateSensor\" attached . The following will all be equivalent : - SimOrientationRateSensor - BaseOrientationRateSensor - Sensor_OrientationRate Important : If multiple sensors are found an array will be returned ( ex . Gload Learjet ) This will also be the name attached to the glue , which can then be used Source code in corl/glues/common/observe_sensor_repeated.py class ObserveSensorRepeated ( BaseAgentPlatformGlue ): \"\"\"Glue to observe a sensor that can offer a variable length output Configuration: sensor: string String of sensor class, sensor base name, or the PluginLibrary group name For example consider a platform which has \"SimOrientationRateSensor\" attached. The following will all be equivalent: - SimOrientationRateSensor - BaseOrientationRateSensor - Sensor_OrientationRate Important: If multiple sensors are found an array will be returned (ex. Gload Learjet) This will also be the name attached to the glue, which can then be used \"\"\" # pylint: disable=too-few-public-methods class Fields : \"\"\" Fields in this glue \"\"\" DIRECT_OBSERVATION = \"direct_observation\" @property def get_validator ( self ) -> typing . Type [ ObserveSensorRepeatedValidator ]: return ObserveSensorRepeatedValidator # TODO: self._sensor.measurement_properties assumes child_space attribute def __init__ ( self , ** kwargs ) -> None : self . config : ObserveSensorRepeatedValidator super () . __init__ ( ** kwargs ) self . _sensor = get_sensor_by_name ( self . _platform , self . config . sensor ) self . _sensor_name : str = self . config . sensor self . out_units = self . config . output_units self . max_len = self . config . max_len @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" # TODO: This may result in ObserveSensor_Sensor_X, which is kinda ugly return \"ObserveSensorRepeated_\" + self . _sensor_name def invalid_value ( self ) -> OrderedDict : \"\"\"Return zeros when invalid \"\"\" arr : typing . List [ float ] = [] d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = arr return d @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" out_dict = {} for field_name , field_unit in self . out_units . items (): out_dict [ field_name ] = GetStrFromUnit ( field_unit ) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = out_dict return d # TODO: Assumes self._sensor.measurement_properties has attribute child_space @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () child_space = gym . spaces . dict . Dict () assert isinstance ( self . _sensor . measurement_properties , RepeatedProp ), \"Unexpected measurement_properties type\" child_space_measure = self . _sensor . measurement_properties . child_space for field_name , field_meas in child_space_measure . items (): if not isinstance ( field_meas , ( DiscreteProp , MultiBinary )): assert isinstance ( field_meas , BoxProp ), \"Unexpected field_meas type\" if field_name not in self . out_units : child_space . spaces [ field_name ] = field_meas . create_space () elif isinstance ( self . out_units [ field_name ], list ): units = self . out_units [ field_name ] assert isinstance ( units , list ), \"Unexpected units type\" child_space . spaces [ field_name ] = field_meas . create_converted_space ( units ) else : child_space . spaces [ field_name ] = field_meas . create_converted_space ([ self . out_units [ field_name ]] * len ( field_meas . low )) else : child_space . spaces [ field_name ] = field_meas . create_space () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = Repeated ( child_space = child_space , max_len = self . config . max_len , ) return d # TODO: Assumes self._sensor.measurement_properties has child_space attribute def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" # NOTE: do not attempt to optimize this function by modifying data in place # you will mess up all other glues that call self._glue.get_observation # a copy is required here sensed_value = self . _sensor . get_measurement () tmp_sensed : typing . List [ typing . Dict [ str , typing . Any ]] = [] append = tmp_sensed . append assert isinstance ( self . _sensor . measurement_properties , RepeatedProp ), \"Unexpected measurement_properties type\" m_props = self . _sensor . measurement_properties . child_space out_units = self . out_units obs_child_space = self . observation_space ()[ self . Fields . DIRECT_OBSERVATION ] . child_space for platform_data in sensed_value : tmp_row = {} for field_name , obs in platform_data . items (): tmp_row [ field_name ] = obs if field_name in out_units : prop = m_props [ field_name ] if isinstance ( prop , BoxProp ): old_unit = prop . unit [ 0 ] assert isinstance ( old_unit , str ) unit_class = GetUnitFromStr ( old_unit ) if unit_class != NoneUnitType . NoneUnit : tmp_row [ field_name ] = Convert ( obs , old_unit , out_units [ field_name ]) if self . config . enable_clip : field_space = obs_child_space . spaces [ field_name ] if isinstance ( field_space , gym . spaces . Box ): tmp_row [ field_name ] = np . clip ( obs , field_space . low , field_space . high ) append ( tmp_row ) # we are going to clip the list to the max_len, so the obs is happy if len ( tmp_sensed ) == self . max_len : break d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = tmp_sensed return d @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"No Actions \"\"\" return None def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ): \"\"\"No Actions \"\"\" return None get_validator : Type [ corl . glues . common . observe_sensor_repeated . ObserveSensorRepeatedValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.common.observe_sensor_repeated.ObserveSensorRepeatedValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs Fields \u00a4 Fields in this glue Source code in corl/glues/common/observe_sensor_repeated.py class Fields : \"\"\" Fields in this glue \"\"\" DIRECT_OBSERVATION = \"direct_observation\" action_space ( self ) \u00a4 No Actions Source code in corl/glues/common/observe_sensor_repeated.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"No Actions \"\"\" return None apply_action ( self , action , observation ) \u00a4 No Actions Source code in corl/glues/common/observe_sensor_repeated.py def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ): \"\"\"No Actions \"\"\" return None get_observation ( self ) \u00a4 Observation Values Source code in corl/glues/common/observe_sensor_repeated.py def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" # NOTE: do not attempt to optimize this function by modifying data in place # you will mess up all other glues that call self._glue.get_observation # a copy is required here sensed_value = self . _sensor . get_measurement () tmp_sensed : typing . List [ typing . Dict [ str , typing . Any ]] = [] append = tmp_sensed . append assert isinstance ( self . _sensor . measurement_properties , RepeatedProp ), \"Unexpected measurement_properties type\" m_props = self . _sensor . measurement_properties . child_space out_units = self . out_units obs_child_space = self . observation_space ()[ self . Fields . DIRECT_OBSERVATION ] . child_space for platform_data in sensed_value : tmp_row = {} for field_name , obs in platform_data . items (): tmp_row [ field_name ] = obs if field_name in out_units : prop = m_props [ field_name ] if isinstance ( prop , BoxProp ): old_unit = prop . unit [ 0 ] assert isinstance ( old_unit , str ) unit_class = GetUnitFromStr ( old_unit ) if unit_class != NoneUnitType . NoneUnit : tmp_row [ field_name ] = Convert ( obs , old_unit , out_units [ field_name ]) if self . config . enable_clip : field_space = obs_child_space . spaces [ field_name ] if isinstance ( field_space , gym . spaces . Box ): tmp_row [ field_name ] = np . clip ( obs , field_space . low , field_space . high ) append ( tmp_row ) # we are going to clip the list to the max_len, so the obs is happy if len ( tmp_sensed ) == self . max_len : break d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = tmp_sensed return d get_unique_name ( self ) \u00a4 Class method that retreives the unique name for the glue instance Source code in corl/glues/common/observe_sensor_repeated.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" # TODO: This may result in ObserveSensor_Sensor_X, which is kinda ugly return \"ObserveSensorRepeated_\" + self . _sensor_name invalid_value ( self ) \u00a4 Return zeros when invalid Source code in corl/glues/common/observe_sensor_repeated.py def invalid_value ( self ) -> OrderedDict : \"\"\"Return zeros when invalid \"\"\" arr : typing . List [ float ] = [] d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = arr return d observation_space ( self ) \u00a4 Observation Space Source code in corl/glues/common/observe_sensor_repeated.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () child_space = gym . spaces . dict . Dict () assert isinstance ( self . _sensor . measurement_properties , RepeatedProp ), \"Unexpected measurement_properties type\" child_space_measure = self . _sensor . measurement_properties . child_space for field_name , field_meas in child_space_measure . items (): if not isinstance ( field_meas , ( DiscreteProp , MultiBinary )): assert isinstance ( field_meas , BoxProp ), \"Unexpected field_meas type\" if field_name not in self . out_units : child_space . spaces [ field_name ] = field_meas . create_space () elif isinstance ( self . out_units [ field_name ], list ): units = self . out_units [ field_name ] assert isinstance ( units , list ), \"Unexpected units type\" child_space . spaces [ field_name ] = field_meas . create_converted_space ( units ) else : child_space . spaces [ field_name ] = field_meas . create_converted_space ([ self . out_units [ field_name ]] * len ( field_meas . low )) else : child_space . spaces [ field_name ] = field_meas . create_space () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = Repeated ( child_space = child_space , max_len = self . config . max_len , ) return d observation_units ( self ) \u00a4 Units of the sensors in this glue Source code in corl/glues/common/observe_sensor_repeated.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" out_dict = {} for field_name , field_unit in self . out_units . items (): out_dict [ field_name ] = GetStrFromUnit ( field_unit ) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = out_dict return d ObserveSensorRepeatedValidator ( BaseAgentPlatformGlueValidator ) pydantic-model \u00a4 sensor: which sensor to find on the platform output_units: unit to convert the output data to max_len: the maximum length to allow the observation space to reach enable_clip: enables clipping for spaces that support cliping Source code in corl/glues/common/observe_sensor_repeated.py class ObserveSensorRepeatedValidator ( BaseAgentPlatformGlueValidator ): \"\"\" sensor: which sensor to find on the platform output_units: unit to convert the output data to max_len: the maximum length to allow the observation space to reach enable_clip: enables clipping for spaces that support cliping \"\"\" sensor : str output_units : typing . Dict [ str , enum . Enum ] = {} max_len : int = 10 enable_clip : bool = False @validator ( 'output_units' , always = True , pre = True ) def validate_output_units ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"output_units validator\"\"\" sensor_obj = get_sensor_by_name ( values [ 'platform' ], values [ 'sensor' ]) child_space = sensor_obj . measurement_properties . child_space units = {} if v is not None : # Configuration provides output units, convert string units to enum.Enum units # Assert that input configuration is a dict assert isinstance ( v , dict ), \"Configuration output_units must be a dict\" for field_name , value in v . items (): if field_name not in child_space : raise RuntimeError ( f \"output units were provided for repeated field { field_name } , but \" f \"that field is not in this repeated space which has keys { child_space . keys () } \" ) prop = child_space [ field_name ] if not isinstance ( prop , ( DiscreteProp , MultiBinary )): assert isinstance ( prop , BoxProp ), \"Unexpected field_space type\" if np . isscalar ( prop . high ) or len ( prop . high ) == 1 : if value is not None and not isinstance ( value , str ): raise RuntimeError ( \"output units must be provided as a scalar\" ) else : raise RuntimeError ( \"repeated field space unit conversions with lists of output types not currently implemented\" ) assert isinstance ( GetUnitFromStr ( prop . unit [ 0 ]), type ( GetUnitFromStr ( value ))), \"Unexpected unit dimension\" units [ field_name ] = GetUnitFromStr ( value ) # Fill in any output units that were not provided with values from the sensor for field_name , prop in child_space . items (): if field_name in units : # This field name already provided so skip continue if hasattr ( prop , 'unit' ): # Sensor measurement properties has unit attribute, get the default unit if isinstance ( prop . unit , str ): units [ field_name ] = GetUnitFromStr ( prop . unit ) . DEFAULT elif isinstance ( prop . unit , list ): units [ field_name ] = GetUnitFromStr ( prop . unit [ 0 ]) . DEFAULT else : raise RuntimeError ( \"Unexpected property unit type\" ) else : # Sensor measurement properties does not have unit attribute, use NoneUnitType.DEFAULT units [ field_name ] = NoneUnitType . DEFAULT return units validate_output_units ( v , values ) classmethod \u00a4 output_units validator Source code in corl/glues/common/observe_sensor_repeated.py @validator ( 'output_units' , always = True , pre = True ) def validate_output_units ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"output_units validator\"\"\" sensor_obj = get_sensor_by_name ( values [ 'platform' ], values [ 'sensor' ]) child_space = sensor_obj . measurement_properties . child_space units = {} if v is not None : # Configuration provides output units, convert string units to enum.Enum units # Assert that input configuration is a dict assert isinstance ( v , dict ), \"Configuration output_units must be a dict\" for field_name , value in v . items (): if field_name not in child_space : raise RuntimeError ( f \"output units were provided for repeated field { field_name } , but \" f \"that field is not in this repeated space which has keys { child_space . keys () } \" ) prop = child_space [ field_name ] if not isinstance ( prop , ( DiscreteProp , MultiBinary )): assert isinstance ( prop , BoxProp ), \"Unexpected field_space type\" if np . isscalar ( prop . high ) or len ( prop . high ) == 1 : if value is not None and not isinstance ( value , str ): raise RuntimeError ( \"output units must be provided as a scalar\" ) else : raise RuntimeError ( \"repeated field space unit conversions with lists of output types not currently implemented\" ) assert isinstance ( GetUnitFromStr ( prop . unit [ 0 ]), type ( GetUnitFromStr ( value ))), \"Unexpected unit dimension\" units [ field_name ] = GetUnitFromStr ( value ) # Fill in any output units that were not provided with values from the sensor for field_name , prop in child_space . items (): if field_name in units : # This field name already provided so skip continue if hasattr ( prop , 'unit' ): # Sensor measurement properties has unit attribute, get the default unit if isinstance ( prop . unit , str ): units [ field_name ] = GetUnitFromStr ( prop . unit ) . DEFAULT elif isinstance ( prop . unit , list ): units [ field_name ] = GetUnitFromStr ( prop . unit [ 0 ]) . DEFAULT else : raise RuntimeError ( \"Unexpected property unit type\" ) else : # Sensor measurement properties does not have unit attribute, use NoneUnitType.DEFAULT units [ field_name ] = NoneUnitType . DEFAULT return units","title":"Observe sensor repeated"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated","text":"Glue to observe a sensor that can offer a variable length output Configuration string String of sensor class, sensor base name, or the PluginLibrary group name For example consider a platform which has \"SimOrientationRateSensor\" attached . The following will all be equivalent : - SimOrientationRateSensor - BaseOrientationRateSensor - Sensor_OrientationRate Important : If multiple sensors are found an array will be returned ( ex . Gload Learjet ) This will also be the name attached to the glue , which can then be used Source code in corl/glues/common/observe_sensor_repeated.py class ObserveSensorRepeated ( BaseAgentPlatformGlue ): \"\"\"Glue to observe a sensor that can offer a variable length output Configuration: sensor: string String of sensor class, sensor base name, or the PluginLibrary group name For example consider a platform which has \"SimOrientationRateSensor\" attached. The following will all be equivalent: - SimOrientationRateSensor - BaseOrientationRateSensor - Sensor_OrientationRate Important: If multiple sensors are found an array will be returned (ex. Gload Learjet) This will also be the name attached to the glue, which can then be used \"\"\" # pylint: disable=too-few-public-methods class Fields : \"\"\" Fields in this glue \"\"\" DIRECT_OBSERVATION = \"direct_observation\" @property def get_validator ( self ) -> typing . Type [ ObserveSensorRepeatedValidator ]: return ObserveSensorRepeatedValidator # TODO: self._sensor.measurement_properties assumes child_space attribute def __init__ ( self , ** kwargs ) -> None : self . config : ObserveSensorRepeatedValidator super () . __init__ ( ** kwargs ) self . _sensor = get_sensor_by_name ( self . _platform , self . config . sensor ) self . _sensor_name : str = self . config . sensor self . out_units = self . config . output_units self . max_len = self . config . max_len @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" # TODO: This may result in ObserveSensor_Sensor_X, which is kinda ugly return \"ObserveSensorRepeated_\" + self . _sensor_name def invalid_value ( self ) -> OrderedDict : \"\"\"Return zeros when invalid \"\"\" arr : typing . List [ float ] = [] d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = arr return d @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" out_dict = {} for field_name , field_unit in self . out_units . items (): out_dict [ field_name ] = GetStrFromUnit ( field_unit ) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = out_dict return d # TODO: Assumes self._sensor.measurement_properties has attribute child_space @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () child_space = gym . spaces . dict . Dict () assert isinstance ( self . _sensor . measurement_properties , RepeatedProp ), \"Unexpected measurement_properties type\" child_space_measure = self . _sensor . measurement_properties . child_space for field_name , field_meas in child_space_measure . items (): if not isinstance ( field_meas , ( DiscreteProp , MultiBinary )): assert isinstance ( field_meas , BoxProp ), \"Unexpected field_meas type\" if field_name not in self . out_units : child_space . spaces [ field_name ] = field_meas . create_space () elif isinstance ( self . out_units [ field_name ], list ): units = self . out_units [ field_name ] assert isinstance ( units , list ), \"Unexpected units type\" child_space . spaces [ field_name ] = field_meas . create_converted_space ( units ) else : child_space . spaces [ field_name ] = field_meas . create_converted_space ([ self . out_units [ field_name ]] * len ( field_meas . low )) else : child_space . spaces [ field_name ] = field_meas . create_space () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = Repeated ( child_space = child_space , max_len = self . config . max_len , ) return d # TODO: Assumes self._sensor.measurement_properties has child_space attribute def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" # NOTE: do not attempt to optimize this function by modifying data in place # you will mess up all other glues that call self._glue.get_observation # a copy is required here sensed_value = self . _sensor . get_measurement () tmp_sensed : typing . List [ typing . Dict [ str , typing . Any ]] = [] append = tmp_sensed . append assert isinstance ( self . _sensor . measurement_properties , RepeatedProp ), \"Unexpected measurement_properties type\" m_props = self . _sensor . measurement_properties . child_space out_units = self . out_units obs_child_space = self . observation_space ()[ self . Fields . DIRECT_OBSERVATION ] . child_space for platform_data in sensed_value : tmp_row = {} for field_name , obs in platform_data . items (): tmp_row [ field_name ] = obs if field_name in out_units : prop = m_props [ field_name ] if isinstance ( prop , BoxProp ): old_unit = prop . unit [ 0 ] assert isinstance ( old_unit , str ) unit_class = GetUnitFromStr ( old_unit ) if unit_class != NoneUnitType . NoneUnit : tmp_row [ field_name ] = Convert ( obs , old_unit , out_units [ field_name ]) if self . config . enable_clip : field_space = obs_child_space . spaces [ field_name ] if isinstance ( field_space , gym . spaces . Box ): tmp_row [ field_name ] = np . clip ( obs , field_space . low , field_space . high ) append ( tmp_row ) # we are going to clip the list to the max_len, so the obs is happy if len ( tmp_sensed ) == self . max_len : break d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = tmp_sensed return d @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"No Actions \"\"\" return None def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ): \"\"\"No Actions \"\"\" return None","title":"ObserveSensorRepeated"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.common.observe_sensor_repeated.ObserveSensorRepeatedValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.Fields","text":"Fields in this glue Source code in corl/glues/common/observe_sensor_repeated.py class Fields : \"\"\" Fields in this glue \"\"\" DIRECT_OBSERVATION = \"direct_observation\"","title":"Fields"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.action_space","text":"No Actions Source code in corl/glues/common/observe_sensor_repeated.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"No Actions \"\"\" return None","title":"action_space()"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.apply_action","text":"No Actions Source code in corl/glues/common/observe_sensor_repeated.py def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ): \"\"\"No Actions \"\"\" return None","title":"apply_action()"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.get_observation","text":"Observation Values Source code in corl/glues/common/observe_sensor_repeated.py def get_observation ( self ) -> OrderedDict : \"\"\"Observation Values \"\"\" # NOTE: do not attempt to optimize this function by modifying data in place # you will mess up all other glues that call self._glue.get_observation # a copy is required here sensed_value = self . _sensor . get_measurement () tmp_sensed : typing . List [ typing . Dict [ str , typing . Any ]] = [] append = tmp_sensed . append assert isinstance ( self . _sensor . measurement_properties , RepeatedProp ), \"Unexpected measurement_properties type\" m_props = self . _sensor . measurement_properties . child_space out_units = self . out_units obs_child_space = self . observation_space ()[ self . Fields . DIRECT_OBSERVATION ] . child_space for platform_data in sensed_value : tmp_row = {} for field_name , obs in platform_data . items (): tmp_row [ field_name ] = obs if field_name in out_units : prop = m_props [ field_name ] if isinstance ( prop , BoxProp ): old_unit = prop . unit [ 0 ] assert isinstance ( old_unit , str ) unit_class = GetUnitFromStr ( old_unit ) if unit_class != NoneUnitType . NoneUnit : tmp_row [ field_name ] = Convert ( obs , old_unit , out_units [ field_name ]) if self . config . enable_clip : field_space = obs_child_space . spaces [ field_name ] if isinstance ( field_space , gym . spaces . Box ): tmp_row [ field_name ] = np . clip ( obs , field_space . low , field_space . high ) append ( tmp_row ) # we are going to clip the list to the max_len, so the obs is happy if len ( tmp_sensed ) == self . max_len : break d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = tmp_sensed return d","title":"get_observation()"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.get_unique_name","text":"Class method that retreives the unique name for the glue instance Source code in corl/glues/common/observe_sensor_repeated.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" # TODO: This may result in ObserveSensor_Sensor_X, which is kinda ugly return \"ObserveSensorRepeated_\" + self . _sensor_name","title":"get_unique_name()"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.invalid_value","text":"Return zeros when invalid Source code in corl/glues/common/observe_sensor_repeated.py def invalid_value ( self ) -> OrderedDict : \"\"\"Return zeros when invalid \"\"\" arr : typing . List [ float ] = [] d = OrderedDict () d [ self . Fields . DIRECT_OBSERVATION ] = arr return d","title":"invalid_value()"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.observation_space","text":"Observation Space Source code in corl/glues/common/observe_sensor_repeated.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation Space \"\"\" d = gym . spaces . dict . Dict () child_space = gym . spaces . dict . Dict () assert isinstance ( self . _sensor . measurement_properties , RepeatedProp ), \"Unexpected measurement_properties type\" child_space_measure = self . _sensor . measurement_properties . child_space for field_name , field_meas in child_space_measure . items (): if not isinstance ( field_meas , ( DiscreteProp , MultiBinary )): assert isinstance ( field_meas , BoxProp ), \"Unexpected field_meas type\" if field_name not in self . out_units : child_space . spaces [ field_name ] = field_meas . create_space () elif isinstance ( self . out_units [ field_name ], list ): units = self . out_units [ field_name ] assert isinstance ( units , list ), \"Unexpected units type\" child_space . spaces [ field_name ] = field_meas . create_converted_space ( units ) else : child_space . spaces [ field_name ] = field_meas . create_converted_space ([ self . out_units [ field_name ]] * len ( field_meas . low )) else : child_space . spaces [ field_name ] = field_meas . create_space () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = Repeated ( child_space = child_space , max_len = self . config . max_len , ) return d","title":"observation_space()"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeated.observation_units","text":"Units of the sensors in this glue Source code in corl/glues/common/observe_sensor_repeated.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Units of the sensors in this glue \"\"\" out_dict = {} for field_name , field_unit in self . out_units . items (): out_dict [ field_name ] = GetStrFromUnit ( field_unit ) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . DIRECT_OBSERVATION ] = out_dict return d","title":"observation_units()"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeatedValidator","text":"sensor: which sensor to find on the platform output_units: unit to convert the output data to max_len: the maximum length to allow the observation space to reach enable_clip: enables clipping for spaces that support cliping Source code in corl/glues/common/observe_sensor_repeated.py class ObserveSensorRepeatedValidator ( BaseAgentPlatformGlueValidator ): \"\"\" sensor: which sensor to find on the platform output_units: unit to convert the output data to max_len: the maximum length to allow the observation space to reach enable_clip: enables clipping for spaces that support cliping \"\"\" sensor : str output_units : typing . Dict [ str , enum . Enum ] = {} max_len : int = 10 enable_clip : bool = False @validator ( 'output_units' , always = True , pre = True ) def validate_output_units ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"output_units validator\"\"\" sensor_obj = get_sensor_by_name ( values [ 'platform' ], values [ 'sensor' ]) child_space = sensor_obj . measurement_properties . child_space units = {} if v is not None : # Configuration provides output units, convert string units to enum.Enum units # Assert that input configuration is a dict assert isinstance ( v , dict ), \"Configuration output_units must be a dict\" for field_name , value in v . items (): if field_name not in child_space : raise RuntimeError ( f \"output units were provided for repeated field { field_name } , but \" f \"that field is not in this repeated space which has keys { child_space . keys () } \" ) prop = child_space [ field_name ] if not isinstance ( prop , ( DiscreteProp , MultiBinary )): assert isinstance ( prop , BoxProp ), \"Unexpected field_space type\" if np . isscalar ( prop . high ) or len ( prop . high ) == 1 : if value is not None and not isinstance ( value , str ): raise RuntimeError ( \"output units must be provided as a scalar\" ) else : raise RuntimeError ( \"repeated field space unit conversions with lists of output types not currently implemented\" ) assert isinstance ( GetUnitFromStr ( prop . unit [ 0 ]), type ( GetUnitFromStr ( value ))), \"Unexpected unit dimension\" units [ field_name ] = GetUnitFromStr ( value ) # Fill in any output units that were not provided with values from the sensor for field_name , prop in child_space . items (): if field_name in units : # This field name already provided so skip continue if hasattr ( prop , 'unit' ): # Sensor measurement properties has unit attribute, get the default unit if isinstance ( prop . unit , str ): units [ field_name ] = GetUnitFromStr ( prop . unit ) . DEFAULT elif isinstance ( prop . unit , list ): units [ field_name ] = GetUnitFromStr ( prop . unit [ 0 ]) . DEFAULT else : raise RuntimeError ( \"Unexpected property unit type\" ) else : # Sensor measurement properties does not have unit attribute, use NoneUnitType.DEFAULT units [ field_name ] = NoneUnitType . DEFAULT return units","title":"ObserveSensorRepeatedValidator"},{"location":"reference/glues/common/observe_sensor_repeated/#corl.glues.common.observe_sensor_repeated.ObserveSensorRepeatedValidator.validate_output_units","text":"output_units validator Source code in corl/glues/common/observe_sensor_repeated.py @validator ( 'output_units' , always = True , pre = True ) def validate_output_units ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"output_units validator\"\"\" sensor_obj = get_sensor_by_name ( values [ 'platform' ], values [ 'sensor' ]) child_space = sensor_obj . measurement_properties . child_space units = {} if v is not None : # Configuration provides output units, convert string units to enum.Enum units # Assert that input configuration is a dict assert isinstance ( v , dict ), \"Configuration output_units must be a dict\" for field_name , value in v . items (): if field_name not in child_space : raise RuntimeError ( f \"output units were provided for repeated field { field_name } , but \" f \"that field is not in this repeated space which has keys { child_space . keys () } \" ) prop = child_space [ field_name ] if not isinstance ( prop , ( DiscreteProp , MultiBinary )): assert isinstance ( prop , BoxProp ), \"Unexpected field_space type\" if np . isscalar ( prop . high ) or len ( prop . high ) == 1 : if value is not None and not isinstance ( value , str ): raise RuntimeError ( \"output units must be provided as a scalar\" ) else : raise RuntimeError ( \"repeated field space unit conversions with lists of output types not currently implemented\" ) assert isinstance ( GetUnitFromStr ( prop . unit [ 0 ]), type ( GetUnitFromStr ( value ))), \"Unexpected unit dimension\" units [ field_name ] = GetUnitFromStr ( value ) # Fill in any output units that were not provided with values from the sensor for field_name , prop in child_space . items (): if field_name in units : # This field name already provided so skip continue if hasattr ( prop , 'unit' ): # Sensor measurement properties has unit attribute, get the default unit if isinstance ( prop . unit , str ): units [ field_name ] = GetUnitFromStr ( prop . unit ) . DEFAULT elif isinstance ( prop . unit , list ): units [ field_name ] = GetUnitFromStr ( prop . unit [ 0 ]) . DEFAULT else : raise RuntimeError ( \"Unexpected property unit type\" ) else : # Sensor measurement properties does not have unit attribute, use NoneUnitType.DEFAULT units [ field_name ] = NoneUnitType . DEFAULT return units","title":"validate_output_units()"},{"location":"reference/glues/common/projected_quantity/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Sensors for OpenAIGymSimulator ProjectedQuantity ( BaseDictWrapperGlue ) \u00a4 Glue that takes a quantity and a number of angles and projects the quantity in the direction of the angles given. Source code in corl/glues/common/projected_quantity.py class ProjectedQuantity ( BaseDictWrapperGlue ): \"\"\" Glue that takes a quantity and a number of angles and projects the quantity in the direction of the angles given. \"\"\" class Fields : \"\"\" Fields in this glue \"\"\" PROJECTED_QUANTITY = \"projected_quantity\" def __init__ ( self , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) if 'quantity' not in self . glues () . keys (): raise KeyError ( 'Missing key: quantity' ) @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : wrapped_name = self . glues ()[ \"quantity\" ] . get_unique_name () if wrapped_name is None : return \"ProjectedQuantity\" return \"Projected_\" + wrapped_name @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : wrapped_space = list ( self . glues ()[ \"quantity\" ] . observation_space () . spaces . values ())[ 0 ] max_wrapped_obs = np . amax ( np . array ([ np . abs ( wrapped_space . low ), np . abs ( wrapped_space . high )])) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . PROJECTED_QUANTITY ] = gym . spaces . Box ( - max_wrapped_obs , max_wrapped_obs , shape = ( 1 , ), dtype = np . float32 ) return d @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\" Returns Dict space with the units of the 'quantity' glue \"\"\" d = gym . spaces . dict . Dict () quantity_glue = self . glues ()[ 'quantity' ] try : d . spaces [ self . Fields . PROJECTED_QUANTITY ] = quantity_glue . config . output_units except AttributeError : d . spaces [ self . Fields . PROJECTED_QUANTITY ] = quantity_glue . observation_units () return d def get_observation ( self ) -> typing . OrderedDict [ str , np . ndarray ]: d = OrderedDict () observations = { k : list ( v . get_observation () . values ())[ 0 ] for k , v in self . glues () . items ()} # type: ignore[union-attr] projected_value = observations . pop ( 'quantity' ) projected_value *= np . prod ( np . cos ( list ( observations . values ()))) d [ self . Fields . PROJECTED_QUANTITY ] = projected_value return d def action_space ( self ): ... def apply_action ( self , action , observation ): ... Fields \u00a4 Fields in this glue Source code in corl/glues/common/projected_quantity.py class Fields : \"\"\" Fields in this glue \"\"\" PROJECTED_QUANTITY = \"projected_quantity\" action_space ( self ) \u00a4 Build the action space for the controller, etc. that defines the action given to apply_action Returns \u00a4 gym.spaces.Space The gym Space that defines the action given to the apply_action function Source code in corl/glues/common/projected_quantity.py def action_space ( self ): ... apply_action ( self , action , observation ) \u00a4 Apply the action for the controller, etc. Parameters \u00a4 action The action for the class to apply to the platform observation The current observations before appling the action Source code in corl/glues/common/projected_quantity.py def apply_action ( self , action , observation ): ... get_observation ( self ) \u00a4 Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns \u00a4 EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/common/projected_quantity.py def get_observation ( self ) -> typing . OrderedDict [ str , np . ndarray ]: d = OrderedDict () observations = { k : list ( v . get_observation () . values ())[ 0 ] for k , v in self . glues () . items ()} # type: ignore[union-attr] projected_value = observations . pop ( 'quantity' ) projected_value *= np . prod ( np . cos ( list ( observations . values ()))) d [ self . Fields . PROJECTED_QUANTITY ] = projected_value return d observation_units ( self ) \u00a4 Returns Dict space with the units of the 'quantity' glue Source code in corl/glues/common/projected_quantity.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\" Returns Dict space with the units of the 'quantity' glue \"\"\" d = gym . spaces . dict . Dict () quantity_glue = self . glues ()[ 'quantity' ] try : d . spaces [ self . Fields . PROJECTED_QUANTITY ] = quantity_glue . config . output_units except AttributeError : d . spaces [ self . Fields . PROJECTED_QUANTITY ] = quantity_glue . observation_units () return d","title":"Projected quantity"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity","text":"Glue that takes a quantity and a number of angles and projects the quantity in the direction of the angles given. Source code in corl/glues/common/projected_quantity.py class ProjectedQuantity ( BaseDictWrapperGlue ): \"\"\" Glue that takes a quantity and a number of angles and projects the quantity in the direction of the angles given. \"\"\" class Fields : \"\"\" Fields in this glue \"\"\" PROJECTED_QUANTITY = \"projected_quantity\" def __init__ ( self , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) if 'quantity' not in self . glues () . keys (): raise KeyError ( 'Missing key: quantity' ) @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : wrapped_name = self . glues ()[ \"quantity\" ] . get_unique_name () if wrapped_name is None : return \"ProjectedQuantity\" return \"Projected_\" + wrapped_name @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : wrapped_space = list ( self . glues ()[ \"quantity\" ] . observation_space () . spaces . values ())[ 0 ] max_wrapped_obs = np . amax ( np . array ([ np . abs ( wrapped_space . low ), np . abs ( wrapped_space . high )])) d = gym . spaces . dict . Dict () d . spaces [ self . Fields . PROJECTED_QUANTITY ] = gym . spaces . Box ( - max_wrapped_obs , max_wrapped_obs , shape = ( 1 , ), dtype = np . float32 ) return d @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\" Returns Dict space with the units of the 'quantity' glue \"\"\" d = gym . spaces . dict . Dict () quantity_glue = self . glues ()[ 'quantity' ] try : d . spaces [ self . Fields . PROJECTED_QUANTITY ] = quantity_glue . config . output_units except AttributeError : d . spaces [ self . Fields . PROJECTED_QUANTITY ] = quantity_glue . observation_units () return d def get_observation ( self ) -> typing . OrderedDict [ str , np . ndarray ]: d = OrderedDict () observations = { k : list ( v . get_observation () . values ())[ 0 ] for k , v in self . glues () . items ()} # type: ignore[union-attr] projected_value = observations . pop ( 'quantity' ) projected_value *= np . prod ( np . cos ( list ( observations . values ()))) d [ self . Fields . PROJECTED_QUANTITY ] = projected_value return d def action_space ( self ): ... def apply_action ( self , action , observation ): ...","title":"ProjectedQuantity"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity.Fields","text":"Fields in this glue Source code in corl/glues/common/projected_quantity.py class Fields : \"\"\" Fields in this glue \"\"\" PROJECTED_QUANTITY = \"projected_quantity\"","title":"Fields"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity.action_space","text":"Build the action space for the controller, etc. that defines the action given to apply_action","title":"action_space()"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity.action_space--returns","text":"gym.spaces.Space The gym Space that defines the action given to the apply_action function Source code in corl/glues/common/projected_quantity.py def action_space ( self ): ...","title":"Returns"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity.apply_action","text":"Apply the action for the controller, etc.","title":"apply_action()"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity.apply_action--parameters","text":"action The action for the class to apply to the platform observation The current observations before appling the action Source code in corl/glues/common/projected_quantity.py def apply_action ( self , action , observation ): ...","title":"Parameters"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity.get_observation","text":"Get the actual observation for the platform using the state of the platform, controller, sensors, etc.","title":"get_observation()"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity.get_observation--returns","text":"EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/common/projected_quantity.py def get_observation ( self ) -> typing . OrderedDict [ str , np . ndarray ]: d = OrderedDict () observations = { k : list ( v . get_observation () . values ())[ 0 ] for k , v in self . glues () . items ()} # type: ignore[union-attr] projected_value = observations . pop ( 'quantity' ) projected_value *= np . prod ( np . cos ( list ( observations . values ()))) d [ self . Fields . PROJECTED_QUANTITY ] = projected_value return d","title":"Returns"},{"location":"reference/glues/common/projected_quantity/#corl.glues.common.projected_quantity.ProjectedQuantity.observation_units","text":"Returns Dict space with the units of the 'quantity' glue Source code in corl/glues/common/projected_quantity.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\" Returns Dict space with the units of the 'quantity' glue \"\"\" d = gym . spaces . dict . Dict () quantity_glue = self . glues ()[ 'quantity' ] try : d . spaces [ self . Fields . PROJECTED_QUANTITY ] = quantity_glue . config . output_units except AttributeError : d . spaces [ self . Fields . PROJECTED_QUANTITY ] = quantity_glue . observation_units () return d","title":"observation_units()"},{"location":"reference/glues/common/target_value/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. LimitConfigValidator ( BaseModel ) pydantic-model \u00a4 minimum: defines the min the obs of the glue may produce maximum: defines the max the obs of the glue may produce unit: defines the unit for the limits clip: defines if the glue should clip the obs values to between min/max Source code in corl/glues/common/target_value.py class LimitConfigValidator ( BaseModel ): \"\"\" minimum: defines the min the obs of the glue may produce maximum: defines the max the obs of the glue may produce unit: defines the unit for the limits clip: defines if the glue should clip the obs values to between min/max \"\"\" minimum : float maximum : float unit : typing . Optional [ str ] clip : bool = False TargetValue ( BaseAgentPlatformGlue ) \u00a4 Glue class for an observation of some constant value Source code in corl/glues/common/target_value.py class TargetValue ( BaseAgentPlatformGlue ): \"\"\" Glue class for an observation of some constant value \"\"\" # pylint: disable=too-few-public-methods class Fields : \"\"\" field data \"\"\" TARGET_VALUE = \"target_value\" def __init__ ( self , ** kwargs ) -> None : self . config : TargetValueValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ TargetValueValidator ]: return TargetValueValidator @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differentiate it from other glues. \"\"\" return f \" { self . config . name } TargetValue\" def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () d [ self . Fields . TARGET_VALUE ] = np . asarray ([ self . config . target_value ], dtype = np . float32 ) return d @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Return a space dictionary that indicates the units of the observation \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . TARGET_VALUE ] = [ self . config . unit ] return d @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Describes observation space for a single scalar continuous value Returns: gym.spaces.Space -- The rllib policy observation space \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . TARGET_VALUE ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) return d def get_observation ( self ) -> OrderedDict : \"\"\"Constructs a portion of the observation space Arguments: platforms {List[BasePlatform]} -- List of current platforms. Gauranteed to be length of 1 and set to self.obs_agent Returns: OrderedDict -- Dictionary containing observation in <FIELD> key \"\"\" d = OrderedDict () d [ self . Fields . TARGET_VALUE ] = np . asarray ([ self . config . target_value ], dtype = np . float32 ) return d def action_space ( self ): ... def apply_action ( self , action , observation ): ... get_validator : Type [ corl . glues . common . target_value . TargetValueValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.common.target_value.TargetValueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs Fields \u00a4 field data Source code in corl/glues/common/target_value.py class Fields : \"\"\" field data \"\"\" TARGET_VALUE = \"target_value\" action_space ( self ) \u00a4 Build the action space for the controller, etc. that defines the action given to apply_action Returns \u00a4 gym.spaces.Space The gym Space that defines the action given to the apply_action function Source code in corl/glues/common/target_value.py def action_space ( self ): ... apply_action ( self , action , observation ) \u00a4 Apply the action for the controller, etc. Parameters \u00a4 action The action for the class to apply to the platform observation The current observations before appling the action Source code in corl/glues/common/target_value.py def apply_action ( self , action , observation ): ... get_observation ( self ) \u00a4 Constructs a portion of the observation space Returns: Type Description OrderedDict OrderedDict -- Dictionary containing observation in key Source code in corl/glues/common/target_value.py def get_observation ( self ) -> OrderedDict : \"\"\"Constructs a portion of the observation space Arguments: platforms {List[BasePlatform]} -- List of current platforms. Gauranteed to be length of 1 and set to self.obs_agent Returns: OrderedDict -- Dictionary containing observation in <FIELD> key \"\"\" d = OrderedDict () d [ self . Fields . TARGET_VALUE ] = np . asarray ([ self . config . target_value ], dtype = np . float32 ) return d get_unique_name ( self ) \u00a4 Provies a unique name of the glue to differentiate it from other glues. Source code in corl/glues/common/target_value.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differentiate it from other glues. \"\"\" return f \" { self . config . name } TargetValue\" invalid_value ( self ) \u00a4 When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: Type Description OrderedDict OrderedDict -- Dictionary with entry containing 1D array Source code in corl/glues/common/target_value.py def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () d [ self . Fields . TARGET_VALUE ] = np . asarray ([ self . config . target_value ], dtype = np . float32 ) return d observation_space ( self ) \u00a4 Describes observation space for a single scalar continuous value Returns: Type Description Space gym.spaces.Space -- The rllib policy observation space Source code in corl/glues/common/target_value.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Describes observation space for a single scalar continuous value Returns: gym.spaces.Space -- The rllib policy observation space \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . TARGET_VALUE ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) return d observation_units ( self ) \u00a4 Return a space dictionary that indicates the units of the observation Source code in corl/glues/common/target_value.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Return a space dictionary that indicates the units of the observation \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . TARGET_VALUE ] = [ self . config . unit ] return d TargetValueValidator ( BaseAgentPlatformGlueValidator ) pydantic-model \u00a4 target_value: the Value this glue should produce this episode unit: the unit of the value produced Source code in corl/glues/common/target_value.py class TargetValueValidator ( BaseAgentPlatformGlueValidator ): \"\"\" target_value: the Value this glue should produce this episode unit: the unit of the value produced \"\"\" target_value : float unit : str limit : LimitConfigValidator","title":"Target value"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.LimitConfigValidator","text":"minimum: defines the min the obs of the glue may produce maximum: defines the max the obs of the glue may produce unit: defines the unit for the limits clip: defines if the glue should clip the obs values to between min/max Source code in corl/glues/common/target_value.py class LimitConfigValidator ( BaseModel ): \"\"\" minimum: defines the min the obs of the glue may produce maximum: defines the max the obs of the glue may produce unit: defines the unit for the limits clip: defines if the glue should clip the obs values to between min/max \"\"\" minimum : float maximum : float unit : typing . Optional [ str ] clip : bool = False","title":"LimitConfigValidator"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue","text":"Glue class for an observation of some constant value Source code in corl/glues/common/target_value.py class TargetValue ( BaseAgentPlatformGlue ): \"\"\" Glue class for an observation of some constant value \"\"\" # pylint: disable=too-few-public-methods class Fields : \"\"\" field data \"\"\" TARGET_VALUE = \"target_value\" def __init__ ( self , ** kwargs ) -> None : self . config : TargetValueValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ TargetValueValidator ]: return TargetValueValidator @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differentiate it from other glues. \"\"\" return f \" { self . config . name } TargetValue\" def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () d [ self . Fields . TARGET_VALUE ] = np . asarray ([ self . config . target_value ], dtype = np . float32 ) return d @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Return a space dictionary that indicates the units of the observation \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . TARGET_VALUE ] = [ self . config . unit ] return d @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Describes observation space for a single scalar continuous value Returns: gym.spaces.Space -- The rllib policy observation space \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . TARGET_VALUE ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) return d def get_observation ( self ) -> OrderedDict : \"\"\"Constructs a portion of the observation space Arguments: platforms {List[BasePlatform]} -- List of current platforms. Gauranteed to be length of 1 and set to self.obs_agent Returns: OrderedDict -- Dictionary containing observation in <FIELD> key \"\"\" d = OrderedDict () d [ self . Fields . TARGET_VALUE ] = np . asarray ([ self . config . target_value ], dtype = np . float32 ) return d def action_space ( self ): ... def apply_action ( self , action , observation ): ...","title":"TargetValue"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.common.target_value.TargetValueValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.Fields","text":"field data Source code in corl/glues/common/target_value.py class Fields : \"\"\" field data \"\"\" TARGET_VALUE = \"target_value\"","title":"Fields"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.action_space","text":"Build the action space for the controller, etc. that defines the action given to apply_action","title":"action_space()"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.action_space--returns","text":"gym.spaces.Space The gym Space that defines the action given to the apply_action function Source code in corl/glues/common/target_value.py def action_space ( self ): ...","title":"Returns"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.apply_action","text":"Apply the action for the controller, etc.","title":"apply_action()"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.apply_action--parameters","text":"action The action for the class to apply to the platform observation The current observations before appling the action Source code in corl/glues/common/target_value.py def apply_action ( self , action , observation ): ...","title":"Parameters"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.get_observation","text":"Constructs a portion of the observation space Returns: Type Description OrderedDict OrderedDict -- Dictionary containing observation in key Source code in corl/glues/common/target_value.py def get_observation ( self ) -> OrderedDict : \"\"\"Constructs a portion of the observation space Arguments: platforms {List[BasePlatform]} -- List of current platforms. Gauranteed to be length of 1 and set to self.obs_agent Returns: OrderedDict -- Dictionary containing observation in <FIELD> key \"\"\" d = OrderedDict () d [ self . Fields . TARGET_VALUE ] = np . asarray ([ self . config . target_value ], dtype = np . float32 ) return d","title":"get_observation()"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.get_unique_name","text":"Provies a unique name of the glue to differentiate it from other glues. Source code in corl/glues/common/target_value.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Provies a unique name of the glue to differentiate it from other glues. \"\"\" return f \" { self . config . name } TargetValue\"","title":"get_unique_name()"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.invalid_value","text":"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: Type Description OrderedDict OrderedDict -- Dictionary with entry containing 1D array Source code in corl/glues/common/target_value.py def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () d [ self . Fields . TARGET_VALUE ] = np . asarray ([ self . config . target_value ], dtype = np . float32 ) return d","title":"invalid_value()"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.observation_space","text":"Describes observation space for a single scalar continuous value Returns: Type Description Space gym.spaces.Space -- The rllib policy observation space Source code in corl/glues/common/target_value.py @lru_cache ( maxsize = 1 ) def observation_space ( self ) -> gym . spaces . Space : \"\"\"Describes observation space for a single scalar continuous value Returns: gym.spaces.Space -- The rllib policy observation space \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . TARGET_VALUE ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) return d","title":"observation_space()"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValue.observation_units","text":"Return a space dictionary that indicates the units of the observation Source code in corl/glues/common/target_value.py @lru_cache ( maxsize = 1 ) def observation_units ( self ): \"\"\"Return a space dictionary that indicates the units of the observation \"\"\" d = gym . spaces . dict . Dict () d . spaces [ self . Fields . TARGET_VALUE ] = [ self . config . unit ] return d","title":"observation_units()"},{"location":"reference/glues/common/target_value/#corl.glues.common.target_value.TargetValueValidator","text":"target_value: the Value this glue should produce this episode unit: the unit of the value produced Source code in corl/glues/common/target_value.py class TargetValueValidator ( BaseAgentPlatformGlueValidator ): \"\"\" target_value: the Value this glue should produce this episode unit: the unit of the value produced \"\"\" target_value : float unit : str limit : LimitConfigValidator","title":"TargetValueValidator"},{"location":"reference/glues/common/target_value_difference/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. TargetValueDifference ( BaseDictWrapperGlue ) \u00a4 Glue class for an observation of some constant value Source code in corl/glues/common/target_value_difference.py class TargetValueDifference ( BaseDictWrapperGlue ): \"\"\" Glue class for an observation of some constant value \"\"\" SENSOR_STR = \"sensor\" TARGET_STR = \"target\" class Fields : # pylint: disable=too-few-public-methods \"\"\" field data \"\"\" TARGET_VALUE_DIFF = \"target_value_diff\" def __init__ ( self , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) keys = self . glues () . keys () if not isinstance ( self . config . target_value , float ) and len ( self . glues ()) != 2 : # type: ignore raise ValueError ( \"self.config.target_value not defined - expecting two glues to get target from 2nd glue\" ) if not isinstance ( self . config . target_value , float ) and len ( self . glues ()) == 2 : # type: ignore if TargetValueDifference . TARGET_STR not in keys : raise KeyError ( f \"Expecting to see { TargetValueDifference . TARGET_STR } in keys - { keys } \" ) if isinstance ( self . config . target_value , float ) and len ( self . glues ()) != 1 : # type: ignore raise ValueError ( \"self.config.target_value defined - expecting one glue\" ) if TargetValueDifference . SENSOR_STR not in keys : raise KeyError ( f \"Expecting to see { TargetValueDifference . SENSOR_STR } in keys - { keys } \" ) self . _logger = logging . getLogger ( TargetValueDifference . __name__ ) @property def get_validator ( self ) -> typing . Type [ TargetValueDifferenceValidator ]: \"\"\"Return validator\"\"\" return TargetValueDifferenceValidator @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retrieves the unique name for the glue instance \"\"\" unique_name = self . config . unique_name # type: ignore if unique_name : return unique_name wrapped_glue_name = self . glues ()[ TargetValueDifference . SENSOR_STR ] . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"Diff\" def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () if isinstance ( self . config . target_value , float ): # type: ignore target_value = self . config . target_value # type: ignore else : target_value = self . glues ()[ TargetValueDifference . TARGET_STR ] . get_observation () # type: ignore d [ self . Fields . TARGET_VALUE ] = np . asarray ([ target_value ], dtype = np . float32 ) # type: ignore return d @lru_cache ( maxsize = 1 ) def observation_space ( self ): \"\"\"Observation space\"\"\" space = self . glues ()[ TargetValueDifference . SENSOR_STR ] . observation_space () d = gym . spaces . dict . Dict () if len ( space . spaces ) > 1 : raise RuntimeError ( \"Target value difference can only wrap a glue with one output\" ) for field in space . spaces : d . spaces [ field + \"_diff\" ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) d . spaces [ field + \"_diff_invalid\" ] = gym . spaces . Discrete ( 2 ) return d def get_observation ( self ): \"\"\"Get observation\"\"\" def get_wrap_diff ( A , B ): \"\"\"Returns the min diff angle RAD A Deg A RAD A Deg B abs_diff_mod_360 Diff 1.047197551 60 2.094395102 120 60 60 2.094395102 120 1.047197551 60 60 60 -2.094395102 -120 -1.047197551 -60 60 60 6.108652382 350 0.1745329252 10 340 20 -2.967059728 -170 2.967059728 170 340 20 Arguments: A {float} -- Angle 1 - rad or deg B {float} -- Angle 2 - rad or deg Returns: float -- the min diff angle \"\"\" # Convert to degrees if needed. temp_a = math . degrees ( A ) if self . config . is_rad else A temp_b = math . degrees ( B ) if self . config . is_rad else B if self . config . method == 1 : # Compute the diff as abs min angle (always positive) abs_diff_mod_360 = abs ( temp_a - temp_b ) % 360 result = 360 - abs_diff_mod_360 if abs_diff_mod_360 > 180 else abs_diff_mod_360 else : # Compute the diff as min angle maintain sign for direction diff = temp_a - temp_b result = ( diff + 180 ) % 360 - 180 # Return the diff in deg if radians if needed. return math . radians ( result ) if self . config . is_rad else result glue = self . glues ()[ TargetValueDifference . SENSOR_STR ] obs = glue . get_observation () observation_units = None if hasattr ( glue , \"observation_units\" ): observation_units = glue . observation_units () if observation_units and self . config . unit : observation_unit = None length = 0 for item in observation_units : observation_unit = observation_units [ item ][ 0 ] length += 1 if length != 1 : raise RuntimeError ( f \"observation_units not of length 1: { observation_units } \" ) if units . GetUnitFromStr ( observation_unit ) != units . GetUnitFromStr ( self . config . unit ): raise RuntimeError ( f \"Target value units [ { self . config . unit } ] and observation units [ { observation_unit } ] don't match\" ) target_invalid = False target_name = \" \" if isinstance ( self . config . target_value , float ): target_value = self . config . target_value else : # TODO make this more configurable... assumes 1 value for target if hasattr ( self . glues ()[ TargetValueDifference . TARGET_STR ], \"_sensor\" ): target_invalid = not self . glues ()[ TargetValueDifference . TARGET_STR ] . _sensor . valid # pylint: disable=protected-access target_name = self . glues ()[ TargetValueDifference . TARGET_STR ] . _sensor_name # pylint: disable=protected-access target_value = self . glues ()[ TargetValueDifference . TARGET_STR ] . get_observation ()[ self . config . target_value_field ][ self . config . target_value_index ] # I would think that this would be needed by the target value but is already converted. # if units.GetUnitFromStr(self.glues()[1]._sensor._properties.unit[0]) != units.GetUnitFromStr(self.config.unit): # target_value = units.Convert( target_value, # units.GetUnitFromStr(self.glues()[1]._sensor._properties.unit[0]), # units.GetUnitFromStr(self.config.unit)) d = OrderedDict () if target_invalid : for field , value in obs . items (): d [ field + \"_diff\" ] = np . array ([ self . config . limit . minimum ], dtype = np . float32 ) d [ field + \"_diff_invalid\" ] = target_invalid else : for field , value in obs . items (): if self . config . is_wrap : # Note: this always returns the min angle diff and is positive diff = get_wrap_diff ( target_value , value [ self . config . value_index ]) else : diff = target_value - value [ self . config . value_index ] if self . config . is_abs : diff = abs ( diff ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::obs = { value [ self . config . value_index ] } \" ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::target = { target_value } \" ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::diff = { diff } \" ) d [ field + \"_diff\" ] = np . array ([ diff ], dtype = np . float32 ) d [ field + \"_diff_invalid\" ] = glue . agent_removed () return d @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"Action space\"\"\" return None def apply_action ( self , action , observation ): # pylint: disable=unused-argument \"\"\"Apply action\"\"\" return None get_validator : Type [ corl . glues . common . target_value_difference . TargetValueDifferenceValidator ] property readonly \u00a4 Return validator Fields \u00a4 field data Source code in corl/glues/common/target_value_difference.py class Fields : # pylint: disable=too-few-public-methods \"\"\" field data \"\"\" TARGET_VALUE_DIFF = \"target_value_diff\" action_space ( self ) \u00a4 Action space Source code in corl/glues/common/target_value_difference.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"Action space\"\"\" return None apply_action ( self , action , observation ) \u00a4 Apply action Source code in corl/glues/common/target_value_difference.py def apply_action ( self , action , observation ): # pylint: disable=unused-argument \"\"\"Apply action\"\"\" return None get_observation ( self ) \u00a4 Get observation Source code in corl/glues/common/target_value_difference.py def get_observation ( self ): \"\"\"Get observation\"\"\" def get_wrap_diff ( A , B ): \"\"\"Returns the min diff angle RAD A Deg A RAD A Deg B abs_diff_mod_360 Diff 1.047197551 60 2.094395102 120 60 60 2.094395102 120 1.047197551 60 60 60 -2.094395102 -120 -1.047197551 -60 60 60 6.108652382 350 0.1745329252 10 340 20 -2.967059728 -170 2.967059728 170 340 20 Arguments: A {float} -- Angle 1 - rad or deg B {float} -- Angle 2 - rad or deg Returns: float -- the min diff angle \"\"\" # Convert to degrees if needed. temp_a = math . degrees ( A ) if self . config . is_rad else A temp_b = math . degrees ( B ) if self . config . is_rad else B if self . config . method == 1 : # Compute the diff as abs min angle (always positive) abs_diff_mod_360 = abs ( temp_a - temp_b ) % 360 result = 360 - abs_diff_mod_360 if abs_diff_mod_360 > 180 else abs_diff_mod_360 else : # Compute the diff as min angle maintain sign for direction diff = temp_a - temp_b result = ( diff + 180 ) % 360 - 180 # Return the diff in deg if radians if needed. return math . radians ( result ) if self . config . is_rad else result glue = self . glues ()[ TargetValueDifference . SENSOR_STR ] obs = glue . get_observation () observation_units = None if hasattr ( glue , \"observation_units\" ): observation_units = glue . observation_units () if observation_units and self . config . unit : observation_unit = None length = 0 for item in observation_units : observation_unit = observation_units [ item ][ 0 ] length += 1 if length != 1 : raise RuntimeError ( f \"observation_units not of length 1: { observation_units } \" ) if units . GetUnitFromStr ( observation_unit ) != units . GetUnitFromStr ( self . config . unit ): raise RuntimeError ( f \"Target value units [ { self . config . unit } ] and observation units [ { observation_unit } ] don't match\" ) target_invalid = False target_name = \" \" if isinstance ( self . config . target_value , float ): target_value = self . config . target_value else : # TODO make this more configurable... assumes 1 value for target if hasattr ( self . glues ()[ TargetValueDifference . TARGET_STR ], \"_sensor\" ): target_invalid = not self . glues ()[ TargetValueDifference . TARGET_STR ] . _sensor . valid # pylint: disable=protected-access target_name = self . glues ()[ TargetValueDifference . TARGET_STR ] . _sensor_name # pylint: disable=protected-access target_value = self . glues ()[ TargetValueDifference . TARGET_STR ] . get_observation ()[ self . config . target_value_field ][ self . config . target_value_index ] # I would think that this would be needed by the target value but is already converted. # if units.GetUnitFromStr(self.glues()[1]._sensor._properties.unit[0]) != units.GetUnitFromStr(self.config.unit): # target_value = units.Convert( target_value, # units.GetUnitFromStr(self.glues()[1]._sensor._properties.unit[0]), # units.GetUnitFromStr(self.config.unit)) d = OrderedDict () if target_invalid : for field , value in obs . items (): d [ field + \"_diff\" ] = np . array ([ self . config . limit . minimum ], dtype = np . float32 ) d [ field + \"_diff_invalid\" ] = target_invalid else : for field , value in obs . items (): if self . config . is_wrap : # Note: this always returns the min angle diff and is positive diff = get_wrap_diff ( target_value , value [ self . config . value_index ]) else : diff = target_value - value [ self . config . value_index ] if self . config . is_abs : diff = abs ( diff ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::obs = { value [ self . config . value_index ] } \" ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::target = { target_value } \" ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::diff = { diff } \" ) d [ field + \"_diff\" ] = np . array ([ diff ], dtype = np . float32 ) d [ field + \"_diff_invalid\" ] = glue . agent_removed () return d get_unique_name ( self ) \u00a4 Class method that retrieves the unique name for the glue instance Source code in corl/glues/common/target_value_difference.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retrieves the unique name for the glue instance \"\"\" unique_name = self . config . unique_name # type: ignore if unique_name : return unique_name wrapped_glue_name = self . glues ()[ TargetValueDifference . SENSOR_STR ] . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"Diff\" invalid_value ( self ) \u00a4 When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: Type Description OrderedDict OrderedDict -- Dictionary with entry containing 1D array Source code in corl/glues/common/target_value_difference.py def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () if isinstance ( self . config . target_value , float ): # type: ignore target_value = self . config . target_value # type: ignore else : target_value = self . glues ()[ TargetValueDifference . TARGET_STR ] . get_observation () # type: ignore d [ self . Fields . TARGET_VALUE ] = np . asarray ([ target_value ], dtype = np . float32 ) # type: ignore return d observation_space ( self ) \u00a4 Observation space Source code in corl/glues/common/target_value_difference.py @lru_cache ( maxsize = 1 ) def observation_space ( self ): \"\"\"Observation space\"\"\" space = self . glues ()[ TargetValueDifference . SENSOR_STR ] . observation_space () d = gym . spaces . dict . Dict () if len ( space . spaces ) > 1 : raise RuntimeError ( \"Target value difference can only wrap a glue with one output\" ) for field in space . spaces : d . spaces [ field + \"_diff\" ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) d . spaces [ field + \"_diff_invalid\" ] = gym . spaces . Discrete ( 2 ) return d TargetValueDifferenceValidator ( BaseDictWrapperGlueValidator ) pydantic-model \u00a4 target_value: float - Value to report the difference around unit: str - unit for the target value value_index: - index to extract the value from an ndarray extractor is_wrap: bool - if the target value difference needs to wrap around the limits is_rad: bool - if the read observation is in radians method: int - TODO: fill this is is_abs: bool - if the difference to the target value should be reported as absolute value limit: LimitConfigValidator - limit configuration unique_name: str - optional unique name for a target value difference glue Source code in corl/glues/common/target_value_difference.py class TargetValueDifferenceValidator ( BaseDictWrapperGlueValidator ): \"\"\" target_value: float - Value to report the difference around unit: str - unit for the target value value_index: - index to extract the value from an ndarray extractor is_wrap: bool - if the target value difference needs to wrap around the limits is_rad: bool - if the read observation is in radians method: int - TODO: fill this is is_abs: bool - if the difference to the target value should be reported as absolute value limit: LimitConfigValidator - limit configuration unique_name: str - optional unique name for a target value difference glue \"\"\" target_value : typing . Optional [ float ] = None target_value_index : typing . Optional [ int ] = 0 target_value_field : typing . Optional [ str ] = \"direct_observation\" unit : str value_index : int = 0 is_wrap : bool = False is_rad : bool = False method : int = 0 is_abs : bool = False limit : LimitConfigValidator unique_name : str = ''","title":"Target value difference"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference","text":"Glue class for an observation of some constant value Source code in corl/glues/common/target_value_difference.py class TargetValueDifference ( BaseDictWrapperGlue ): \"\"\" Glue class for an observation of some constant value \"\"\" SENSOR_STR = \"sensor\" TARGET_STR = \"target\" class Fields : # pylint: disable=too-few-public-methods \"\"\" field data \"\"\" TARGET_VALUE_DIFF = \"target_value_diff\" def __init__ ( self , ** kwargs ) -> None : super () . __init__ ( ** kwargs ) keys = self . glues () . keys () if not isinstance ( self . config . target_value , float ) and len ( self . glues ()) != 2 : # type: ignore raise ValueError ( \"self.config.target_value not defined - expecting two glues to get target from 2nd glue\" ) if not isinstance ( self . config . target_value , float ) and len ( self . glues ()) == 2 : # type: ignore if TargetValueDifference . TARGET_STR not in keys : raise KeyError ( f \"Expecting to see { TargetValueDifference . TARGET_STR } in keys - { keys } \" ) if isinstance ( self . config . target_value , float ) and len ( self . glues ()) != 1 : # type: ignore raise ValueError ( \"self.config.target_value defined - expecting one glue\" ) if TargetValueDifference . SENSOR_STR not in keys : raise KeyError ( f \"Expecting to see { TargetValueDifference . SENSOR_STR } in keys - { keys } \" ) self . _logger = logging . getLogger ( TargetValueDifference . __name__ ) @property def get_validator ( self ) -> typing . Type [ TargetValueDifferenceValidator ]: \"\"\"Return validator\"\"\" return TargetValueDifferenceValidator @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retrieves the unique name for the glue instance \"\"\" unique_name = self . config . unique_name # type: ignore if unique_name : return unique_name wrapped_glue_name = self . glues ()[ TargetValueDifference . SENSOR_STR ] . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"Diff\" def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () if isinstance ( self . config . target_value , float ): # type: ignore target_value = self . config . target_value # type: ignore else : target_value = self . glues ()[ TargetValueDifference . TARGET_STR ] . get_observation () # type: ignore d [ self . Fields . TARGET_VALUE ] = np . asarray ([ target_value ], dtype = np . float32 ) # type: ignore return d @lru_cache ( maxsize = 1 ) def observation_space ( self ): \"\"\"Observation space\"\"\" space = self . glues ()[ TargetValueDifference . SENSOR_STR ] . observation_space () d = gym . spaces . dict . Dict () if len ( space . spaces ) > 1 : raise RuntimeError ( \"Target value difference can only wrap a glue with one output\" ) for field in space . spaces : d . spaces [ field + \"_diff\" ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) d . spaces [ field + \"_diff_invalid\" ] = gym . spaces . Discrete ( 2 ) return d def get_observation ( self ): \"\"\"Get observation\"\"\" def get_wrap_diff ( A , B ): \"\"\"Returns the min diff angle RAD A Deg A RAD A Deg B abs_diff_mod_360 Diff 1.047197551 60 2.094395102 120 60 60 2.094395102 120 1.047197551 60 60 60 -2.094395102 -120 -1.047197551 -60 60 60 6.108652382 350 0.1745329252 10 340 20 -2.967059728 -170 2.967059728 170 340 20 Arguments: A {float} -- Angle 1 - rad or deg B {float} -- Angle 2 - rad or deg Returns: float -- the min diff angle \"\"\" # Convert to degrees if needed. temp_a = math . degrees ( A ) if self . config . is_rad else A temp_b = math . degrees ( B ) if self . config . is_rad else B if self . config . method == 1 : # Compute the diff as abs min angle (always positive) abs_diff_mod_360 = abs ( temp_a - temp_b ) % 360 result = 360 - abs_diff_mod_360 if abs_diff_mod_360 > 180 else abs_diff_mod_360 else : # Compute the diff as min angle maintain sign for direction diff = temp_a - temp_b result = ( diff + 180 ) % 360 - 180 # Return the diff in deg if radians if needed. return math . radians ( result ) if self . config . is_rad else result glue = self . glues ()[ TargetValueDifference . SENSOR_STR ] obs = glue . get_observation () observation_units = None if hasattr ( glue , \"observation_units\" ): observation_units = glue . observation_units () if observation_units and self . config . unit : observation_unit = None length = 0 for item in observation_units : observation_unit = observation_units [ item ][ 0 ] length += 1 if length != 1 : raise RuntimeError ( f \"observation_units not of length 1: { observation_units } \" ) if units . GetUnitFromStr ( observation_unit ) != units . GetUnitFromStr ( self . config . unit ): raise RuntimeError ( f \"Target value units [ { self . config . unit } ] and observation units [ { observation_unit } ] don't match\" ) target_invalid = False target_name = \" \" if isinstance ( self . config . target_value , float ): target_value = self . config . target_value else : # TODO make this more configurable... assumes 1 value for target if hasattr ( self . glues ()[ TargetValueDifference . TARGET_STR ], \"_sensor\" ): target_invalid = not self . glues ()[ TargetValueDifference . TARGET_STR ] . _sensor . valid # pylint: disable=protected-access target_name = self . glues ()[ TargetValueDifference . TARGET_STR ] . _sensor_name # pylint: disable=protected-access target_value = self . glues ()[ TargetValueDifference . TARGET_STR ] . get_observation ()[ self . config . target_value_field ][ self . config . target_value_index ] # I would think that this would be needed by the target value but is already converted. # if units.GetUnitFromStr(self.glues()[1]._sensor._properties.unit[0]) != units.GetUnitFromStr(self.config.unit): # target_value = units.Convert( target_value, # units.GetUnitFromStr(self.glues()[1]._sensor._properties.unit[0]), # units.GetUnitFromStr(self.config.unit)) d = OrderedDict () if target_invalid : for field , value in obs . items (): d [ field + \"_diff\" ] = np . array ([ self . config . limit . minimum ], dtype = np . float32 ) d [ field + \"_diff_invalid\" ] = target_invalid else : for field , value in obs . items (): if self . config . is_wrap : # Note: this always returns the min angle diff and is positive diff = get_wrap_diff ( target_value , value [ self . config . value_index ]) else : diff = target_value - value [ self . config . value_index ] if self . config . is_abs : diff = abs ( diff ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::obs = { value [ self . config . value_index ] } \" ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::target = { target_value } \" ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::diff = { diff } \" ) d [ field + \"_diff\" ] = np . array ([ diff ], dtype = np . float32 ) d [ field + \"_diff_invalid\" ] = glue . agent_removed () return d @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"Action space\"\"\" return None def apply_action ( self , action , observation ): # pylint: disable=unused-argument \"\"\"Apply action\"\"\" return None","title":"TargetValueDifference"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference.get_validator","text":"Return validator","title":"get_validator"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference.Fields","text":"field data Source code in corl/glues/common/target_value_difference.py class Fields : # pylint: disable=too-few-public-methods \"\"\" field data \"\"\" TARGET_VALUE_DIFF = \"target_value_diff\"","title":"Fields"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference.action_space","text":"Action space Source code in corl/glues/common/target_value_difference.py @lru_cache ( maxsize = 1 ) def action_space ( self ) -> gym . spaces . Space : \"\"\"Action space\"\"\" return None","title":"action_space()"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference.apply_action","text":"Apply action Source code in corl/glues/common/target_value_difference.py def apply_action ( self , action , observation ): # pylint: disable=unused-argument \"\"\"Apply action\"\"\" return None","title":"apply_action()"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference.get_observation","text":"Get observation Source code in corl/glues/common/target_value_difference.py def get_observation ( self ): \"\"\"Get observation\"\"\" def get_wrap_diff ( A , B ): \"\"\"Returns the min diff angle RAD A Deg A RAD A Deg B abs_diff_mod_360 Diff 1.047197551 60 2.094395102 120 60 60 2.094395102 120 1.047197551 60 60 60 -2.094395102 -120 -1.047197551 -60 60 60 6.108652382 350 0.1745329252 10 340 20 -2.967059728 -170 2.967059728 170 340 20 Arguments: A {float} -- Angle 1 - rad or deg B {float} -- Angle 2 - rad or deg Returns: float -- the min diff angle \"\"\" # Convert to degrees if needed. temp_a = math . degrees ( A ) if self . config . is_rad else A temp_b = math . degrees ( B ) if self . config . is_rad else B if self . config . method == 1 : # Compute the diff as abs min angle (always positive) abs_diff_mod_360 = abs ( temp_a - temp_b ) % 360 result = 360 - abs_diff_mod_360 if abs_diff_mod_360 > 180 else abs_diff_mod_360 else : # Compute the diff as min angle maintain sign for direction diff = temp_a - temp_b result = ( diff + 180 ) % 360 - 180 # Return the diff in deg if radians if needed. return math . radians ( result ) if self . config . is_rad else result glue = self . glues ()[ TargetValueDifference . SENSOR_STR ] obs = glue . get_observation () observation_units = None if hasattr ( glue , \"observation_units\" ): observation_units = glue . observation_units () if observation_units and self . config . unit : observation_unit = None length = 0 for item in observation_units : observation_unit = observation_units [ item ][ 0 ] length += 1 if length != 1 : raise RuntimeError ( f \"observation_units not of length 1: { observation_units } \" ) if units . GetUnitFromStr ( observation_unit ) != units . GetUnitFromStr ( self . config . unit ): raise RuntimeError ( f \"Target value units [ { self . config . unit } ] and observation units [ { observation_unit } ] don't match\" ) target_invalid = False target_name = \" \" if isinstance ( self . config . target_value , float ): target_value = self . config . target_value else : # TODO make this more configurable... assumes 1 value for target if hasattr ( self . glues ()[ TargetValueDifference . TARGET_STR ], \"_sensor\" ): target_invalid = not self . glues ()[ TargetValueDifference . TARGET_STR ] . _sensor . valid # pylint: disable=protected-access target_name = self . glues ()[ TargetValueDifference . TARGET_STR ] . _sensor_name # pylint: disable=protected-access target_value = self . glues ()[ TargetValueDifference . TARGET_STR ] . get_observation ()[ self . config . target_value_field ][ self . config . target_value_index ] # I would think that this would be needed by the target value but is already converted. # if units.GetUnitFromStr(self.glues()[1]._sensor._properties.unit[0]) != units.GetUnitFromStr(self.config.unit): # target_value = units.Convert( target_value, # units.GetUnitFromStr(self.glues()[1]._sensor._properties.unit[0]), # units.GetUnitFromStr(self.config.unit)) d = OrderedDict () if target_invalid : for field , value in obs . items (): d [ field + \"_diff\" ] = np . array ([ self . config . limit . minimum ], dtype = np . float32 ) d [ field + \"_diff_invalid\" ] = target_invalid else : for field , value in obs . items (): if self . config . is_wrap : # Note: this always returns the min angle diff and is positive diff = get_wrap_diff ( target_value , value [ self . config . value_index ]) else : diff = target_value - value [ self . config . value_index ] if self . config . is_abs : diff = abs ( diff ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::obs = { value [ self . config . value_index ] } \" ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::target = { target_value } \" ) self . _logger . debug ( f \" { TargetValueDifference . __name__ } :: { target_name } ::diff = { diff } \" ) d [ field + \"_diff\" ] = np . array ([ diff ], dtype = np . float32 ) d [ field + \"_diff_invalid\" ] = glue . agent_removed () return d","title":"get_observation()"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference.get_unique_name","text":"Class method that retrieves the unique name for the glue instance Source code in corl/glues/common/target_value_difference.py @lru_cache ( maxsize = 1 ) def get_unique_name ( self ) -> str : \"\"\"Class method that retrieves the unique name for the glue instance \"\"\" unique_name = self . config . unique_name # type: ignore if unique_name : return unique_name wrapped_glue_name = self . glues ()[ TargetValueDifference . SENSOR_STR ] . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"Diff\"","title":"get_unique_name()"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference.invalid_value","text":"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: Type Description OrderedDict OrderedDict -- Dictionary with entry containing 1D array Source code in corl/glues/common/target_value_difference.py def invalid_value ( self ) -> OrderedDict : \"\"\"When invalid return a value of 0 TODO: this may need to be self.min in the case that the minimum is larger than 0 (i.e. a harddeck) Returns: OrderedDict -- Dictionary with <FIELD> entry containing 1D array \"\"\" d = OrderedDict () if isinstance ( self . config . target_value , float ): # type: ignore target_value = self . config . target_value # type: ignore else : target_value = self . glues ()[ TargetValueDifference . TARGET_STR ] . get_observation () # type: ignore d [ self . Fields . TARGET_VALUE ] = np . asarray ([ target_value ], dtype = np . float32 ) # type: ignore return d","title":"invalid_value()"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifference.observation_space","text":"Observation space Source code in corl/glues/common/target_value_difference.py @lru_cache ( maxsize = 1 ) def observation_space ( self ): \"\"\"Observation space\"\"\" space = self . glues ()[ TargetValueDifference . SENSOR_STR ] . observation_space () d = gym . spaces . dict . Dict () if len ( space . spaces ) > 1 : raise RuntimeError ( \"Target value difference can only wrap a glue with one output\" ) for field in space . spaces : d . spaces [ field + \"_diff\" ] = gym . spaces . Box ( self . config . limit . minimum , self . config . limit . maximum , shape = ( 1 , ), dtype = np . float32 ) d . spaces [ field + \"_diff_invalid\" ] = gym . spaces . Discrete ( 2 ) return d","title":"observation_space()"},{"location":"reference/glues/common/target_value_difference/#corl.glues.common.target_value_difference.TargetValueDifferenceValidator","text":"target_value: float - Value to report the difference around unit: str - unit for the target value value_index: - index to extract the value from an ndarray extractor is_wrap: bool - if the target value difference needs to wrap around the limits is_rad: bool - if the read observation is in radians method: int - TODO: fill this is is_abs: bool - if the difference to the target value should be reported as absolute value limit: LimitConfigValidator - limit configuration unique_name: str - optional unique name for a target value difference glue Source code in corl/glues/common/target_value_difference.py class TargetValueDifferenceValidator ( BaseDictWrapperGlueValidator ): \"\"\" target_value: float - Value to report the difference around unit: str - unit for the target value value_index: - index to extract the value from an ndarray extractor is_wrap: bool - if the target value difference needs to wrap around the limits is_rad: bool - if the read observation is in radians method: int - TODO: fill this is is_abs: bool - if the difference to the target value should be reported as absolute value limit: LimitConfigValidator - limit configuration unique_name: str - optional unique name for a target value difference glue \"\"\" target_value : typing . Optional [ float ] = None target_value_index : typing . Optional [ int ] = 0 target_value_field : typing . Optional [ str ] = \"direct_observation\" unit : str value_index : int = 0 is_wrap : bool = False is_rad : bool = False method : int = 0 is_abs : bool = False limit : LimitConfigValidator unique_name : str = ''","title":"TargetValueDifferenceValidator"},{"location":"reference/glues/controller_wrappers/__init__/","text":"","title":"Controller Wrappers"},{"location":"reference/glues/controller_wrappers/delta_controller/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. AvailablePlatforms DeltaAction ( BaseWrapperGlue ) \u00a4 DeltaAction is a glue class that wraps another glue class. It treats the actions passed to it as a delta from the last action command rather E.G. if the wrapped action space has has throttle as one of the controls, then a delta action of 0.2 would move the absolute throttle position 0.2 higher than it was at the end of the last step. Source code in corl/glues/controller_wrappers/delta_controller.py class DeltaAction ( BaseWrapperGlue ): \"\"\" DeltaAction is a glue class that wraps another glue class. It treats the actions passed to it as a delta from the last action command rather E.G. if the wrapped action space has has throttle as one of the controls, then a delta action of 0.2 would move the absolute throttle position 0.2 higher than it was at the end of the last step. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : DeltaActionValidator super () . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( DeltaAction . __name__ ) self . step_size = EnvSpaceUtil . convert_config_param_to_space ( action_space = self . glue () . action_space (), parameter = self . config . step_size ) self . _is_wrap = self . config . is_wrap self . saved_action_deltas : EnvSpaceUtil . sample_type = OrderedDict () for space_name , space in self . action_space () . items (): self . saved_action_deltas [ space_name ] = space . low inner_glue : BaseAgentControllerGlue = typing . cast ( BaseAgentControllerGlue , self . glue ()) if not isinstance ( inner_glue , BaseAgentControllerGlue ): raise TypeError ( f \"Inner glue is not a BaseAgentControllerGlue, but rather { type ( inner_glue ) . __name__ } \" ) @property def get_validator ( self ) -> typing . Type [ DeltaActionValidator ]: \"\"\"Return validator\"\"\" return DeltaActionValidator @lru_cache () def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" wrapped_glue_name = self . glue () . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"Delta\" def get_observation ( self ) -> EnvSpaceUtil . sample_type : \"\"\"Get observation\"\"\" return { \"absolute\" : self . glue () . get_observation (), \"delta\" : self . saved_action_deltas } @lru_cache () def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation space\"\"\" return gym . spaces . Dict ({ \"absolute\" : self . glue () . observation_space (), \"delta\" : self . action_space ()}) @lru_cache () def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" # get the action space from the parent original_action_space = self . glue () . action_space () # log the original action space self . _logger . debug ( f \"action_space: { original_action_space } \" ) # zero mean the space so we can scale it easier zero_mean_space = EnvSpaceUtil . zero_mean_space ( original_action_space ) # scale the size of the unbiased space for space_name , space in zero_mean_space . items (): zero_mean_space [ space_name ] = EnvSpaceUtil . scale_space ( space , scale = self . step_size [ space_name ]) return zero_mean_space def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ) -> None : \"\"\" Apply the action for the controller, etc. \"\"\" self . _logger . debug ( f \"apply_action: { action } \" ) inner_glue : BaseAgentControllerGlue = typing . cast ( BaseAgentControllerGlue , self . glue ()) if not isinstance ( inner_glue , BaseAgentControllerGlue ): raise TypeError ( f \"Inner glue is not a BaseAgentControllerGlue, but rather { type ( inner_glue ) . __name__ } \" ) last_absolute_action = inner_glue . get_applied_control () absolute_action = EnvSpaceUtil . add_space_samples ( space_template = self . action_space (), space_sample1 = action , space_sample2 = last_absolute_action , ) absolute_action = EnvSpaceUtil . clip_space_sample_to_space ( absolute_action , inner_glue . action_space (), self . _is_wrap ) self . saved_action_deltas = action inner_glue . apply_action ( absolute_action , observation ) def get_info_dict ( self ): \"\"\" Get the user specified metadata/metrics/etc. \"\"\" return {} get_validator : Type [ corl . glues . controller_wrappers . delta_controller . DeltaActionValidator ] property readonly \u00a4 Return validator action_space ( self ) \u00a4 Build the action space for the controller, etc. Source code in corl/glues/controller_wrappers/delta_controller.py @lru_cache () def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" # get the action space from the parent original_action_space = self . glue () . action_space () # log the original action space self . _logger . debug ( f \"action_space: { original_action_space } \" ) # zero mean the space so we can scale it easier zero_mean_space = EnvSpaceUtil . zero_mean_space ( original_action_space ) # scale the size of the unbiased space for space_name , space in zero_mean_space . items (): zero_mean_space [ space_name ] = EnvSpaceUtil . scale_space ( space , scale = self . step_size [ space_name ]) return zero_mean_space apply_action ( self , action , observation ) \u00a4 Apply the action for the controller, etc. Source code in corl/glues/controller_wrappers/delta_controller.py def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ) -> None : \"\"\" Apply the action for the controller, etc. \"\"\" self . _logger . debug ( f \"apply_action: { action } \" ) inner_glue : BaseAgentControllerGlue = typing . cast ( BaseAgentControllerGlue , self . glue ()) if not isinstance ( inner_glue , BaseAgentControllerGlue ): raise TypeError ( f \"Inner glue is not a BaseAgentControllerGlue, but rather { type ( inner_glue ) . __name__ } \" ) last_absolute_action = inner_glue . get_applied_control () absolute_action = EnvSpaceUtil . add_space_samples ( space_template = self . action_space (), space_sample1 = action , space_sample2 = last_absolute_action , ) absolute_action = EnvSpaceUtil . clip_space_sample_to_space ( absolute_action , inner_glue . action_space (), self . _is_wrap ) self . saved_action_deltas = action inner_glue . apply_action ( absolute_action , observation ) get_info_dict ( self ) \u00a4 Get the user specified metadata/metrics/etc. Source code in corl/glues/controller_wrappers/delta_controller.py def get_info_dict ( self ): \"\"\" Get the user specified metadata/metrics/etc. \"\"\" return {} get_observation ( self ) \u00a4 Get observation Source code in corl/glues/controller_wrappers/delta_controller.py def get_observation ( self ) -> EnvSpaceUtil . sample_type : \"\"\"Get observation\"\"\" return { \"absolute\" : self . glue () . get_observation (), \"delta\" : self . saved_action_deltas } get_unique_name ( self ) \u00a4 Class method that retreives the unique name for the glue instance Source code in corl/glues/controller_wrappers/delta_controller.py @lru_cache () def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" wrapped_glue_name = self . glue () . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"Delta\" observation_space ( self ) \u00a4 Observation space Source code in corl/glues/controller_wrappers/delta_controller.py @lru_cache () def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation space\"\"\" return gym . spaces . Dict ({ \"absolute\" : self . glue () . observation_space (), \"delta\" : self . action_space ()}) DeltaActionValidator ( BaseWrapperGlueValidator ) pydantic-model \u00a4 A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space . e . g . A throttle DeltaAction . apply_action ( [ 0 . 2 ] ) with step_size = [. 05 ] would move the absolute throttle position to 0 . 01 higher than it was at the end of the last step . Source code in corl/glues/controller_wrappers/delta_controller.py class DeltaActionValidator ( BaseWrapperGlueValidator ): \"\"\" step_size: A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space. e.g. A throttle DeltaAction.apply_action([0.2]) with step_size=[.05] would move the absolute throttle position to 0.01 higher than it was at the end of the last step. \"\"\" step_size : float = 1.0 is_wrap : bool @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"DeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v check_step_scale ( v ) classmethod \u00a4 verifies range of step scale values Source code in corl/glues/controller_wrappers/delta_controller.py @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"DeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"Delta controller"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaAction","text":"DeltaAction is a glue class that wraps another glue class. It treats the actions passed to it as a delta from the last action command rather E.G. if the wrapped action space has has throttle as one of the controls, then a delta action of 0.2 would move the absolute throttle position 0.2 higher than it was at the end of the last step. Source code in corl/glues/controller_wrappers/delta_controller.py class DeltaAction ( BaseWrapperGlue ): \"\"\" DeltaAction is a glue class that wraps another glue class. It treats the actions passed to it as a delta from the last action command rather E.G. if the wrapped action space has has throttle as one of the controls, then a delta action of 0.2 would move the absolute throttle position 0.2 higher than it was at the end of the last step. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : DeltaActionValidator super () . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( DeltaAction . __name__ ) self . step_size = EnvSpaceUtil . convert_config_param_to_space ( action_space = self . glue () . action_space (), parameter = self . config . step_size ) self . _is_wrap = self . config . is_wrap self . saved_action_deltas : EnvSpaceUtil . sample_type = OrderedDict () for space_name , space in self . action_space () . items (): self . saved_action_deltas [ space_name ] = space . low inner_glue : BaseAgentControllerGlue = typing . cast ( BaseAgentControllerGlue , self . glue ()) if not isinstance ( inner_glue , BaseAgentControllerGlue ): raise TypeError ( f \"Inner glue is not a BaseAgentControllerGlue, but rather { type ( inner_glue ) . __name__ } \" ) @property def get_validator ( self ) -> typing . Type [ DeltaActionValidator ]: \"\"\"Return validator\"\"\" return DeltaActionValidator @lru_cache () def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" wrapped_glue_name = self . glue () . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"Delta\" def get_observation ( self ) -> EnvSpaceUtil . sample_type : \"\"\"Get observation\"\"\" return { \"absolute\" : self . glue () . get_observation (), \"delta\" : self . saved_action_deltas } @lru_cache () def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation space\"\"\" return gym . spaces . Dict ({ \"absolute\" : self . glue () . observation_space (), \"delta\" : self . action_space ()}) @lru_cache () def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" # get the action space from the parent original_action_space = self . glue () . action_space () # log the original action space self . _logger . debug ( f \"action_space: { original_action_space } \" ) # zero mean the space so we can scale it easier zero_mean_space = EnvSpaceUtil . zero_mean_space ( original_action_space ) # scale the size of the unbiased space for space_name , space in zero_mean_space . items (): zero_mean_space [ space_name ] = EnvSpaceUtil . scale_space ( space , scale = self . step_size [ space_name ]) return zero_mean_space def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ) -> None : \"\"\" Apply the action for the controller, etc. \"\"\" self . _logger . debug ( f \"apply_action: { action } \" ) inner_glue : BaseAgentControllerGlue = typing . cast ( BaseAgentControllerGlue , self . glue ()) if not isinstance ( inner_glue , BaseAgentControllerGlue ): raise TypeError ( f \"Inner glue is not a BaseAgentControllerGlue, but rather { type ( inner_glue ) . __name__ } \" ) last_absolute_action = inner_glue . get_applied_control () absolute_action = EnvSpaceUtil . add_space_samples ( space_template = self . action_space (), space_sample1 = action , space_sample2 = last_absolute_action , ) absolute_action = EnvSpaceUtil . clip_space_sample_to_space ( absolute_action , inner_glue . action_space (), self . _is_wrap ) self . saved_action_deltas = action inner_glue . apply_action ( absolute_action , observation ) def get_info_dict ( self ): \"\"\" Get the user specified metadata/metrics/etc. \"\"\" return {}","title":"DeltaAction"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaAction.get_validator","text":"Return validator","title":"get_validator"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaAction.action_space","text":"Build the action space for the controller, etc. Source code in corl/glues/controller_wrappers/delta_controller.py @lru_cache () def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" # get the action space from the parent original_action_space = self . glue () . action_space () # log the original action space self . _logger . debug ( f \"action_space: { original_action_space } \" ) # zero mean the space so we can scale it easier zero_mean_space = EnvSpaceUtil . zero_mean_space ( original_action_space ) # scale the size of the unbiased space for space_name , space in zero_mean_space . items (): zero_mean_space [ space_name ] = EnvSpaceUtil . scale_space ( space , scale = self . step_size [ space_name ]) return zero_mean_space","title":"action_space()"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaAction.apply_action","text":"Apply the action for the controller, etc. Source code in corl/glues/controller_wrappers/delta_controller.py def apply_action ( self , action : EnvSpaceUtil . sample_type , observation : EnvSpaceUtil . sample_type ) -> None : \"\"\" Apply the action for the controller, etc. \"\"\" self . _logger . debug ( f \"apply_action: { action } \" ) inner_glue : BaseAgentControllerGlue = typing . cast ( BaseAgentControllerGlue , self . glue ()) if not isinstance ( inner_glue , BaseAgentControllerGlue ): raise TypeError ( f \"Inner glue is not a BaseAgentControllerGlue, but rather { type ( inner_glue ) . __name__ } \" ) last_absolute_action = inner_glue . get_applied_control () absolute_action = EnvSpaceUtil . add_space_samples ( space_template = self . action_space (), space_sample1 = action , space_sample2 = last_absolute_action , ) absolute_action = EnvSpaceUtil . clip_space_sample_to_space ( absolute_action , inner_glue . action_space (), self . _is_wrap ) self . saved_action_deltas = action inner_glue . apply_action ( absolute_action , observation )","title":"apply_action()"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaAction.get_info_dict","text":"Get the user specified metadata/metrics/etc. Source code in corl/glues/controller_wrappers/delta_controller.py def get_info_dict ( self ): \"\"\" Get the user specified metadata/metrics/etc. \"\"\" return {}","title":"get_info_dict()"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaAction.get_observation","text":"Get observation Source code in corl/glues/controller_wrappers/delta_controller.py def get_observation ( self ) -> EnvSpaceUtil . sample_type : \"\"\"Get observation\"\"\" return { \"absolute\" : self . glue () . get_observation (), \"delta\" : self . saved_action_deltas }","title":"get_observation()"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaAction.get_unique_name","text":"Class method that retreives the unique name for the glue instance Source code in corl/glues/controller_wrappers/delta_controller.py @lru_cache () def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" wrapped_glue_name = self . glue () . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"Delta\"","title":"get_unique_name()"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaAction.observation_space","text":"Observation space Source code in corl/glues/controller_wrappers/delta_controller.py @lru_cache () def observation_space ( self ) -> gym . spaces . Space : \"\"\"Observation space\"\"\" return gym . spaces . Dict ({ \"absolute\" : self . glue () . observation_space (), \"delta\" : self . action_space ()})","title":"observation_space()"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaActionValidator","text":"A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space . e . g . A throttle DeltaAction . apply_action ( [ 0 . 2 ] ) with step_size = [. 05 ] would move the absolute throttle position to 0 . 01 higher than it was at the end of the last step . Source code in corl/glues/controller_wrappers/delta_controller.py class DeltaActionValidator ( BaseWrapperGlueValidator ): \"\"\" step_size: A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space. e.g. A throttle DeltaAction.apply_action([0.2]) with step_size=[.05] would move the absolute throttle position to 0.01 higher than it was at the end of the last step. \"\"\" step_size : float = 1.0 is_wrap : bool @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"DeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"DeltaActionValidator"},{"location":"reference/glues/controller_wrappers/delta_controller/#corl.glues.controller_wrappers.delta_controller.DeltaActionValidator.check_step_scale","text":"verifies range of step scale values Source code in corl/glues/controller_wrappers/delta_controller.py @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"DeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"check_step_scale()"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. AvailablePlatforms RelativeObsDeltaAction ( BaseMultiWrapperGlue ) \u00a4 RelativeObsDeltaAction is a glue class that wraps another glue class. It treats the actions passed to it as a delta from a linked observation E.G. if the wrapped action space has has roll as one of the controls, then a delta action of 0.2 would move the absolute roll position 0.2 higher than it is as measured by the linked roll sensor. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py class RelativeObsDeltaAction ( BaseMultiWrapperGlue ): \"\"\" RelativeObsDeltaAction is a glue class that wraps another glue class. It treats the actions passed to it as a delta from a linked observation E.G. if the wrapped action space has has roll as one of the controls, then a delta action of 0.2 would move the absolute roll position 0.2 higher than it is as measured by the linked roll sensor. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : RelativeObsDeltaActionValidator super () . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( RelativeObsDeltaAction . __name__ ) if len ( self . glues ()) != 2 : raise RuntimeError ( f \"Error: RelativeObsDeltaAction expected 2 wrapped glues, got { len ( self . glues ()) } \" ) self . controller : ControllerGlue = typing . cast ( ControllerGlue , self . glues ()[ 0 ]) if not isinstance ( self . controller , ControllerGlue ): raise RuntimeError ( f \"Error: RelativeObsDeltaAction expects the first wrapped glue to be a ControllerGlue, got { self . controller } \" ) self . relative_obs_glue : ObserveSensor = typing . cast ( ObserveSensor , self . glues ()[ 1 ]) if not isinstance ( self . relative_obs_glue , ObserveSensor ): raise RuntimeError ( f \"Error: RelativeObsDeltaAction expects the second wrapped glue to be a ObserveSensor, got { self . relative_obs_glue } \" ) # verify that the config setup is not going to get the user into a situation where they are # only accessing one part of the obs but applying that obs as the base position for multiple actions if self . config . obs_index and len ( list ( self . controller . action_space () . values ())[ 0 ] . low ) != 1 : raise RuntimeError ( f \"ERROR: your glue { self . get_unique_name () } has an action space length of more than 1, \" \"but you specified though obs_index to access only 1 component of the obs \" \"from the wrapped observe Sensor, to fix this error in your config for this glue define 'obs_index': null\" ) self . step_size = EnvSpaceUtil . convert_config_param_to_space ( action_space = self . controller . action_space (), parameter = self . config . step_size ) self . _is_wrap = self . config . is_wrap self . saved_action_deltas = OrderedDict () for space_name , space in self . action_space () . items (): if self . config . initial_value is not None : self . saved_action_deltas [ space_name ] = np . asarray ([ self . config . initial_value ], dtype = np . float32 ) else : self . saved_action_deltas [ space_name ] = space . low @property def get_validator ( self ) -> typing . Type [ RelativeObsDeltaActionValidator ]: return RelativeObsDeltaActionValidator @lru_cache () def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" wrapped_glue_name = self . controller . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"RelativeDelta\" def get_observation ( self ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict ]: return { \"absolute\" : self . controller . get_observation (), \"delta\" : self . saved_action_deltas , } @lru_cache () def observation_space ( self ) -> gym . spaces . Space : return gym . spaces . Dict ({ \"absolute\" : self . controller . observation_space (), \"delta\" : self . action_space ()}) @lru_cache () def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" # get the action space from the parent original_action_space = self . controller . action_space () # log the original action space self . _logger . debug ( f \"action_space: { original_action_space } \" ) # zero mean the space so we can scale it easier zero_mean_space = EnvSpaceUtil . zero_mean_space ( original_action_space ) # scale the size of the unbiased space for space_name , space in zero_mean_space . items (): zero_mean_space [ space_name ] = EnvSpaceUtil . scale_space ( space , scale = self . step_size [ space_name ]) return zero_mean_space # TODO: assumes self.controller._control_properties has unit attribute def apply_action ( self , action , observation ) -> None : \"\"\" Apply the action for the controller, etc. \"\"\" self . _logger . debug ( f \"apply_action: { action } \" ) current_observation = self . relative_obs_glue . get_observation ()[ \"direct_observation\" ] # all units in an array must be the same, so this assumption is ok obs_units = self . relative_obs_glue . observation_units ()[ \"direct_observation\" ][ 0 ] if self . config . obs_index : current_observation = current_observation [ self . config . obs_index ] assert isinstance ( self . controller . _control_properties , BoxProp ), \"Unexpected control_properties type\" # pylint: disable=W0212 out_unit = self . controller . _control_properties . unit [ 0 ] # pylint: disable=W0212 assert isinstance ( out_unit , str ) unit_converted_obs = Convert ( current_observation , obs_units , out_unit ) new_base_obs = OrderedDict () for control in action . keys (): new_base_obs [ control ] = unit_converted_obs self . saved_action_deltas = action absolute_action = EnvSpaceUtil . add_space_samples ( space_template = self . action_space (), space_sample1 = action , space_sample2 = new_base_obs , ) absolute_action = EnvSpaceUtil . clip_space_sample_to_space ( absolute_action , self . controller . action_space (), self . _is_wrap ) try : self . controller . apply_action ( absolute_action , observation ) except Exception as exc : # Purpose - add additional debugging information and re-raise the exception raise ValueError ( f ' \\n ' f 'action= { action } \\n ' f 'current_observation= { current_observation } \\n ' f 'obs_unit= { obs_units } \\n ' f 'out_unit= { out_unit } \\n ' f 'action_space= { self . action_space () } \\n ' f 'controller_action_space= { self . controller . action_space () } \\n ' f 'is_wrap= { self . _is_wrap } \\n ' ) from exc def get_info_dict ( self ): \"\"\" Get the user specified metadata/metrics/etc. \"\"\" return {} get_validator : Type [ corl . glues . controller_wrappers . obs_relative_delta_controller . RelativeObsDeltaActionValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaActionValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs action_space ( self ) \u00a4 Build the action space for the controller, etc. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py @lru_cache () def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" # get the action space from the parent original_action_space = self . controller . action_space () # log the original action space self . _logger . debug ( f \"action_space: { original_action_space } \" ) # zero mean the space so we can scale it easier zero_mean_space = EnvSpaceUtil . zero_mean_space ( original_action_space ) # scale the size of the unbiased space for space_name , space in zero_mean_space . items (): zero_mean_space [ space_name ] = EnvSpaceUtil . scale_space ( space , scale = self . step_size [ space_name ]) return zero_mean_space apply_action ( self , action , observation ) \u00a4 Apply the action for the controller, etc. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py def apply_action ( self , action , observation ) -> None : \"\"\" Apply the action for the controller, etc. \"\"\" self . _logger . debug ( f \"apply_action: { action } \" ) current_observation = self . relative_obs_glue . get_observation ()[ \"direct_observation\" ] # all units in an array must be the same, so this assumption is ok obs_units = self . relative_obs_glue . observation_units ()[ \"direct_observation\" ][ 0 ] if self . config . obs_index : current_observation = current_observation [ self . config . obs_index ] assert isinstance ( self . controller . _control_properties , BoxProp ), \"Unexpected control_properties type\" # pylint: disable=W0212 out_unit = self . controller . _control_properties . unit [ 0 ] # pylint: disable=W0212 assert isinstance ( out_unit , str ) unit_converted_obs = Convert ( current_observation , obs_units , out_unit ) new_base_obs = OrderedDict () for control in action . keys (): new_base_obs [ control ] = unit_converted_obs self . saved_action_deltas = action absolute_action = EnvSpaceUtil . add_space_samples ( space_template = self . action_space (), space_sample1 = action , space_sample2 = new_base_obs , ) absolute_action = EnvSpaceUtil . clip_space_sample_to_space ( absolute_action , self . controller . action_space (), self . _is_wrap ) try : self . controller . apply_action ( absolute_action , observation ) except Exception as exc : # Purpose - add additional debugging information and re-raise the exception raise ValueError ( f ' \\n ' f 'action= { action } \\n ' f 'current_observation= { current_observation } \\n ' f 'obs_unit= { obs_units } \\n ' f 'out_unit= { out_unit } \\n ' f 'action_space= { self . action_space () } \\n ' f 'controller_action_space= { self . controller . action_space () } \\n ' f 'is_wrap= { self . _is_wrap } \\n ' ) from exc get_info_dict ( self ) \u00a4 Get the user specified metadata/metrics/etc. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py def get_info_dict ( self ): \"\"\" Get the user specified metadata/metrics/etc. \"\"\" return {} get_observation ( self ) \u00a4 Get the actual observation for the platform using the state of the platform, controller, sensors, etc. Returns \u00a4 EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py def get_observation ( self ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict ]: return { \"absolute\" : self . controller . get_observation (), \"delta\" : self . saved_action_deltas , } get_unique_name ( self ) \u00a4 Class method that retreives the unique name for the glue instance Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py @lru_cache () def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" wrapped_glue_name = self . controller . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"RelativeDelta\" RelativeObsDeltaActionValidator ( BaseMultiWrapperGlueValidator ) pydantic-model \u00a4 A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space . e . g . A throttle DeltaAction . apply_action ( [ 0 . 2 ] ) with step_size = [. 05 ] would move the absolute throttle position to 0 . 01 higher than it was at the end of the last step . Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py class RelativeObsDeltaActionValidator ( BaseMultiWrapperGlueValidator ): \"\"\" step_size: A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space. e.g. A throttle DeltaAction.apply_action([0.2]) with step_size=[.05] would move the absolute throttle position to 0.01 higher than it was at the end of the last step. \"\"\" step_size : float = 1.0 obs_index : typing . Optional [ int ] = 0 is_wrap : bool = False initial_value : typing . Optional [ float ] = None @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"RelativeObsDeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v check_step_scale ( v ) classmethod \u00a4 verifies range of step scale values Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"RelativeObsDeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"Obs relative delta controller"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaAction","text":"RelativeObsDeltaAction is a glue class that wraps another glue class. It treats the actions passed to it as a delta from a linked observation E.G. if the wrapped action space has has roll as one of the controls, then a delta action of 0.2 would move the absolute roll position 0.2 higher than it is as measured by the linked roll sensor. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py class RelativeObsDeltaAction ( BaseMultiWrapperGlue ): \"\"\" RelativeObsDeltaAction is a glue class that wraps another glue class. It treats the actions passed to it as a delta from a linked observation E.G. if the wrapped action space has has roll as one of the controls, then a delta action of 0.2 would move the absolute roll position 0.2 higher than it is as measured by the linked roll sensor. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : RelativeObsDeltaActionValidator super () . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( RelativeObsDeltaAction . __name__ ) if len ( self . glues ()) != 2 : raise RuntimeError ( f \"Error: RelativeObsDeltaAction expected 2 wrapped glues, got { len ( self . glues ()) } \" ) self . controller : ControllerGlue = typing . cast ( ControllerGlue , self . glues ()[ 0 ]) if not isinstance ( self . controller , ControllerGlue ): raise RuntimeError ( f \"Error: RelativeObsDeltaAction expects the first wrapped glue to be a ControllerGlue, got { self . controller } \" ) self . relative_obs_glue : ObserveSensor = typing . cast ( ObserveSensor , self . glues ()[ 1 ]) if not isinstance ( self . relative_obs_glue , ObserveSensor ): raise RuntimeError ( f \"Error: RelativeObsDeltaAction expects the second wrapped glue to be a ObserveSensor, got { self . relative_obs_glue } \" ) # verify that the config setup is not going to get the user into a situation where they are # only accessing one part of the obs but applying that obs as the base position for multiple actions if self . config . obs_index and len ( list ( self . controller . action_space () . values ())[ 0 ] . low ) != 1 : raise RuntimeError ( f \"ERROR: your glue { self . get_unique_name () } has an action space length of more than 1, \" \"but you specified though obs_index to access only 1 component of the obs \" \"from the wrapped observe Sensor, to fix this error in your config for this glue define 'obs_index': null\" ) self . step_size = EnvSpaceUtil . convert_config_param_to_space ( action_space = self . controller . action_space (), parameter = self . config . step_size ) self . _is_wrap = self . config . is_wrap self . saved_action_deltas = OrderedDict () for space_name , space in self . action_space () . items (): if self . config . initial_value is not None : self . saved_action_deltas [ space_name ] = np . asarray ([ self . config . initial_value ], dtype = np . float32 ) else : self . saved_action_deltas [ space_name ] = space . low @property def get_validator ( self ) -> typing . Type [ RelativeObsDeltaActionValidator ]: return RelativeObsDeltaActionValidator @lru_cache () def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" wrapped_glue_name = self . controller . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"RelativeDelta\" def get_observation ( self ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict ]: return { \"absolute\" : self . controller . get_observation (), \"delta\" : self . saved_action_deltas , } @lru_cache () def observation_space ( self ) -> gym . spaces . Space : return gym . spaces . Dict ({ \"absolute\" : self . controller . observation_space (), \"delta\" : self . action_space ()}) @lru_cache () def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" # get the action space from the parent original_action_space = self . controller . action_space () # log the original action space self . _logger . debug ( f \"action_space: { original_action_space } \" ) # zero mean the space so we can scale it easier zero_mean_space = EnvSpaceUtil . zero_mean_space ( original_action_space ) # scale the size of the unbiased space for space_name , space in zero_mean_space . items (): zero_mean_space [ space_name ] = EnvSpaceUtil . scale_space ( space , scale = self . step_size [ space_name ]) return zero_mean_space # TODO: assumes self.controller._control_properties has unit attribute def apply_action ( self , action , observation ) -> None : \"\"\" Apply the action for the controller, etc. \"\"\" self . _logger . debug ( f \"apply_action: { action } \" ) current_observation = self . relative_obs_glue . get_observation ()[ \"direct_observation\" ] # all units in an array must be the same, so this assumption is ok obs_units = self . relative_obs_glue . observation_units ()[ \"direct_observation\" ][ 0 ] if self . config . obs_index : current_observation = current_observation [ self . config . obs_index ] assert isinstance ( self . controller . _control_properties , BoxProp ), \"Unexpected control_properties type\" # pylint: disable=W0212 out_unit = self . controller . _control_properties . unit [ 0 ] # pylint: disable=W0212 assert isinstance ( out_unit , str ) unit_converted_obs = Convert ( current_observation , obs_units , out_unit ) new_base_obs = OrderedDict () for control in action . keys (): new_base_obs [ control ] = unit_converted_obs self . saved_action_deltas = action absolute_action = EnvSpaceUtil . add_space_samples ( space_template = self . action_space (), space_sample1 = action , space_sample2 = new_base_obs , ) absolute_action = EnvSpaceUtil . clip_space_sample_to_space ( absolute_action , self . controller . action_space (), self . _is_wrap ) try : self . controller . apply_action ( absolute_action , observation ) except Exception as exc : # Purpose - add additional debugging information and re-raise the exception raise ValueError ( f ' \\n ' f 'action= { action } \\n ' f 'current_observation= { current_observation } \\n ' f 'obs_unit= { obs_units } \\n ' f 'out_unit= { out_unit } \\n ' f 'action_space= { self . action_space () } \\n ' f 'controller_action_space= { self . controller . action_space () } \\n ' f 'is_wrap= { self . _is_wrap } \\n ' ) from exc def get_info_dict ( self ): \"\"\" Get the user specified metadata/metrics/etc. \"\"\" return {}","title":"RelativeObsDeltaAction"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaAction.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaActionValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaAction.action_space","text":"Build the action space for the controller, etc. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py @lru_cache () def action_space ( self ) -> gym . spaces . Space : \"\"\" Build the action space for the controller, etc. \"\"\" # get the action space from the parent original_action_space = self . controller . action_space () # log the original action space self . _logger . debug ( f \"action_space: { original_action_space } \" ) # zero mean the space so we can scale it easier zero_mean_space = EnvSpaceUtil . zero_mean_space ( original_action_space ) # scale the size of the unbiased space for space_name , space in zero_mean_space . items (): zero_mean_space [ space_name ] = EnvSpaceUtil . scale_space ( space , scale = self . step_size [ space_name ]) return zero_mean_space","title":"action_space()"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaAction.apply_action","text":"Apply the action for the controller, etc. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py def apply_action ( self , action , observation ) -> None : \"\"\" Apply the action for the controller, etc. \"\"\" self . _logger . debug ( f \"apply_action: { action } \" ) current_observation = self . relative_obs_glue . get_observation ()[ \"direct_observation\" ] # all units in an array must be the same, so this assumption is ok obs_units = self . relative_obs_glue . observation_units ()[ \"direct_observation\" ][ 0 ] if self . config . obs_index : current_observation = current_observation [ self . config . obs_index ] assert isinstance ( self . controller . _control_properties , BoxProp ), \"Unexpected control_properties type\" # pylint: disable=W0212 out_unit = self . controller . _control_properties . unit [ 0 ] # pylint: disable=W0212 assert isinstance ( out_unit , str ) unit_converted_obs = Convert ( current_observation , obs_units , out_unit ) new_base_obs = OrderedDict () for control in action . keys (): new_base_obs [ control ] = unit_converted_obs self . saved_action_deltas = action absolute_action = EnvSpaceUtil . add_space_samples ( space_template = self . action_space (), space_sample1 = action , space_sample2 = new_base_obs , ) absolute_action = EnvSpaceUtil . clip_space_sample_to_space ( absolute_action , self . controller . action_space (), self . _is_wrap ) try : self . controller . apply_action ( absolute_action , observation ) except Exception as exc : # Purpose - add additional debugging information and re-raise the exception raise ValueError ( f ' \\n ' f 'action= { action } \\n ' f 'current_observation= { current_observation } \\n ' f 'obs_unit= { obs_units } \\n ' f 'out_unit= { out_unit } \\n ' f 'action_space= { self . action_space () } \\n ' f 'controller_action_space= { self . controller . action_space () } \\n ' f 'is_wrap= { self . _is_wrap } \\n ' ) from exc","title":"apply_action()"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaAction.get_info_dict","text":"Get the user specified metadata/metrics/etc. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py def get_info_dict ( self ): \"\"\" Get the user specified metadata/metrics/etc. \"\"\" return {}","title":"get_info_dict()"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaAction.get_observation","text":"Get the actual observation for the platform using the state of the platform, controller, sensors, etc.","title":"get_observation()"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaAction.get_observation--returns","text":"EnvSpaceUtil.sample_type The actual observation for this platform from this glue class Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py def get_observation ( self ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict ]: return { \"absolute\" : self . controller . get_observation (), \"delta\" : self . saved_action_deltas , }","title":"Returns"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaAction.get_unique_name","text":"Class method that retreives the unique name for the glue instance Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py @lru_cache () def get_unique_name ( self ) -> str : \"\"\"Class method that retreives the unique name for the glue instance \"\"\" wrapped_glue_name = self . controller . get_unique_name () if wrapped_glue_name is None : return None return wrapped_glue_name + \"RelativeDelta\"","title":"get_unique_name()"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaActionValidator","text":"A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space . e . g . A throttle DeltaAction . apply_action ( [ 0 . 2 ] ) with step_size = [. 05 ] would move the absolute throttle position to 0 . 01 higher than it was at the end of the last step . Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py class RelativeObsDeltaActionValidator ( BaseMultiWrapperGlueValidator ): \"\"\" step_size: A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space. e.g. A throttle DeltaAction.apply_action([0.2]) with step_size=[.05] would move the absolute throttle position to 0.01 higher than it was at the end of the last step. \"\"\" step_size : float = 1.0 obs_index : typing . Optional [ int ] = 0 is_wrap : bool = False initial_value : typing . Optional [ float ] = None @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"RelativeObsDeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"RelativeObsDeltaActionValidator"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller/#corl.glues.controller_wrappers.obs_relative_delta_controller.RelativeObsDeltaActionValidator.check_step_scale","text":"verifies range of step scale values Source code in corl/glues/controller_wrappers/obs_relative_delta_controller.py @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"RelativeObsDeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"check_step_scale()"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller_dict/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. AvailablePlatforms RelativeObsDeltaActionDict ( BaseDictWrapperGlue , RelativeObsDeltaAction ) \u00a4 RelativeObsDeltaActionDict is a glue class that wraps another glue class. It treats the actions passed to it as a delta from a linked observation E.G. if the wrapped action space has has roll as one of the controls, then a delta action of 0.2 would move the absolute roll position 0.2 higher than it is as measured by the linked roll sensor. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller_dict.py class RelativeObsDeltaActionDict ( BaseDictWrapperGlue , RelativeObsDeltaAction ): # type: ignore[misc] \"\"\" RelativeObsDeltaActionDict is a glue class that wraps another glue class. It treats the actions passed to it as a delta from a linked observation E.G. if the wrapped action space has has roll as one of the controls, then a delta action of 0.2 would move the absolute roll position 0.2 higher than it is as measured by the linked roll sensor. \"\"\" def __init__ ( self , ** kwargs ) -> None : # pylint: disable=super-init-not-called self . config : RelativeObsDeltaActionDictValidator # type: ignore[assignment] BaseDictWrapperGlue . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( RelativeObsDeltaActionDict . __name__ ) controller_keys = self . glues () . keys () if 'controller' not in controller_keys : raise KeyError ( 'Missing key: controller' ) if 'sensor' not in controller_keys : raise KeyError ( 'Missing key: sensor' ) self . controller : ControllerGlue = typing . cast ( ControllerGlue , self . glues ()[ \"controller\" ]) if not isinstance ( self . controller , ControllerGlue ): raise RuntimeError ( \"Error: RelativeObsDeltaActionDict expects the glue wrapped on the 'controller' key to be a ControllerGlue, \" f \"got { self . controller } \" ) self . relative_obs_glue : ObserveSensor = typing . cast ( ObserveSensor , self . glues ()[ \"sensor\" ]) if not isinstance ( self . relative_obs_glue , ObserveSensor ): raise RuntimeError ( \"Error: RelativeObsDeltaActionDict expects the glue wrapped on the 'sensor' key to be a ObserveSensor, \" f \"got { self . relative_obs_glue } \" ) # verify that the config setup is not going to get the user into a situation where they are # only accessing one part of the obs but applying that obs as the base position for multiple actions if self . config . obs_index and len ( list ( self . controller . action_space () . values ())[ 0 ] . low ) != 1 : raise RuntimeError ( f \"ERROR: your glue { self . get_unique_name () } has an action space length of more than 1, \" \"but you specified though obs_index to access only 1 component of the obs \" \"from the wrapped observe Sensor, to fix this error in your config for this glue define 'obs_index': null\" ) self . step_size = EnvSpaceUtil . convert_config_param_to_space ( action_space = self . controller . action_space (), parameter = self . config . step_size ) self . _is_wrap = self . config . is_wrap self . saved_action_deltas = OrderedDict () for space_name , space in self . action_space () . items (): if self . config . initial_value is not None : self . saved_action_deltas [ space_name ] = np . asarray ([ self . config . initial_value ], dtype = np . float32 ) else : self . saved_action_deltas [ space_name ] = space . low @property def get_validator ( self ) -> typing . Type [ RelativeObsDeltaActionDictValidator ]: # type: ignore[override] return RelativeObsDeltaActionDictValidator get_validator : Type [ corl . glues . controller_wrappers . obs_relative_delta_controller_dict . RelativeObsDeltaActionDictValidator ] property readonly \u00a4 returns the validator for this class Returns: Type Description Type[corl.glues.controller_wrappers.obs_relative_delta_controller_dict.RelativeObsDeltaActionDictValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs RelativeObsDeltaActionDictValidator ( BaseDictWrapperGlueValidator ) pydantic-model \u00a4 A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space . e . g . A throttle DeltaAction . apply_action ( [ 0 . 2 ] ) with step_size = [. 05 ] would move the absolute throttle position to 0 . 01 higher than it was at the end of the last step . Source code in corl/glues/controller_wrappers/obs_relative_delta_controller_dict.py class RelativeObsDeltaActionDictValidator ( BaseDictWrapperGlueValidator ): \"\"\" step_size: A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space. e.g. A throttle DeltaAction.apply_action([0.2]) with step_size=[.05] would move the absolute throttle position to 0.01 higher than it was at the end of the last step. \"\"\" step_size : float = 1.0 obs_index : typing . Optional [ int ] = 0 is_wrap : bool = False initial_value : typing . Optional [ float ] = None @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"RelativeObsDeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v check_step_scale ( v ) classmethod \u00a4 verifies range of step scale values Source code in corl/glues/controller_wrappers/obs_relative_delta_controller_dict.py @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"RelativeObsDeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"Obs relative delta controller dict"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller_dict/#corl.glues.controller_wrappers.obs_relative_delta_controller_dict.RelativeObsDeltaActionDict","text":"RelativeObsDeltaActionDict is a glue class that wraps another glue class. It treats the actions passed to it as a delta from a linked observation E.G. if the wrapped action space has has roll as one of the controls, then a delta action of 0.2 would move the absolute roll position 0.2 higher than it is as measured by the linked roll sensor. Source code in corl/glues/controller_wrappers/obs_relative_delta_controller_dict.py class RelativeObsDeltaActionDict ( BaseDictWrapperGlue , RelativeObsDeltaAction ): # type: ignore[misc] \"\"\" RelativeObsDeltaActionDict is a glue class that wraps another glue class. It treats the actions passed to it as a delta from a linked observation E.G. if the wrapped action space has has roll as one of the controls, then a delta action of 0.2 would move the absolute roll position 0.2 higher than it is as measured by the linked roll sensor. \"\"\" def __init__ ( self , ** kwargs ) -> None : # pylint: disable=super-init-not-called self . config : RelativeObsDeltaActionDictValidator # type: ignore[assignment] BaseDictWrapperGlue . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( RelativeObsDeltaActionDict . __name__ ) controller_keys = self . glues () . keys () if 'controller' not in controller_keys : raise KeyError ( 'Missing key: controller' ) if 'sensor' not in controller_keys : raise KeyError ( 'Missing key: sensor' ) self . controller : ControllerGlue = typing . cast ( ControllerGlue , self . glues ()[ \"controller\" ]) if not isinstance ( self . controller , ControllerGlue ): raise RuntimeError ( \"Error: RelativeObsDeltaActionDict expects the glue wrapped on the 'controller' key to be a ControllerGlue, \" f \"got { self . controller } \" ) self . relative_obs_glue : ObserveSensor = typing . cast ( ObserveSensor , self . glues ()[ \"sensor\" ]) if not isinstance ( self . relative_obs_glue , ObserveSensor ): raise RuntimeError ( \"Error: RelativeObsDeltaActionDict expects the glue wrapped on the 'sensor' key to be a ObserveSensor, \" f \"got { self . relative_obs_glue } \" ) # verify that the config setup is not going to get the user into a situation where they are # only accessing one part of the obs but applying that obs as the base position for multiple actions if self . config . obs_index and len ( list ( self . controller . action_space () . values ())[ 0 ] . low ) != 1 : raise RuntimeError ( f \"ERROR: your glue { self . get_unique_name () } has an action space length of more than 1, \" \"but you specified though obs_index to access only 1 component of the obs \" \"from the wrapped observe Sensor, to fix this error in your config for this glue define 'obs_index': null\" ) self . step_size = EnvSpaceUtil . convert_config_param_to_space ( action_space = self . controller . action_space (), parameter = self . config . step_size ) self . _is_wrap = self . config . is_wrap self . saved_action_deltas = OrderedDict () for space_name , space in self . action_space () . items (): if self . config . initial_value is not None : self . saved_action_deltas [ space_name ] = np . asarray ([ self . config . initial_value ], dtype = np . float32 ) else : self . saved_action_deltas [ space_name ] = space . low @property def get_validator ( self ) -> typing . Type [ RelativeObsDeltaActionDictValidator ]: # type: ignore[override] return RelativeObsDeltaActionDictValidator","title":"RelativeObsDeltaActionDict"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller_dict/#corl.glues.controller_wrappers.obs_relative_delta_controller_dict.RelativeObsDeltaActionDict.get_validator","text":"returns the validator for this class Returns: Type Description Type[corl.glues.controller_wrappers.obs_relative_delta_controller_dict.RelativeObsDeltaActionDictValidator] BaseAgentGlueValidator -- A pydantic validator to be used to validate kwargs","title":"get_validator"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller_dict/#corl.glues.controller_wrappers.obs_relative_delta_controller_dict.RelativeObsDeltaActionDictValidator","text":"A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space . e . g . A throttle DeltaAction . apply_action ( [ 0 . 2 ] ) with step_size = [. 05 ] would move the absolute throttle position to 0 . 01 higher than it was at the end of the last step . Source code in corl/glues/controller_wrappers/obs_relative_delta_controller_dict.py class RelativeObsDeltaActionDictValidator ( BaseDictWrapperGlueValidator ): \"\"\" step_size: A dict that contains a floating point scalar for each action in the action space, by which the corresponding delta action is scaled prior to converting the action to the wrapped space. e.g. A throttle DeltaAction.apply_action([0.2]) with step_size=[.05] would move the absolute throttle position to 0.01 higher than it was at the end of the last step. \"\"\" step_size : float = 1.0 obs_index : typing . Optional [ int ] = 0 is_wrap : bool = False initial_value : typing . Optional [ float ] = None @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"RelativeObsDeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"RelativeObsDeltaActionDictValidator"},{"location":"reference/glues/controller_wrappers/obs_relative_delta_controller_dict/#corl.glues.controller_wrappers.obs_relative_delta_controller_dict.RelativeObsDeltaActionDictValidator.check_step_scale","text":"verifies range of step scale values Source code in corl/glues/controller_wrappers/obs_relative_delta_controller_dict.py @validator ( \"step_size\" ) @classmethod def check_step_scale ( cls , v ): \"\"\" verifies range of step scale values \"\"\" if v >= 1.0 or v < 0 : raise ValueError ( \"RelativeObsDeltaActionValidator got step size of more that 1.0 or less than 0\" ) return v","title":"check_step_scale()"},{"location":"reference/libraries/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Libraries"},{"location":"reference/libraries/cleanup/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. cleanup ( fn ) \u00a4 Returns an object that will call fn when it goes out of scope Source code in corl/libraries/cleanup.py def cleanup ( fn : typing . Callable ): \"\"\"Returns an object that will call fn when it goes out of scope \"\"\" class ScopedDestructor : \"\"\"calls fn when this goes out of scope\"\"\" def __init__ ( self ): self . abort = False def __del__ ( self ): if not self . abort : fn () return ScopedDestructor ()","title":"Cleanup"},{"location":"reference/libraries/cleanup/#corl.libraries.cleanup.cleanup","text":"Returns an object that will call fn when it goes out of scope Source code in corl/libraries/cleanup.py def cleanup ( fn : typing . Callable ): \"\"\"Returns an object that will call fn when it goes out of scope \"\"\" class ScopedDestructor : \"\"\"calls fn when this goes out of scope\"\"\" def __init__ ( self ): self . abort = False def __del__ ( self ): if not self . abort : fn () return ScopedDestructor ()","title":"cleanup()"},{"location":"reference/libraries/collection_utils/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. collection utils get_dictionary_subset ( data_dict , keys ) \u00a4 Create dictionary of input tupe with subset of values Parameters \u00a4 data_dict typing.Union[typing.Dict, typing.OrderedDict]: src dictionary keys list: list of keys defining subset Returns \u00a4 dictionary of input tupe with subset of values Source code in corl/libraries/collection_utils.py def get_dictionary_subset ( data_dict : typing . Union [ typing . Dict , typing . OrderedDict ], keys : typing . List ) -> typing . OrderedDict : \"\"\" Create dictionary of input tupe with subset of values Parameters ---------- data_dict typing.Union[typing.Dict, typing.OrderedDict]: src dictionary keys list: list of keys defining subset Returns ------- dictionary of input tupe with subset of values \"\"\" return collections . OrderedDict ({ key : data_dict [ key ] for key in keys if key in data_dict })","title":"Collection utils"},{"location":"reference/libraries/collection_utils/#corl.libraries.collection_utils.get_dictionary_subset","text":"Create dictionary of input tupe with subset of values","title":"get_dictionary_subset()"},{"location":"reference/libraries/collection_utils/#corl.libraries.collection_utils.get_dictionary_subset--parameters","text":"data_dict typing.Union[typing.Dict, typing.OrderedDict]: src dictionary keys list: list of keys defining subset","title":"Parameters"},{"location":"reference/libraries/collection_utils/#corl.libraries.collection_utils.get_dictionary_subset--returns","text":"dictionary of input tupe with subset of values Source code in corl/libraries/collection_utils.py def get_dictionary_subset ( data_dict : typing . Union [ typing . Dict , typing . OrderedDict ], keys : typing . List ) -> typing . OrderedDict : \"\"\" Create dictionary of input tupe with subset of values Parameters ---------- data_dict typing.Union[typing.Dict, typing.OrderedDict]: src dictionary keys list: list of keys defining subset Returns ------- dictionary of input tupe with subset of values \"\"\" return collections . OrderedDict ({ key : data_dict [ key ] for key in keys if key in data_dict })","title":"Returns"},{"location":"reference/libraries/env_common/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Environment Common Module EnvCommonStrings \u00a4 Environment Common Strings Source code in corl/libraries/env_common.py class EnvCommonStrings : \"\"\"Environment Common Strings \"\"\" SLANT_RANGE_RATE = \"range_change_rate\" SLANT_RANGE = \"range\" ASPECT_ANGLE = \"aspect_angle\" CROSS_ANGLE = \"heading_cross_angle\" RELATIVE_ELEVATION = \"relative_elevation\" ALT_ERROR = \"alt_error\" SUM_ALT_ERROR = \"sum_alt_error\" ALT_ERROR_DIFF = \"alt_error_diff\" EnvCommonValues \u00a4 Environment Common Values Source code in corl/libraries/env_common.py class EnvCommonValues : \"\"\"Environment Common Values \"\"\" METERS_TO_FEET = 3.28084 FEET_TO_METERS = 0.3048 MACH_TO_METERS_PER_SECOND = 343 NAUTICAL_MILE_TO_METERS = 1852 NAUTICAL_MILE_TO_KILOMETERS = 1.852 METERS_TO_NAUTICAL_MILE = 0.000539957 DEG_TO_RAD = 0.01745329251994329576923 RAD_TO_DEG = 57.29577951308232087721 METERS_PER_SECOND_TO_KNOTS = 1.94384 FEET_PER_SECOND_TO_KNOTS = 0.592484 KNOTS_TO_METERS_PER_SECOND = 0.514444 METERS_TO_KILOMETERS = 0.001","title":"Env common"},{"location":"reference/libraries/env_common/#corl.libraries.env_common.EnvCommonStrings","text":"Environment Common Strings Source code in corl/libraries/env_common.py class EnvCommonStrings : \"\"\"Environment Common Strings \"\"\" SLANT_RANGE_RATE = \"range_change_rate\" SLANT_RANGE = \"range\" ASPECT_ANGLE = \"aspect_angle\" CROSS_ANGLE = \"heading_cross_angle\" RELATIVE_ELEVATION = \"relative_elevation\" ALT_ERROR = \"alt_error\" SUM_ALT_ERROR = \"sum_alt_error\" ALT_ERROR_DIFF = \"alt_error_diff\"","title":"EnvCommonStrings"},{"location":"reference/libraries/env_common/#corl.libraries.env_common.EnvCommonValues","text":"Environment Common Values Source code in corl/libraries/env_common.py class EnvCommonValues : \"\"\"Environment Common Values \"\"\" METERS_TO_FEET = 3.28084 FEET_TO_METERS = 0.3048 MACH_TO_METERS_PER_SECOND = 343 NAUTICAL_MILE_TO_METERS = 1852 NAUTICAL_MILE_TO_KILOMETERS = 1.852 METERS_TO_NAUTICAL_MILE = 0.000539957 DEG_TO_RAD = 0.01745329251994329576923 RAD_TO_DEG = 57.29577951308232087721 METERS_PER_SECOND_TO_KNOTS = 1.94384 FEET_PER_SECOND_TO_KNOTS = 0.592484 KNOTS_TO_METERS_PER_SECOND = 0.514444 METERS_TO_KILOMETERS = 0.001","title":"EnvCommonValues"},{"location":"reference/libraries/env_func_base/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Functors are objects the that can be treated as though they are a function. When to use functors? - Functors are used when you want to hide/abstract the real implementation. Let\u2019s say you want to call the different functions depending on the input but you don\u2019t want the user code to make explicit calls to those different functions. This is the ideal situation where functors can help. - In this scenario, we can go for a functor which internally calls the most suitable function depending on the input. - Now if later, none of functions to be called increases, then it would be just a simple change in the backend code without disturbing any of the user code. Thus functors help in creating maintainable, decoupled and extendable codes. EnvFuncBase \u00a4 base definition for env functions Source code in corl/libraries/env_func_base.py class EnvFuncBase : \"\"\" base definition for env functions \"\"\" def reset ( self ): # pylint: disable=no-self-use \"\"\" Base reset function for items such as rewards and dones \"\"\" ... @property def name ( self ) -> str : \"\"\" gets the name fo the functor Returns ------- str The name of the functor \"\"\" return type ( self ) . __name__ name : str property readonly \u00a4 gets the name fo the functor Returns \u00a4 str The name of the functor reset ( self ) \u00a4 Base reset function for items such as rewards and dones Source code in corl/libraries/env_func_base.py def reset ( self ): # pylint: disable=no-self-use \"\"\" Base reset function for items such as rewards and dones \"\"\" ...","title":"Env func base"},{"location":"reference/libraries/env_func_base/#corl.libraries.env_func_base.EnvFuncBase","text":"base definition for env functions Source code in corl/libraries/env_func_base.py class EnvFuncBase : \"\"\" base definition for env functions \"\"\" def reset ( self ): # pylint: disable=no-self-use \"\"\" Base reset function for items such as rewards and dones \"\"\" ... @property def name ( self ) -> str : \"\"\" gets the name fo the functor Returns ------- str The name of the functor \"\"\" return type ( self ) . __name__","title":"EnvFuncBase"},{"location":"reference/libraries/env_func_base/#corl.libraries.env_func_base.EnvFuncBase.name","text":"gets the name fo the functor","title":"name"},{"location":"reference/libraries/env_func_base/#corl.libraries.env_func_base.EnvFuncBase.name--returns","text":"str The name of the functor","title":"Returns"},{"location":"reference/libraries/env_func_base/#corl.libraries.env_func_base.EnvFuncBase.reset","text":"Base reset function for items such as rewards and dones Source code in corl/libraries/env_func_base.py def reset ( self ): # pylint: disable=no-self-use \"\"\" Base reset function for items such as rewards and dones \"\"\" ...","title":"reset()"},{"location":"reference/libraries/env_space_util/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. ENV Space Util Module EnvSpaceUtil \u00a4 ENV Space Util Source code in corl/libraries/env_space_util.py class EnvSpaceUtil : # pylint: disable=R0904 \"\"\" ENV Space Util \"\"\" _logger = logging . getLogger ( \"EnvSpaceUtil\" ) sample_type = typing . Union [ OrderedDict , dict , tuple , np . ndarray , list ] @staticmethod def deep_sanity_check_space_sample ( # pylint: disable=R0912 space : gym . spaces . Space , sample : sample_type , key_stack : str = \"\" , ) -> None : \"\"\"Ensure space sample is consistent with space. This will give a traceback of the exact space that failed Parameters ---------- space: gym.spaces.Dict the space we expect the sample to conform to sample: sample_type sample that we are checking if it belongs to the given space key_stack: str string of the keys we have used when getting to this current spot in the observation_space, observation this is used for recursive calls do not set in the initial call to this function \"\"\" if isinstance ( space , gym . spaces . Dict ): if not isinstance ( sample , ( StateDict , OrderedDict , dict )): raise ValueError ( f \"space { key_stack } = { space } was a gym.spaces.Dict type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a StateDict, OrderedDict, or dict\" ) for key , value in sample . items (): EnvSpaceUtil . deep_sanity_check_space_sample ( space . spaces [ key ], value , key_stack = f \" { key_stack } [ { key } ]\" ) elif isinstance ( space , gym . spaces . Tuple ): if not isinstance ( sample , tuple ): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Tuple type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a tuple type\" ) for idx , value in enumerate ( sample ): EnvSpaceUtil . deep_sanity_check_space_sample ( space . spaces [ idx ], value , key_stack = f \" { key_stack } [ { idx } ]\" ) elif isinstance ( space , gym . spaces . Discrete ): if not isinstance ( sample , ( int , np . integer , np . ndarray )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Discrete type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not an int or np.integer or np.ndarray\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . n } \" ) elif isinstance ( space , gym . spaces . Box ): if not isinstance ( sample , ( np . ndarray , list , np . floating )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Box type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.float type\" ) if not space . contains ( sample ): sample_dtype = getattr ( sample , 'dtype' , None ) raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . low } { space . high } \" f \"space dtype is { space . dtype } , sample dtype is { sample_dtype } \" ) elif isinstance ( space , gym . spaces . MultiBinary ): if not isinstance ( sample , ( np . ndarray , list , np . integer )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.MultiBinary type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.integer type\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . n } \" ) elif isinstance ( space , gym . spaces . MultiDiscrete ): if not isinstance ( sample , ( np . ndarray , list , np . integer )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.MultiDiscrete type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.integer type\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . nvec } \" ) elif isinstance ( space , Repeated ): if not isinstance ( sample , list ): raise ValueError ( f \"space { key_stack } = { space } is a ray.rllib.utils.spaces.repeated.Repeated type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a list type\" ) for idx , item in enumerate ( sample ): EnvSpaceUtil . deep_sanity_check_space_sample ( space . child_space , item , key_stack = f \" { key_stack } [ { idx } ]\" ) @staticmethod def sanity_check_space_sample ( space , sample ): \"\"\"[summary] Parameters ---------- space : [type] [description] sample : [type] [description] Raises ------ RuntimeError [description] \"\"\" if not space . contains ( sample ): raise RuntimeError ( f \"sample of { sample } does not meet space { space } setup\" ) @staticmethod def deep_merge_dict ( source : dict , destination : dict ): \"\"\" Merget two dictionaries that also can contain sub dictionaries. This function returns the second dict but it also modifies it in place run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , OrderedDict ()) EnvSpaceUtil . deep_merge_dict ( value , node ) else : destination [ key ] = value return destination @staticmethod def scale_space ( space : gym . spaces . Space , scale : float ) -> gym . spaces . Space : \"\"\" Multiplies the low and high properties of all the Boxes in the given gym space by the scale input Parameters ---------- space: gym.spaces.Space the gym space to scale the Boxes of scale: float what to multiply the Box low and high by Returns ------- gym.spaces.Space the scaled gym space \"\"\" # TODO: this copy probably doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . scale_space ( value , scale = scale ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_thing = [ EnvSpaceUtil . scale_space ( sp , scale = scale ) for sp in space . spaces ] val = gym . spaces . Tuple ( tuple ( new_thing )) elif isinstance ( space , gym . spaces . Box ): scaled_box = gym . spaces . Box ( low = np . multiply ( space . low , scale ) . astype ( np . float32 ), high = np . multiply ( space . high , scale ) . astype ( np . float32 ), shape = space . shape , dtype = np . float32 ) val = scaled_box return val @staticmethod def zero_mean_space ( space : gym . spaces . Space ) -> gym . spaces . Space : \"\"\" Returns a space object where every Box instance has its low and high shifted to be zero mean Parameters ---------- space: gym.spaces.Space The gym space to zero mean Returns ------- gym.spaces.Space A gym space the same as the input but with the Box instances shifted \"\"\" # TODO: this copy doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . zero_mean_space ( value ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_thing = [ EnvSpaceUtil . zero_mean_space ( sp ) for sp in space . spaces ] val = gym . spaces . Tuple ( tuple ( new_thing )) elif isinstance ( space , gym . spaces . Box ): mean = ( space . high + space . low ) / 2 zero_mean_box = gym . spaces . Box ( low = space . low - mean , high = space . high - mean , shape = space . shape , dtype = np . float32 ) val = zero_mean_box return val @staticmethod def space_box_min_maxer ( space_likes : typing . Tuple [ gym . spaces . Space ], out_min : float = - 1.0 , out_max : float = 1.0 , ) -> gym . spaces . Space : \"\"\" Makes a gym box to the out_min and out_max range Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space] the gym space to turn all boxes into the scaled space out_min: float the new low for the boxes out_max: float the new high for the boxes Returns ------- gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed \"\"\" space_arg = space_likes [ 0 ] if isinstance ( space_arg , gym . spaces . Box ): return gym . spaces . Box ( low = out_min , high = out_max , shape = space_arg . shape , dtype = np . float32 ) return copy . deepcopy ( space_arg ) @staticmethod def normalize_space ( space : gym . spaces . Space , out_min =- 1 , out_max = 1 , ) -> gym . spaces . Space : \"\"\" This is a convenience wrapper for box_scaler Parameters ---------- space: gym.spaces.Space the gym space to turn all boxes into the scaled space out_min: float the new low for the boxes out_max: float the new high for the boxes Returns ------- gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . space_box_min_maxer , space_likes = ( space , ), out_min = out_min , out_max = out_max , return_space = True , ) @staticmethod def get_zero_sample_from_space ( space : gym . spaces . Space ) -> sample_type : \"\"\" Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters ---------- space: gym.spaces.Space The gym space to zero sample from Returns ------- sample_type The instance of the gym space but all Box spaces are sampled as zero \"\"\" val = space . sample () if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . get_zero_sample_from_space ( value ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . get_zero_sample_from_space ( sp ) for sp in space . spaces ] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): val = np . zeros ( shape = space . shape , dtype = np . float32 ) return val @staticmethod def get_mean_sample_from_space ( space : gym . spaces . Space ) -> sample_type : \"\"\" Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters ---------- space: gym.spaces.Space The gym space to zero sample from Returns ------- sample_type The instance of the gym space but all Box spaces are sampled as zero \"\"\" val = space . sample () if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . get_zero_sample_from_space ( value ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . get_zero_sample_from_space ( sp ) for sp in space . spaces ] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): val = ( space . high + space . low ) / 2.0 return val @staticmethod def add_space_samples ( space_template : gym . spaces . Space , space_sample1 : sample_type , space_sample2 : sample_type , ) -> sample_type : \"\"\" Adds together two instances of gym spaces. This only adds the ndarray or list types (that were sampled from Box) If the object is not a ndarray or list then the value of space_sample1 is returned by default Parameters ---------- space_template: gym.spaces.Space The template to use for adding these space instances. This is to determine the difference between a Box and Discrete or MultiDiscrete or MultiBinary space_sample1: sample_type The first instance to add space_sample2: sample_type The second instance to add Returns ------- sample_type an instance of the space object but with all the Box types added \"\"\" # if not type(space_sample1) == type(space_sample2): # raise ValueError('space instances must be of same type') # TODO: I want to check they are the same type but dict and OrderedDict should match which makes this annoying val : EnvSpaceUtil . sample_type if isinstance ( space_template , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space_template . spaces . items (): value1 = space_sample1 [ key ] value2 = space_sample2 [ key ] new_dict [ key ] = EnvSpaceUtil . add_space_samples ( space_value , value1 , value2 ) val = new_dict elif isinstance ( space_template , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . add_space_samples ( * args ) for args in zip ( space_template , space_sample1 , space_sample2 )] val = tuple ( new_tuple ) elif isinstance ( space_template , gym . spaces . Box ): if isinstance ( space_sample1 , np . ndarray ): val = np . array ( space_sample1 + space_sample2 ) elif isinstance ( space_sample1 , list ): val = [ value1 + value2 for value1 , value2 in zip ( space_sample1 , space_sample2 )] else : val = copy . deepcopy ( space_sample1 ) return val @staticmethod def clip_space_sample_to_space ( space_sample : sample_type , space : gym . spaces . Space , is_wrap : bool = False ) -> sample_type : \"\"\" Clips a space instance to a given space. After this the space should contain the space instance Parameters ---------- space_sample: sample_type the space instance we are going to clip space: gym.spaces.Space the gym space to clip the instance to. Returns ------- sample_type the clipped space instance \"\"\" val = copy . deepcopy ( space_sample ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space . spaces . items (): space_sample_value = space_sample [ key ] new_dict [ key ] = EnvSpaceUtil . clip_space_sample_to_space ( space_sample_value , space_value , is_wrap ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . clip_space_sample_to_space ( siv , sv , is_wrap ) for siv , sv in zip ( space_sample , space )] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): if is_wrap : # Takes care of the case where the controls wrap at the min/max value # Example: Range = 0-1, value = 1.1 ----> clipping puts to .1 if space_sample > space . high : val = space . low + ( space_sample - space . high ) elif space_sample < space . low : val = space . high - ( space . low - space_sample ) else : # Takes care of the case where the controls saturate at the min/max value # Example: Range = 0-1, value = 1.1 ----> clipping puts to 1 assert isinstance ( space_sample , ( Sequence , np . ndarray )), \\ f 'Box spaces must have sequence samples, received { type ( space_sample ) . __name__ } ' val = np . clip ( a = space_sample , a_min = space . low , a_max = space . high ) return val @staticmethod def turn_box_into_tuple_of_discretes ( space : gym . spaces . Space , num_actions : typing . Union [ int , typing . List [ int ], Iterable , dict ] = 10 ) -> gym . spaces . Space : \"\"\" Takes a gym space and replaces any Box types with MultiDiscrete types with the same number of possible of discrete as the box and each discrete has the same number of possible actions = num_actions Parameters ---------- space num_actions: int, list, dict how many discrete actions to give to each possible discrete in the MultiDiscrete Returns ------- gym.spaces.Space A gym space where all the Box types are replaced with MultiDiscrete types \"\"\" # first pass through the code num_actions is an int or list, this code turns it into an # iterator for the rest of the recursive calls if isinstance ( num_actions , int ): num_actions = repeat ( num_actions ) elif isinstance ( num_actions , ( list , tuple )): num_actions = iter ( num_actions ) # TODO: this copy doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space . spaces . items (): if isinstance ( num_actions , dict ): new_dict [ key ] = EnvSpaceUtil . turn_box_into_tuple_of_discretes ( space = space_value , num_actions = num_actions [ key ]) else : new_dict [ key ] = EnvSpaceUtil . turn_box_into_tuple_of_discretes ( space = space_value , num_actions = num_actions ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . turn_box_into_tuple_of_discretes ( sv , num_actions = num_actions ) for sv in space ] if len ( new_tuple ) == 1 and isinstance ( new_tuple [ 0 ], gym . spaces . Tuple ): val = new_tuple [ 0 ] else : val = gym . spaces . Tuple ( tuple ( new_tuple )) elif isinstance ( space , gym . spaces . Box ): assert isinstance ( num_actions , Iterator ), f 'num_actions must be iterator, received { type ( num_actions ) . __name__ } ' val = gym . spaces . Tuple ([ Discrete ( np . asarray ( next ( num_actions ))) for _ in range ( 0 , space . low . size )]) return val # TODO: maybe use this function in more places? maybe not? it could be slower? @staticmethod def iterate_over_space_likes ( func , space_likes : typing . Tuple [ typing . Union [ gym . spaces . Space , sample_type ], ... ], return_space : bool , * func_args , ** func_kwargs , ) -> typing . Union [ gym . spaces . Space , sample_type ]: \"\"\" Iterates over space_likes which are tuple, dicts or the gym equivalent. When it encounters an actual item that is not a container it calls the func method. put any args, or kwargs you want to give to func in the overall call and they will be forwarded Parameters ---------- func: the function to apply space_likes: typing.Tuple[typing.Union[gym.spaces.Space, sample_type], ...] the spaces to iterate over. They must have the same keywords for dicts and number of items for tuples return_space: bool if true the containers will be gym space equivalents func_args the arguments to give to func func_kwargs the keyword arguments to give to func Returns ------- The contained result by calling func and stuffing back into the tuples and dicts in the call if return_space=True the containers are gym spaces \"\"\" first_space_like = space_likes [ 0 ] val = None if isinstance ( first_space_like , ( gym . spaces . Dict , dict , OrderedDict )): new_dict = OrderedDict () keys : typing . KeysView if isinstance ( first_space_like , gym . spaces . Dict ): keys = first_space_like . spaces . keys () else : keys = first_space_like . keys () for key in keys : new_space_likes = tuple ( spacer [ key ] for spacer in space_likes ) new_dict [ key ] = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = new_space_likes , return_space = return_space , * func_args , ** func_kwargs , ) val = gym . spaces . Dict ( spaces = new_dict ) if return_space else new_dict elif isinstance ( first_space_like , ( gym . spaces . Tuple , tuple )): new_tuple = [ EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = new_space_likes , return_space = return_space , * func_args , ** func_kwargs , ) for new_space_likes in zip ( * space_likes ) ] val = ( gym . spaces . Tuple ( tuple ( new_tuple )) if return_space else tuple ( new_tuple )) elif isinstance ( first_space_like , Repeated ): # if space_likes is longer than 1 that means that return_space = False # if there is only the space that means we need to generate just the space # itself and can use the second path, however for the case where we have a sample # it comes in as a list and we must iterate over the entire repeated list and process it if len ( space_likes ) > 1 : repeated_samples = space_likes [ 1 ] assert isinstance ( repeated_samples , MutableSequence ), \\ f 'repeated_samples must be MutableSequence, received { type ( repeated_samples ) . __name__ } ' for indx , sample in enumerate ( repeated_samples ): repeated_samples [ indx ] = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = ( first_space_like . child_space , sample ), return_space = return_space , * func_args , ** func_kwargs , ) val = repeated_samples else : new_child_space = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = ( first_space_like . child_space , ), return_space = return_space , * func_args , ** func_kwargs , ) val = Repeated ( child_space = new_child_space , max_len = first_space_like . max_len ) else : val = func ( space_likes , * func_args , ** func_kwargs ) return val @staticmethod def turn_orig_space_box_to_cont_sample ( space_likes : typing . Tuple [ gym . spaces . Space , gym . spaces . Space , sample_type ]) -> sample_type : \"\"\" Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns ------- sample_type: the sample space that is now a continuous version of the discrete sample space according to the cont space \"\"\" ( original_space_arg , discrete_only_space_arg , space_sample_arg ) = space_likes if isinstance ( original_space_arg , gym . spaces . Box ) and ( isinstance ( discrete_only_space_arg , ( gym . spaces . MultiDiscrete , gym . spaces . Discrete ))): if isinstance ( discrete_only_space_arg , gym . spaces . MultiDiscrete ): possible_n = discrete_only_space_arg . nvec elif isinstance ( discrete_only_space_arg , gym . spaces . Discrete ): possible_n = discrete_only_space_arg . n else : raise RuntimeError ( \"This should not be reachable\" ) action = space_sample_arg low = original_space_arg . low high = original_space_arg . high new_cont_sample = ( action / ( possible_n - 1 )) * ( high - low ) + low return new_cont_sample return copy . deepcopy ( space_sample_arg ) @staticmethod def turn_orig_space_box_to_cont_sample_powerspace ( space_likes : typing . Tuple [ gym . spaces . Space , gym . spaces . Space , sample_type , sample_type ] ) -> sample_type : \"\"\" Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns ------- sample_type: the sample space that is now a continuous version of the discrete sample space according to the cont space \"\"\" # There is a test built for this function. Please keep the test up to date for any changes you make. ( original_space_arg , discrete_only_space_arg , space_sample_arg , pow_n ) = space_likes if isinstance ( original_space_arg , gym . spaces . Box ) and ( isinstance ( discrete_only_space_arg , ( gym . spaces . Tuple , gym . spaces . Discrete ))): if isinstance ( discrete_only_space_arg , gym . spaces . Tuple ): possible_n = np . asarray ([ x . n for x in discrete_only_space_arg ]) pow_n = np . asarray ( pow_n ) discrete_sample = np . asarray ( space_sample_arg ) elif isinstance ( discrete_only_space_arg , gym . spaces . Discrete ): possible_n = np . asarray ([ discrete_only_space_arg . n ]) pow_n = np . asarray ([ pow_n ]) discrete_sample = np . asarray ([ space_sample_arg ]) else : raise RuntimeError ( \"This should not be reachable\" ) low = original_space_arg . low high = original_space_arg . high if any ( low > high ): raise RuntimeError ( \"lower bounds of space somehow higher than high bounds\" ) if not all ( possible_n % 2 ): if any ( pow_n > 1 ): raise RuntimeError ( \"The exponential power factor not supported when possible_n % 2\" ) difference = high - low movement_per_space_sample = difference / possible_n new_cont_sample = low + ( movement_per_space_sample * space_sample_arg ) return new_cont_sample if any ( pow_n <= 0 ): raise RuntimeError ( \"The exponential power factor used to stretch/shrink the discrete space must be greater than zero\" ) if any ( - low != high ) or any ( low >= high ): raise RuntimeError ( \"Currently only support symmetric space about zero for power space setup - TODO\" ) p_high = np . power ( high , 1 / pow_n . astype ( float )) p_low = - p_high # Since we have specified the space must be symetric around 0 new_cont_sample = ( discrete_sample / ( possible_n - 1 )) * ( p_high - p_low ) + p_low new_cont_sample = np . power ( np . abs ( new_cont_sample ), pow_n ) * np . sign ( new_cont_sample ) return new_cont_sample return copy . deepcopy ( space_sample_arg ) @staticmethod def turn_discrete_action_back_to_cont ( original_space : gym . spaces . Space , discrete_only_space : gym . spaces . Space , space_sample : sample_type , ) -> sample_type : \"\"\" This is a convenience wrapper for turn_orig_space_box_to_cont_sample Parameters ---------- original_space: gym.spaces.Space the continuous space discrete_only_space: gym.spaces.Space the discrete space space_sample: sample_type the sample of the discrete space Returns ------- sample_type: the continuous version of the discrete sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( EnvSpaceUtil . turn_orig_space_box_to_cont_sample , space_likes = ( original_space , discrete_only_space , space_sample ), return_space = False , ) @staticmethod def turn_discrete_action_back_to_cont_powerspace ( original_space : gym . spaces . Space , discrete_only_space : gym . spaces . Space , space_sample : sample_type , pow_n : dict ) -> sample_type : \"\"\" This is a convenience wrapper for turn_orig_space_box_to_cont_sample_powerspace Parameters ---------- original_space: gym.spaces.Space the continuous space discrete_only_space: gym.spaces.Space the discrete space space_sample: sample_type the sample of the discrete space Returns ------- sample_type: the continuous version of the discrete sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( EnvSpaceUtil . turn_orig_space_box_to_cont_sample_powerspace , space_likes = ( original_space , discrete_only_space , space_sample , pow_n ), return_space = False , ) @staticmethod def box_scaler ( space_likes : typing . Tuple [ gym . spaces . Space , sample_type ], out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This scales a box space to be between the out_min and out_max arguments Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale out_min: float the minimum of the output scaling out_max: float the maximum of the output scaling Returns ------- sample_type: the scaled sample with min of out_min and max of out_max \"\"\" ( space_arg , space_sample_arg ) = space_likes if isinstance ( space_arg , gym . spaces . Box ) and space_arg . is_bounded (): val = space_sample_arg in_min = space_arg . low in_max = space_arg . high norm_value = ( out_max - out_min ) * ( val - in_min ) / ( in_max - in_min ) + out_min return norm_value . astype ( np . float32 ) return copy . deepcopy ( space_sample_arg ) @staticmethod def scale_sample_from_space ( space : gym . spaces . Space , space_sample : sample_type , out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This is a convenience wrapper for box_scaler Parameters ---------- space: gym.spaces.Space the space to use for the input min and max space_sample: sample_type the space sample to scale out_min: float the minimum of the output scaling out_max: float the maximum of the output scaling Returns ------- sample_type: the scaled sample with min of out_min and max of out_max (this is in dicts and tuples the same as space_sample was) \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . box_scaler , space_likes = ( space , space_sample ), out_min = out_min , out_max = out_max , return_space = False , ) @staticmethod def box_unscaler ( space_likes : typing . Tuple [ gym . spaces . Space , sample_type ], out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" Unscales the space_sample according to be the scale of the input space. In this sense out_min and out_max are the min max of the sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale out_min: float the minimum of the sample out_max: float the maximum of the sample Returns ------- space_type: the unscaled sample \"\"\" ( space_arg , space_sample_arg ) = space_likes if isinstance ( space_arg , gym . spaces . Box ): norm_value = space_sample_arg assert isinstance ( norm_value , np . ndarray ), f 'norm_value must be np.ndarray, received { type ( norm_value ) . __name__ } ' in_min = space_arg . low in_max = space_arg . high val = ( norm_value - out_min ) * ( in_max - in_min ) / ( out_max - out_min ) + in_min return val return copy . deepcopy ( space_sample_arg ) @staticmethod def unscale_sample_from_space ( space : gym . spaces . Space , space_sample : sample_type , out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This is a convenience wrapper for box_unscaler Parameters ---------- space: gym.spaces.Space the gym space we will unscale to space_sample: sample_type the sample we want to unscale. thus this is a scaled version of the input space with a min,max defined by the arguments out_min,out_max out_min: float the minimum of the sample out_max: float the maximum of the sample Returns ------- sample_type: the unscaled version of the space_sample. Thus the space should now contain this output sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . box_unscaler , space_likes = ( space , space_sample ), out_min = out_min , out_max = out_max , return_space = False , ) @staticmethod def convert_config_param_to_space ( action_space : gym . spaces . Space , parameter : typing . Union [ int , float , list , dict ]) -> dict : \"\"\" This is a a convert for parameters used in action space conversions Parameters ---------- space: gym.spaces.Space the gym space that defines the actions parameter: typing.Union[int, float, list, dict] the parameter as defined in a a config Returns ------- dict: actions are dicts, the output is a dict with the parameters set for each key \"\"\" action_params = {} sample_action = action_space . sample () . items () if isinstance ( parameter , ( int , float )): parameter = [ parameter ] # change to list if needed if len ( parameter ) == 1 : # if length 1, broadcast to the proper length for key , value in sample_action : action_params [ key ] = parameter * len ( value ) elif isinstance ( parameter , list ): if len ( sample_action ) != 1 : raise ValueError ( f \"list configs can only be applied to action space dicts with single key: { sample_action } \" ) for key , value in sample_action : if len ( value ) != len ( parameter ): raise ValueError ( f \"config length does not match action length of { len ( value ) } . { parameter } \" ) action_params [ key ] = parameter elif isinstance ( parameter , dict ): # pylint: disable=too-many-nested-blocks for key , value in sample_action : if key not in parameter : if isinstance ( key , tuple ): # parameters may be specfied using tuple values as keys action_params [ key ] = np . zeros ( len ( key )) # type: ignore[assignment] for idx , sub_key in enumerate ( key ): if sub_key not in parameter : raise ValueError ( f \"action space key not in config key: { sub_key } , config: { parameter } \" ) action_params [ key ][ idx ] = parameter [ sub_key ] else : raise ValueError ( f \"action space key not in config key: { key } , config: { parameter } \" ) elif len ( value ) != len ( parameter [ key ]): raise ValueError ( f \"config value length for key { key } does not match action length of { len ( value ) } . { parameter } \" ) else : action_params [ key ] = parameter [ key ] return action_params add_space_samples ( space_template , space_sample1 , space_sample2 ) staticmethod \u00a4 Adds together two instances of gym spaces. This only adds the ndarray or list types (that were sampled from Box) If the object is not a ndarray or list then the value of space_sample1 is returned by default Parameters gym.spaces.Space The template to use for adding these space instances. This is to determine the difference between a Box and Discrete or MultiDiscrete or MultiBinary sample_type The first instance to add sample_type The second instance to add Returns \u00a4 sample_type an instance of the space object but with all the Box types added Source code in corl/libraries/env_space_util.py @staticmethod def add_space_samples ( space_template : gym . spaces . Space , space_sample1 : sample_type , space_sample2 : sample_type , ) -> sample_type : \"\"\" Adds together two instances of gym spaces. This only adds the ndarray or list types (that were sampled from Box) If the object is not a ndarray or list then the value of space_sample1 is returned by default Parameters ---------- space_template: gym.spaces.Space The template to use for adding these space instances. This is to determine the difference between a Box and Discrete or MultiDiscrete or MultiBinary space_sample1: sample_type The first instance to add space_sample2: sample_type The second instance to add Returns ------- sample_type an instance of the space object but with all the Box types added \"\"\" # if not type(space_sample1) == type(space_sample2): # raise ValueError('space instances must be of same type') # TODO: I want to check they are the same type but dict and OrderedDict should match which makes this annoying val : EnvSpaceUtil . sample_type if isinstance ( space_template , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space_template . spaces . items (): value1 = space_sample1 [ key ] value2 = space_sample2 [ key ] new_dict [ key ] = EnvSpaceUtil . add_space_samples ( space_value , value1 , value2 ) val = new_dict elif isinstance ( space_template , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . add_space_samples ( * args ) for args in zip ( space_template , space_sample1 , space_sample2 )] val = tuple ( new_tuple ) elif isinstance ( space_template , gym . spaces . Box ): if isinstance ( space_sample1 , np . ndarray ): val = np . array ( space_sample1 + space_sample2 ) elif isinstance ( space_sample1 , list ): val = [ value1 + value2 for value1 , value2 in zip ( space_sample1 , space_sample2 )] else : val = copy . deepcopy ( space_sample1 ) return val box_scaler ( space_likes , out_min =- 1 , out_max = 1 ) staticmethod \u00a4 This scales a box space to be between the out_min and out_max arguments Parameters \u00a4 typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale float the minimum of the output scaling float the maximum of the output scaling Returns \u00a4 Sample_type the scaled sample with min of out_min and max of out_max Source code in corl/libraries/env_space_util.py @staticmethod def box_scaler ( space_likes : typing . Tuple [ gym . spaces . Space , sample_type ], out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This scales a box space to be between the out_min and out_max arguments Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale out_min: float the minimum of the output scaling out_max: float the maximum of the output scaling Returns ------- sample_type: the scaled sample with min of out_min and max of out_max \"\"\" ( space_arg , space_sample_arg ) = space_likes if isinstance ( space_arg , gym . spaces . Box ) and space_arg . is_bounded (): val = space_sample_arg in_min = space_arg . low in_max = space_arg . high norm_value = ( out_max - out_min ) * ( val - in_min ) / ( in_max - in_min ) + out_min return norm_value . astype ( np . float32 ) return copy . deepcopy ( space_sample_arg ) box_unscaler ( space_likes , out_min =- 1 , out_max = 1 ) staticmethod \u00a4 Unscales the space_sample according to be the scale of the input space. In this sense out_min and out_max are the min max of the sample Parameters \u00a4 typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale float the minimum of the sample float the maximum of the sample Returns \u00a4 Space_type the unscaled sample Source code in corl/libraries/env_space_util.py @staticmethod def box_unscaler ( space_likes : typing . Tuple [ gym . spaces . Space , sample_type ], out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" Unscales the space_sample according to be the scale of the input space. In this sense out_min and out_max are the min max of the sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale out_min: float the minimum of the sample out_max: float the maximum of the sample Returns ------- space_type: the unscaled sample \"\"\" ( space_arg , space_sample_arg ) = space_likes if isinstance ( space_arg , gym . spaces . Box ): norm_value = space_sample_arg assert isinstance ( norm_value , np . ndarray ), f 'norm_value must be np.ndarray, received { type ( norm_value ) . __name__ } ' in_min = space_arg . low in_max = space_arg . high val = ( norm_value - out_min ) * ( in_max - in_min ) / ( out_max - out_min ) + in_min return val return copy . deepcopy ( space_sample_arg ) clip_space_sample_to_space ( space_sample , space , is_wrap = False ) staticmethod \u00a4 Clips a space instance to a given space. After this the space should contain the space instance Parameters \u00a4 sample_type the space instance we are going to clip gym.spaces.Space the gym space to clip the instance to. Returns \u00a4 sample_type the clipped space instance Source code in corl/libraries/env_space_util.py @staticmethod def clip_space_sample_to_space ( space_sample : sample_type , space : gym . spaces . Space , is_wrap : bool = False ) -> sample_type : \"\"\" Clips a space instance to a given space. After this the space should contain the space instance Parameters ---------- space_sample: sample_type the space instance we are going to clip space: gym.spaces.Space the gym space to clip the instance to. Returns ------- sample_type the clipped space instance \"\"\" val = copy . deepcopy ( space_sample ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space . spaces . items (): space_sample_value = space_sample [ key ] new_dict [ key ] = EnvSpaceUtil . clip_space_sample_to_space ( space_sample_value , space_value , is_wrap ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . clip_space_sample_to_space ( siv , sv , is_wrap ) for siv , sv in zip ( space_sample , space )] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): if is_wrap : # Takes care of the case where the controls wrap at the min/max value # Example: Range = 0-1, value = 1.1 ----> clipping puts to .1 if space_sample > space . high : val = space . low + ( space_sample - space . high ) elif space_sample < space . low : val = space . high - ( space . low - space_sample ) else : # Takes care of the case where the controls saturate at the min/max value # Example: Range = 0-1, value = 1.1 ----> clipping puts to 1 assert isinstance ( space_sample , ( Sequence , np . ndarray )), \\ f 'Box spaces must have sequence samples, received { type ( space_sample ) . __name__ } ' val = np . clip ( a = space_sample , a_min = space . low , a_max = space . high ) return val convert_config_param_to_space ( action_space , parameter ) staticmethod \u00a4 This is a a convert for parameters used in action space conversions Parameters \u00a4 gym.spaces.Space the gym space that defines the actions typing.Union[int, float, list, dict] the parameter as defined in a a config Returns \u00a4 Dict actions are dicts, the output is a dict with the parameters set for each key Source code in corl/libraries/env_space_util.py @staticmethod def convert_config_param_to_space ( action_space : gym . spaces . Space , parameter : typing . Union [ int , float , list , dict ]) -> dict : \"\"\" This is a a convert for parameters used in action space conversions Parameters ---------- space: gym.spaces.Space the gym space that defines the actions parameter: typing.Union[int, float, list, dict] the parameter as defined in a a config Returns ------- dict: actions are dicts, the output is a dict with the parameters set for each key \"\"\" action_params = {} sample_action = action_space . sample () . items () if isinstance ( parameter , ( int , float )): parameter = [ parameter ] # change to list if needed if len ( parameter ) == 1 : # if length 1, broadcast to the proper length for key , value in sample_action : action_params [ key ] = parameter * len ( value ) elif isinstance ( parameter , list ): if len ( sample_action ) != 1 : raise ValueError ( f \"list configs can only be applied to action space dicts with single key: { sample_action } \" ) for key , value in sample_action : if len ( value ) != len ( parameter ): raise ValueError ( f \"config length does not match action length of { len ( value ) } . { parameter } \" ) action_params [ key ] = parameter elif isinstance ( parameter , dict ): # pylint: disable=too-many-nested-blocks for key , value in sample_action : if key not in parameter : if isinstance ( key , tuple ): # parameters may be specfied using tuple values as keys action_params [ key ] = np . zeros ( len ( key )) # type: ignore[assignment] for idx , sub_key in enumerate ( key ): if sub_key not in parameter : raise ValueError ( f \"action space key not in config key: { sub_key } , config: { parameter } \" ) action_params [ key ][ idx ] = parameter [ sub_key ] else : raise ValueError ( f \"action space key not in config key: { key } , config: { parameter } \" ) elif len ( value ) != len ( parameter [ key ]): raise ValueError ( f \"config value length for key { key } does not match action length of { len ( value ) } . { parameter } \" ) else : action_params [ key ] = parameter [ key ] return action_params deep_merge_dict ( source , destination ) staticmethod \u00a4 Merget two dictionaries that also can contain sub dictionaries. This function returns the second dict but it also modifies it in place run me with nosetests --with-doctest file.py a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True Source code in corl/libraries/env_space_util.py @staticmethod def deep_merge_dict ( source : dict , destination : dict ): \"\"\" Merget two dictionaries that also can contain sub dictionaries. This function returns the second dict but it also modifies it in place run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , OrderedDict ()) EnvSpaceUtil . deep_merge_dict ( value , node ) else : destination [ key ] = value return destination deep_sanity_check_space_sample ( space , sample , key_stack = '' ) staticmethod \u00a4 Ensure space sample is consistent with space. This will give a traceback of the exact space that failed Parameters \u00a4 gym.spaces.Dict the space we expect the sample to conform to sample_type sample that we are checking if it belongs to the given space str string of the keys we have used when getting to this current spot in the observation_space, observation this is used for recursive calls do not set in the initial call to this function Source code in corl/libraries/env_space_util.py @staticmethod def deep_sanity_check_space_sample ( # pylint: disable=R0912 space : gym . spaces . Space , sample : sample_type , key_stack : str = \"\" , ) -> None : \"\"\"Ensure space sample is consistent with space. This will give a traceback of the exact space that failed Parameters ---------- space: gym.spaces.Dict the space we expect the sample to conform to sample: sample_type sample that we are checking if it belongs to the given space key_stack: str string of the keys we have used when getting to this current spot in the observation_space, observation this is used for recursive calls do not set in the initial call to this function \"\"\" if isinstance ( space , gym . spaces . Dict ): if not isinstance ( sample , ( StateDict , OrderedDict , dict )): raise ValueError ( f \"space { key_stack } = { space } was a gym.spaces.Dict type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a StateDict, OrderedDict, or dict\" ) for key , value in sample . items (): EnvSpaceUtil . deep_sanity_check_space_sample ( space . spaces [ key ], value , key_stack = f \" { key_stack } [ { key } ]\" ) elif isinstance ( space , gym . spaces . Tuple ): if not isinstance ( sample , tuple ): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Tuple type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a tuple type\" ) for idx , value in enumerate ( sample ): EnvSpaceUtil . deep_sanity_check_space_sample ( space . spaces [ idx ], value , key_stack = f \" { key_stack } [ { idx } ]\" ) elif isinstance ( space , gym . spaces . Discrete ): if not isinstance ( sample , ( int , np . integer , np . ndarray )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Discrete type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not an int or np.integer or np.ndarray\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . n } \" ) elif isinstance ( space , gym . spaces . Box ): if not isinstance ( sample , ( np . ndarray , list , np . floating )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Box type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.float type\" ) if not space . contains ( sample ): sample_dtype = getattr ( sample , 'dtype' , None ) raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . low } { space . high } \" f \"space dtype is { space . dtype } , sample dtype is { sample_dtype } \" ) elif isinstance ( space , gym . spaces . MultiBinary ): if not isinstance ( sample , ( np . ndarray , list , np . integer )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.MultiBinary type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.integer type\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . n } \" ) elif isinstance ( space , gym . spaces . MultiDiscrete ): if not isinstance ( sample , ( np . ndarray , list , np . integer )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.MultiDiscrete type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.integer type\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . nvec } \" ) elif isinstance ( space , Repeated ): if not isinstance ( sample , list ): raise ValueError ( f \"space { key_stack } = { space } is a ray.rllib.utils.spaces.repeated.Repeated type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a list type\" ) for idx , item in enumerate ( sample ): EnvSpaceUtil . deep_sanity_check_space_sample ( space . child_space , item , key_stack = f \" { key_stack } [ { idx } ]\" ) get_mean_sample_from_space ( space ) staticmethod \u00a4 Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters \u00a4 gym.spaces.Space The gym space to zero sample from Returns \u00a4 sample_type The instance of the gym space but all Box spaces are sampled as zero Source code in corl/libraries/env_space_util.py @staticmethod def get_mean_sample_from_space ( space : gym . spaces . Space ) -> sample_type : \"\"\" Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters ---------- space: gym.spaces.Space The gym space to zero sample from Returns ------- sample_type The instance of the gym space but all Box spaces are sampled as zero \"\"\" val = space . sample () if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . get_zero_sample_from_space ( value ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . get_zero_sample_from_space ( sp ) for sp in space . spaces ] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): val = ( space . high + space . low ) / 2.0 return val get_zero_sample_from_space ( space ) staticmethod \u00a4 Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters \u00a4 gym.spaces.Space The gym space to zero sample from Returns \u00a4 sample_type The instance of the gym space but all Box spaces are sampled as zero Source code in corl/libraries/env_space_util.py @staticmethod def get_zero_sample_from_space ( space : gym . spaces . Space ) -> sample_type : \"\"\" Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters ---------- space: gym.spaces.Space The gym space to zero sample from Returns ------- sample_type The instance of the gym space but all Box spaces are sampled as zero \"\"\" val = space . sample () if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . get_zero_sample_from_space ( value ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . get_zero_sample_from_space ( sp ) for sp in space . spaces ] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): val = np . zeros ( shape = space . shape , dtype = np . float32 ) return val iterate_over_space_likes ( func , space_likes , return_space , * func_args , ** func_kwargs ) staticmethod \u00a4 Iterates over space_likes which are tuple, dicts or the gym equivalent. When it encounters an actual item that is not a container it calls the func method. put any args, or kwargs you want to give to func in the overall call and they will be forwarded Parameters \u00a4 Func the function to apply typing.Tuple[typing.Union[gym.spaces.Space, sample_type], ...] the spaces to iterate over. They must have the same keywords for dicts and number of items for tuples bool if true the containers will be gym space equivalents func_args the arguments to give to func func_kwargs the keyword arguments to give to func Returns \u00a4 The contained result by calling func and stuffing back into the tuples and dicts in the call if return_space=True the containers are gym spaces Source code in corl/libraries/env_space_util.py @staticmethod def iterate_over_space_likes ( func , space_likes : typing . Tuple [ typing . Union [ gym . spaces . Space , sample_type ], ... ], return_space : bool , * func_args , ** func_kwargs , ) -> typing . Union [ gym . spaces . Space , sample_type ]: \"\"\" Iterates over space_likes which are tuple, dicts or the gym equivalent. When it encounters an actual item that is not a container it calls the func method. put any args, or kwargs you want to give to func in the overall call and they will be forwarded Parameters ---------- func: the function to apply space_likes: typing.Tuple[typing.Union[gym.spaces.Space, sample_type], ...] the spaces to iterate over. They must have the same keywords for dicts and number of items for tuples return_space: bool if true the containers will be gym space equivalents func_args the arguments to give to func func_kwargs the keyword arguments to give to func Returns ------- The contained result by calling func and stuffing back into the tuples and dicts in the call if return_space=True the containers are gym spaces \"\"\" first_space_like = space_likes [ 0 ] val = None if isinstance ( first_space_like , ( gym . spaces . Dict , dict , OrderedDict )): new_dict = OrderedDict () keys : typing . KeysView if isinstance ( first_space_like , gym . spaces . Dict ): keys = first_space_like . spaces . keys () else : keys = first_space_like . keys () for key in keys : new_space_likes = tuple ( spacer [ key ] for spacer in space_likes ) new_dict [ key ] = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = new_space_likes , return_space = return_space , * func_args , ** func_kwargs , ) val = gym . spaces . Dict ( spaces = new_dict ) if return_space else new_dict elif isinstance ( first_space_like , ( gym . spaces . Tuple , tuple )): new_tuple = [ EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = new_space_likes , return_space = return_space , * func_args , ** func_kwargs , ) for new_space_likes in zip ( * space_likes ) ] val = ( gym . spaces . Tuple ( tuple ( new_tuple )) if return_space else tuple ( new_tuple )) elif isinstance ( first_space_like , Repeated ): # if space_likes is longer than 1 that means that return_space = False # if there is only the space that means we need to generate just the space # itself and can use the second path, however for the case where we have a sample # it comes in as a list and we must iterate over the entire repeated list and process it if len ( space_likes ) > 1 : repeated_samples = space_likes [ 1 ] assert isinstance ( repeated_samples , MutableSequence ), \\ f 'repeated_samples must be MutableSequence, received { type ( repeated_samples ) . __name__ } ' for indx , sample in enumerate ( repeated_samples ): repeated_samples [ indx ] = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = ( first_space_like . child_space , sample ), return_space = return_space , * func_args , ** func_kwargs , ) val = repeated_samples else : new_child_space = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = ( first_space_like . child_space , ), return_space = return_space , * func_args , ** func_kwargs , ) val = Repeated ( child_space = new_child_space , max_len = first_space_like . max_len ) else : val = func ( space_likes , * func_args , ** func_kwargs ) return val normalize_space ( space , out_min =- 1 , out_max = 1 ) staticmethod \u00a4 This is a convenience wrapper for box_scaler Parameters \u00a4 gym.spaces.Space the gym space to turn all boxes into the scaled space float the new low for the boxes float the new high for the boxes Returns \u00a4 gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed Source code in corl/libraries/env_space_util.py @staticmethod def normalize_space ( space : gym . spaces . Space , out_min =- 1 , out_max = 1 , ) -> gym . spaces . Space : \"\"\" This is a convenience wrapper for box_scaler Parameters ---------- space: gym.spaces.Space the gym space to turn all boxes into the scaled space out_min: float the new low for the boxes out_max: float the new high for the boxes Returns ------- gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . space_box_min_maxer , space_likes = ( space , ), out_min = out_min , out_max = out_max , return_space = True , ) sanity_check_space_sample ( space , sample ) staticmethod \u00a4 [summary] Parameters \u00a4 space : [type] [description] sample : [type] [description] Raises \u00a4 RuntimeError [description] Source code in corl/libraries/env_space_util.py @staticmethod def sanity_check_space_sample ( space , sample ): \"\"\"[summary] Parameters ---------- space : [type] [description] sample : [type] [description] Raises ------ RuntimeError [description] \"\"\" if not space . contains ( sample ): raise RuntimeError ( f \"sample of { sample } does not meet space { space } setup\" ) scale_sample_from_space ( space , space_sample , out_min =- 1 , out_max = 1 ) staticmethod \u00a4 This is a convenience wrapper for box_scaler Parameters \u00a4 gym.spaces.Space the space to use for the input min and max sample_type the space sample to scale float the minimum of the output scaling float the maximum of the output scaling Returns \u00a4 Sample_type the scaled sample with min of out_min and max of out_max (this is in dicts and tuples the same as space_sample was) Source code in corl/libraries/env_space_util.py @staticmethod def scale_sample_from_space ( space : gym . spaces . Space , space_sample : sample_type , out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This is a convenience wrapper for box_scaler Parameters ---------- space: gym.spaces.Space the space to use for the input min and max space_sample: sample_type the space sample to scale out_min: float the minimum of the output scaling out_max: float the maximum of the output scaling Returns ------- sample_type: the scaled sample with min of out_min and max of out_max (this is in dicts and tuples the same as space_sample was) \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . box_scaler , space_likes = ( space , space_sample ), out_min = out_min , out_max = out_max , return_space = False , ) scale_space ( space , scale ) staticmethod \u00a4 Multiplies the low and high properties of all the Boxes in the given gym space by the scale input Parameters gym.spaces.Space the gym space to scale the Boxes of float what to multiply the Box low and high by Returns \u00a4 gym.spaces.Space the scaled gym space Source code in corl/libraries/env_space_util.py @staticmethod def scale_space ( space : gym . spaces . Space , scale : float ) -> gym . spaces . Space : \"\"\" Multiplies the low and high properties of all the Boxes in the given gym space by the scale input Parameters ---------- space: gym.spaces.Space the gym space to scale the Boxes of scale: float what to multiply the Box low and high by Returns ------- gym.spaces.Space the scaled gym space \"\"\" # TODO: this copy probably doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . scale_space ( value , scale = scale ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_thing = [ EnvSpaceUtil . scale_space ( sp , scale = scale ) for sp in space . spaces ] val = gym . spaces . Tuple ( tuple ( new_thing )) elif isinstance ( space , gym . spaces . Box ): scaled_box = gym . spaces . Box ( low = np . multiply ( space . low , scale ) . astype ( np . float32 ), high = np . multiply ( space . high , scale ) . astype ( np . float32 ), shape = space . shape , dtype = np . float32 ) val = scaled_box return val space_box_min_maxer ( space_likes , out_min =- 1.0 , out_max = 1.0 ) staticmethod \u00a4 Makes a gym box to the out_min and out_max range Parameters \u00a4 typing.Tuple[gym.spaces.Space] the gym space to turn all boxes into the scaled space float the new low for the boxes float the new high for the boxes Returns \u00a4 gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed Source code in corl/libraries/env_space_util.py @staticmethod def space_box_min_maxer ( space_likes : typing . Tuple [ gym . spaces . Space ], out_min : float = - 1.0 , out_max : float = 1.0 , ) -> gym . spaces . Space : \"\"\" Makes a gym box to the out_min and out_max range Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space] the gym space to turn all boxes into the scaled space out_min: float the new low for the boxes out_max: float the new high for the boxes Returns ------- gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed \"\"\" space_arg = space_likes [ 0 ] if isinstance ( space_arg , gym . spaces . Box ): return gym . spaces . Box ( low = out_min , high = out_max , shape = space_arg . shape , dtype = np . float32 ) return copy . deepcopy ( space_arg ) turn_box_into_tuple_of_discretes ( space , num_actions = 10 ) staticmethod \u00a4 Takes a gym space and replaces any Box types with MultiDiscrete types with the same number of possible of discrete as the box and each discrete has the same number of possible actions = num_actions Parameters space int, list, dict how many discrete actions to give to each possible discrete in the MultiDiscrete Returns \u00a4 gym.spaces.Space A gym space where all the Box types are replaced with MultiDiscrete types Source code in corl/libraries/env_space_util.py @staticmethod def turn_box_into_tuple_of_discretes ( space : gym . spaces . Space , num_actions : typing . Union [ int , typing . List [ int ], Iterable , dict ] = 10 ) -> gym . spaces . Space : \"\"\" Takes a gym space and replaces any Box types with MultiDiscrete types with the same number of possible of discrete as the box and each discrete has the same number of possible actions = num_actions Parameters ---------- space num_actions: int, list, dict how many discrete actions to give to each possible discrete in the MultiDiscrete Returns ------- gym.spaces.Space A gym space where all the Box types are replaced with MultiDiscrete types \"\"\" # first pass through the code num_actions is an int or list, this code turns it into an # iterator for the rest of the recursive calls if isinstance ( num_actions , int ): num_actions = repeat ( num_actions ) elif isinstance ( num_actions , ( list , tuple )): num_actions = iter ( num_actions ) # TODO: this copy doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space . spaces . items (): if isinstance ( num_actions , dict ): new_dict [ key ] = EnvSpaceUtil . turn_box_into_tuple_of_discretes ( space = space_value , num_actions = num_actions [ key ]) else : new_dict [ key ] = EnvSpaceUtil . turn_box_into_tuple_of_discretes ( space = space_value , num_actions = num_actions ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . turn_box_into_tuple_of_discretes ( sv , num_actions = num_actions ) for sv in space ] if len ( new_tuple ) == 1 and isinstance ( new_tuple [ 0 ], gym . spaces . Tuple ): val = new_tuple [ 0 ] else : val = gym . spaces . Tuple ( tuple ( new_tuple )) elif isinstance ( space , gym . spaces . Box ): assert isinstance ( num_actions , Iterator ), f 'num_actions must be iterator, received { type ( num_actions ) . __name__ } ' val = gym . spaces . Tuple ([ Discrete ( np . asarray ( next ( num_actions ))) for _ in range ( 0 , space . low . size )]) return val turn_discrete_action_back_to_cont ( original_space , discrete_only_space , space_sample ) staticmethod \u00a4 This is a convenience wrapper for turn_orig_space_box_to_cont_sample Parameters \u00a4 gym.spaces.Space the continuous space gym.spaces.Space the discrete space sample_type the sample of the discrete space Returns \u00a4 Sample_type the continuous version of the discrete sample Source code in corl/libraries/env_space_util.py @staticmethod def turn_discrete_action_back_to_cont ( original_space : gym . spaces . Space , discrete_only_space : gym . spaces . Space , space_sample : sample_type , ) -> sample_type : \"\"\" This is a convenience wrapper for turn_orig_space_box_to_cont_sample Parameters ---------- original_space: gym.spaces.Space the continuous space discrete_only_space: gym.spaces.Space the discrete space space_sample: sample_type the sample of the discrete space Returns ------- sample_type: the continuous version of the discrete sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( EnvSpaceUtil . turn_orig_space_box_to_cont_sample , space_likes = ( original_space , discrete_only_space , space_sample ), return_space = False , ) turn_discrete_action_back_to_cont_powerspace ( original_space , discrete_only_space , space_sample , pow_n ) staticmethod \u00a4 This is a convenience wrapper for turn_orig_space_box_to_cont_sample_powerspace Parameters \u00a4 gym.spaces.Space the continuous space gym.spaces.Space the discrete space sample_type the sample of the discrete space Returns \u00a4 Sample_type the continuous version of the discrete sample Source code in corl/libraries/env_space_util.py @staticmethod def turn_discrete_action_back_to_cont_powerspace ( original_space : gym . spaces . Space , discrete_only_space : gym . spaces . Space , space_sample : sample_type , pow_n : dict ) -> sample_type : \"\"\" This is a convenience wrapper for turn_orig_space_box_to_cont_sample_powerspace Parameters ---------- original_space: gym.spaces.Space the continuous space discrete_only_space: gym.spaces.Space the discrete space space_sample: sample_type the sample of the discrete space Returns ------- sample_type: the continuous version of the discrete sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( EnvSpaceUtil . turn_orig_space_box_to_cont_sample_powerspace , space_likes = ( original_space , discrete_only_space , space_sample , pow_n ), return_space = False , ) turn_orig_space_box_to_cont_sample ( space_likes ) staticmethod \u00a4 Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters \u00a4 typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns \u00a4 Sample_type the sample space that is now a continuous version of the discrete sample space according to the cont space Source code in corl/libraries/env_space_util.py @staticmethod def turn_orig_space_box_to_cont_sample ( space_likes : typing . Tuple [ gym . spaces . Space , gym . spaces . Space , sample_type ]) -> sample_type : \"\"\" Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns ------- sample_type: the sample space that is now a continuous version of the discrete sample space according to the cont space \"\"\" ( original_space_arg , discrete_only_space_arg , space_sample_arg ) = space_likes if isinstance ( original_space_arg , gym . spaces . Box ) and ( isinstance ( discrete_only_space_arg , ( gym . spaces . MultiDiscrete , gym . spaces . Discrete ))): if isinstance ( discrete_only_space_arg , gym . spaces . MultiDiscrete ): possible_n = discrete_only_space_arg . nvec elif isinstance ( discrete_only_space_arg , gym . spaces . Discrete ): possible_n = discrete_only_space_arg . n else : raise RuntimeError ( \"This should not be reachable\" ) action = space_sample_arg low = original_space_arg . low high = original_space_arg . high new_cont_sample = ( action / ( possible_n - 1 )) * ( high - low ) + low return new_cont_sample return copy . deepcopy ( space_sample_arg ) turn_orig_space_box_to_cont_sample_powerspace ( space_likes ) staticmethod \u00a4 Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters \u00a4 typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns \u00a4 Sample_type the sample space that is now a continuous version of the discrete sample space according to the cont space Source code in corl/libraries/env_space_util.py @staticmethod def turn_orig_space_box_to_cont_sample_powerspace ( space_likes : typing . Tuple [ gym . spaces . Space , gym . spaces . Space , sample_type , sample_type ] ) -> sample_type : \"\"\" Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns ------- sample_type: the sample space that is now a continuous version of the discrete sample space according to the cont space \"\"\" # There is a test built for this function. Please keep the test up to date for any changes you make. ( original_space_arg , discrete_only_space_arg , space_sample_arg , pow_n ) = space_likes if isinstance ( original_space_arg , gym . spaces . Box ) and ( isinstance ( discrete_only_space_arg , ( gym . spaces . Tuple , gym . spaces . Discrete ))): if isinstance ( discrete_only_space_arg , gym . spaces . Tuple ): possible_n = np . asarray ([ x . n for x in discrete_only_space_arg ]) pow_n = np . asarray ( pow_n ) discrete_sample = np . asarray ( space_sample_arg ) elif isinstance ( discrete_only_space_arg , gym . spaces . Discrete ): possible_n = np . asarray ([ discrete_only_space_arg . n ]) pow_n = np . asarray ([ pow_n ]) discrete_sample = np . asarray ([ space_sample_arg ]) else : raise RuntimeError ( \"This should not be reachable\" ) low = original_space_arg . low high = original_space_arg . high if any ( low > high ): raise RuntimeError ( \"lower bounds of space somehow higher than high bounds\" ) if not all ( possible_n % 2 ): if any ( pow_n > 1 ): raise RuntimeError ( \"The exponential power factor not supported when possible_n % 2\" ) difference = high - low movement_per_space_sample = difference / possible_n new_cont_sample = low + ( movement_per_space_sample * space_sample_arg ) return new_cont_sample if any ( pow_n <= 0 ): raise RuntimeError ( \"The exponential power factor used to stretch/shrink the discrete space must be greater than zero\" ) if any ( - low != high ) or any ( low >= high ): raise RuntimeError ( \"Currently only support symmetric space about zero for power space setup - TODO\" ) p_high = np . power ( high , 1 / pow_n . astype ( float )) p_low = - p_high # Since we have specified the space must be symetric around 0 new_cont_sample = ( discrete_sample / ( possible_n - 1 )) * ( p_high - p_low ) + p_low new_cont_sample = np . power ( np . abs ( new_cont_sample ), pow_n ) * np . sign ( new_cont_sample ) return new_cont_sample return copy . deepcopy ( space_sample_arg ) unscale_sample_from_space ( space , space_sample , out_min =- 1 , out_max = 1 ) staticmethod \u00a4 This is a convenience wrapper for box_unscaler Parameters \u00a4 gym.spaces.Space the gym space we will unscale to sample_type the sample we want to unscale. thus this is a scaled version of the input space with a min,max defined by the arguments out_min,out_max float the minimum of the sample float the maximum of the sample Returns \u00a4 Sample_type the unscaled version of the space_sample. Thus the space should now contain this output sample Source code in corl/libraries/env_space_util.py @staticmethod def unscale_sample_from_space ( space : gym . spaces . Space , space_sample : sample_type , out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This is a convenience wrapper for box_unscaler Parameters ---------- space: gym.spaces.Space the gym space we will unscale to space_sample: sample_type the sample we want to unscale. thus this is a scaled version of the input space with a min,max defined by the arguments out_min,out_max out_min: float the minimum of the sample out_max: float the maximum of the sample Returns ------- sample_type: the unscaled version of the space_sample. Thus the space should now contain this output sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . box_unscaler , space_likes = ( space , space_sample ), out_min = out_min , out_max = out_max , return_space = False , ) zero_mean_space ( space ) staticmethod \u00a4 Returns a space object where every Box instance has its low and high shifted to be zero mean Parameters gym.spaces.Space The gym space to zero mean Returns \u00a4 gym.spaces.Space A gym space the same as the input but with the Box instances shifted Source code in corl/libraries/env_space_util.py @staticmethod def zero_mean_space ( space : gym . spaces . Space ) -> gym . spaces . Space : \"\"\" Returns a space object where every Box instance has its low and high shifted to be zero mean Parameters ---------- space: gym.spaces.Space The gym space to zero mean Returns ------- gym.spaces.Space A gym space the same as the input but with the Box instances shifted \"\"\" # TODO: this copy doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . zero_mean_space ( value ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_thing = [ EnvSpaceUtil . zero_mean_space ( sp ) for sp in space . spaces ] val = gym . spaces . Tuple ( tuple ( new_thing )) elif isinstance ( space , gym . spaces . Box ): mean = ( space . high + space . low ) / 2 zero_mean_box = gym . spaces . Box ( low = space . low - mean , high = space . high - mean , shape = space . shape , dtype = np . float32 ) val = zero_mean_box return val","title":"Env space util"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil","text":"ENV Space Util Source code in corl/libraries/env_space_util.py class EnvSpaceUtil : # pylint: disable=R0904 \"\"\" ENV Space Util \"\"\" _logger = logging . getLogger ( \"EnvSpaceUtil\" ) sample_type = typing . Union [ OrderedDict , dict , tuple , np . ndarray , list ] @staticmethod def deep_sanity_check_space_sample ( # pylint: disable=R0912 space : gym . spaces . Space , sample : sample_type , key_stack : str = \"\" , ) -> None : \"\"\"Ensure space sample is consistent with space. This will give a traceback of the exact space that failed Parameters ---------- space: gym.spaces.Dict the space we expect the sample to conform to sample: sample_type sample that we are checking if it belongs to the given space key_stack: str string of the keys we have used when getting to this current spot in the observation_space, observation this is used for recursive calls do not set in the initial call to this function \"\"\" if isinstance ( space , gym . spaces . Dict ): if not isinstance ( sample , ( StateDict , OrderedDict , dict )): raise ValueError ( f \"space { key_stack } = { space } was a gym.spaces.Dict type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a StateDict, OrderedDict, or dict\" ) for key , value in sample . items (): EnvSpaceUtil . deep_sanity_check_space_sample ( space . spaces [ key ], value , key_stack = f \" { key_stack } [ { key } ]\" ) elif isinstance ( space , gym . spaces . Tuple ): if not isinstance ( sample , tuple ): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Tuple type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a tuple type\" ) for idx , value in enumerate ( sample ): EnvSpaceUtil . deep_sanity_check_space_sample ( space . spaces [ idx ], value , key_stack = f \" { key_stack } [ { idx } ]\" ) elif isinstance ( space , gym . spaces . Discrete ): if not isinstance ( sample , ( int , np . integer , np . ndarray )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Discrete type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not an int or np.integer or np.ndarray\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . n } \" ) elif isinstance ( space , gym . spaces . Box ): if not isinstance ( sample , ( np . ndarray , list , np . floating )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Box type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.float type\" ) if not space . contains ( sample ): sample_dtype = getattr ( sample , 'dtype' , None ) raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . low } { space . high } \" f \"space dtype is { space . dtype } , sample dtype is { sample_dtype } \" ) elif isinstance ( space , gym . spaces . MultiBinary ): if not isinstance ( sample , ( np . ndarray , list , np . integer )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.MultiBinary type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.integer type\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . n } \" ) elif isinstance ( space , gym . spaces . MultiDiscrete ): if not isinstance ( sample , ( np . ndarray , list , np . integer )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.MultiDiscrete type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.integer type\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . nvec } \" ) elif isinstance ( space , Repeated ): if not isinstance ( sample , list ): raise ValueError ( f \"space { key_stack } = { space } is a ray.rllib.utils.spaces.repeated.Repeated type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a list type\" ) for idx , item in enumerate ( sample ): EnvSpaceUtil . deep_sanity_check_space_sample ( space . child_space , item , key_stack = f \" { key_stack } [ { idx } ]\" ) @staticmethod def sanity_check_space_sample ( space , sample ): \"\"\"[summary] Parameters ---------- space : [type] [description] sample : [type] [description] Raises ------ RuntimeError [description] \"\"\" if not space . contains ( sample ): raise RuntimeError ( f \"sample of { sample } does not meet space { space } setup\" ) @staticmethod def deep_merge_dict ( source : dict , destination : dict ): \"\"\" Merget two dictionaries that also can contain sub dictionaries. This function returns the second dict but it also modifies it in place run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , OrderedDict ()) EnvSpaceUtil . deep_merge_dict ( value , node ) else : destination [ key ] = value return destination @staticmethod def scale_space ( space : gym . spaces . Space , scale : float ) -> gym . spaces . Space : \"\"\" Multiplies the low and high properties of all the Boxes in the given gym space by the scale input Parameters ---------- space: gym.spaces.Space the gym space to scale the Boxes of scale: float what to multiply the Box low and high by Returns ------- gym.spaces.Space the scaled gym space \"\"\" # TODO: this copy probably doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . scale_space ( value , scale = scale ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_thing = [ EnvSpaceUtil . scale_space ( sp , scale = scale ) for sp in space . spaces ] val = gym . spaces . Tuple ( tuple ( new_thing )) elif isinstance ( space , gym . spaces . Box ): scaled_box = gym . spaces . Box ( low = np . multiply ( space . low , scale ) . astype ( np . float32 ), high = np . multiply ( space . high , scale ) . astype ( np . float32 ), shape = space . shape , dtype = np . float32 ) val = scaled_box return val @staticmethod def zero_mean_space ( space : gym . spaces . Space ) -> gym . spaces . Space : \"\"\" Returns a space object where every Box instance has its low and high shifted to be zero mean Parameters ---------- space: gym.spaces.Space The gym space to zero mean Returns ------- gym.spaces.Space A gym space the same as the input but with the Box instances shifted \"\"\" # TODO: this copy doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . zero_mean_space ( value ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_thing = [ EnvSpaceUtil . zero_mean_space ( sp ) for sp in space . spaces ] val = gym . spaces . Tuple ( tuple ( new_thing )) elif isinstance ( space , gym . spaces . Box ): mean = ( space . high + space . low ) / 2 zero_mean_box = gym . spaces . Box ( low = space . low - mean , high = space . high - mean , shape = space . shape , dtype = np . float32 ) val = zero_mean_box return val @staticmethod def space_box_min_maxer ( space_likes : typing . Tuple [ gym . spaces . Space ], out_min : float = - 1.0 , out_max : float = 1.0 , ) -> gym . spaces . Space : \"\"\" Makes a gym box to the out_min and out_max range Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space] the gym space to turn all boxes into the scaled space out_min: float the new low for the boxes out_max: float the new high for the boxes Returns ------- gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed \"\"\" space_arg = space_likes [ 0 ] if isinstance ( space_arg , gym . spaces . Box ): return gym . spaces . Box ( low = out_min , high = out_max , shape = space_arg . shape , dtype = np . float32 ) return copy . deepcopy ( space_arg ) @staticmethod def normalize_space ( space : gym . spaces . Space , out_min =- 1 , out_max = 1 , ) -> gym . spaces . Space : \"\"\" This is a convenience wrapper for box_scaler Parameters ---------- space: gym.spaces.Space the gym space to turn all boxes into the scaled space out_min: float the new low for the boxes out_max: float the new high for the boxes Returns ------- gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . space_box_min_maxer , space_likes = ( space , ), out_min = out_min , out_max = out_max , return_space = True , ) @staticmethod def get_zero_sample_from_space ( space : gym . spaces . Space ) -> sample_type : \"\"\" Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters ---------- space: gym.spaces.Space The gym space to zero sample from Returns ------- sample_type The instance of the gym space but all Box spaces are sampled as zero \"\"\" val = space . sample () if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . get_zero_sample_from_space ( value ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . get_zero_sample_from_space ( sp ) for sp in space . spaces ] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): val = np . zeros ( shape = space . shape , dtype = np . float32 ) return val @staticmethod def get_mean_sample_from_space ( space : gym . spaces . Space ) -> sample_type : \"\"\" Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters ---------- space: gym.spaces.Space The gym space to zero sample from Returns ------- sample_type The instance of the gym space but all Box spaces are sampled as zero \"\"\" val = space . sample () if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . get_zero_sample_from_space ( value ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . get_zero_sample_from_space ( sp ) for sp in space . spaces ] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): val = ( space . high + space . low ) / 2.0 return val @staticmethod def add_space_samples ( space_template : gym . spaces . Space , space_sample1 : sample_type , space_sample2 : sample_type , ) -> sample_type : \"\"\" Adds together two instances of gym spaces. This only adds the ndarray or list types (that were sampled from Box) If the object is not a ndarray or list then the value of space_sample1 is returned by default Parameters ---------- space_template: gym.spaces.Space The template to use for adding these space instances. This is to determine the difference between a Box and Discrete or MultiDiscrete or MultiBinary space_sample1: sample_type The first instance to add space_sample2: sample_type The second instance to add Returns ------- sample_type an instance of the space object but with all the Box types added \"\"\" # if not type(space_sample1) == type(space_sample2): # raise ValueError('space instances must be of same type') # TODO: I want to check they are the same type but dict and OrderedDict should match which makes this annoying val : EnvSpaceUtil . sample_type if isinstance ( space_template , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space_template . spaces . items (): value1 = space_sample1 [ key ] value2 = space_sample2 [ key ] new_dict [ key ] = EnvSpaceUtil . add_space_samples ( space_value , value1 , value2 ) val = new_dict elif isinstance ( space_template , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . add_space_samples ( * args ) for args in zip ( space_template , space_sample1 , space_sample2 )] val = tuple ( new_tuple ) elif isinstance ( space_template , gym . spaces . Box ): if isinstance ( space_sample1 , np . ndarray ): val = np . array ( space_sample1 + space_sample2 ) elif isinstance ( space_sample1 , list ): val = [ value1 + value2 for value1 , value2 in zip ( space_sample1 , space_sample2 )] else : val = copy . deepcopy ( space_sample1 ) return val @staticmethod def clip_space_sample_to_space ( space_sample : sample_type , space : gym . spaces . Space , is_wrap : bool = False ) -> sample_type : \"\"\" Clips a space instance to a given space. After this the space should contain the space instance Parameters ---------- space_sample: sample_type the space instance we are going to clip space: gym.spaces.Space the gym space to clip the instance to. Returns ------- sample_type the clipped space instance \"\"\" val = copy . deepcopy ( space_sample ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space . spaces . items (): space_sample_value = space_sample [ key ] new_dict [ key ] = EnvSpaceUtil . clip_space_sample_to_space ( space_sample_value , space_value , is_wrap ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . clip_space_sample_to_space ( siv , sv , is_wrap ) for siv , sv in zip ( space_sample , space )] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): if is_wrap : # Takes care of the case where the controls wrap at the min/max value # Example: Range = 0-1, value = 1.1 ----> clipping puts to .1 if space_sample > space . high : val = space . low + ( space_sample - space . high ) elif space_sample < space . low : val = space . high - ( space . low - space_sample ) else : # Takes care of the case where the controls saturate at the min/max value # Example: Range = 0-1, value = 1.1 ----> clipping puts to 1 assert isinstance ( space_sample , ( Sequence , np . ndarray )), \\ f 'Box spaces must have sequence samples, received { type ( space_sample ) . __name__ } ' val = np . clip ( a = space_sample , a_min = space . low , a_max = space . high ) return val @staticmethod def turn_box_into_tuple_of_discretes ( space : gym . spaces . Space , num_actions : typing . Union [ int , typing . List [ int ], Iterable , dict ] = 10 ) -> gym . spaces . Space : \"\"\" Takes a gym space and replaces any Box types with MultiDiscrete types with the same number of possible of discrete as the box and each discrete has the same number of possible actions = num_actions Parameters ---------- space num_actions: int, list, dict how many discrete actions to give to each possible discrete in the MultiDiscrete Returns ------- gym.spaces.Space A gym space where all the Box types are replaced with MultiDiscrete types \"\"\" # first pass through the code num_actions is an int or list, this code turns it into an # iterator for the rest of the recursive calls if isinstance ( num_actions , int ): num_actions = repeat ( num_actions ) elif isinstance ( num_actions , ( list , tuple )): num_actions = iter ( num_actions ) # TODO: this copy doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space . spaces . items (): if isinstance ( num_actions , dict ): new_dict [ key ] = EnvSpaceUtil . turn_box_into_tuple_of_discretes ( space = space_value , num_actions = num_actions [ key ]) else : new_dict [ key ] = EnvSpaceUtil . turn_box_into_tuple_of_discretes ( space = space_value , num_actions = num_actions ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . turn_box_into_tuple_of_discretes ( sv , num_actions = num_actions ) for sv in space ] if len ( new_tuple ) == 1 and isinstance ( new_tuple [ 0 ], gym . spaces . Tuple ): val = new_tuple [ 0 ] else : val = gym . spaces . Tuple ( tuple ( new_tuple )) elif isinstance ( space , gym . spaces . Box ): assert isinstance ( num_actions , Iterator ), f 'num_actions must be iterator, received { type ( num_actions ) . __name__ } ' val = gym . spaces . Tuple ([ Discrete ( np . asarray ( next ( num_actions ))) for _ in range ( 0 , space . low . size )]) return val # TODO: maybe use this function in more places? maybe not? it could be slower? @staticmethod def iterate_over_space_likes ( func , space_likes : typing . Tuple [ typing . Union [ gym . spaces . Space , sample_type ], ... ], return_space : bool , * func_args , ** func_kwargs , ) -> typing . Union [ gym . spaces . Space , sample_type ]: \"\"\" Iterates over space_likes which are tuple, dicts or the gym equivalent. When it encounters an actual item that is not a container it calls the func method. put any args, or kwargs you want to give to func in the overall call and they will be forwarded Parameters ---------- func: the function to apply space_likes: typing.Tuple[typing.Union[gym.spaces.Space, sample_type], ...] the spaces to iterate over. They must have the same keywords for dicts and number of items for tuples return_space: bool if true the containers will be gym space equivalents func_args the arguments to give to func func_kwargs the keyword arguments to give to func Returns ------- The contained result by calling func and stuffing back into the tuples and dicts in the call if return_space=True the containers are gym spaces \"\"\" first_space_like = space_likes [ 0 ] val = None if isinstance ( first_space_like , ( gym . spaces . Dict , dict , OrderedDict )): new_dict = OrderedDict () keys : typing . KeysView if isinstance ( first_space_like , gym . spaces . Dict ): keys = first_space_like . spaces . keys () else : keys = first_space_like . keys () for key in keys : new_space_likes = tuple ( spacer [ key ] for spacer in space_likes ) new_dict [ key ] = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = new_space_likes , return_space = return_space , * func_args , ** func_kwargs , ) val = gym . spaces . Dict ( spaces = new_dict ) if return_space else new_dict elif isinstance ( first_space_like , ( gym . spaces . Tuple , tuple )): new_tuple = [ EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = new_space_likes , return_space = return_space , * func_args , ** func_kwargs , ) for new_space_likes in zip ( * space_likes ) ] val = ( gym . spaces . Tuple ( tuple ( new_tuple )) if return_space else tuple ( new_tuple )) elif isinstance ( first_space_like , Repeated ): # if space_likes is longer than 1 that means that return_space = False # if there is only the space that means we need to generate just the space # itself and can use the second path, however for the case where we have a sample # it comes in as a list and we must iterate over the entire repeated list and process it if len ( space_likes ) > 1 : repeated_samples = space_likes [ 1 ] assert isinstance ( repeated_samples , MutableSequence ), \\ f 'repeated_samples must be MutableSequence, received { type ( repeated_samples ) . __name__ } ' for indx , sample in enumerate ( repeated_samples ): repeated_samples [ indx ] = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = ( first_space_like . child_space , sample ), return_space = return_space , * func_args , ** func_kwargs , ) val = repeated_samples else : new_child_space = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = ( first_space_like . child_space , ), return_space = return_space , * func_args , ** func_kwargs , ) val = Repeated ( child_space = new_child_space , max_len = first_space_like . max_len ) else : val = func ( space_likes , * func_args , ** func_kwargs ) return val @staticmethod def turn_orig_space_box_to_cont_sample ( space_likes : typing . Tuple [ gym . spaces . Space , gym . spaces . Space , sample_type ]) -> sample_type : \"\"\" Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns ------- sample_type: the sample space that is now a continuous version of the discrete sample space according to the cont space \"\"\" ( original_space_arg , discrete_only_space_arg , space_sample_arg ) = space_likes if isinstance ( original_space_arg , gym . spaces . Box ) and ( isinstance ( discrete_only_space_arg , ( gym . spaces . MultiDiscrete , gym . spaces . Discrete ))): if isinstance ( discrete_only_space_arg , gym . spaces . MultiDiscrete ): possible_n = discrete_only_space_arg . nvec elif isinstance ( discrete_only_space_arg , gym . spaces . Discrete ): possible_n = discrete_only_space_arg . n else : raise RuntimeError ( \"This should not be reachable\" ) action = space_sample_arg low = original_space_arg . low high = original_space_arg . high new_cont_sample = ( action / ( possible_n - 1 )) * ( high - low ) + low return new_cont_sample return copy . deepcopy ( space_sample_arg ) @staticmethod def turn_orig_space_box_to_cont_sample_powerspace ( space_likes : typing . Tuple [ gym . spaces . Space , gym . spaces . Space , sample_type , sample_type ] ) -> sample_type : \"\"\" Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns ------- sample_type: the sample space that is now a continuous version of the discrete sample space according to the cont space \"\"\" # There is a test built for this function. Please keep the test up to date for any changes you make. ( original_space_arg , discrete_only_space_arg , space_sample_arg , pow_n ) = space_likes if isinstance ( original_space_arg , gym . spaces . Box ) and ( isinstance ( discrete_only_space_arg , ( gym . spaces . Tuple , gym . spaces . Discrete ))): if isinstance ( discrete_only_space_arg , gym . spaces . Tuple ): possible_n = np . asarray ([ x . n for x in discrete_only_space_arg ]) pow_n = np . asarray ( pow_n ) discrete_sample = np . asarray ( space_sample_arg ) elif isinstance ( discrete_only_space_arg , gym . spaces . Discrete ): possible_n = np . asarray ([ discrete_only_space_arg . n ]) pow_n = np . asarray ([ pow_n ]) discrete_sample = np . asarray ([ space_sample_arg ]) else : raise RuntimeError ( \"This should not be reachable\" ) low = original_space_arg . low high = original_space_arg . high if any ( low > high ): raise RuntimeError ( \"lower bounds of space somehow higher than high bounds\" ) if not all ( possible_n % 2 ): if any ( pow_n > 1 ): raise RuntimeError ( \"The exponential power factor not supported when possible_n % 2\" ) difference = high - low movement_per_space_sample = difference / possible_n new_cont_sample = low + ( movement_per_space_sample * space_sample_arg ) return new_cont_sample if any ( pow_n <= 0 ): raise RuntimeError ( \"The exponential power factor used to stretch/shrink the discrete space must be greater than zero\" ) if any ( - low != high ) or any ( low >= high ): raise RuntimeError ( \"Currently only support symmetric space about zero for power space setup - TODO\" ) p_high = np . power ( high , 1 / pow_n . astype ( float )) p_low = - p_high # Since we have specified the space must be symetric around 0 new_cont_sample = ( discrete_sample / ( possible_n - 1 )) * ( p_high - p_low ) + p_low new_cont_sample = np . power ( np . abs ( new_cont_sample ), pow_n ) * np . sign ( new_cont_sample ) return new_cont_sample return copy . deepcopy ( space_sample_arg ) @staticmethod def turn_discrete_action_back_to_cont ( original_space : gym . spaces . Space , discrete_only_space : gym . spaces . Space , space_sample : sample_type , ) -> sample_type : \"\"\" This is a convenience wrapper for turn_orig_space_box_to_cont_sample Parameters ---------- original_space: gym.spaces.Space the continuous space discrete_only_space: gym.spaces.Space the discrete space space_sample: sample_type the sample of the discrete space Returns ------- sample_type: the continuous version of the discrete sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( EnvSpaceUtil . turn_orig_space_box_to_cont_sample , space_likes = ( original_space , discrete_only_space , space_sample ), return_space = False , ) @staticmethod def turn_discrete_action_back_to_cont_powerspace ( original_space : gym . spaces . Space , discrete_only_space : gym . spaces . Space , space_sample : sample_type , pow_n : dict ) -> sample_type : \"\"\" This is a convenience wrapper for turn_orig_space_box_to_cont_sample_powerspace Parameters ---------- original_space: gym.spaces.Space the continuous space discrete_only_space: gym.spaces.Space the discrete space space_sample: sample_type the sample of the discrete space Returns ------- sample_type: the continuous version of the discrete sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( EnvSpaceUtil . turn_orig_space_box_to_cont_sample_powerspace , space_likes = ( original_space , discrete_only_space , space_sample , pow_n ), return_space = False , ) @staticmethod def box_scaler ( space_likes : typing . Tuple [ gym . spaces . Space , sample_type ], out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This scales a box space to be between the out_min and out_max arguments Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale out_min: float the minimum of the output scaling out_max: float the maximum of the output scaling Returns ------- sample_type: the scaled sample with min of out_min and max of out_max \"\"\" ( space_arg , space_sample_arg ) = space_likes if isinstance ( space_arg , gym . spaces . Box ) and space_arg . is_bounded (): val = space_sample_arg in_min = space_arg . low in_max = space_arg . high norm_value = ( out_max - out_min ) * ( val - in_min ) / ( in_max - in_min ) + out_min return norm_value . astype ( np . float32 ) return copy . deepcopy ( space_sample_arg ) @staticmethod def scale_sample_from_space ( space : gym . spaces . Space , space_sample : sample_type , out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This is a convenience wrapper for box_scaler Parameters ---------- space: gym.spaces.Space the space to use for the input min and max space_sample: sample_type the space sample to scale out_min: float the minimum of the output scaling out_max: float the maximum of the output scaling Returns ------- sample_type: the scaled sample with min of out_min and max of out_max (this is in dicts and tuples the same as space_sample was) \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . box_scaler , space_likes = ( space , space_sample ), out_min = out_min , out_max = out_max , return_space = False , ) @staticmethod def box_unscaler ( space_likes : typing . Tuple [ gym . spaces . Space , sample_type ], out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" Unscales the space_sample according to be the scale of the input space. In this sense out_min and out_max are the min max of the sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale out_min: float the minimum of the sample out_max: float the maximum of the sample Returns ------- space_type: the unscaled sample \"\"\" ( space_arg , space_sample_arg ) = space_likes if isinstance ( space_arg , gym . spaces . Box ): norm_value = space_sample_arg assert isinstance ( norm_value , np . ndarray ), f 'norm_value must be np.ndarray, received { type ( norm_value ) . __name__ } ' in_min = space_arg . low in_max = space_arg . high val = ( norm_value - out_min ) * ( in_max - in_min ) / ( out_max - out_min ) + in_min return val return copy . deepcopy ( space_sample_arg ) @staticmethod def unscale_sample_from_space ( space : gym . spaces . Space , space_sample : sample_type , out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This is a convenience wrapper for box_unscaler Parameters ---------- space: gym.spaces.Space the gym space we will unscale to space_sample: sample_type the sample we want to unscale. thus this is a scaled version of the input space with a min,max defined by the arguments out_min,out_max out_min: float the minimum of the sample out_max: float the maximum of the sample Returns ------- sample_type: the unscaled version of the space_sample. Thus the space should now contain this output sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . box_unscaler , space_likes = ( space , space_sample ), out_min = out_min , out_max = out_max , return_space = False , ) @staticmethod def convert_config_param_to_space ( action_space : gym . spaces . Space , parameter : typing . Union [ int , float , list , dict ]) -> dict : \"\"\" This is a a convert for parameters used in action space conversions Parameters ---------- space: gym.spaces.Space the gym space that defines the actions parameter: typing.Union[int, float, list, dict] the parameter as defined in a a config Returns ------- dict: actions are dicts, the output is a dict with the parameters set for each key \"\"\" action_params = {} sample_action = action_space . sample () . items () if isinstance ( parameter , ( int , float )): parameter = [ parameter ] # change to list if needed if len ( parameter ) == 1 : # if length 1, broadcast to the proper length for key , value in sample_action : action_params [ key ] = parameter * len ( value ) elif isinstance ( parameter , list ): if len ( sample_action ) != 1 : raise ValueError ( f \"list configs can only be applied to action space dicts with single key: { sample_action } \" ) for key , value in sample_action : if len ( value ) != len ( parameter ): raise ValueError ( f \"config length does not match action length of { len ( value ) } . { parameter } \" ) action_params [ key ] = parameter elif isinstance ( parameter , dict ): # pylint: disable=too-many-nested-blocks for key , value in sample_action : if key not in parameter : if isinstance ( key , tuple ): # parameters may be specfied using tuple values as keys action_params [ key ] = np . zeros ( len ( key )) # type: ignore[assignment] for idx , sub_key in enumerate ( key ): if sub_key not in parameter : raise ValueError ( f \"action space key not in config key: { sub_key } , config: { parameter } \" ) action_params [ key ][ idx ] = parameter [ sub_key ] else : raise ValueError ( f \"action space key not in config key: { key } , config: { parameter } \" ) elif len ( value ) != len ( parameter [ key ]): raise ValueError ( f \"config value length for key { key } does not match action length of { len ( value ) } . { parameter } \" ) else : action_params [ key ] = parameter [ key ] return action_params","title":"EnvSpaceUtil"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.add_space_samples","text":"Adds together two instances of gym spaces. This only adds the ndarray or list types (that were sampled from Box) If the object is not a ndarray or list then the value of space_sample1 is returned by default Parameters gym.spaces.Space The template to use for adding these space instances. This is to determine the difference between a Box and Discrete or MultiDiscrete or MultiBinary sample_type The first instance to add sample_type The second instance to add","title":"add_space_samples()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.add_space_samples--returns","text":"sample_type an instance of the space object but with all the Box types added Source code in corl/libraries/env_space_util.py @staticmethod def add_space_samples ( space_template : gym . spaces . Space , space_sample1 : sample_type , space_sample2 : sample_type , ) -> sample_type : \"\"\" Adds together two instances of gym spaces. This only adds the ndarray or list types (that were sampled from Box) If the object is not a ndarray or list then the value of space_sample1 is returned by default Parameters ---------- space_template: gym.spaces.Space The template to use for adding these space instances. This is to determine the difference between a Box and Discrete or MultiDiscrete or MultiBinary space_sample1: sample_type The first instance to add space_sample2: sample_type The second instance to add Returns ------- sample_type an instance of the space object but with all the Box types added \"\"\" # if not type(space_sample1) == type(space_sample2): # raise ValueError('space instances must be of same type') # TODO: I want to check they are the same type but dict and OrderedDict should match which makes this annoying val : EnvSpaceUtil . sample_type if isinstance ( space_template , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space_template . spaces . items (): value1 = space_sample1 [ key ] value2 = space_sample2 [ key ] new_dict [ key ] = EnvSpaceUtil . add_space_samples ( space_value , value1 , value2 ) val = new_dict elif isinstance ( space_template , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . add_space_samples ( * args ) for args in zip ( space_template , space_sample1 , space_sample2 )] val = tuple ( new_tuple ) elif isinstance ( space_template , gym . spaces . Box ): if isinstance ( space_sample1 , np . ndarray ): val = np . array ( space_sample1 + space_sample2 ) elif isinstance ( space_sample1 , list ): val = [ value1 + value2 for value1 , value2 in zip ( space_sample1 , space_sample2 )] else : val = copy . deepcopy ( space_sample1 ) return val","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.box_scaler","text":"This scales a box space to be between the out_min and out_max arguments","title":"box_scaler()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.box_scaler--parameters","text":"typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale float the minimum of the output scaling float the maximum of the output scaling","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.box_scaler--returns","text":"Sample_type the scaled sample with min of out_min and max of out_max Source code in corl/libraries/env_space_util.py @staticmethod def box_scaler ( space_likes : typing . Tuple [ gym . spaces . Space , sample_type ], out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This scales a box space to be between the out_min and out_max arguments Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale out_min: float the minimum of the output scaling out_max: float the maximum of the output scaling Returns ------- sample_type: the scaled sample with min of out_min and max of out_max \"\"\" ( space_arg , space_sample_arg ) = space_likes if isinstance ( space_arg , gym . spaces . Box ) and space_arg . is_bounded (): val = space_sample_arg in_min = space_arg . low in_max = space_arg . high norm_value = ( out_max - out_min ) * ( val - in_min ) / ( in_max - in_min ) + out_min return norm_value . astype ( np . float32 ) return copy . deepcopy ( space_sample_arg )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.box_unscaler","text":"Unscales the space_sample according to be the scale of the input space. In this sense out_min and out_max are the min max of the sample","title":"box_unscaler()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.box_unscaler--parameters","text":"typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale float the minimum of the sample float the maximum of the sample","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.box_unscaler--returns","text":"Space_type the unscaled sample Source code in corl/libraries/env_space_util.py @staticmethod def box_unscaler ( space_likes : typing . Tuple [ gym . spaces . Space , sample_type ], out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" Unscales the space_sample according to be the scale of the input space. In this sense out_min and out_max are the min max of the sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, sample_type] the first is the gym spade to determine the input min and max the second is the sample of this space to scale out_min: float the minimum of the sample out_max: float the maximum of the sample Returns ------- space_type: the unscaled sample \"\"\" ( space_arg , space_sample_arg ) = space_likes if isinstance ( space_arg , gym . spaces . Box ): norm_value = space_sample_arg assert isinstance ( norm_value , np . ndarray ), f 'norm_value must be np.ndarray, received { type ( norm_value ) . __name__ } ' in_min = space_arg . low in_max = space_arg . high val = ( norm_value - out_min ) * ( in_max - in_min ) / ( out_max - out_min ) + in_min return val return copy . deepcopy ( space_sample_arg )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.clip_space_sample_to_space","text":"Clips a space instance to a given space. After this the space should contain the space instance","title":"clip_space_sample_to_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.clip_space_sample_to_space--parameters","text":"sample_type the space instance we are going to clip gym.spaces.Space the gym space to clip the instance to.","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.clip_space_sample_to_space--returns","text":"sample_type the clipped space instance Source code in corl/libraries/env_space_util.py @staticmethod def clip_space_sample_to_space ( space_sample : sample_type , space : gym . spaces . Space , is_wrap : bool = False ) -> sample_type : \"\"\" Clips a space instance to a given space. After this the space should contain the space instance Parameters ---------- space_sample: sample_type the space instance we are going to clip space: gym.spaces.Space the gym space to clip the instance to. Returns ------- sample_type the clipped space instance \"\"\" val = copy . deepcopy ( space_sample ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space . spaces . items (): space_sample_value = space_sample [ key ] new_dict [ key ] = EnvSpaceUtil . clip_space_sample_to_space ( space_sample_value , space_value , is_wrap ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . clip_space_sample_to_space ( siv , sv , is_wrap ) for siv , sv in zip ( space_sample , space )] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): if is_wrap : # Takes care of the case where the controls wrap at the min/max value # Example: Range = 0-1, value = 1.1 ----> clipping puts to .1 if space_sample > space . high : val = space . low + ( space_sample - space . high ) elif space_sample < space . low : val = space . high - ( space . low - space_sample ) else : # Takes care of the case where the controls saturate at the min/max value # Example: Range = 0-1, value = 1.1 ----> clipping puts to 1 assert isinstance ( space_sample , ( Sequence , np . ndarray )), \\ f 'Box spaces must have sequence samples, received { type ( space_sample ) . __name__ } ' val = np . clip ( a = space_sample , a_min = space . low , a_max = space . high ) return val","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.convert_config_param_to_space","text":"This is a a convert for parameters used in action space conversions","title":"convert_config_param_to_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.convert_config_param_to_space--parameters","text":"gym.spaces.Space the gym space that defines the actions typing.Union[int, float, list, dict] the parameter as defined in a a config","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.convert_config_param_to_space--returns","text":"Dict actions are dicts, the output is a dict with the parameters set for each key Source code in corl/libraries/env_space_util.py @staticmethod def convert_config_param_to_space ( action_space : gym . spaces . Space , parameter : typing . Union [ int , float , list , dict ]) -> dict : \"\"\" This is a a convert for parameters used in action space conversions Parameters ---------- space: gym.spaces.Space the gym space that defines the actions parameter: typing.Union[int, float, list, dict] the parameter as defined in a a config Returns ------- dict: actions are dicts, the output is a dict with the parameters set for each key \"\"\" action_params = {} sample_action = action_space . sample () . items () if isinstance ( parameter , ( int , float )): parameter = [ parameter ] # change to list if needed if len ( parameter ) == 1 : # if length 1, broadcast to the proper length for key , value in sample_action : action_params [ key ] = parameter * len ( value ) elif isinstance ( parameter , list ): if len ( sample_action ) != 1 : raise ValueError ( f \"list configs can only be applied to action space dicts with single key: { sample_action } \" ) for key , value in sample_action : if len ( value ) != len ( parameter ): raise ValueError ( f \"config length does not match action length of { len ( value ) } . { parameter } \" ) action_params [ key ] = parameter elif isinstance ( parameter , dict ): # pylint: disable=too-many-nested-blocks for key , value in sample_action : if key not in parameter : if isinstance ( key , tuple ): # parameters may be specfied using tuple values as keys action_params [ key ] = np . zeros ( len ( key )) # type: ignore[assignment] for idx , sub_key in enumerate ( key ): if sub_key not in parameter : raise ValueError ( f \"action space key not in config key: { sub_key } , config: { parameter } \" ) action_params [ key ][ idx ] = parameter [ sub_key ] else : raise ValueError ( f \"action space key not in config key: { key } , config: { parameter } \" ) elif len ( value ) != len ( parameter [ key ]): raise ValueError ( f \"config value length for key { key } does not match action length of { len ( value ) } . { parameter } \" ) else : action_params [ key ] = parameter [ key ] return action_params","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.deep_merge_dict","text":"Merget two dictionaries that also can contain sub dictionaries. This function returns the second dict but it also modifies it in place run me with nosetests --with-doctest file.py a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True Source code in corl/libraries/env_space_util.py @staticmethod def deep_merge_dict ( source : dict , destination : dict ): \"\"\" Merget two dictionaries that also can contain sub dictionaries. This function returns the second dict but it also modifies it in place run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , OrderedDict ()) EnvSpaceUtil . deep_merge_dict ( value , node ) else : destination [ key ] = value return destination","title":"deep_merge_dict()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.deep_sanity_check_space_sample","text":"Ensure space sample is consistent with space. This will give a traceback of the exact space that failed","title":"deep_sanity_check_space_sample()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.deep_sanity_check_space_sample--parameters","text":"gym.spaces.Dict the space we expect the sample to conform to sample_type sample that we are checking if it belongs to the given space str string of the keys we have used when getting to this current spot in the observation_space, observation this is used for recursive calls do not set in the initial call to this function Source code in corl/libraries/env_space_util.py @staticmethod def deep_sanity_check_space_sample ( # pylint: disable=R0912 space : gym . spaces . Space , sample : sample_type , key_stack : str = \"\" , ) -> None : \"\"\"Ensure space sample is consistent with space. This will give a traceback of the exact space that failed Parameters ---------- space: gym.spaces.Dict the space we expect the sample to conform to sample: sample_type sample that we are checking if it belongs to the given space key_stack: str string of the keys we have used when getting to this current spot in the observation_space, observation this is used for recursive calls do not set in the initial call to this function \"\"\" if isinstance ( space , gym . spaces . Dict ): if not isinstance ( sample , ( StateDict , OrderedDict , dict )): raise ValueError ( f \"space { key_stack } = { space } was a gym.spaces.Dict type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a StateDict, OrderedDict, or dict\" ) for key , value in sample . items (): EnvSpaceUtil . deep_sanity_check_space_sample ( space . spaces [ key ], value , key_stack = f \" { key_stack } [ { key } ]\" ) elif isinstance ( space , gym . spaces . Tuple ): if not isinstance ( sample , tuple ): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Tuple type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a tuple type\" ) for idx , value in enumerate ( sample ): EnvSpaceUtil . deep_sanity_check_space_sample ( space . spaces [ idx ], value , key_stack = f \" { key_stack } [ { idx } ]\" ) elif isinstance ( space , gym . spaces . Discrete ): if not isinstance ( sample , ( int , np . integer , np . ndarray )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Discrete type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not an int or np.integer or np.ndarray\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . n } \" ) elif isinstance ( space , gym . spaces . Box ): if not isinstance ( sample , ( np . ndarray , list , np . floating )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.Box type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.float type\" ) if not space . contains ( sample ): sample_dtype = getattr ( sample , 'dtype' , None ) raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . low } { space . high } \" f \"space dtype is { space . dtype } , sample dtype is { sample_dtype } \" ) elif isinstance ( space , gym . spaces . MultiBinary ): if not isinstance ( sample , ( np . ndarray , list , np . integer )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.MultiBinary type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.integer type\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . n } \" ) elif isinstance ( space , gym . spaces . MultiDiscrete ): if not isinstance ( sample , ( np . ndarray , list , np . integer )): raise ValueError ( f \"space { key_stack } = { space } is a gym.spaces.MultiDiscrete type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a np.ndarray, list, or np.integer type\" ) if not space . contains ( sample ): raise ValueError ( f \"sample { key_stack } has value of { sample } however \" f \"space { key_stack } has space definition of { space } { space . nvec } \" ) elif isinstance ( space , Repeated ): if not isinstance ( sample , list ): raise ValueError ( f \"space { key_stack } = { space } is a ray.rllib.utils.spaces.repeated.Repeated type but \\n \" f \"sample { key_stack } = { sample } \\n \" f \"is a { type ( sample ) } type and not a list type\" ) for idx , item in enumerate ( sample ): EnvSpaceUtil . deep_sanity_check_space_sample ( space . child_space , item , key_stack = f \" { key_stack } [ { idx } ]\" )","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.get_mean_sample_from_space","text":"Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it.","title":"get_mean_sample_from_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.get_mean_sample_from_space--parameters","text":"gym.spaces.Space The gym space to zero sample from","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.get_mean_sample_from_space--returns","text":"sample_type The instance of the gym space but all Box spaces are sampled as zero Source code in corl/libraries/env_space_util.py @staticmethod def get_mean_sample_from_space ( space : gym . spaces . Space ) -> sample_type : \"\"\" Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters ---------- space: gym.spaces.Space The gym space to zero sample from Returns ------- sample_type The instance of the gym space but all Box spaces are sampled as zero \"\"\" val = space . sample () if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . get_zero_sample_from_space ( value ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . get_zero_sample_from_space ( sp ) for sp in space . spaces ] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): val = ( space . high + space . low ) / 2.0 return val","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.get_zero_sample_from_space","text":"Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it.","title":"get_zero_sample_from_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.get_zero_sample_from_space--parameters","text":"gym.spaces.Space The gym space to zero sample from","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.get_zero_sample_from_space--returns","text":"sample_type The instance of the gym space but all Box spaces are sampled as zero Source code in corl/libraries/env_space_util.py @staticmethod def get_zero_sample_from_space ( space : gym . spaces . Space ) -> sample_type : \"\"\" Given a gym space returns an instance of that space but instead of sampling from the gym space, returns all zeros. If the space is not a Box and we cannot iterate over it then we will sample from it. Parameters ---------- space: gym.spaces.Space The gym space to zero sample from Returns ------- sample_type The instance of the gym space but all Box spaces are sampled as zero \"\"\" val = space . sample () if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . get_zero_sample_from_space ( value ) val = new_dict elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . get_zero_sample_from_space ( sp ) for sp in space . spaces ] val = tuple ( new_tuple ) elif isinstance ( space , gym . spaces . Box ): val = np . zeros ( shape = space . shape , dtype = np . float32 ) return val","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.iterate_over_space_likes","text":"Iterates over space_likes which are tuple, dicts or the gym equivalent. When it encounters an actual item that is not a container it calls the func method. put any args, or kwargs you want to give to func in the overall call and they will be forwarded","title":"iterate_over_space_likes()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.iterate_over_space_likes--parameters","text":"Func the function to apply typing.Tuple[typing.Union[gym.spaces.Space, sample_type], ...] the spaces to iterate over. They must have the same keywords for dicts and number of items for tuples bool if true the containers will be gym space equivalents func_args the arguments to give to func func_kwargs the keyword arguments to give to func","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.iterate_over_space_likes--returns","text":"The contained result by calling func and stuffing back into the tuples and dicts in the call if return_space=True the containers are gym spaces Source code in corl/libraries/env_space_util.py @staticmethod def iterate_over_space_likes ( func , space_likes : typing . Tuple [ typing . Union [ gym . spaces . Space , sample_type ], ... ], return_space : bool , * func_args , ** func_kwargs , ) -> typing . Union [ gym . spaces . Space , sample_type ]: \"\"\" Iterates over space_likes which are tuple, dicts or the gym equivalent. When it encounters an actual item that is not a container it calls the func method. put any args, or kwargs you want to give to func in the overall call and they will be forwarded Parameters ---------- func: the function to apply space_likes: typing.Tuple[typing.Union[gym.spaces.Space, sample_type], ...] the spaces to iterate over. They must have the same keywords for dicts and number of items for tuples return_space: bool if true the containers will be gym space equivalents func_args the arguments to give to func func_kwargs the keyword arguments to give to func Returns ------- The contained result by calling func and stuffing back into the tuples and dicts in the call if return_space=True the containers are gym spaces \"\"\" first_space_like = space_likes [ 0 ] val = None if isinstance ( first_space_like , ( gym . spaces . Dict , dict , OrderedDict )): new_dict = OrderedDict () keys : typing . KeysView if isinstance ( first_space_like , gym . spaces . Dict ): keys = first_space_like . spaces . keys () else : keys = first_space_like . keys () for key in keys : new_space_likes = tuple ( spacer [ key ] for spacer in space_likes ) new_dict [ key ] = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = new_space_likes , return_space = return_space , * func_args , ** func_kwargs , ) val = gym . spaces . Dict ( spaces = new_dict ) if return_space else new_dict elif isinstance ( first_space_like , ( gym . spaces . Tuple , tuple )): new_tuple = [ EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = new_space_likes , return_space = return_space , * func_args , ** func_kwargs , ) for new_space_likes in zip ( * space_likes ) ] val = ( gym . spaces . Tuple ( tuple ( new_tuple )) if return_space else tuple ( new_tuple )) elif isinstance ( first_space_like , Repeated ): # if space_likes is longer than 1 that means that return_space = False # if there is only the space that means we need to generate just the space # itself and can use the second path, however for the case where we have a sample # it comes in as a list and we must iterate over the entire repeated list and process it if len ( space_likes ) > 1 : repeated_samples = space_likes [ 1 ] assert isinstance ( repeated_samples , MutableSequence ), \\ f 'repeated_samples must be MutableSequence, received { type ( repeated_samples ) . __name__ } ' for indx , sample in enumerate ( repeated_samples ): repeated_samples [ indx ] = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = ( first_space_like . child_space , sample ), return_space = return_space , * func_args , ** func_kwargs , ) val = repeated_samples else : new_child_space = EnvSpaceUtil . iterate_over_space_likes ( # type: ignore[misc] func , space_likes = ( first_space_like . child_space , ), return_space = return_space , * func_args , ** func_kwargs , ) val = Repeated ( child_space = new_child_space , max_len = first_space_like . max_len ) else : val = func ( space_likes , * func_args , ** func_kwargs ) return val","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.normalize_space","text":"This is a convenience wrapper for box_scaler","title":"normalize_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.normalize_space--parameters","text":"gym.spaces.Space the gym space to turn all boxes into the scaled space float the new low for the boxes float the new high for the boxes","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.normalize_space--returns","text":"gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed Source code in corl/libraries/env_space_util.py @staticmethod def normalize_space ( space : gym . spaces . Space , out_min =- 1 , out_max = 1 , ) -> gym . spaces . Space : \"\"\" This is a convenience wrapper for box_scaler Parameters ---------- space: gym.spaces.Space the gym space to turn all boxes into the scaled space out_min: float the new low for the boxes out_max: float the new high for the boxes Returns ------- gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . space_box_min_maxer , space_likes = ( space , ), out_min = out_min , out_max = out_max , return_space = True , )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.sanity_check_space_sample","text":"[summary]","title":"sanity_check_space_sample()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.sanity_check_space_sample--parameters","text":"space : [type] [description] sample : [type] [description]","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.sanity_check_space_sample--raises","text":"RuntimeError [description] Source code in corl/libraries/env_space_util.py @staticmethod def sanity_check_space_sample ( space , sample ): \"\"\"[summary] Parameters ---------- space : [type] [description] sample : [type] [description] Raises ------ RuntimeError [description] \"\"\" if not space . contains ( sample ): raise RuntimeError ( f \"sample of { sample } does not meet space { space } setup\" )","title":"Raises"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.scale_sample_from_space","text":"This is a convenience wrapper for box_scaler","title":"scale_sample_from_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.scale_sample_from_space--parameters","text":"gym.spaces.Space the space to use for the input min and max sample_type the space sample to scale float the minimum of the output scaling float the maximum of the output scaling","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.scale_sample_from_space--returns","text":"Sample_type the scaled sample with min of out_min and max of out_max (this is in dicts and tuples the same as space_sample was) Source code in corl/libraries/env_space_util.py @staticmethod def scale_sample_from_space ( space : gym . spaces . Space , space_sample : sample_type , out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This is a convenience wrapper for box_scaler Parameters ---------- space: gym.spaces.Space the space to use for the input min and max space_sample: sample_type the space sample to scale out_min: float the minimum of the output scaling out_max: float the maximum of the output scaling Returns ------- sample_type: the scaled sample with min of out_min and max of out_max (this is in dicts and tuples the same as space_sample was) \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . box_scaler , space_likes = ( space , space_sample ), out_min = out_min , out_max = out_max , return_space = False , )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.scale_space","text":"Multiplies the low and high properties of all the Boxes in the given gym space by the scale input Parameters gym.spaces.Space the gym space to scale the Boxes of float what to multiply the Box low and high by","title":"scale_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.scale_space--returns","text":"gym.spaces.Space the scaled gym space Source code in corl/libraries/env_space_util.py @staticmethod def scale_space ( space : gym . spaces . Space , scale : float ) -> gym . spaces . Space : \"\"\" Multiplies the low and high properties of all the Boxes in the given gym space by the scale input Parameters ---------- space: gym.spaces.Space the gym space to scale the Boxes of scale: float what to multiply the Box low and high by Returns ------- gym.spaces.Space the scaled gym space \"\"\" # TODO: this copy probably doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . scale_space ( value , scale = scale ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_thing = [ EnvSpaceUtil . scale_space ( sp , scale = scale ) for sp in space . spaces ] val = gym . spaces . Tuple ( tuple ( new_thing )) elif isinstance ( space , gym . spaces . Box ): scaled_box = gym . spaces . Box ( low = np . multiply ( space . low , scale ) . astype ( np . float32 ), high = np . multiply ( space . high , scale ) . astype ( np . float32 ), shape = space . shape , dtype = np . float32 ) val = scaled_box return val","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.space_box_min_maxer","text":"Makes a gym box to the out_min and out_max range","title":"space_box_min_maxer()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.space_box_min_maxer--parameters","text":"typing.Tuple[gym.spaces.Space] the gym space to turn all boxes into the scaled space float the new low for the boxes float the new high for the boxes","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.space_box_min_maxer--returns","text":"gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed Source code in corl/libraries/env_space_util.py @staticmethod def space_box_min_maxer ( space_likes : typing . Tuple [ gym . spaces . Space ], out_min : float = - 1.0 , out_max : float = 1.0 , ) -> gym . spaces . Space : \"\"\" Makes a gym box to the out_min and out_max range Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space] the gym space to turn all boxes into the scaled space out_min: float the new low for the boxes out_max: float the new high for the boxes Returns ------- gym.spaces.Space: the new gym spaces where all boxes have had their bounds changed \"\"\" space_arg = space_likes [ 0 ] if isinstance ( space_arg , gym . spaces . Box ): return gym . spaces . Box ( low = out_min , high = out_max , shape = space_arg . shape , dtype = np . float32 ) return copy . deepcopy ( space_arg )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_box_into_tuple_of_discretes","text":"Takes a gym space and replaces any Box types with MultiDiscrete types with the same number of possible of discrete as the box and each discrete has the same number of possible actions = num_actions Parameters space int, list, dict how many discrete actions to give to each possible discrete in the MultiDiscrete","title":"turn_box_into_tuple_of_discretes()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_box_into_tuple_of_discretes--returns","text":"gym.spaces.Space A gym space where all the Box types are replaced with MultiDiscrete types Source code in corl/libraries/env_space_util.py @staticmethod def turn_box_into_tuple_of_discretes ( space : gym . spaces . Space , num_actions : typing . Union [ int , typing . List [ int ], Iterable , dict ] = 10 ) -> gym . spaces . Space : \"\"\" Takes a gym space and replaces any Box types with MultiDiscrete types with the same number of possible of discrete as the box and each discrete has the same number of possible actions = num_actions Parameters ---------- space num_actions: int, list, dict how many discrete actions to give to each possible discrete in the MultiDiscrete Returns ------- gym.spaces.Space A gym space where all the Box types are replaced with MultiDiscrete types \"\"\" # first pass through the code num_actions is an int or list, this code turns it into an # iterator for the rest of the recursive calls if isinstance ( num_actions , int ): num_actions = repeat ( num_actions ) elif isinstance ( num_actions , ( list , tuple )): num_actions = iter ( num_actions ) # TODO: this copy doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , space_value in space . spaces . items (): if isinstance ( num_actions , dict ): new_dict [ key ] = EnvSpaceUtil . turn_box_into_tuple_of_discretes ( space = space_value , num_actions = num_actions [ key ]) else : new_dict [ key ] = EnvSpaceUtil . turn_box_into_tuple_of_discretes ( space = space_value , num_actions = num_actions ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_tuple = [ EnvSpaceUtil . turn_box_into_tuple_of_discretes ( sv , num_actions = num_actions ) for sv in space ] if len ( new_tuple ) == 1 and isinstance ( new_tuple [ 0 ], gym . spaces . Tuple ): val = new_tuple [ 0 ] else : val = gym . spaces . Tuple ( tuple ( new_tuple )) elif isinstance ( space , gym . spaces . Box ): assert isinstance ( num_actions , Iterator ), f 'num_actions must be iterator, received { type ( num_actions ) . __name__ } ' val = gym . spaces . Tuple ([ Discrete ( np . asarray ( next ( num_actions ))) for _ in range ( 0 , space . low . size )]) return val","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_discrete_action_back_to_cont","text":"This is a convenience wrapper for turn_orig_space_box_to_cont_sample","title":"turn_discrete_action_back_to_cont()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_discrete_action_back_to_cont--parameters","text":"gym.spaces.Space the continuous space gym.spaces.Space the discrete space sample_type the sample of the discrete space","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_discrete_action_back_to_cont--returns","text":"Sample_type the continuous version of the discrete sample Source code in corl/libraries/env_space_util.py @staticmethod def turn_discrete_action_back_to_cont ( original_space : gym . spaces . Space , discrete_only_space : gym . spaces . Space , space_sample : sample_type , ) -> sample_type : \"\"\" This is a convenience wrapper for turn_orig_space_box_to_cont_sample Parameters ---------- original_space: gym.spaces.Space the continuous space discrete_only_space: gym.spaces.Space the discrete space space_sample: sample_type the sample of the discrete space Returns ------- sample_type: the continuous version of the discrete sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( EnvSpaceUtil . turn_orig_space_box_to_cont_sample , space_likes = ( original_space , discrete_only_space , space_sample ), return_space = False , )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_discrete_action_back_to_cont_powerspace","text":"This is a convenience wrapper for turn_orig_space_box_to_cont_sample_powerspace","title":"turn_discrete_action_back_to_cont_powerspace()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_discrete_action_back_to_cont_powerspace--parameters","text":"gym.spaces.Space the continuous space gym.spaces.Space the discrete space sample_type the sample of the discrete space","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_discrete_action_back_to_cont_powerspace--returns","text":"Sample_type the continuous version of the discrete sample Source code in corl/libraries/env_space_util.py @staticmethod def turn_discrete_action_back_to_cont_powerspace ( original_space : gym . spaces . Space , discrete_only_space : gym . spaces . Space , space_sample : sample_type , pow_n : dict ) -> sample_type : \"\"\" This is a convenience wrapper for turn_orig_space_box_to_cont_sample_powerspace Parameters ---------- original_space: gym.spaces.Space the continuous space discrete_only_space: gym.spaces.Space the discrete space space_sample: sample_type the sample of the discrete space Returns ------- sample_type: the continuous version of the discrete sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( EnvSpaceUtil . turn_orig_space_box_to_cont_sample_powerspace , space_likes = ( original_space , discrete_only_space , space_sample , pow_n ), return_space = False , )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_orig_space_box_to_cont_sample","text":"Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample","title":"turn_orig_space_box_to_cont_sample()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_orig_space_box_to_cont_sample--parameters","text":"typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_orig_space_box_to_cont_sample--returns","text":"Sample_type the sample space that is now a continuous version of the discrete sample space according to the cont space Source code in corl/libraries/env_space_util.py @staticmethod def turn_orig_space_box_to_cont_sample ( space_likes : typing . Tuple [ gym . spaces . Space , gym . spaces . Space , sample_type ]) -> sample_type : \"\"\" Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns ------- sample_type: the sample space that is now a continuous version of the discrete sample space according to the cont space \"\"\" ( original_space_arg , discrete_only_space_arg , space_sample_arg ) = space_likes if isinstance ( original_space_arg , gym . spaces . Box ) and ( isinstance ( discrete_only_space_arg , ( gym . spaces . MultiDiscrete , gym . spaces . Discrete ))): if isinstance ( discrete_only_space_arg , gym . spaces . MultiDiscrete ): possible_n = discrete_only_space_arg . nvec elif isinstance ( discrete_only_space_arg , gym . spaces . Discrete ): possible_n = discrete_only_space_arg . n else : raise RuntimeError ( \"This should not be reachable\" ) action = space_sample_arg low = original_space_arg . low high = original_space_arg . high new_cont_sample = ( action / ( possible_n - 1 )) * ( high - low ) + low return new_cont_sample return copy . deepcopy ( space_sample_arg )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_orig_space_box_to_cont_sample_powerspace","text":"Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample","title":"turn_orig_space_box_to_cont_sample_powerspace()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_orig_space_box_to_cont_sample_powerspace--parameters","text":"typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.turn_orig_space_box_to_cont_sample_powerspace--returns","text":"Sample_type the sample space that is now a continuous version of the discrete sample space according to the cont space Source code in corl/libraries/env_space_util.py @staticmethod def turn_orig_space_box_to_cont_sample_powerspace ( space_likes : typing . Tuple [ gym . spaces . Space , gym . spaces . Space , sample_type , sample_type ] ) -> sample_type : \"\"\" Given a continuous space and a discrete space and a sample of the discrete space, this function turns the discrete sample into a continuous sample Parameters ---------- space_likes: typing.Tuple[gym.spaces.Space, gym.spaces.Space, sample_type] the first is the original gym space that is continuous, the second is the new gym space with discrete objects the last is the space sample that is of discrete types Returns ------- sample_type: the sample space that is now a continuous version of the discrete sample space according to the cont space \"\"\" # There is a test built for this function. Please keep the test up to date for any changes you make. ( original_space_arg , discrete_only_space_arg , space_sample_arg , pow_n ) = space_likes if isinstance ( original_space_arg , gym . spaces . Box ) and ( isinstance ( discrete_only_space_arg , ( gym . spaces . Tuple , gym . spaces . Discrete ))): if isinstance ( discrete_only_space_arg , gym . spaces . Tuple ): possible_n = np . asarray ([ x . n for x in discrete_only_space_arg ]) pow_n = np . asarray ( pow_n ) discrete_sample = np . asarray ( space_sample_arg ) elif isinstance ( discrete_only_space_arg , gym . spaces . Discrete ): possible_n = np . asarray ([ discrete_only_space_arg . n ]) pow_n = np . asarray ([ pow_n ]) discrete_sample = np . asarray ([ space_sample_arg ]) else : raise RuntimeError ( \"This should not be reachable\" ) low = original_space_arg . low high = original_space_arg . high if any ( low > high ): raise RuntimeError ( \"lower bounds of space somehow higher than high bounds\" ) if not all ( possible_n % 2 ): if any ( pow_n > 1 ): raise RuntimeError ( \"The exponential power factor not supported when possible_n % 2\" ) difference = high - low movement_per_space_sample = difference / possible_n new_cont_sample = low + ( movement_per_space_sample * space_sample_arg ) return new_cont_sample if any ( pow_n <= 0 ): raise RuntimeError ( \"The exponential power factor used to stretch/shrink the discrete space must be greater than zero\" ) if any ( - low != high ) or any ( low >= high ): raise RuntimeError ( \"Currently only support symmetric space about zero for power space setup - TODO\" ) p_high = np . power ( high , 1 / pow_n . astype ( float )) p_low = - p_high # Since we have specified the space must be symetric around 0 new_cont_sample = ( discrete_sample / ( possible_n - 1 )) * ( p_high - p_low ) + p_low new_cont_sample = np . power ( np . abs ( new_cont_sample ), pow_n ) * np . sign ( new_cont_sample ) return new_cont_sample return copy . deepcopy ( space_sample_arg )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.unscale_sample_from_space","text":"This is a convenience wrapper for box_unscaler","title":"unscale_sample_from_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.unscale_sample_from_space--parameters","text":"gym.spaces.Space the gym space we will unscale to sample_type the sample we want to unscale. thus this is a scaled version of the input space with a min,max defined by the arguments out_min,out_max float the minimum of the sample float the maximum of the sample","title":"Parameters"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.unscale_sample_from_space--returns","text":"Sample_type the unscaled version of the space_sample. Thus the space should now contain this output sample Source code in corl/libraries/env_space_util.py @staticmethod def unscale_sample_from_space ( space : gym . spaces . Space , space_sample : sample_type , out_min : float = - 1 , out_max : float = 1 , ) -> sample_type : \"\"\" This is a convenience wrapper for box_unscaler Parameters ---------- space: gym.spaces.Space the gym space we will unscale to space_sample: sample_type the sample we want to unscale. thus this is a scaled version of the input space with a min,max defined by the arguments out_min,out_max out_min: float the minimum of the sample out_max: float the maximum of the sample Returns ------- sample_type: the unscaled version of the space_sample. Thus the space should now contain this output sample \"\"\" return EnvSpaceUtil . iterate_over_space_likes ( func = EnvSpaceUtil . box_unscaler , space_likes = ( space , space_sample ), out_min = out_min , out_max = out_max , return_space = False , )","title":"Returns"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.zero_mean_space","text":"Returns a space object where every Box instance has its low and high shifted to be zero mean Parameters gym.spaces.Space The gym space to zero mean","title":"zero_mean_space()"},{"location":"reference/libraries/env_space_util/#corl.libraries.env_space_util.EnvSpaceUtil.zero_mean_space--returns","text":"gym.spaces.Space A gym space the same as the input but with the Box instances shifted Source code in corl/libraries/env_space_util.py @staticmethod def zero_mean_space ( space : gym . spaces . Space ) -> gym . spaces . Space : \"\"\" Returns a space object where every Box instance has its low and high shifted to be zero mean Parameters ---------- space: gym.spaces.Space The gym space to zero mean Returns ------- gym.spaces.Space A gym space the same as the input but with the Box instances shifted \"\"\" # TODO: this copy doesn't actually work but I can dream val = copy . deepcopy ( space ) if isinstance ( space , gym . spaces . Dict ): new_dict = OrderedDict () for key , value in space . spaces . items (): new_dict [ key ] = EnvSpaceUtil . zero_mean_space ( value ) val = gym . spaces . Dict ( spaces = new_dict ) elif isinstance ( space , gym . spaces . Tuple ): new_thing = [ EnvSpaceUtil . zero_mean_space ( sp ) for sp in space . spaces ] val = gym . spaces . Tuple ( tuple ( new_thing )) elif isinstance ( space , gym . spaces . Box ): mean = ( space . high + space . low ) / 2 zero_mean_box = gym . spaces . Box ( low = space . low - mean , high = space . high - mean , shape = space . shape , dtype = np . float32 ) val = zero_mean_box return val","title":"Returns"},{"location":"reference/libraries/environment_dict/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Environment Dict Module Callback \u00a4 Callback provides basic callback processing for all reward and done functions Source code in corl/libraries/environment_dict.py class Callback : \"\"\" Callback provides basic callback processing for all reward and done functions \"\"\" def __init__ ( self , funcs : typing . Optional [ typing . Sequence [ typing . Callable ]] = None ) -> None : \"\"\" __init__ constructor Parameters ---------- funcs : typing.List[typing.Callable], optional List of callale functions, by default None \"\"\" self . _process_callbacks : typing . List [ typing . Callable ] = [] self . _logger = logging . getLogger ( Callback . __name__ ) if funcs : self . register_funcs ( funcs ) def register_func ( self , func : typing . Callable ) -> None : \"\"\" register_func registers a function to the list of valid functions Parameters ---------- func : typing.Callable The callable function \"\"\" # MTB - 10/15/2020 - There seems to be an issue with this when combining multiple reward sets # if func is a callable the expression func in self._process_callbacks will always return True if isinstance ( func , Callback ): self . _process_callbacks . append ( func ) else : if func not in self . _process_callbacks : self . _process_callbacks . append ( func ) else : warnings . warn ( \"Ignoring a duplicate callback given\" ) def register_funcs ( self , funcs : typing . Optional [ typing . Sequence [ typing . Callable ]]): \"\"\" register_func registers a list of functions to the list of valid functions Parameters ---------- func : typing.Callable The callable function \"\"\" for func in funcs or []: self . register_func ( func ) def unregister_func ( self , func : typing . Callable ) -> None : \"\"\" unregister callbacks from processing Parameters ---------- key : str The callback string to remove \"\"\" if func in self . _process_callbacks : self . _process_callbacks . remove ( func ) def unregister_funcs ( self ) -> None : \"\"\" unregister callbacks from processing \"\"\" self . _process_callbacks . clear () def reset_funcs ( self ): \"\"\" reset_funcs func is a callable then attempt to reset its state. Parameters ---------- key : str The callback string to remove \"\"\" for func in self . _process_callbacks : reset_op = getattr ( func , \"reset\" , None ) if callable ( reset_op ): func . reset () @property def process_callbacks ( self ) -> typing . List [ typing . Callable ]: \"\"\" process_callbacks gets the current callbacks Returns ------- typing.List[typing.Callable] Current list of callbacks \"\"\" return self . _process_callbacks process_callbacks : List [ Callable ] property readonly \u00a4 process_callbacks gets the current callbacks Returns \u00a4 typing.List[typing.Callable] Current list of callbacks __init__ ( self , funcs = None ) special \u00a4 init constructor Parameters \u00a4 funcs : typing.List[typing.Callable], optional List of callale functions, by default None Source code in corl/libraries/environment_dict.py def __init__ ( self , funcs : typing . Optional [ typing . Sequence [ typing . Callable ]] = None ) -> None : \"\"\" __init__ constructor Parameters ---------- funcs : typing.List[typing.Callable], optional List of callale functions, by default None \"\"\" self . _process_callbacks : typing . List [ typing . Callable ] = [] self . _logger = logging . getLogger ( Callback . __name__ ) if funcs : self . register_funcs ( funcs ) register_func ( self , func ) \u00a4 register_func registers a function to the list of valid functions Parameters \u00a4 func : typing.Callable The callable function Source code in corl/libraries/environment_dict.py def register_func ( self , func : typing . Callable ) -> None : \"\"\" register_func registers a function to the list of valid functions Parameters ---------- func : typing.Callable The callable function \"\"\" # MTB - 10/15/2020 - There seems to be an issue with this when combining multiple reward sets # if func is a callable the expression func in self._process_callbacks will always return True if isinstance ( func , Callback ): self . _process_callbacks . append ( func ) else : if func not in self . _process_callbacks : self . _process_callbacks . append ( func ) else : warnings . warn ( \"Ignoring a duplicate callback given\" ) register_funcs ( self , funcs ) \u00a4 register_func registers a list of functions to the list of valid functions Parameters \u00a4 func : typing.Callable The callable function Source code in corl/libraries/environment_dict.py def register_funcs ( self , funcs : typing . Optional [ typing . Sequence [ typing . Callable ]]): \"\"\" register_func registers a list of functions to the list of valid functions Parameters ---------- func : typing.Callable The callable function \"\"\" for func in funcs or []: self . register_func ( func ) reset_funcs ( self ) \u00a4 reset_funcs func is a callable then attempt to reset its state. Parameters \u00a4 key : str The callback string to remove Source code in corl/libraries/environment_dict.py def reset_funcs ( self ): \"\"\" reset_funcs func is a callable then attempt to reset its state. Parameters ---------- key : str The callback string to remove \"\"\" for func in self . _process_callbacks : reset_op = getattr ( func , \"reset\" , None ) if callable ( reset_op ): func . reset () unregister_func ( self , func ) \u00a4 unregister callbacks from processing Parameters \u00a4 key : str The callback string to remove Source code in corl/libraries/environment_dict.py def unregister_func ( self , func : typing . Callable ) -> None : \"\"\" unregister callbacks from processing Parameters ---------- key : str The callback string to remove \"\"\" if func in self . _process_callbacks : self . _process_callbacks . remove ( func ) unregister_funcs ( self ) \u00a4 unregister callbacks from processing Source code in corl/libraries/environment_dict.py def unregister_funcs ( self ) -> None : \"\"\" unregister callbacks from processing \"\"\" self . _process_callbacks . clear () DoneDict ( EnvDict ) \u00a4 [summary] Parameters \u00a4 EnvDict : [type] [description] Source code in corl/libraries/environment_dict.py class DoneDict ( EnvDict ): \"\"\"[summary] Parameters ---------- EnvDict : [type] [description] \"\"\" def __init__ ( self , processing_funcs : typing . Sequence [ typing . Callable ] = None , reduce_fn : typing . Callable = None , reduce_fn_kwargs = None , ** kwargs , ) -> None : super () . __init__ ( processing_funcs = processing_funcs , reduce_fn = reduce_fn , reduce_fn_kwargs = reduce_fn_kwargs , ** kwargs ) self . _agent_filter : typing . Optional [ typing . Iterable [ str ]] = None def __call__ ( self , * args , ** kwargs ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" __call__ Callable function for the done dictionary type Returns ------- typing.Tuple[OrderedDict, OrderedDict] The done information \"\"\" r = super () . __call__ ( * args , ** kwargs ) # Check for bool type in return value for key , value in r [ 0 ] . items (): if isinstance ( value , np . bool_ ): r [ 0 ][ key ] = bool ( value ) elif not isinstance ( value , bool ): raise TypeError ( \"DoneDict __call__ return is not type bool for key: {} \" . format ( key )) for key0 , value0 in r [ 1 ] . items (): for key1 , value1 in value0 . items (): if isinstance ( value1 , np . bool_ ): r [ 1 ][ key0 ][ key1 ] = bool ( value1 ) elif not isinstance ( value1 , bool ): raise TypeError ( \"DoneDict __call__ return is not type bool for key: {} / {} \" . format ( key0 , key1 )) return r @property def _filtered_process_callbacks ( self ) -> typing . List [ typing . Callable ]: if self . _agent_filter is None : return super () . _filtered_process_callbacks # Avoid circular import from corl.dones.done_func_base import DoneFuncBase # pylint: disable=import-outside-toplevel return [ x for x in self . _process_callbacks if not isinstance ( x , DoneFuncBase ) or x . agent in self . _agent_filter ] def set_alive_agents ( self , alive_agents : typing . Iterable [ str ]) -> None : \"\"\"Specify which agents are alive This is used to determine which callbacks to call. Parameters ---------- alive_agents : typing.Iterable[str] Agents that are currently alive. \"\"\" self . _agent_filter = alive_agents def _reduce ( self , r , ** kwargs ): self . _reduce_fn = self . _reduce_fn or np . any tmp = StateDict . stack_values ( r ) tmp = { k : self . _reduce_fn ( v , ** kwargs ) for k , v in tmp . items ()} return StateDict ( sorted ( tmp . items ())) __call__ ( self , * args , ** kwargs ) special \u00a4 call Callable function for the done dictionary type Returns \u00a4 typing.Tuple[OrderedDict, OrderedDict] The done information Source code in corl/libraries/environment_dict.py def __call__ ( self , * args , ** kwargs ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" __call__ Callable function for the done dictionary type Returns ------- typing.Tuple[OrderedDict, OrderedDict] The done information \"\"\" r = super () . __call__ ( * args , ** kwargs ) # Check for bool type in return value for key , value in r [ 0 ] . items (): if isinstance ( value , np . bool_ ): r [ 0 ][ key ] = bool ( value ) elif not isinstance ( value , bool ): raise TypeError ( \"DoneDict __call__ return is not type bool for key: {} \" . format ( key )) for key0 , value0 in r [ 1 ] . items (): for key1 , value1 in value0 . items (): if isinstance ( value1 , np . bool_ ): r [ 1 ][ key0 ][ key1 ] = bool ( value1 ) elif not isinstance ( value1 , bool ): raise TypeError ( \"DoneDict __call__ return is not type bool for key: {} / {} \" . format ( key0 , key1 )) return r set_alive_agents ( self , alive_agents ) \u00a4 Specify which agents are alive This is used to determine which callbacks to call. Parameters \u00a4 alive_agents : typing.Iterable[str] Agents that are currently alive. Source code in corl/libraries/environment_dict.py def set_alive_agents ( self , alive_agents : typing . Iterable [ str ]) -> None : \"\"\"Specify which agents are alive This is used to determine which callbacks to call. Parameters ---------- alive_agents : typing.Iterable[str] Agents that are currently alive. \"\"\" self . _agent_filter = alive_agents EnvDict ( StateDict , Callback ) \u00a4 [summary] Parameters \u00a4 StateDict : [type] [description] Callback : [type] [description] Returns \u00a4 [type] [description] Source code in corl/libraries/environment_dict.py class EnvDict ( StateDict , Callback ): \"\"\"[summary] Parameters ---------- StateDict : [type] [description] Callback : [type] [description] Returns ------- [type] [description] \"\"\" # list of items to exclude when generating the keys, items, values EXCLUDE_KEYS = [ \"_default_kwargs\" , \"_reduce_fn\" , \"_reduce_fn_kwargs\" , \"_processing_funcs\" , \"_recurse\" , ] class DuplicateName ( RuntimeError ): \"\"\"Exception class for callbacks with duplicate names\"\"\" def __init__ ( self , processing_funcs : typing . Sequence [ typing . Callable ] = None , reduce_fn : typing . Callable = None , reduce_fn_kwargs = None , ** kwargs , ) -> None : \"\"\" __init__ environment dictionary constructor Parameters ---------- processing_funcs : typing.List[typing.Callable], optional List of functions to call by the environment, by default None reduce_fn : typing.Callable, optional function used to reduce the results, by default None reduce_fn_kwargs : [type], optional [description], by default None \"\"\" self . _default_kwargs = None self . _reduce_fn = reduce_fn self . _reduce_fn_kwargs = reduce_fn_kwargs or {} self . _set_default_kwargs ( kwargs ) StateDict . __init__ ( self , ** kwargs ) Callback . __init__ ( self , processing_funcs ) def __call__ ( self , * args , ** kwargs ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" __call__ Callable function for the environment dictionary type Returns ------- typing.Tuple[OrderedDict, OrderedDict] The reduced rewards and theret information \"\"\" r = [ self . _default_kwargs , ] ret_info : OrderedDict = OrderedDict () def merge ( source , destination ): \"\"\" run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , {}) merge ( value , node ) else : destination [ key ] = value return destination for func in self . _filtered_process_callbacks : if isinstance ( func , OrderedDict ): ret = func ( * args , ** kwargs ) rew , info = ret r . append ( rew ) ret_info = merge ( ret_info , info ) for func in self . _filtered_process_callbacks : if not isinstance ( func , OrderedDict ): ret = func ( * args , ** kwargs ) # single value r . append ( copy . deepcopy ( ret )) try : name = func . __name__ except : # noqa: E722 # pylint: disable=bare-except name = func . name # type: ignore # This only affects the info dictionary that is returned. As the code below merges the output for all agents together, # the __all__ entry would be overwritten to only provide the information of the last agent, which could be confusing or # inaccurate. Therefore, remove __all__ from the returned information. if '__all__' in ret : del ret [ '__all__' ] if name in ret_info . keys (): common_keys = set ( ret . keys ()) & set ( ret_info [ name ] . keys ()) if common_keys : raise self . DuplicateName ( f ' { name } has common keys: { common_keys } ' ) ret_info [ name ] . update ( ** ret ) else : ret_info [ name ] = ret # TODO link with reduce and ret info return self . _reduce ( r , ** self . _reduce_fn_kwargs ), ret_info # type: ignore @property def _filtered_process_callbacks ( self ) -> typing . List [ typing . Callable ]: \"\"\"Set of callbacks that have been filtered by subclass logic. Default implementation is all callbacks Returns ------- typing.List[typing.Callable] Callbacks to apply \"\"\" return self . _process_callbacks @abc . abstractmethod def _reduce ( self , r : typing . Callable , ** kwargs ): \"\"\" _reduce user defined reduce function for processing Parameters ---------- r : typing.Callable The reduce function to use \"\"\" ... def _set_default_kwargs ( self , kwargs ): \"\"\" _set_default_kwargs [summary] [extended_summary] Parameters ---------- kwargs : [type] [description] \"\"\" if self . _default_kwargs is None : self . _default_kwargs = copy . deepcopy ( kwargs ) def reset ( self ): \"\"\" reset [summary] \"\"\" # EnvDict.__init__(self, **self._default_kwargs) self . reset_funcs () def _filtered_self ( self ): tmp = { k : v for k , v in super () . items () if k not in self . EXCLUDE_KEYS } tmp = StateDict ( tmp ) return tmp def keys ( self ): return self . _filtered_self () . keys () def values ( self ): return self . _filtered_self () . values () def items ( self ): return self . _filtered_self () . items () def to_dict ( self ): return self . _filtered_self () . to_dict () @property def name ( self ): \"\"\"[summary] Returns ------- [type] [description] \"\"\" return type ( self ) . __name__ @staticmethod def observation_deep_copy ( d1 , d2 ): \"\"\"[summary] Parameters ---------- d1 : [type] [description] d2 : [type] [description] \"\"\" # d1 = pickle.loads(pickle.dumps(d2)) for ( k1 , v1 ), ( k2 , v2 ) in zip ( d1 . items (), d2 . items ()): if isinstance ( v1 , dict ): EnvDict . observation_deep_copy ( v1 , v2 ) else : d1 [ k1 ] = copy . deepcopy ( d2 [ k2 ]) name property readonly \u00a4 [summary] Returns \u00a4 [type] [description] DuplicateName ( RuntimeError ) \u00a4 Exception class for callbacks with duplicate names Source code in corl/libraries/environment_dict.py class DuplicateName ( RuntimeError ): \"\"\"Exception class for callbacks with duplicate names\"\"\" __call__ ( self , * args , ** kwargs ) special \u00a4 call Callable function for the environment dictionary type Returns \u00a4 typing.Tuple[OrderedDict, OrderedDict] The reduced rewards and theret information Source code in corl/libraries/environment_dict.py def __call__ ( self , * args , ** kwargs ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" __call__ Callable function for the environment dictionary type Returns ------- typing.Tuple[OrderedDict, OrderedDict] The reduced rewards and theret information \"\"\" r = [ self . _default_kwargs , ] ret_info : OrderedDict = OrderedDict () def merge ( source , destination ): \"\"\" run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , {}) merge ( value , node ) else : destination [ key ] = value return destination for func in self . _filtered_process_callbacks : if isinstance ( func , OrderedDict ): ret = func ( * args , ** kwargs ) rew , info = ret r . append ( rew ) ret_info = merge ( ret_info , info ) for func in self . _filtered_process_callbacks : if not isinstance ( func , OrderedDict ): ret = func ( * args , ** kwargs ) # single value r . append ( copy . deepcopy ( ret )) try : name = func . __name__ except : # noqa: E722 # pylint: disable=bare-except name = func . name # type: ignore # This only affects the info dictionary that is returned. As the code below merges the output for all agents together, # the __all__ entry would be overwritten to only provide the information of the last agent, which could be confusing or # inaccurate. Therefore, remove __all__ from the returned information. if '__all__' in ret : del ret [ '__all__' ] if name in ret_info . keys (): common_keys = set ( ret . keys ()) & set ( ret_info [ name ] . keys ()) if common_keys : raise self . DuplicateName ( f ' { name } has common keys: { common_keys } ' ) ret_info [ name ] . update ( ** ret ) else : ret_info [ name ] = ret # TODO link with reduce and ret info return self . _reduce ( r , ** self . _reduce_fn_kwargs ), ret_info # type: ignore __init__ ( self , processing_funcs = None , reduce_fn = None , reduce_fn_kwargs = None , ** kwargs ) special \u00a4 init environment dictionary constructor Parameters \u00a4 processing_funcs : typing.List[typing.Callable], optional List of functions to call by the environment, by default None reduce_fn : typing.Callable, optional function used to reduce the results, by default None reduce_fn_kwargs : [type], optional [description], by default None Source code in corl/libraries/environment_dict.py def __init__ ( self , processing_funcs : typing . Sequence [ typing . Callable ] = None , reduce_fn : typing . Callable = None , reduce_fn_kwargs = None , ** kwargs , ) -> None : \"\"\" __init__ environment dictionary constructor Parameters ---------- processing_funcs : typing.List[typing.Callable], optional List of functions to call by the environment, by default None reduce_fn : typing.Callable, optional function used to reduce the results, by default None reduce_fn_kwargs : [type], optional [description], by default None \"\"\" self . _default_kwargs = None self . _reduce_fn = reduce_fn self . _reduce_fn_kwargs = reduce_fn_kwargs or {} self . _set_default_kwargs ( kwargs ) StateDict . __init__ ( self , ** kwargs ) Callback . __init__ ( self , processing_funcs ) items ( self ) \u00a4 D.items() -> a set-like object providing a view on D's items Source code in corl/libraries/environment_dict.py def items ( self ): return self . _filtered_self () . items () keys ( self ) \u00a4 D.keys() -> a set-like object providing a view on D's keys Source code in corl/libraries/environment_dict.py def keys ( self ): return self . _filtered_self () . keys () observation_deep_copy ( d1 , d2 ) staticmethod \u00a4 [summary] Parameters \u00a4 d1 : [type] [description] d2 : [type] [description] Source code in corl/libraries/environment_dict.py @staticmethod def observation_deep_copy ( d1 , d2 ): \"\"\"[summary] Parameters ---------- d1 : [type] [description] d2 : [type] [description] \"\"\" # d1 = pickle.loads(pickle.dumps(d2)) for ( k1 , v1 ), ( k2 , v2 ) in zip ( d1 . items (), d2 . items ()): if isinstance ( v1 , dict ): EnvDict . observation_deep_copy ( v1 , v2 ) else : d1 [ k1 ] = copy . deepcopy ( d2 [ k2 ]) reset ( self ) \u00a4 reset [summary] Source code in corl/libraries/environment_dict.py def reset ( self ): \"\"\" reset [summary] \"\"\" # EnvDict.__init__(self, **self._default_kwargs) self . reset_funcs () to_dict ( self ) \u00a4 Converts to a dictionary Returns \u00a4 dict the dict form Source code in corl/libraries/environment_dict.py def to_dict ( self ): return self . _filtered_self () . to_dict () values ( self ) \u00a4 D.values() -> an object providing a view on D's values Source code in corl/libraries/environment_dict.py def values ( self ): return self . _filtered_self () . values () InfoDict ( EnvDict ) \u00a4 [summary] Parameters \u00a4 EnvDict : [type] [description] Source code in corl/libraries/environment_dict.py class InfoDict ( EnvDict ): \"\"\"[summary] Parameters ---------- EnvDict : [type] [description] \"\"\" def _reduce ( self , r , ** kwargs ): self . _reduce_fn = self . _reduce_fn or ( lambda x : {}) tmp = StateDict . stack_values ( r ) tmp = { k : self . _reduce_fn ( v , ** kwargs ) for k , v in tmp . items ()} return StateDict ( sorted ( tmp . items ()), recurse = False ) RewardDict ( EnvDict ) \u00a4 [summary] Parameters \u00a4 EnvDict : [type] [description] Returns \u00a4 [type] [description] Source code in corl/libraries/environment_dict.py class RewardDict ( EnvDict ): \"\"\"[summary] Parameters ---------- EnvDict : [type] [description] Returns ------- [type] [description] \"\"\" SCALE_KEY = \"SCALE\" def __init__ ( self , processing_funcs : typing . Sequence [ typing . Callable ] = None , reduce_fn : typing . Callable = None , reduce_fn_kwargs = None , ** kwargs , ) -> None : super () . __init__ ( processing_funcs = processing_funcs , reduce_fn = reduce_fn , reduce_fn_kwargs = reduce_fn_kwargs , ** kwargs ) self . _agent_filter : typing . Optional [ typing . Iterable [ str ]] = None @property def _filtered_process_callbacks ( self ) -> typing . List [ typing . Callable ]: if self . _agent_filter is None : return super () . _filtered_process_callbacks # Avoid circular import from corl.rewards.reward_func_base import RewardFuncBase # pylint: disable=import-outside-toplevel # TODO: Make it so that RewardFuncBase has agent_id as part of its base class API # Need to disable yapf because it puts this all on a single line, which then fails pylint because the line is too long. # yapf: disable return [ x for x in self . _process_callbacks if not isinstance ( x , RewardFuncBase ) or x . _agent_id in self . _agent_filter ] # pylint: disable=protected-access # yapf: enable def set_alive_agents ( self , alive_agents : typing . Iterable [ str ]) -> None : \"\"\"Specify which agents are alive This is used to determine which callbacks to call. Parameters ---------- alive_agents : typing.Iterable[str] Agents that are currently alive. \"\"\" self . _agent_filter = alive_agents def _reduce ( self , r , ** kwargs ): scale = 1.0 if RewardDict . SCALE_KEY in kwargs : scale = kwargs [ RewardDict . SCALE_KEY ] del kwargs [ RewardDict . SCALE_KEY ] self . _reduce_fn = self . _reduce_fn or np . sum tmp = StateDict . stack_values ( r ) tmp = { k : self . _reduce_fn ( v , ** kwargs ) / scale for k , v in tmp . items ()} return OrderedDict ( sorted ( tmp . items ())) def set_scale_down ( self , scale : int ): \"\"\" set_scale_down sets a value for the reward dict to scale down the rewards by Parameters ---------- scale : int The value to divide all rewards in the reward dict by \"\"\" self . _reduce_fn_kwargs [ RewardDict . SCALE_KEY ] = scale set_alive_agents ( self , alive_agents ) \u00a4 Specify which agents are alive This is used to determine which callbacks to call. Parameters \u00a4 alive_agents : typing.Iterable[str] Agents that are currently alive. Source code in corl/libraries/environment_dict.py def set_alive_agents ( self , alive_agents : typing . Iterable [ str ]) -> None : \"\"\"Specify which agents are alive This is used to determine which callbacks to call. Parameters ---------- alive_agents : typing.Iterable[str] Agents that are currently alive. \"\"\" self . _agent_filter = alive_agents set_scale_down ( self , scale ) \u00a4 set_scale_down sets a value for the reward dict to scale down the rewards by Parameters \u00a4 scale : int The value to divide all rewards in the reward dict by Source code in corl/libraries/environment_dict.py def set_scale_down ( self , scale : int ): \"\"\" set_scale_down sets a value for the reward dict to scale down the rewards by Parameters ---------- scale : int The value to divide all rewards in the reward dict by \"\"\" self . _reduce_fn_kwargs [ RewardDict . SCALE_KEY ] = scale","title":"Environment dict"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback","text":"Callback provides basic callback processing for all reward and done functions Source code in corl/libraries/environment_dict.py class Callback : \"\"\" Callback provides basic callback processing for all reward and done functions \"\"\" def __init__ ( self , funcs : typing . Optional [ typing . Sequence [ typing . Callable ]] = None ) -> None : \"\"\" __init__ constructor Parameters ---------- funcs : typing.List[typing.Callable], optional List of callale functions, by default None \"\"\" self . _process_callbacks : typing . List [ typing . Callable ] = [] self . _logger = logging . getLogger ( Callback . __name__ ) if funcs : self . register_funcs ( funcs ) def register_func ( self , func : typing . Callable ) -> None : \"\"\" register_func registers a function to the list of valid functions Parameters ---------- func : typing.Callable The callable function \"\"\" # MTB - 10/15/2020 - There seems to be an issue with this when combining multiple reward sets # if func is a callable the expression func in self._process_callbacks will always return True if isinstance ( func , Callback ): self . _process_callbacks . append ( func ) else : if func not in self . _process_callbacks : self . _process_callbacks . append ( func ) else : warnings . warn ( \"Ignoring a duplicate callback given\" ) def register_funcs ( self , funcs : typing . Optional [ typing . Sequence [ typing . Callable ]]): \"\"\" register_func registers a list of functions to the list of valid functions Parameters ---------- func : typing.Callable The callable function \"\"\" for func in funcs or []: self . register_func ( func ) def unregister_func ( self , func : typing . Callable ) -> None : \"\"\" unregister callbacks from processing Parameters ---------- key : str The callback string to remove \"\"\" if func in self . _process_callbacks : self . _process_callbacks . remove ( func ) def unregister_funcs ( self ) -> None : \"\"\" unregister callbacks from processing \"\"\" self . _process_callbacks . clear () def reset_funcs ( self ): \"\"\" reset_funcs func is a callable then attempt to reset its state. Parameters ---------- key : str The callback string to remove \"\"\" for func in self . _process_callbacks : reset_op = getattr ( func , \"reset\" , None ) if callable ( reset_op ): func . reset () @property def process_callbacks ( self ) -> typing . List [ typing . Callable ]: \"\"\" process_callbacks gets the current callbacks Returns ------- typing.List[typing.Callable] Current list of callbacks \"\"\" return self . _process_callbacks","title":"Callback"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.process_callbacks","text":"process_callbacks gets the current callbacks","title":"process_callbacks"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.process_callbacks--returns","text":"typing.List[typing.Callable] Current list of callbacks","title":"Returns"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.__init__","text":"init constructor","title":"__init__()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.__init__--parameters","text":"funcs : typing.List[typing.Callable], optional List of callale functions, by default None Source code in corl/libraries/environment_dict.py def __init__ ( self , funcs : typing . Optional [ typing . Sequence [ typing . Callable ]] = None ) -> None : \"\"\" __init__ constructor Parameters ---------- funcs : typing.List[typing.Callable], optional List of callale functions, by default None \"\"\" self . _process_callbacks : typing . List [ typing . Callable ] = [] self . _logger = logging . getLogger ( Callback . __name__ ) if funcs : self . register_funcs ( funcs )","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.register_func","text":"register_func registers a function to the list of valid functions","title":"register_func()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.register_func--parameters","text":"func : typing.Callable The callable function Source code in corl/libraries/environment_dict.py def register_func ( self , func : typing . Callable ) -> None : \"\"\" register_func registers a function to the list of valid functions Parameters ---------- func : typing.Callable The callable function \"\"\" # MTB - 10/15/2020 - There seems to be an issue with this when combining multiple reward sets # if func is a callable the expression func in self._process_callbacks will always return True if isinstance ( func , Callback ): self . _process_callbacks . append ( func ) else : if func not in self . _process_callbacks : self . _process_callbacks . append ( func ) else : warnings . warn ( \"Ignoring a duplicate callback given\" )","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.register_funcs","text":"register_func registers a list of functions to the list of valid functions","title":"register_funcs()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.register_funcs--parameters","text":"func : typing.Callable The callable function Source code in corl/libraries/environment_dict.py def register_funcs ( self , funcs : typing . Optional [ typing . Sequence [ typing . Callable ]]): \"\"\" register_func registers a list of functions to the list of valid functions Parameters ---------- func : typing.Callable The callable function \"\"\" for func in funcs or []: self . register_func ( func )","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.reset_funcs","text":"reset_funcs func is a callable then attempt to reset its state.","title":"reset_funcs()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.reset_funcs--parameters","text":"key : str The callback string to remove Source code in corl/libraries/environment_dict.py def reset_funcs ( self ): \"\"\" reset_funcs func is a callable then attempt to reset its state. Parameters ---------- key : str The callback string to remove \"\"\" for func in self . _process_callbacks : reset_op = getattr ( func , \"reset\" , None ) if callable ( reset_op ): func . reset ()","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.unregister_func","text":"unregister callbacks from processing","title":"unregister_func()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.unregister_func--parameters","text":"key : str The callback string to remove Source code in corl/libraries/environment_dict.py def unregister_func ( self , func : typing . Callable ) -> None : \"\"\" unregister callbacks from processing Parameters ---------- key : str The callback string to remove \"\"\" if func in self . _process_callbacks : self . _process_callbacks . remove ( func )","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.Callback.unregister_funcs","text":"unregister callbacks from processing Source code in corl/libraries/environment_dict.py def unregister_funcs ( self ) -> None : \"\"\" unregister callbacks from processing \"\"\" self . _process_callbacks . clear ()","title":"unregister_funcs()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.DoneDict","text":"[summary]","title":"DoneDict"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.DoneDict--parameters","text":"EnvDict : [type] [description] Source code in corl/libraries/environment_dict.py class DoneDict ( EnvDict ): \"\"\"[summary] Parameters ---------- EnvDict : [type] [description] \"\"\" def __init__ ( self , processing_funcs : typing . Sequence [ typing . Callable ] = None , reduce_fn : typing . Callable = None , reduce_fn_kwargs = None , ** kwargs , ) -> None : super () . __init__ ( processing_funcs = processing_funcs , reduce_fn = reduce_fn , reduce_fn_kwargs = reduce_fn_kwargs , ** kwargs ) self . _agent_filter : typing . Optional [ typing . Iterable [ str ]] = None def __call__ ( self , * args , ** kwargs ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" __call__ Callable function for the done dictionary type Returns ------- typing.Tuple[OrderedDict, OrderedDict] The done information \"\"\" r = super () . __call__ ( * args , ** kwargs ) # Check for bool type in return value for key , value in r [ 0 ] . items (): if isinstance ( value , np . bool_ ): r [ 0 ][ key ] = bool ( value ) elif not isinstance ( value , bool ): raise TypeError ( \"DoneDict __call__ return is not type bool for key: {} \" . format ( key )) for key0 , value0 in r [ 1 ] . items (): for key1 , value1 in value0 . items (): if isinstance ( value1 , np . bool_ ): r [ 1 ][ key0 ][ key1 ] = bool ( value1 ) elif not isinstance ( value1 , bool ): raise TypeError ( \"DoneDict __call__ return is not type bool for key: {} / {} \" . format ( key0 , key1 )) return r @property def _filtered_process_callbacks ( self ) -> typing . List [ typing . Callable ]: if self . _agent_filter is None : return super () . _filtered_process_callbacks # Avoid circular import from corl.dones.done_func_base import DoneFuncBase # pylint: disable=import-outside-toplevel return [ x for x in self . _process_callbacks if not isinstance ( x , DoneFuncBase ) or x . agent in self . _agent_filter ] def set_alive_agents ( self , alive_agents : typing . Iterable [ str ]) -> None : \"\"\"Specify which agents are alive This is used to determine which callbacks to call. Parameters ---------- alive_agents : typing.Iterable[str] Agents that are currently alive. \"\"\" self . _agent_filter = alive_agents def _reduce ( self , r , ** kwargs ): self . _reduce_fn = self . _reduce_fn or np . any tmp = StateDict . stack_values ( r ) tmp = { k : self . _reduce_fn ( v , ** kwargs ) for k , v in tmp . items ()} return StateDict ( sorted ( tmp . items ()))","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.DoneDict.__call__","text":"call Callable function for the done dictionary type","title":"__call__()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.DoneDict.__call__--returns","text":"typing.Tuple[OrderedDict, OrderedDict] The done information Source code in corl/libraries/environment_dict.py def __call__ ( self , * args , ** kwargs ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" __call__ Callable function for the done dictionary type Returns ------- typing.Tuple[OrderedDict, OrderedDict] The done information \"\"\" r = super () . __call__ ( * args , ** kwargs ) # Check for bool type in return value for key , value in r [ 0 ] . items (): if isinstance ( value , np . bool_ ): r [ 0 ][ key ] = bool ( value ) elif not isinstance ( value , bool ): raise TypeError ( \"DoneDict __call__ return is not type bool for key: {} \" . format ( key )) for key0 , value0 in r [ 1 ] . items (): for key1 , value1 in value0 . items (): if isinstance ( value1 , np . bool_ ): r [ 1 ][ key0 ][ key1 ] = bool ( value1 ) elif not isinstance ( value1 , bool ): raise TypeError ( \"DoneDict __call__ return is not type bool for key: {} / {} \" . format ( key0 , key1 )) return r","title":"Returns"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.DoneDict.set_alive_agents","text":"Specify which agents are alive This is used to determine which callbacks to call.","title":"set_alive_agents()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.DoneDict.set_alive_agents--parameters","text":"alive_agents : typing.Iterable[str] Agents that are currently alive. Source code in corl/libraries/environment_dict.py def set_alive_agents ( self , alive_agents : typing . Iterable [ str ]) -> None : \"\"\"Specify which agents are alive This is used to determine which callbacks to call. Parameters ---------- alive_agents : typing.Iterable[str] Agents that are currently alive. \"\"\" self . _agent_filter = alive_agents","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict","text":"[summary]","title":"EnvDict"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict--parameters","text":"StateDict : [type] [description] Callback : [type] [description]","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict--returns","text":"[type] [description] Source code in corl/libraries/environment_dict.py class EnvDict ( StateDict , Callback ): \"\"\"[summary] Parameters ---------- StateDict : [type] [description] Callback : [type] [description] Returns ------- [type] [description] \"\"\" # list of items to exclude when generating the keys, items, values EXCLUDE_KEYS = [ \"_default_kwargs\" , \"_reduce_fn\" , \"_reduce_fn_kwargs\" , \"_processing_funcs\" , \"_recurse\" , ] class DuplicateName ( RuntimeError ): \"\"\"Exception class for callbacks with duplicate names\"\"\" def __init__ ( self , processing_funcs : typing . Sequence [ typing . Callable ] = None , reduce_fn : typing . Callable = None , reduce_fn_kwargs = None , ** kwargs , ) -> None : \"\"\" __init__ environment dictionary constructor Parameters ---------- processing_funcs : typing.List[typing.Callable], optional List of functions to call by the environment, by default None reduce_fn : typing.Callable, optional function used to reduce the results, by default None reduce_fn_kwargs : [type], optional [description], by default None \"\"\" self . _default_kwargs = None self . _reduce_fn = reduce_fn self . _reduce_fn_kwargs = reduce_fn_kwargs or {} self . _set_default_kwargs ( kwargs ) StateDict . __init__ ( self , ** kwargs ) Callback . __init__ ( self , processing_funcs ) def __call__ ( self , * args , ** kwargs ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" __call__ Callable function for the environment dictionary type Returns ------- typing.Tuple[OrderedDict, OrderedDict] The reduced rewards and theret information \"\"\" r = [ self . _default_kwargs , ] ret_info : OrderedDict = OrderedDict () def merge ( source , destination ): \"\"\" run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , {}) merge ( value , node ) else : destination [ key ] = value return destination for func in self . _filtered_process_callbacks : if isinstance ( func , OrderedDict ): ret = func ( * args , ** kwargs ) rew , info = ret r . append ( rew ) ret_info = merge ( ret_info , info ) for func in self . _filtered_process_callbacks : if not isinstance ( func , OrderedDict ): ret = func ( * args , ** kwargs ) # single value r . append ( copy . deepcopy ( ret )) try : name = func . __name__ except : # noqa: E722 # pylint: disable=bare-except name = func . name # type: ignore # This only affects the info dictionary that is returned. As the code below merges the output for all agents together, # the __all__ entry would be overwritten to only provide the information of the last agent, which could be confusing or # inaccurate. Therefore, remove __all__ from the returned information. if '__all__' in ret : del ret [ '__all__' ] if name in ret_info . keys (): common_keys = set ( ret . keys ()) & set ( ret_info [ name ] . keys ()) if common_keys : raise self . DuplicateName ( f ' { name } has common keys: { common_keys } ' ) ret_info [ name ] . update ( ** ret ) else : ret_info [ name ] = ret # TODO link with reduce and ret info return self . _reduce ( r , ** self . _reduce_fn_kwargs ), ret_info # type: ignore @property def _filtered_process_callbacks ( self ) -> typing . List [ typing . Callable ]: \"\"\"Set of callbacks that have been filtered by subclass logic. Default implementation is all callbacks Returns ------- typing.List[typing.Callable] Callbacks to apply \"\"\" return self . _process_callbacks @abc . abstractmethod def _reduce ( self , r : typing . Callable , ** kwargs ): \"\"\" _reduce user defined reduce function for processing Parameters ---------- r : typing.Callable The reduce function to use \"\"\" ... def _set_default_kwargs ( self , kwargs ): \"\"\" _set_default_kwargs [summary] [extended_summary] Parameters ---------- kwargs : [type] [description] \"\"\" if self . _default_kwargs is None : self . _default_kwargs = copy . deepcopy ( kwargs ) def reset ( self ): \"\"\" reset [summary] \"\"\" # EnvDict.__init__(self, **self._default_kwargs) self . reset_funcs () def _filtered_self ( self ): tmp = { k : v for k , v in super () . items () if k not in self . EXCLUDE_KEYS } tmp = StateDict ( tmp ) return tmp def keys ( self ): return self . _filtered_self () . keys () def values ( self ): return self . _filtered_self () . values () def items ( self ): return self . _filtered_self () . items () def to_dict ( self ): return self . _filtered_self () . to_dict () @property def name ( self ): \"\"\"[summary] Returns ------- [type] [description] \"\"\" return type ( self ) . __name__ @staticmethod def observation_deep_copy ( d1 , d2 ): \"\"\"[summary] Parameters ---------- d1 : [type] [description] d2 : [type] [description] \"\"\" # d1 = pickle.loads(pickle.dumps(d2)) for ( k1 , v1 ), ( k2 , v2 ) in zip ( d1 . items (), d2 . items ()): if isinstance ( v1 , dict ): EnvDict . observation_deep_copy ( v1 , v2 ) else : d1 [ k1 ] = copy . deepcopy ( d2 [ k2 ])","title":"Returns"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.name","text":"[summary]","title":"name"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.name--returns","text":"[type] [description]","title":"Returns"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.DuplicateName","text":"Exception class for callbacks with duplicate names Source code in corl/libraries/environment_dict.py class DuplicateName ( RuntimeError ): \"\"\"Exception class for callbacks with duplicate names\"\"\"","title":"DuplicateName"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.__call__","text":"call Callable function for the environment dictionary type","title":"__call__()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.__call__--returns","text":"typing.Tuple[OrderedDict, OrderedDict] The reduced rewards and theret information Source code in corl/libraries/environment_dict.py def __call__ ( self , * args , ** kwargs ) -> typing . Tuple [ OrderedDict , OrderedDict ]: \"\"\" __call__ Callable function for the environment dictionary type Returns ------- typing.Tuple[OrderedDict, OrderedDict] The reduced rewards and theret information \"\"\" r = [ self . _default_kwargs , ] ret_info : OrderedDict = OrderedDict () def merge ( source , destination ): \"\"\" run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , {}) merge ( value , node ) else : destination [ key ] = value return destination for func in self . _filtered_process_callbacks : if isinstance ( func , OrderedDict ): ret = func ( * args , ** kwargs ) rew , info = ret r . append ( rew ) ret_info = merge ( ret_info , info ) for func in self . _filtered_process_callbacks : if not isinstance ( func , OrderedDict ): ret = func ( * args , ** kwargs ) # single value r . append ( copy . deepcopy ( ret )) try : name = func . __name__ except : # noqa: E722 # pylint: disable=bare-except name = func . name # type: ignore # This only affects the info dictionary that is returned. As the code below merges the output for all agents together, # the __all__ entry would be overwritten to only provide the information of the last agent, which could be confusing or # inaccurate. Therefore, remove __all__ from the returned information. if '__all__' in ret : del ret [ '__all__' ] if name in ret_info . keys (): common_keys = set ( ret . keys ()) & set ( ret_info [ name ] . keys ()) if common_keys : raise self . DuplicateName ( f ' { name } has common keys: { common_keys } ' ) ret_info [ name ] . update ( ** ret ) else : ret_info [ name ] = ret # TODO link with reduce and ret info return self . _reduce ( r , ** self . _reduce_fn_kwargs ), ret_info # type: ignore","title":"Returns"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.__init__","text":"init environment dictionary constructor","title":"__init__()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.__init__--parameters","text":"processing_funcs : typing.List[typing.Callable], optional List of functions to call by the environment, by default None reduce_fn : typing.Callable, optional function used to reduce the results, by default None reduce_fn_kwargs : [type], optional [description], by default None Source code in corl/libraries/environment_dict.py def __init__ ( self , processing_funcs : typing . Sequence [ typing . Callable ] = None , reduce_fn : typing . Callable = None , reduce_fn_kwargs = None , ** kwargs , ) -> None : \"\"\" __init__ environment dictionary constructor Parameters ---------- processing_funcs : typing.List[typing.Callable], optional List of functions to call by the environment, by default None reduce_fn : typing.Callable, optional function used to reduce the results, by default None reduce_fn_kwargs : [type], optional [description], by default None \"\"\" self . _default_kwargs = None self . _reduce_fn = reduce_fn self . _reduce_fn_kwargs = reduce_fn_kwargs or {} self . _set_default_kwargs ( kwargs ) StateDict . __init__ ( self , ** kwargs ) Callback . __init__ ( self , processing_funcs )","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.items","text":"D.items() -> a set-like object providing a view on D's items Source code in corl/libraries/environment_dict.py def items ( self ): return self . _filtered_self () . items ()","title":"items()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.keys","text":"D.keys() -> a set-like object providing a view on D's keys Source code in corl/libraries/environment_dict.py def keys ( self ): return self . _filtered_self () . keys ()","title":"keys()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.observation_deep_copy","text":"[summary]","title":"observation_deep_copy()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.observation_deep_copy--parameters","text":"d1 : [type] [description] d2 : [type] [description] Source code in corl/libraries/environment_dict.py @staticmethod def observation_deep_copy ( d1 , d2 ): \"\"\"[summary] Parameters ---------- d1 : [type] [description] d2 : [type] [description] \"\"\" # d1 = pickle.loads(pickle.dumps(d2)) for ( k1 , v1 ), ( k2 , v2 ) in zip ( d1 . items (), d2 . items ()): if isinstance ( v1 , dict ): EnvDict . observation_deep_copy ( v1 , v2 ) else : d1 [ k1 ] = copy . deepcopy ( d2 [ k2 ])","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.reset","text":"reset [summary] Source code in corl/libraries/environment_dict.py def reset ( self ): \"\"\" reset [summary] \"\"\" # EnvDict.__init__(self, **self._default_kwargs) self . reset_funcs ()","title":"reset()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.to_dict","text":"Converts to a dictionary","title":"to_dict()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.to_dict--returns","text":"dict the dict form Source code in corl/libraries/environment_dict.py def to_dict ( self ): return self . _filtered_self () . to_dict ()","title":"Returns"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.EnvDict.values","text":"D.values() -> an object providing a view on D's values Source code in corl/libraries/environment_dict.py def values ( self ): return self . _filtered_self () . values ()","title":"values()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.InfoDict","text":"[summary]","title":"InfoDict"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.InfoDict--parameters","text":"EnvDict : [type] [description] Source code in corl/libraries/environment_dict.py class InfoDict ( EnvDict ): \"\"\"[summary] Parameters ---------- EnvDict : [type] [description] \"\"\" def _reduce ( self , r , ** kwargs ): self . _reduce_fn = self . _reduce_fn or ( lambda x : {}) tmp = StateDict . stack_values ( r ) tmp = { k : self . _reduce_fn ( v , ** kwargs ) for k , v in tmp . items ()} return StateDict ( sorted ( tmp . items ()), recurse = False )","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.RewardDict","text":"[summary]","title":"RewardDict"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.RewardDict--parameters","text":"EnvDict : [type] [description]","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.RewardDict--returns","text":"[type] [description] Source code in corl/libraries/environment_dict.py class RewardDict ( EnvDict ): \"\"\"[summary] Parameters ---------- EnvDict : [type] [description] Returns ------- [type] [description] \"\"\" SCALE_KEY = \"SCALE\" def __init__ ( self , processing_funcs : typing . Sequence [ typing . Callable ] = None , reduce_fn : typing . Callable = None , reduce_fn_kwargs = None , ** kwargs , ) -> None : super () . __init__ ( processing_funcs = processing_funcs , reduce_fn = reduce_fn , reduce_fn_kwargs = reduce_fn_kwargs , ** kwargs ) self . _agent_filter : typing . Optional [ typing . Iterable [ str ]] = None @property def _filtered_process_callbacks ( self ) -> typing . List [ typing . Callable ]: if self . _agent_filter is None : return super () . _filtered_process_callbacks # Avoid circular import from corl.rewards.reward_func_base import RewardFuncBase # pylint: disable=import-outside-toplevel # TODO: Make it so that RewardFuncBase has agent_id as part of its base class API # Need to disable yapf because it puts this all on a single line, which then fails pylint because the line is too long. # yapf: disable return [ x for x in self . _process_callbacks if not isinstance ( x , RewardFuncBase ) or x . _agent_id in self . _agent_filter ] # pylint: disable=protected-access # yapf: enable def set_alive_agents ( self , alive_agents : typing . Iterable [ str ]) -> None : \"\"\"Specify which agents are alive This is used to determine which callbacks to call. Parameters ---------- alive_agents : typing.Iterable[str] Agents that are currently alive. \"\"\" self . _agent_filter = alive_agents def _reduce ( self , r , ** kwargs ): scale = 1.0 if RewardDict . SCALE_KEY in kwargs : scale = kwargs [ RewardDict . SCALE_KEY ] del kwargs [ RewardDict . SCALE_KEY ] self . _reduce_fn = self . _reduce_fn or np . sum tmp = StateDict . stack_values ( r ) tmp = { k : self . _reduce_fn ( v , ** kwargs ) / scale for k , v in tmp . items ()} return OrderedDict ( sorted ( tmp . items ())) def set_scale_down ( self , scale : int ): \"\"\" set_scale_down sets a value for the reward dict to scale down the rewards by Parameters ---------- scale : int The value to divide all rewards in the reward dict by \"\"\" self . _reduce_fn_kwargs [ RewardDict . SCALE_KEY ] = scale","title":"Returns"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.RewardDict.set_alive_agents","text":"Specify which agents are alive This is used to determine which callbacks to call.","title":"set_alive_agents()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.RewardDict.set_alive_agents--parameters","text":"alive_agents : typing.Iterable[str] Agents that are currently alive. Source code in corl/libraries/environment_dict.py def set_alive_agents ( self , alive_agents : typing . Iterable [ str ]) -> None : \"\"\"Specify which agents are alive This is used to determine which callbacks to call. Parameters ---------- alive_agents : typing.Iterable[str] Agents that are currently alive. \"\"\" self . _agent_filter = alive_agents","title":"Parameters"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.RewardDict.set_scale_down","text":"set_scale_down sets a value for the reward dict to scale down the rewards by","title":"set_scale_down()"},{"location":"reference/libraries/environment_dict/#corl.libraries.environment_dict.RewardDict.set_scale_down--parameters","text":"scale : int The value to divide all rewards in the reward dict by Source code in corl/libraries/environment_dict.py def set_scale_down ( self , scale : int ): \"\"\" set_scale_down sets a value for the reward dict to scale down the rewards by Parameters ---------- scale : int The value to divide all rewards in the reward dict by \"\"\" self . _reduce_fn_kwargs [ RewardDict . SCALE_KEY ] = scale","title":"Parameters"},{"location":"reference/libraries/factory/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Factory ( BaseModel ) pydantic-model \u00a4 Factory class to allow subclass creation with pydantic TODO: Create a simple motivating example Source code in corl/libraries/factory.py class Factory ( BaseModel ): \"\"\"Factory class to allow subclass creation with pydantic TODO: Create a simple motivating example \"\"\" type : PyObject config : Dict [ str , Any ] = {} def build ( self , ** kwargs ): \"\"\"Build the object contained within this factory.\"\"\" return self . type ( ** self . config , ** kwargs ) @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build () build ( self , ** kwargs ) \u00a4 Build the object contained within this factory. Source code in corl/libraries/factory.py def build ( self , ** kwargs ): \"\"\"Build the object contained within this factory.\"\"\" return self . type ( ** self . config , ** kwargs ) resolve_factory ( v ) classmethod \u00a4 Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) Source code in corl/libraries/factory.py @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build ()","title":"Factory"},{"location":"reference/libraries/factory/#corl.libraries.factory.Factory","text":"Factory class to allow subclass creation with pydantic TODO: Create a simple motivating example Source code in corl/libraries/factory.py class Factory ( BaseModel ): \"\"\"Factory class to allow subclass creation with pydantic TODO: Create a simple motivating example \"\"\" type : PyObject config : Dict [ str , Any ] = {} def build ( self , ** kwargs ): \"\"\"Build the object contained within this factory.\"\"\" return self . type ( ** self . config , ** kwargs ) @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build ()","title":"Factory"},{"location":"reference/libraries/factory/#corl.libraries.factory.Factory.build","text":"Build the object contained within this factory. Source code in corl/libraries/factory.py def build ( self , ** kwargs ): \"\"\"Build the object contained within this factory.\"\"\" return self . type ( ** self . config , ** kwargs )","title":"build()"},{"location":"reference/libraries/factory/#corl.libraries.factory.Factory.resolve_factory","text":"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) Source code in corl/libraries/factory.py @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build ()","title":"resolve_factory()"},{"location":"reference/libraries/functor/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Functor ( BaseModel ) pydantic-model \u00a4 name: The optional name of the functor. class: The class of the functor. config: The functor's specific configuration dictionary. Source code in corl/libraries/functor.py class Functor ( BaseModel ): \"\"\" - name: The optional name of the functor. - class: The class of the functor. - config: The functor's specific configuration dictionary. \"\"\" functor : PyObject name : Annotated [ str , Field ( strip_whitespace = True , min_length = 1 )] = '' config : Dict [ str , ObjectStoreElem ] = {} references : Dict [ str , str ] = {} # This might not be necessary after Parameter has pydantic validation class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True extra = 'forbid' @validator ( 'name' , pre = True , always = True ) def name_from_functor ( cls , v , values ): \"\"\"Create default value for name from the functor name\"\"\" # \"'functor' not in values\" means PyObject failed and an error is coming # Check for it here because that error is more helpful than a KeyError here. if not v and 'functor' in values : return values [ 'functor' ] . __name__ return v resolve_factory = validator ( 'config' , pre = True , each_item = True , allow_reuse = True )( Factory . resolve_factory ) def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description \"\"\" functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , ** functor_args , ** kwargs ) def resolve_storage_and_references ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ): \"\"\" Resolve parameter storage and references to get direct functor arguments.\"\"\" functor_args : Dict [ str , Any ] = {} functor_units = getattr ( self . functor , 'REQUIRED_UNITS' , {}) # Is there a units field in the configuration file? config_units = self . config . get ( 'unit' , None ) if config_units is not None and not isinstance ( config_units , str ): raise TypeError ( f 'Units of { self . name } are \" { config_units } \", which is not a string' ) for arg_name , arg_value in self . config . items (): # Resolve parameter values if isinstance ( arg_value , Parameter ): for source in param_sources : if arg_name in source . get ( self . name , {}): resolved_value = source [ self . name ][ arg_name ] break else : # This \"else\" means \"no break encountered\", which means that the argument was not found in any source raise RuntimeError ( f 'Could not resolve argument { arg_name } for { self . name } from the parameter sources' ) else : resolved_value = arg_value # Resolve units functor_args [ arg_name ] = self . _resolve_units ( name = arg_name , value = resolved_value , functor_units = functor_units , config_units = config_units , error_name = self . name ) for ref_dest , ref_src in self . references . items (): # Resolve references for source in ref_sources : if ref_src in source : ref_obj = source [ ref_src ] if not isinstance ( ref_obj , Parameter ): break else : # This \"else\" means \"no break encountered\", which means either: # 1. ref_src was not found in any source # 2. ref_src was found; however, it was an unresolved Parameter raise RuntimeError ( f 'Could not find { ref_src } for { self . name } in the reference storage' ) # Resolve units functor_args [ ref_dest ] = self . _resolve_units ( name = ref_dest , value = ref_obj , functor_units = functor_units , config_units = config_units , error_name = self . name ) return functor_args def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" parameters = { k : v for k , v in self . config . items () if isinstance ( v , Parameter )} if parameters : if self . name in parameter_store : raise ValueError ( f 'Duplicate functor name in parameter store: { self . name } ' ) parameter_store [ self . name ] = parameters @staticmethod def _resolve_units ( name : str , value : ObjectStoreElem , functor_units : Dict [ str , Union [ None , Literal [ True ], enum . Enum ]], config_units : Optional [ str ], error_name : str ) -> Any : assert not isinstance ( value , Parameter ) functor_arg : Any # Determine what to pass in to functor if isinstance ( value , ValueWithUnits ): # Value has units if name in functor_units : # Extra local variable required so that MyPy does proper type reduction in the conditional block below. these_units = functor_units [ name ] if these_units is True : # Require it to be actually boolean True, not just \"truthy\" functor_arg = value else : functor_arg = value . as_units ( these_units ) elif config_units is not None : functor_arg = value . as_units ( config_units ) elif value . units is NoneUnitType . NoneUnit : functor_arg = value . value else : raise RuntimeError ( f 'Argument { name } of { error_name } has units { value . units } , but none are expected' ) else : # Value has no units # Functor requires them and the unit is not None if functor_units . get ( name , NoneUnitType . NoneUnit ) not in [ None , NoneUnitType . NoneUnit ]: raise RuntimeError ( f 'Argument { name } of { error_name } is missing required units' ) # Functor does not require them functor_arg = value return functor_arg Config \u00a4 Allow arbitrary types for Parameter Source code in corl/libraries/functor.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True extra = 'forbid' add_to_parameter_store ( self , parameter_store ) \u00a4 Add the parameters of this functor to an external parameter store. Parameters \u00a4 parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. Source code in corl/libraries/functor.py def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" parameters = { k : v for k , v in self . config . items () if isinstance ( v , Parameter )} if parameters : if self . name in parameter_store : raise ValueError ( f 'Duplicate functor name in parameter store: { self . name } ' ) parameter_store [ self . name ] = parameters create_functor_object ( self , param_sources = (), ref_sources = (), ** kwargs ) \u00a4 Create the object with this functor TODO: Better description Source code in corl/libraries/functor.py def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description \"\"\" functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , ** functor_args , ** kwargs ) name_from_functor ( v , values ) classmethod \u00a4 Create default value for name from the functor name Source code in corl/libraries/functor.py @validator ( 'name' , pre = True , always = True ) def name_from_functor ( cls , v , values ): \"\"\"Create default value for name from the functor name\"\"\" # \"'functor' not in values\" means PyObject failed and an error is coming # Check for it here because that error is more helpful than a KeyError here. if not v and 'functor' in values : return values [ 'functor' ] . __name__ return v resolve_factory ( v ) classmethod \u00a4 Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) Source code in corl/libraries/functor.py @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build () resolve_storage_and_references ( self , param_sources = (), ref_sources = ()) \u00a4 Resolve parameter storage and references to get direct functor arguments. Source code in corl/libraries/functor.py def resolve_storage_and_references ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ): \"\"\" Resolve parameter storage and references to get direct functor arguments.\"\"\" functor_args : Dict [ str , Any ] = {} functor_units = getattr ( self . functor , 'REQUIRED_UNITS' , {}) # Is there a units field in the configuration file? config_units = self . config . get ( 'unit' , None ) if config_units is not None and not isinstance ( config_units , str ): raise TypeError ( f 'Units of { self . name } are \" { config_units } \", which is not a string' ) for arg_name , arg_value in self . config . items (): # Resolve parameter values if isinstance ( arg_value , Parameter ): for source in param_sources : if arg_name in source . get ( self . name , {}): resolved_value = source [ self . name ][ arg_name ] break else : # This \"else\" means \"no break encountered\", which means that the argument was not found in any source raise RuntimeError ( f 'Could not resolve argument { arg_name } for { self . name } from the parameter sources' ) else : resolved_value = arg_value # Resolve units functor_args [ arg_name ] = self . _resolve_units ( name = arg_name , value = resolved_value , functor_units = functor_units , config_units = config_units , error_name = self . name ) for ref_dest , ref_src in self . references . items (): # Resolve references for source in ref_sources : if ref_src in source : ref_obj = source [ ref_src ] if not isinstance ( ref_obj , Parameter ): break else : # This \"else\" means \"no break encountered\", which means either: # 1. ref_src was not found in any source # 2. ref_src was found; however, it was an unresolved Parameter raise RuntimeError ( f 'Could not find { ref_src } for { self . name } in the reference storage' ) # Resolve units functor_args [ ref_dest ] = self . _resolve_units ( name = ref_dest , value = ref_obj , functor_units = functor_units , config_units = config_units , error_name = self . name ) return functor_args FunctorDictWrapper ( Functor ) pydantic-model \u00a4 wrapped: The dict of functor or functor wrapper configurations wrapped by this wrapper Source code in corl/libraries/functor.py class FunctorDictWrapper ( Functor ): \"\"\" wrapped: The dict of functor or functor wrapper configurations wrapped by this wrapper \"\"\" wrapped : Dict [ str , Union [ 'FunctorDictWrapper' , FunctorMultiWrapper , FunctorWrapper , Functor ]] @validator ( 'wrapped' , pre = True ) def adjust_wrapped_name ( cls , v ): \"\"\"Use the dictionary wrapping key as the name if none is provided.\"\"\" if not isinstance ( v , Mapping_Type ): return v # this pre validator is pretty dangerous in terms of # what it will attempt to run on, because a regular wrapper # has a dictionary it will attempt to perform this operation # on regular wrappers, so we are just going to verify that # this is not a regular wrapper by checking for a functor appearing # in wrapped if \"functor\" in v . keys (): return v for key , value in v . items (): try : if 'name' not in value : value [ 'name' ] = key except : # pylint: disable=bare-except # noqa: E722 pass return v def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_funcs = { k : v . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) for k , v in self . wrapped . items () } functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_funcs , ** functor_args , ** kwargs ) def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) for wrapped in self . wrapped . values (): wrapped . add_to_parameter_store ( parameter_store ) add_to_parameter_store ( self , parameter_store ) \u00a4 Add the parameters of this functor to an external parameter store. Parameters \u00a4 parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. Source code in corl/libraries/functor.py def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) for wrapped in self . wrapped . values (): wrapped . add_to_parameter_store ( parameter_store ) adjust_wrapped_name ( v ) classmethod \u00a4 Use the dictionary wrapping key as the name if none is provided. Source code in corl/libraries/functor.py @validator ( 'wrapped' , pre = True ) def adjust_wrapped_name ( cls , v ): \"\"\"Use the dictionary wrapping key as the name if none is provided.\"\"\" if not isinstance ( v , Mapping_Type ): return v # this pre validator is pretty dangerous in terms of # what it will attempt to run on, because a regular wrapper # has a dictionary it will attempt to perform this operation # on regular wrappers, so we are just going to verify that # this is not a regular wrapper by checking for a functor appearing # in wrapped if \"functor\" in v . keys (): return v for key , value in v . items (): try : if 'name' not in value : value [ 'name' ] = key except : # pylint: disable=bare-except # noqa: E722 pass return v create_functor_object ( self , param_sources = (), ref_sources = (), ** kwargs ) \u00a4 Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches Source code in corl/libraries/functor.py def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_funcs = { k : v . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) for k , v in self . wrapped . items () } functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_funcs , ** functor_args , ** kwargs ) FunctorMultiWrapper ( Functor ) pydantic-model \u00a4 wrapped: The functor or functor wrapper configuration wrapped by this functor wrapper. Source code in corl/libraries/functor.py class FunctorMultiWrapper ( Functor ): \"\"\" wrapped: The functor or functor wrapper configuration wrapped by this functor wrapper. \"\"\" wrapped : List [ Union [ 'FunctorMultiWrapper' , FunctorWrapper , Functor , 'FunctorDictWrapper' ]] def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_funcs = [ x . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) for x in self . wrapped ] functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_funcs , ** functor_args , ** kwargs ) def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) for wrapped in self . wrapped : wrapped . add_to_parameter_store ( parameter_store ) add_to_parameter_store ( self , parameter_store ) \u00a4 Add the parameters of this functor to an external parameter store. Parameters \u00a4 parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. Source code in corl/libraries/functor.py def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) for wrapped in self . wrapped : wrapped . add_to_parameter_store ( parameter_store ) create_functor_object ( self , param_sources = (), ref_sources = (), ** kwargs ) \u00a4 Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches Source code in corl/libraries/functor.py def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_funcs = [ x . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) for x in self . wrapped ] functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_funcs , ** functor_args , ** kwargs ) FunctorWrapper ( Functor ) pydantic-model \u00a4 wrapped: The functor or functor wrapper configuration wrapped by this functor wrapper. Source code in corl/libraries/functor.py class FunctorWrapper ( Functor ): \"\"\" wrapped: The functor or functor wrapper configuration wrapped by this functor wrapper. \"\"\" wrapped : Union [ 'FunctorMultiWrapper' , 'FunctorWrapper' , 'FunctorDictWrapper' , Functor ] def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_func = self . wrapped . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_func , ** functor_args , ** kwargs ) def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) self . wrapped . add_to_parameter_store ( parameter_store ) add_to_parameter_store ( self , parameter_store ) \u00a4 Add the parameters of this functor to an external parameter store. Parameters \u00a4 parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. Source code in corl/libraries/functor.py def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) self . wrapped . add_to_parameter_store ( parameter_store ) create_functor_object ( self , param_sources = (), ref_sources = (), ** kwargs ) \u00a4 Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches Source code in corl/libraries/functor.py def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_func = self . wrapped . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_func , ** functor_args , ** kwargs )","title":"Functor"},{"location":"reference/libraries/functor/#corl.libraries.functor.Functor","text":"name: The optional name of the functor. class: The class of the functor. config: The functor's specific configuration dictionary. Source code in corl/libraries/functor.py class Functor ( BaseModel ): \"\"\" - name: The optional name of the functor. - class: The class of the functor. - config: The functor's specific configuration dictionary. \"\"\" functor : PyObject name : Annotated [ str , Field ( strip_whitespace = True , min_length = 1 )] = '' config : Dict [ str , ObjectStoreElem ] = {} references : Dict [ str , str ] = {} # This might not be necessary after Parameter has pydantic validation class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True extra = 'forbid' @validator ( 'name' , pre = True , always = True ) def name_from_functor ( cls , v , values ): \"\"\"Create default value for name from the functor name\"\"\" # \"'functor' not in values\" means PyObject failed and an error is coming # Check for it here because that error is more helpful than a KeyError here. if not v and 'functor' in values : return values [ 'functor' ] . __name__ return v resolve_factory = validator ( 'config' , pre = True , each_item = True , allow_reuse = True )( Factory . resolve_factory ) def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description \"\"\" functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , ** functor_args , ** kwargs ) def resolve_storage_and_references ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ): \"\"\" Resolve parameter storage and references to get direct functor arguments.\"\"\" functor_args : Dict [ str , Any ] = {} functor_units = getattr ( self . functor , 'REQUIRED_UNITS' , {}) # Is there a units field in the configuration file? config_units = self . config . get ( 'unit' , None ) if config_units is not None and not isinstance ( config_units , str ): raise TypeError ( f 'Units of { self . name } are \" { config_units } \", which is not a string' ) for arg_name , arg_value in self . config . items (): # Resolve parameter values if isinstance ( arg_value , Parameter ): for source in param_sources : if arg_name in source . get ( self . name , {}): resolved_value = source [ self . name ][ arg_name ] break else : # This \"else\" means \"no break encountered\", which means that the argument was not found in any source raise RuntimeError ( f 'Could not resolve argument { arg_name } for { self . name } from the parameter sources' ) else : resolved_value = arg_value # Resolve units functor_args [ arg_name ] = self . _resolve_units ( name = arg_name , value = resolved_value , functor_units = functor_units , config_units = config_units , error_name = self . name ) for ref_dest , ref_src in self . references . items (): # Resolve references for source in ref_sources : if ref_src in source : ref_obj = source [ ref_src ] if not isinstance ( ref_obj , Parameter ): break else : # This \"else\" means \"no break encountered\", which means either: # 1. ref_src was not found in any source # 2. ref_src was found; however, it was an unresolved Parameter raise RuntimeError ( f 'Could not find { ref_src } for { self . name } in the reference storage' ) # Resolve units functor_args [ ref_dest ] = self . _resolve_units ( name = ref_dest , value = ref_obj , functor_units = functor_units , config_units = config_units , error_name = self . name ) return functor_args def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" parameters = { k : v for k , v in self . config . items () if isinstance ( v , Parameter )} if parameters : if self . name in parameter_store : raise ValueError ( f 'Duplicate functor name in parameter store: { self . name } ' ) parameter_store [ self . name ] = parameters @staticmethod def _resolve_units ( name : str , value : ObjectStoreElem , functor_units : Dict [ str , Union [ None , Literal [ True ], enum . Enum ]], config_units : Optional [ str ], error_name : str ) -> Any : assert not isinstance ( value , Parameter ) functor_arg : Any # Determine what to pass in to functor if isinstance ( value , ValueWithUnits ): # Value has units if name in functor_units : # Extra local variable required so that MyPy does proper type reduction in the conditional block below. these_units = functor_units [ name ] if these_units is True : # Require it to be actually boolean True, not just \"truthy\" functor_arg = value else : functor_arg = value . as_units ( these_units ) elif config_units is not None : functor_arg = value . as_units ( config_units ) elif value . units is NoneUnitType . NoneUnit : functor_arg = value . value else : raise RuntimeError ( f 'Argument { name } of { error_name } has units { value . units } , but none are expected' ) else : # Value has no units # Functor requires them and the unit is not None if functor_units . get ( name , NoneUnitType . NoneUnit ) not in [ None , NoneUnitType . NoneUnit ]: raise RuntimeError ( f 'Argument { name } of { error_name } is missing required units' ) # Functor does not require them functor_arg = value return functor_arg","title":"Functor"},{"location":"reference/libraries/functor/#corl.libraries.functor.Functor.Config","text":"Allow arbitrary types for Parameter Source code in corl/libraries/functor.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True extra = 'forbid'","title":"Config"},{"location":"reference/libraries/functor/#corl.libraries.functor.Functor.add_to_parameter_store","text":"Add the parameters of this functor to an external parameter store.","title":"add_to_parameter_store()"},{"location":"reference/libraries/functor/#corl.libraries.functor.Functor.add_to_parameter_store--parameters","text":"parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. Source code in corl/libraries/functor.py def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" parameters = { k : v for k , v in self . config . items () if isinstance ( v , Parameter )} if parameters : if self . name in parameter_store : raise ValueError ( f 'Duplicate functor name in parameter store: { self . name } ' ) parameter_store [ self . name ] = parameters","title":"Parameters"},{"location":"reference/libraries/functor/#corl.libraries.functor.Functor.create_functor_object","text":"Create the object with this functor TODO: Better description Source code in corl/libraries/functor.py def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description \"\"\" functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , ** functor_args , ** kwargs )","title":"create_functor_object()"},{"location":"reference/libraries/functor/#corl.libraries.functor.Functor.name_from_functor","text":"Create default value for name from the functor name Source code in corl/libraries/functor.py @validator ( 'name' , pre = True , always = True ) def name_from_functor ( cls , v , values ): \"\"\"Create default value for name from the functor name\"\"\" # \"'functor' not in values\" means PyObject failed and an error is coming # Check for it here because that error is more helpful than a KeyError here. if not v and 'functor' in values : return values [ 'functor' ] . __name__ return v","title":"name_from_functor()"},{"location":"reference/libraries/functor/#corl.libraries.functor.Functor.resolve_factory","text":"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) Source code in corl/libraries/functor.py @classmethod def resolve_factory ( cls , v ): \"\"\"Validator for converting a factory into the built object. Usage in a pydantic model: resolve_factory = validator('name', pre=True, allow_reuse=True)(Factory.resolve_factory) \"\"\" try : v [ 'type' ] except ( TypeError , KeyError ): # Not something that should be built with the factory return v else : factory = cls ( ** v ) return factory . build ()","title":"resolve_factory()"},{"location":"reference/libraries/functor/#corl.libraries.functor.Functor.resolve_storage_and_references","text":"Resolve parameter storage and references to get direct functor arguments. Source code in corl/libraries/functor.py def resolve_storage_and_references ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ): \"\"\" Resolve parameter storage and references to get direct functor arguments.\"\"\" functor_args : Dict [ str , Any ] = {} functor_units = getattr ( self . functor , 'REQUIRED_UNITS' , {}) # Is there a units field in the configuration file? config_units = self . config . get ( 'unit' , None ) if config_units is not None and not isinstance ( config_units , str ): raise TypeError ( f 'Units of { self . name } are \" { config_units } \", which is not a string' ) for arg_name , arg_value in self . config . items (): # Resolve parameter values if isinstance ( arg_value , Parameter ): for source in param_sources : if arg_name in source . get ( self . name , {}): resolved_value = source [ self . name ][ arg_name ] break else : # This \"else\" means \"no break encountered\", which means that the argument was not found in any source raise RuntimeError ( f 'Could not resolve argument { arg_name } for { self . name } from the parameter sources' ) else : resolved_value = arg_value # Resolve units functor_args [ arg_name ] = self . _resolve_units ( name = arg_name , value = resolved_value , functor_units = functor_units , config_units = config_units , error_name = self . name ) for ref_dest , ref_src in self . references . items (): # Resolve references for source in ref_sources : if ref_src in source : ref_obj = source [ ref_src ] if not isinstance ( ref_obj , Parameter ): break else : # This \"else\" means \"no break encountered\", which means either: # 1. ref_src was not found in any source # 2. ref_src was found; however, it was an unresolved Parameter raise RuntimeError ( f 'Could not find { ref_src } for { self . name } in the reference storage' ) # Resolve units functor_args [ ref_dest ] = self . _resolve_units ( name = ref_dest , value = ref_obj , functor_units = functor_units , config_units = config_units , error_name = self . name ) return functor_args","title":"resolve_storage_and_references()"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorDictWrapper","text":"wrapped: The dict of functor or functor wrapper configurations wrapped by this wrapper Source code in corl/libraries/functor.py class FunctorDictWrapper ( Functor ): \"\"\" wrapped: The dict of functor or functor wrapper configurations wrapped by this wrapper \"\"\" wrapped : Dict [ str , Union [ 'FunctorDictWrapper' , FunctorMultiWrapper , FunctorWrapper , Functor ]] @validator ( 'wrapped' , pre = True ) def adjust_wrapped_name ( cls , v ): \"\"\"Use the dictionary wrapping key as the name if none is provided.\"\"\" if not isinstance ( v , Mapping_Type ): return v # this pre validator is pretty dangerous in terms of # what it will attempt to run on, because a regular wrapper # has a dictionary it will attempt to perform this operation # on regular wrappers, so we are just going to verify that # this is not a regular wrapper by checking for a functor appearing # in wrapped if \"functor\" in v . keys (): return v for key , value in v . items (): try : if 'name' not in value : value [ 'name' ] = key except : # pylint: disable=bare-except # noqa: E722 pass return v def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_funcs = { k : v . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) for k , v in self . wrapped . items () } functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_funcs , ** functor_args , ** kwargs ) def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) for wrapped in self . wrapped . values (): wrapped . add_to_parameter_store ( parameter_store )","title":"FunctorDictWrapper"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorDictWrapper.add_to_parameter_store","text":"Add the parameters of this functor to an external parameter store.","title":"add_to_parameter_store()"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorDictWrapper.add_to_parameter_store--parameters","text":"parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. Source code in corl/libraries/functor.py def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) for wrapped in self . wrapped . values (): wrapped . add_to_parameter_store ( parameter_store )","title":"Parameters"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorDictWrapper.adjust_wrapped_name","text":"Use the dictionary wrapping key as the name if none is provided. Source code in corl/libraries/functor.py @validator ( 'wrapped' , pre = True ) def adjust_wrapped_name ( cls , v ): \"\"\"Use the dictionary wrapping key as the name if none is provided.\"\"\" if not isinstance ( v , Mapping_Type ): return v # this pre validator is pretty dangerous in terms of # what it will attempt to run on, because a regular wrapper # has a dictionary it will attempt to perform this operation # on regular wrappers, so we are just going to verify that # this is not a regular wrapper by checking for a functor appearing # in wrapped if \"functor\" in v . keys (): return v for key , value in v . items (): try : if 'name' not in value : value [ 'name' ] = key except : # pylint: disable=bare-except # noqa: E722 pass return v","title":"adjust_wrapped_name()"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorDictWrapper.create_functor_object","text":"Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches Source code in corl/libraries/functor.py def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_funcs = { k : v . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) for k , v in self . wrapped . items () } functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_funcs , ** functor_args , ** kwargs )","title":"create_functor_object()"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorMultiWrapper","text":"wrapped: The functor or functor wrapper configuration wrapped by this functor wrapper. Source code in corl/libraries/functor.py class FunctorMultiWrapper ( Functor ): \"\"\" wrapped: The functor or functor wrapper configuration wrapped by this functor wrapper. \"\"\" wrapped : List [ Union [ 'FunctorMultiWrapper' , FunctorWrapper , Functor , 'FunctorDictWrapper' ]] def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_funcs = [ x . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) for x in self . wrapped ] functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_funcs , ** functor_args , ** kwargs ) def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) for wrapped in self . wrapped : wrapped . add_to_parameter_store ( parameter_store )","title":"FunctorMultiWrapper"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorMultiWrapper.add_to_parameter_store","text":"Add the parameters of this functor to an external parameter store.","title":"add_to_parameter_store()"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorMultiWrapper.add_to_parameter_store--parameters","text":"parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. Source code in corl/libraries/functor.py def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) for wrapped in self . wrapped : wrapped . add_to_parameter_store ( parameter_store )","title":"Parameters"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorMultiWrapper.create_functor_object","text":"Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches Source code in corl/libraries/functor.py def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_funcs = [ x . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) for x in self . wrapped ] functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_funcs , ** functor_args , ** kwargs )","title":"create_functor_object()"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorWrapper","text":"wrapped: The functor or functor wrapper configuration wrapped by this functor wrapper. Source code in corl/libraries/functor.py class FunctorWrapper ( Functor ): \"\"\" wrapped: The functor or functor wrapper configuration wrapped by this functor wrapper. \"\"\" wrapped : Union [ 'FunctorMultiWrapper' , 'FunctorWrapper' , 'FunctorDictWrapper' , Functor ] def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_func = self . wrapped . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_func , ** functor_args , ** kwargs ) def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) self . wrapped . add_to_parameter_store ( parameter_store )","title":"FunctorWrapper"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorWrapper.add_to_parameter_store","text":"Add the parameters of this functor to an external parameter store.","title":"add_to_parameter_store()"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorWrapper.add_to_parameter_store--parameters","text":"parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. Source code in corl/libraries/functor.py def add_to_parameter_store ( self , parameter_store : Dict [ str , Dict [ str , Parameter ]]) -> None : \"\"\"Add the parameters of this functor to an external parameter store. Parameters ---------- parameter_store : Dict[str, Dict[str, Parameter]] Parameter store to which to add the parameters. The keys of the outer dictionary are functor names. The inner dictionary is the collection of Parameters. \"\"\" super () . add_to_parameter_store ( parameter_store ) self . wrapped . add_to_parameter_store ( parameter_store )","title":"Parameters"},{"location":"reference/libraries/functor/#corl.libraries.functor.FunctorWrapper.create_functor_object","text":"Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches Source code in corl/libraries/functor.py def create_functor_object ( self , param_sources : Sequence [ Mapping [ str , Any ]] = (), ref_sources : Sequence [ Mapping [ str , Any ]] = (), ** kwargs ): \"\"\" Create the object with this functor TODO: Better description TODO: Clean up logic so does not have too-many-branches \"\"\" wrapped_func = self . wrapped . create_functor_object ( param_sources = param_sources , ref_sources = ref_sources , ** kwargs ) functor_args = self . resolve_storage_and_references ( param_sources = param_sources , ref_sources = ref_sources ) return self . functor ( name = self . name , wrapped = wrapped_func , ** functor_args , ** kwargs )","title":"create_functor_object()"},{"location":"reference/libraries/hparam_search_util/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseHparamSearch ( ABC ) \u00a4 Experiment provides an anstract class to run specific types of experiments this allows users to do specific setup steps or to run some sort of custom training loop Source code in corl/libraries/hparam_search_util.py class BaseHparamSearch ( abc . ABC ): \"\"\" Experiment provides an anstract class to run specific types of experiments this allows users to do specific setup steps or to run some sort of custom training loop \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseHparamSearchValidator = self . get_validator ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseHparamSearchValidator ]: \"\"\"Gets the validator Returns ------- typing.Type[BaseHparamSearchValidator] The validator class \"\"\" return BaseHparamSearchValidator @abc . abstractmethod def add_algorithm_hparams ( self , rllib_config : dict , tune_config : dict ) -> None : \"\"\"Updates the configurations based on search alg Parameters ---------- rllib_config : dict The rllib config tune_config : dict The tune config \"\"\" ... get_validator : Type [ corl . libraries . hparam_search_util . BaseHparamSearchValidator ] property readonly \u00a4 Gets the validator Returns \u00a4 typing.Type[BaseHparamSearchValidator] The validator class add_algorithm_hparams ( self , rllib_config , tune_config ) \u00a4 Updates the configurations based on search alg Parameters \u00a4 rllib_config : dict The rllib config tune_config : dict The tune config Source code in corl/libraries/hparam_search_util.py @abc . abstractmethod def add_algorithm_hparams ( self , rllib_config : dict , tune_config : dict ) -> None : \"\"\"Updates the configurations based on search alg Parameters ---------- rllib_config : dict The rllib config tune_config : dict The tune config \"\"\" ... BaseHparamSearchValidator ( BaseModel ) pydantic-model \u00a4 Base Validator to subclass for Experiments subclassing BaseExperiment Source code in corl/libraries/hparam_search_util.py class BaseHparamSearchValidator ( BaseModel ): \"\"\" Base Validator to subclass for Experiments subclassing BaseExperiment \"\"\" ... HparamSearchPPO_AHBS ( BaseHparamSearch ) \u00a4 Asynchronous Hyper Band Example https://docs.ray.io/en/master/tune/examples/includes/async_hyperband_example.html Source code in corl/libraries/hparam_search_util.py class HparamSearchPPO_AHBS ( BaseHparamSearch ): \"\"\"Asynchronous Hyper Band Example https://docs.ray.io/en/master/tune/examples/includes/async_hyperband_example.html \"\"\" def __init__ ( self , ** kwargs ) -> None : super () . __init__ ( kwargs = kwargs ) self . _model_choices = [] if self . config . include_lstm_search : # type: ignore self . _model_choices . append ( ParametersModel . select_lstm_model ) if self . config . inclue_fully_connected_search : # type: ignore self . _model_choices . append ( ParametersModel . select_fully_connected_model ) if self . config . include_frame_stacking_search : # type: ignore self . _model_choices . append ( ParametersModel . select_framestacking_model ) if self . config . include_gtrxl_search : # type: ignore self . _model_choices . append ( ParametersModel . select_gtrxl_model ) @property def get_validator ( self ) -> typing . Type [ HparamSearchValidator_AHBS ]: \"\"\"gets the configuration for AHBS Returns ------- typing.Type[HparamSearchValidator_AHBS] validator \"\"\" return HparamSearchValidator_AHBS def add_algorithm_hparams ( self , rllib_config , tune_config ) -> None : \"\"\"[summary] Parameters ---------- run_or_experiment_config : [type] [description] tune_config : [type] [description] args : [type] [description] \"\"\" ahbs = tune . schedulers . AsyncHyperBandScheduler ( time_attr = self . config . time_attr , # type: ignore metric = self . config . metric , # type: ignore mode = self . config . mode , # type: ignore max_t = self . config . max_t , # type: ignore grace_period = self . config . grace_period , # type: ignore brackets = self . config . brackets # type: ignore ) tune_config [ \"num_samples\" ] = self . config . samples # type: ignore tune_config [ \"scheduler\" ] = ahbs rllib_config . update ( ParametersPPO . ppo_hyperparameters ()) rllib_config [ \"model\" ] = tune . sample_from ( partial ( ParametersModel . select_model , self . _model_choices )) get_validator : Type [ corl . libraries . hparam_search_util . HparamSearchValidator_AHBS ] property readonly \u00a4 gets the configuration for AHBS Returns \u00a4 typing.Type[HparamSearchValidator_AHBS] validator add_algorithm_hparams ( self , rllib_config , tune_config ) \u00a4 [summary] Parameters \u00a4 run_or_experiment_config : [type] [description] tune_config : [type] [description] args : [type] [description] Source code in corl/libraries/hparam_search_util.py def add_algorithm_hparams ( self , rllib_config , tune_config ) -> None : \"\"\"[summary] Parameters ---------- run_or_experiment_config : [type] [description] tune_config : [type] [description] args : [type] [description] \"\"\" ahbs = tune . schedulers . AsyncHyperBandScheduler ( time_attr = self . config . time_attr , # type: ignore metric = self . config . metric , # type: ignore mode = self . config . mode , # type: ignore max_t = self . config . max_t , # type: ignore grace_period = self . config . grace_period , # type: ignore brackets = self . config . brackets # type: ignore ) tune_config [ \"num_samples\" ] = self . config . samples # type: ignore tune_config [ \"scheduler\" ] = ahbs rllib_config . update ( ParametersPPO . ppo_hyperparameters ()) rllib_config [ \"model\" ] = tune . sample_from ( partial ( ParametersModel . select_model , self . _model_choices )) HparamSearchPPO_PBT ( BaseHparamSearch ) \u00a4 PPO PBT Search Space https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Source code in corl/libraries/hparam_search_util.py class HparamSearchPPO_PBT ( BaseHparamSearch ): \"\"\" PPO PBT Search Space https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe \"\"\" @property def get_validator ( self ) -> typing . Type [ HparamSearchValidator_PBT ]: \"\"\"gets the configuration for AHBS Returns ------- typing.Type[HparamSearchValidator_AHBS] validator \"\"\" return HparamSearchValidator_PBT def add_algorithm_hparams ( self , rllib_config , tune_config ) -> None : \"\"\"Adds population based training to the configuration (TBD items to be added - default never add) Parameters ---------- rllib_config : dict The experiment configuration tune_config : dict The tune configuration \"\"\" # Postprocess the perturbed config to ensure it's still valid pbt = tune . schedulers . PopulationBasedTraining ( time_attr = self . config . time_attr , # type: ignore metric = self . config . metric , # type: ignore mode = self . config . mode , # type: ignore perturbation_interval = self . config . perturbation_interval , # type: ignore resample_probability = self . config . resample_probability , # type: ignore burn_in_period = self . config . burn_in_period , # type: ignore log_config = True , # Specifies the mutations of these hyper params hyperparam_mutations = { \"lambda\" : ParametersPPO . LAMBDA_DIST , \"clip_param\" : ParametersPPO . CLIP_PARAM_DIST , \"lr\" : ParametersPPO . LR_DIST , \"num_sgd_iter\" : ParametersPPO . NSGD_DIST , \"sgd_minibatch_size\" : ParametersPPO . SGD_MINIBATCH_SIZE_DIST , \"train_batch_size\" : ParametersPPO . TRAIN_BATCH_SIZE_DIST , \"vf_loss_coeff\" : ParametersPPO . VF_LOSS_COEFF_DIST , \"entropy_coeff\" : ParametersPPO . ENTROPY_COEFF_DIST , \"gamma\" : ParametersPPO . GAMMA_DIST , \"kl_coeff\" : ParametersPPO . KL_COEFF_DIST , \"kl_target\" : ParametersPPO . KL_TARGET_DIST }, custom_explore_fn = ParametersPPO . pbt_ppo_explore ) # These params start off randomly drawn from a set. tune_config [ \"scheduler\" ] = pbt tune_config [ \"num_samples\" ] = self . config . samples # type: ignore rllib_config . update ( ParametersPPO . ppo_hyperparameters ()) get_validator : Type [ corl . libraries . hparam_search_util . HparamSearchValidator_PBT ] property readonly \u00a4 gets the configuration for AHBS Returns \u00a4 typing.Type[HparamSearchValidator_AHBS] validator add_algorithm_hparams ( self , rllib_config , tune_config ) \u00a4 Adds population based training to the configuration (TBD items to be added - default never add) Parameters \u00a4 rllib_config : dict The experiment configuration tune_config : dict The tune configuration Source code in corl/libraries/hparam_search_util.py def add_algorithm_hparams ( self , rllib_config , tune_config ) -> None : \"\"\"Adds population based training to the configuration (TBD items to be added - default never add) Parameters ---------- rllib_config : dict The experiment configuration tune_config : dict The tune configuration \"\"\" # Postprocess the perturbed config to ensure it's still valid pbt = tune . schedulers . PopulationBasedTraining ( time_attr = self . config . time_attr , # type: ignore metric = self . config . metric , # type: ignore mode = self . config . mode , # type: ignore perturbation_interval = self . config . perturbation_interval , # type: ignore resample_probability = self . config . resample_probability , # type: ignore burn_in_period = self . config . burn_in_period , # type: ignore log_config = True , # Specifies the mutations of these hyper params hyperparam_mutations = { \"lambda\" : ParametersPPO . LAMBDA_DIST , \"clip_param\" : ParametersPPO . CLIP_PARAM_DIST , \"lr\" : ParametersPPO . LR_DIST , \"num_sgd_iter\" : ParametersPPO . NSGD_DIST , \"sgd_minibatch_size\" : ParametersPPO . SGD_MINIBATCH_SIZE_DIST , \"train_batch_size\" : ParametersPPO . TRAIN_BATCH_SIZE_DIST , \"vf_loss_coeff\" : ParametersPPO . VF_LOSS_COEFF_DIST , \"entropy_coeff\" : ParametersPPO . ENTROPY_COEFF_DIST , \"gamma\" : ParametersPPO . GAMMA_DIST , \"kl_coeff\" : ParametersPPO . KL_COEFF_DIST , \"kl_target\" : ParametersPPO . KL_TARGET_DIST }, custom_explore_fn = ParametersPPO . pbt_ppo_explore ) # These params start off randomly drawn from a set. tune_config [ \"scheduler\" ] = pbt tune_config [ \"num_samples\" ] = self . config . samples # type: ignore rllib_config . update ( ParametersPPO . ppo_hyperparameters ()) HparamSearchValidator_AHBS ( HparamSearchValidator_Shared ) pydantic-model \u00a4 Base Validator to subclass for search subclassing Source code in corl/libraries/hparam_search_util.py class HparamSearchValidator_AHBS ( HparamSearchValidator_Shared ): \"\"\" Base Validator to subclass for search subclassing \"\"\" # max time units per trial. Trials will be stopped after max_t time units (determined # by time_attr) have passed. max_t : float = 1e7 # Brackets brackets : float = 1 # Only stop trials at least this old in time. The units are the same as the attribute # named by time-attr. grace_period : float = 5e6 include_lstm_search : bool = False inclue_fully_connected_search : bool = True include_frame_stacking_search : bool = False include_gtrxl_search : bool = False HparamSearchValidator_PBT ( HparamSearchValidator_Shared ) pydantic-model \u00a4 Base Validator to subclass for search subclassing Source code in corl/libraries/hparam_search_util.py class HparamSearchValidator_PBT ( HparamSearchValidator_Shared ): \"\"\" Base Validator to subclass for search subclassing \"\"\" # The probability of resampling from the original distribution when applying hyperparam_mutations. # If not resampled, the value will be perturbed by a factor of 1.2 or 0.8 if continuous, or changed # to an adjacent value if discrete. Note that resample_probability by default is 0.25, thus # hyperparameter with a distribution may go out of the specific range. resample_probability : float = 0.25 # (float) \u2013 Models will be considered for perturbation at this interval of time_attr. Note that # perturbation incurs checkpoint overhead, so you shouldn\u2019t set this to be too frequent. perturbation_interval : float = 4 # (float) \u2013 Models will not be considered for perturbation before this interval of time_attr has # passed. This guarantees that models are trained for at least a certain amount of time or timesteps # before being perturbed. burn_in_period : float = 10 HparamSearchValidator_Shared ( BaseHparamSearchValidator ) pydantic-model \u00a4 Base Validator to subclass for search subclassing Source code in corl/libraries/hparam_search_util.py class HparamSearchValidator_Shared ( BaseHparamSearchValidator ): \"\"\" Base Validator to subclass for search subclassing \"\"\" # \"The training result objective value attribute. Stopping procedures will use this attribute.\" metric : str = \"episode_reward_mean\" # One of {min, max}. Determines whether objective is minimizing or maximizing the metric attribute. mode : str = \"max\" # A training result attr to use for comparing time. Note that you can pass in something # non-temporal such as training_iteration as a measure of progress, the only requirement is # that the attribute should increase monotonically. time_attr : str = \"timesteps_total\" # The number of samples to collect during HPARAM search (trials) samples : int = 4 ParametersModel \u00a4 Holds the model parameters Source code in corl/libraries/hparam_search_util.py class ParametersModel : \"\"\"Holds the model parameters \"\"\" FC_LAYER_CHOICES = [ 32 , 64 , 128 , 256 , 512 ] FC_LAYER_COUNT = [ 2 , 3 , 4 , 5 , 6 ] @staticmethod def __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ): model_layers : typing . List = [] for _ in range ( 0 , layer_count ): temp_layers = layer_choices [ MIN_LAYER_INDEX :] if len ( model_layers ) < FC_FILTER_LOWER_VALUES_THRESHOLD else layer_choices if model_layers : model_layers . append ( random . choice ([ x for x in temp_layers if x <= model_layers [ - 1 ]])) else : model_layers . append ( random . choice ( temp_layers )) return model_layers @staticmethod def select_lstm_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"use_lstm\" ] = True model_config [ \"max_seq_len\" ] = random . choice ([ 2 , 3 , 5 , 10 ]) model_config [ \"lstm_cell_size\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"vf_share_layers\" ] = True # model_config[\"vf_share_layers\"] = random.choice([True, False]) # model_config[\"lstm_use_prev_action\"] = random.choice([True, False]) # model_config[\"lstm_use_prev_reward\"] = random.choice([True, False]) # model_config[\"_time_major\"] = random.choice([True, False]) return model_config @staticmethod def select_fully_connected_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" layer_count = random . choice ( ParametersModel . FC_LAYER_COUNT ) layer_choices = ParametersModel . FC_LAYER_CHOICES FC_FILTER_LOWER_VALUES_THRESHOLD = 2 MIN_LAYER_INDEX = 2 model_layers = ParametersModel . __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ) model_config : dict = {} model_config [ \"fcnet_hiddens\" ] = model_layers model_config [ \"fcnet_activation\" ] = random . choice ([ \"relu\" , \"tanh\" ]) return model_config @staticmethod def select_framestacking_model () -> dict : \"\"\"[summary] Returns: dict -- [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"custom_model\" ] = \"TorchFrameStack\" model_config [ \"custom_model_config\" ] = {} # type: ignore model_config [ \"custom_model_config\" ][ \"num_frames\" ] = random . choice ( list ( range ( 1 , 11 ))) model_config [ \"custom_model_config\" ][ \"include_actions\" ] = random . choice ([ True , False ]) model_config [ \"custom_model_config\" ][ \"include_rewards\" ] = random . choice ([ True , False ]) layer_count = random . choice ( ParametersModel . FC_LAYER_COUNT ) layer_choices = ParametersModel . FC_LAYER_CHOICES FC_FILTER_LOWER_VALUES_THRESHOLD = 2 MIN_LAYER_INDEX = 2 model_config [ \"custom_model_config\" ][ \"post_fcnet_hiddens\" ] = ParametersModel . __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ) return model_config @staticmethod def select_gtrxl_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"use_attention\" ] = False model_config [ \"attention_num_transformer_units\" ] = random . choice ( list ( range ( 1 , 6 ))) model_config [ \"attention_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"attention_num_heads\" ] = random . choice ( list ( range ( 1 , 6 ))) model_config [ \"attention_head_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"attention_memory_inference\" ] = 50 model_config [ \"attention_memory_training\" ] = 50 model_config [ \"attention_position_wise_mlp_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) # model_config[\"attention_init_gru_gate_bias\"] = 2.0 model_config [ \"attention_use_n_prev_actions\" ] = random . choice ( list ( range ( 1 , 11 ))) model_config [ \"attention_use_n_prev_rewards\" ] = random . choice ( list ( range ( 1 , 11 ))) return model_config @staticmethod def select_model ( model_choices ) -> dict : \"\"\"The following function provides the start to exploring model configurations. \"\"\" model_config_func = random . choice ( model_choices ) return model_config_func () select_framestacking_model () staticmethod \u00a4 [summary] Returns: Type Description dict dict -- [description] Source code in corl/libraries/hparam_search_util.py @staticmethod def select_framestacking_model () -> dict : \"\"\"[summary] Returns: dict -- [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"custom_model\" ] = \"TorchFrameStack\" model_config [ \"custom_model_config\" ] = {} # type: ignore model_config [ \"custom_model_config\" ][ \"num_frames\" ] = random . choice ( list ( range ( 1 , 11 ))) model_config [ \"custom_model_config\" ][ \"include_actions\" ] = random . choice ([ True , False ]) model_config [ \"custom_model_config\" ][ \"include_rewards\" ] = random . choice ([ True , False ]) layer_count = random . choice ( ParametersModel . FC_LAYER_COUNT ) layer_choices = ParametersModel . FC_LAYER_CHOICES FC_FILTER_LOWER_VALUES_THRESHOLD = 2 MIN_LAYER_INDEX = 2 model_config [ \"custom_model_config\" ][ \"post_fcnet_hiddens\" ] = ParametersModel . __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ) return model_config select_fully_connected_model () staticmethod \u00a4 [summary] Returns \u00a4 dict [description] Source code in corl/libraries/hparam_search_util.py @staticmethod def select_fully_connected_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" layer_count = random . choice ( ParametersModel . FC_LAYER_COUNT ) layer_choices = ParametersModel . FC_LAYER_CHOICES FC_FILTER_LOWER_VALUES_THRESHOLD = 2 MIN_LAYER_INDEX = 2 model_layers = ParametersModel . __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ) model_config : dict = {} model_config [ \"fcnet_hiddens\" ] = model_layers model_config [ \"fcnet_activation\" ] = random . choice ([ \"relu\" , \"tanh\" ]) return model_config select_gtrxl_model () staticmethod \u00a4 [summary] Returns \u00a4 dict [description] Source code in corl/libraries/hparam_search_util.py @staticmethod def select_gtrxl_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"use_attention\" ] = False model_config [ \"attention_num_transformer_units\" ] = random . choice ( list ( range ( 1 , 6 ))) model_config [ \"attention_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"attention_num_heads\" ] = random . choice ( list ( range ( 1 , 6 ))) model_config [ \"attention_head_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"attention_memory_inference\" ] = 50 model_config [ \"attention_memory_training\" ] = 50 model_config [ \"attention_position_wise_mlp_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) # model_config[\"attention_init_gru_gate_bias\"] = 2.0 model_config [ \"attention_use_n_prev_actions\" ] = random . choice ( list ( range ( 1 , 11 ))) model_config [ \"attention_use_n_prev_rewards\" ] = random . choice ( list ( range ( 1 , 11 ))) return model_config select_lstm_model () staticmethod \u00a4 [summary] Returns \u00a4 dict [description] Source code in corl/libraries/hparam_search_util.py @staticmethod def select_lstm_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"use_lstm\" ] = True model_config [ \"max_seq_len\" ] = random . choice ([ 2 , 3 , 5 , 10 ]) model_config [ \"lstm_cell_size\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"vf_share_layers\" ] = True # model_config[\"vf_share_layers\"] = random.choice([True, False]) # model_config[\"lstm_use_prev_action\"] = random.choice([True, False]) # model_config[\"lstm_use_prev_reward\"] = random.choice([True, False]) # model_config[\"_time_major\"] = random.choice([True, False]) return model_config select_model ( model_choices ) staticmethod \u00a4 The following function provides the start to exploring model configurations. Source code in corl/libraries/hparam_search_util.py @staticmethod def select_model ( model_choices ) -> dict : \"\"\"The following function provides the start to exploring model configurations. \"\"\" model_config_func = random . choice ( model_choices ) return model_config_func () ParametersPPO \u00a4 Utility functions for processing hparam searches in the framework for PPO algorithm https://github.com/ray-project/ray/blob/00922817b66ee14ba215972a98f416f3d6fef1ba/rllib/agents/ppo/ppo.py https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe https://blog.openai.com/openai-five/ https://docs.ray.io/en/master/tune/api_docs/trainable.html#advanced-reusing-actors Source code in corl/libraries/hparam_search_util.py class ParametersPPO : \"\"\"Utility functions for processing hparam searches in the framework for PPO algorithm https://github.com/ray-project/ray/blob/00922817b66ee14ba215972a98f416f3d6fef1ba/rllib/agents/ppo/ppo.py https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe https://blog.openai.com/openai-five/ https://docs.ray.io/en/master/tune/api_docs/trainable.html#advanced-reusing-actors \"\"\" LAMBDA_MIN = 0.9 LAMBDA_MAX = 1.0 LAMBDA_DIST = tune . uniform ( LAMBDA_MIN , LAMBDA_MAX ) @staticmethod def LAMBDA_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM GAE Parameter Lambda Range: 0.9 to 1 GAE Parameter Lambda also known as: GAE Parameter (lambda) (PPO Paper), lambda (RLlib), lambda (ppo2 baselines), lambda (ppo baselines), lambda (Unity ML), gae_lambda (TensorForce) \"\"\" return ParametersPPO . LAMBDA_DIST VF_LOSS_COEFF_MIN = 0.5 VF_LOSS_COEFF_MAX = 1.0 VF_LOSS_COEFF_DIST = tune . uniform ( VF_LOSS_COEFF_MIN , VF_LOSS_COEFF_MAX ) @staticmethod def VF_LOSS_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Value Function Coefficient Range: 0.5, 1 Value Function Coefficient also known as: VF coeff. (PPO Paper), vf_loss_coef (RLlib), vf_coef (ppo2 baselines), (ppo baselines: unclear), (Unity ML: unclear), (TensorForce: unclear) \"\"\" return ParametersPPO . VF_LOSS_COEFF_DIST ENTROPY_COEFF_MIN = 0.00 ENTROPY_COEFF_MAX = 0.01 ENTROPY_COEFF_DIST = tune . uniform ( ENTROPY_COEFF_MIN , ENTROPY_COEFF_MAX ) @staticmethod def ENTROPY_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Entropy Coefficient Range: 0 to 0.01 Entropy Coefficient also known as: Entropy coeff. (PPO Paper), entropy_coeff (RLlib), ent_coeff (ppo2 baselines), entcoeff (ppo baselines), beta (Unity ML), entropy_regularization (TensorForce) \"\"\" return ParametersPPO . ENTROPY_COEFF_DIST CLIP_PARAM_MIN = 0.1 CLIP_PARAM_MAX = 0.3 CLIP_PARAM_DIST = tune . choice ([ 0.1 , 0.2 , 0.3 ]) @staticmethod def CLIP_PARAM_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Clipping Range: 0.1, 0.2, 0.3 Clipping also known as: Clipping parameter epsilon (PPO Paper), clip_param (RLlib), cliprange (ppo2 baselines), clip_param (ppo baselines), epsilon (Unity ML), likelihood_ratio_clipping (TensorForce) \"\"\" return ParametersPPO . CLIP_PARAM_DIST KL_TARGET_MIN = 0.003 KL_TARGET_MAX = 0.03 KL_TARGET_DIST = tune . uniform ( KL_TARGET_MIN , KL_TARGET_MAX ) @staticmethod def KL_TARGET_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB \"\"\" return ParametersPPO . KL_COEFF_DIST KL_COEFF_MIN = 0.2 # RLLIB Default KL_COEFF_MAX = 1.0 # https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe KL_COEFF_DIST = tune . uniform ( KL_COEFF_MIN , KL_COEFF_MAX ) @staticmethod def KL_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB \"\"\" return ParametersPPO . KL_COEFF_DIST GAMMA_MIN = 0.8000 GAMMA_MAX = 0.9997 GAMMA_DIST = tune . uniform ( GAMMA_MIN , GAMMA_MAX ) @staticmethod def GAMMA_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Discount Factor Gamma Range: 0.99 (most common), 0.8 to 0.9997 Discount Factor Gamma also known as: Discount (gamma) (PPO Paper), gamma (RLlib), gamma (ppo2 baselines), gamma (ppo baselines), gamma (Unity ML), discount (TensorForce) \"\"\" return ParametersPPO . GAMMA_DIST LR_MIN = 5e-6 LR_MAX = 0.003 LR_DIST = tune . uniform ( LR_MIN , LR_MAX ) @staticmethod def LR_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Learning Rate Range: 0.003 to 5e-6 Learning Rate also known as: Adam stepsize (PPO Paper), sgd_stepsize (RLlib), lr (ppo2 baselines), (ppo baselines: unclear), learning_rate (Unity ML), learning_rate (TensorForce) \"\"\" return ParametersPPO . LR_DIST NSGD_MIN = 3 NSGD_MAX = 30 NSGD_DIST = tune . choice ( list ( range ( NSGD_MIN , NSGD_MAX + 1 ))) @staticmethod def NSGD_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Epoch Range: 3 to 30 Epoch also known as: Num. epochs (PPO paper), num_sgd_iter (RLlib), epochs (ppo2 baselines), optim_epochs (ppo baselines), num_epoch (Unity ML), (TensorForce: unclear) \"\"\" return ParametersPPO . NSGD_DIST SGD_MINIBATCH_SIZE_MIN = 128 SGD_MINIBATCH_SIZE_MAX = 4096 SGD_MINIBATCH_SIZE_DIST = tune . choice ([ 128 , 256 , 512 , 1024 , 2048 , 4096 ]) @staticmethod def SGD_MINIBATCH_SIZE_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM\"\"\" return ParametersPPO . SGD_MINIBATCH_SIZE_DIST TRAIN_BATCH_SIZE_MIN = 4096 TRAIN_BATCH_SIZE_MAX = 160000 TRAIN_BATCH_SIZE_INC = 256 # [4096, 4352, 4608, 4864, 5120, 5376, 5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936, 8192, 8448, # 8704, 8960, 9216, 9472, 9728, 9984, 10240, 10496, 10752, 11008, 11264, 11520, 11776, 12032, 12288, 12544, # 12800, 13056, 13312, 13568, 13824, 14080, 14336, 14592, 14848, 15104, 15360, 15616, 15872, 16128, 16384, # 16640, 16896, 17152, 17408, 17664, 17920, 18176, 18432, 18688, 18944, 19200, 19456, 19712, 19968, 20224, # 20480, 20736, 20992, 21248, 21504, 21760, 22016, 22272, 22528, 22784, 23040, 23296, 23552, 23808, 24064, # 24320, 24576, 24832, 25088, 25344, 25600, 25856, 26112, 26368, 26624, 26880, 27136, 27392, 27648, 27904, # 28160, 28416, 28672, 28928, 29184, 29440, 29696, 29952, 30208, 30464, 30720, 30976, 31232, 31488, 31744, # 32000, 32256, 32512, 32768, 33024, 33280, 33536, 33792, 34048, 34304, 34560, 34816, 35072, 35328, 35584, # 35840, 36096, 36352, 36608, 36864, 37120, 37376, 37632, 37888, 38144, 38400, 38656, 38912, 39168, 39424, # 39680, 39936, 40192, 40448, 40704, 40960, 41216, 41472, 41728, 41984, 42240, 42496, 42752, 43008, 43264, # 43520, 43776, 44032, 44288, 44544, 44800, 45056, 45312, 45568, 45824, 46080, 46336, 46592, 46848, 47104, # 47360, 47616, 47872, 48128, 48384, 48640, 48896, 49152, 49408, 49664, 49920, 50176, 50432, 50688, 50944, # 51200, 51456, 51712, 51968, 52224, 52480, 52736, 52992, 53248, 53504, 53760, 54016, 54272, 54528, 54784, # 55040, 55296, 55552, 55808, 56064, 56320, 56576, 56832, 57088, 57344, 57600, 57856, 58112, 58368, 58624, # 58880, 59136, 59392, 59648, 59904, 60160, 60416, 60672, 60928, 61184, 61440, 61696, 61952, 62208, 62464, # 62720, 62976, 63232, 63488, 63744, 64000, 64256, 64512, 64768, 65024, 65280, 65536, 65792, 66048, 66304, # 66560, 66816, 67072, 67328, 67584, 67840, 68096, 68352, 68608, 68864, 69120, 69376, 69632, 69888, 70144, # 70400, 70656, 70912, 71168, 71424, 71680, 71936, 72192, 72448, 72704, 72960, 73216, 73472, 73728, 73984, # 74240, 74496, 74752, 75008, 75264, 75520, 75776, 76032, 76288, 76544, 76800, 77056, 77312, 77568, 77824, # 78080, 78336, 78592, 78848, 79104, 79360, 79616, 79872, 80128, 80384, 80640, 80896, 81152, 81408, 81664, # 81920, 82176, 82432, 82688, 82944, 83200, 83456, 83712, 83968, 84224, 84480, 84736, 84992, 85248, 85504, # 85760, 86016, 86272, 86528, 86784, 87040, 87296, 87552, 87808, 88064, 88320, 88576, 88832, 89088, 89344, # 89600, 89856, 90112, 90368, 90624, 90880, 91136, 91392, 91648, 91904, 92160, 92416, 92672, 92928, 93184, # 93440, 93696, 93952, 94208, 94464, 94720, 94976, 95232, 95488, 95744, 96000, 96256, 96512, 96768, 97024, # 97280, 97536, 97792, 98048, 98304, 98560, 98816, 99072, 99328, 99584, 99840, 100096, 100352, 100608, 100864, # 101120, 101376, 101632, 101888, 102144, 102400, 102656, 102912, 103168, 103424, 103680, 103936, 104192, # 104448, 104704, 104960, 105216, 105472, 105728, 105984, 106240, 106496, 106752, 107008, 107264, 107520, # 107776, 108032, 108288, 108544, 108800, 109056, 109312, 109568, 109824, 110080, 110336, 110592, 110848, # 111104, 111360, 111616, 111872, 112128, 112384, 112640, 112896, 113152, 113408, 113664, 113920, 114176, # 114432, 114688, 114944, 115200, 115456, 115712, 115968, 116224, 116480, 116736, 116992, 117248, 117504, # 117760, 118016, 118272, 118528, 118784, 119040, 119296, 119552, 119808, 120064, 120320, 120576, 120832, # 121088, 121344, 121600, 121856, 122112, 122368, 122624, 122880, 123136, 123392, 123648, 123904, 124160, # 124416, 124672, 124928, 125184, 125440, 125696, 125952, 126208, 126464, 126720, 126976, 127232, 127488, # 127744, 128000, 128256, 128512, 128768, 129024, 129280, 129536, 129792, 130048, 130304, 130560, 130816, # 131072, 131328, 131584, 131840, 132096, 132352, 132608, 132864, 133120, 133376, 133632, 133888, 134144, # 134400, 134656, 134912, 135168, 135424, 135680, 135936, 136192, 136448, 136704, 136960, 137216, 137472, # 137728, 137984, 138240, 138496, 138752, 139008, 139264, 139520, 139776, 140032, 140288, 140544, 140800, # 141056, 141312, 141568, 141824, 142080, 142336, 142592, 142848, 143104, 143360, 143616, 143872, 144128, # 144384, 144640, 144896, 145152, 145408, 145664, 145920, 146176, 146432, 146688, 146944, 147200, 147456, # 147712, 147968, 148224, 148480, 148736, 148992, 149248, 149504, 149760, 150016, 150272, 150528, 150784, # 151040, 151296, 151552, 151808, 152064, 152320, 152576, 152832, 153088, 153344, 153600, 153856, 154112, # 154368, 154624, 154880, 155136, 155392, 155648, 155904, 156160, 156416, 156672, 156928, 157184, 157440, # 157696, 157952, 158208, 158464, 158720, 158976, 159232, 159488, 159744, 160000] TRAIN_BATCH_SIZE_DIST = tune . choice ( list ( range ( 2 ** 12 , 160000 + TRAIN_BATCH_SIZE_INC , TRAIN_BATCH_SIZE_INC ))) @staticmethod def TRAIN_BATCH_SIZE_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM\"\"\" return ParametersPPO . TRAIN_BATCH_SIZE_DIST @staticmethod def ppo_hyperparameters () -> dict : \"\"\"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: dict -- model configuration \"\"\" ppo_hparams = {} ppo_hparams [ \"lambda\" ] = tune . sample_from ( ParametersPPO . LAMBDA_RANGE ) ppo_hparams [ \"vf_loss_coeff\" ] = tune . sample_from ( ParametersPPO . VF_LOSS_COEFF_RANGE ) ppo_hparams [ \"entropy_coeff\" ] = tune . sample_from ( ParametersPPO . ENTROPY_COEFF_RANGE ) ppo_hparams [ \"clip_param\" ] = tune . sample_from ( ParametersPPO . CLIP_PARAM_RANGE ) ppo_hparams [ \"gamma\" ] = tune . sample_from ( ParametersPPO . GAMMA_RANGE ) ppo_hparams [ \"lr\" ] = tune . sample_from ( ParametersPPO . LR_RANGE ) ppo_hparams [ \"num_sgd_iter\" ] = tune . sample_from ( ParametersPPO . NSGD_RANGE ) ppo_hparams [ \"sgd_minibatch_size\" ] = tune . sample_from ( ParametersPPO . SGD_MINIBATCH_SIZE_RANGE ) ppo_hparams [ \"train_batch_size\" ] = tune . sample_from ( ParametersPPO . TRAIN_BATCH_SIZE_RANGE ) ppo_hparams [ \"kl_coeff\" ] = tune . sample_from ( ParametersPPO . KL_COEFF_RANGE ) ppo_hparams [ \"kl_target\" ] = tune . sample_from ( ParametersPPO . KL_TARGET_RANGE ) return ppo_hparams @staticmethod def sample_ppo_hyperparameters () -> dict : \"\"\"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: dict -- model configuration \"\"\" ppo_hparams = ParametersPPO . ppo_hyperparameters () for k , v in ppo_hparams . items (): ppo_hparams [ k ] = v . sample () return ppo_hparams @staticmethod def pbt_ppo_explore ( config : dict ) -> dict : \"\"\"The following function links to the companion function above. Sets the clipping needed by PBT https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Arguments: config {dict} -- input config Returns: dict -- clipped config \"\"\" def clip_parameter ( config , parameter , parameter_max , parameter_min ): if config [ parameter ] > parameter_max : config [ parameter ] = parameter_max elif config [ parameter ] < parameter_min : config [ parameter ] = parameter_min clip_parameter ( config , \"lambda\" , ParametersPPO . LAMBDA_MAX , ParametersPPO . LAMBDA_MIN ) clip_parameter ( config , \"vf_loss_coeff\" , ParametersPPO . VF_LOSS_COEFF_MAX , ParametersPPO . VF_LOSS_COEFF_MIN ) clip_parameter ( config , \"entropy_coeff\" , ParametersPPO . ENTROPY_COEFF_MAX , ParametersPPO . ENTROPY_COEFF_MIN ) clip_parameter ( config , \"gamma\" , ParametersPPO . GAMMA_MAX , ParametersPPO . GAMMA_MIN ) clip_parameter ( config , \"clip_param\" , ParametersPPO . CLIP_PARAM_MAX , ParametersPPO . CLIP_PARAM_MIN ) clip_parameter ( config , \"lr\" , ParametersPPO . LR_MIN , ParametersPPO . LR_MAX ) clip_parameter ( config , \"kl_coeff\" , ParametersPPO . KL_COEFF_MIN , ParametersPPO . KL_COEFF_MAX ) clip_parameter ( config , \"kl_target\" , ParametersPPO . KL_TARGET_MIN , ParametersPPO . KL_TARGET_MAX ) sgd_minibatch_size_str = \"sgd_minibatch_size\" train_batch_size_str = \"train_batch_size\" num_sgd_iter_str = \"num_sgd_iter\" clip_parameter ( config , num_sgd_iter_str , ParametersPPO . NSGD_MAX , ParametersPPO . NSGD_MIN ) config [ num_sgd_iter_str ] = int ( config [ num_sgd_iter_str ]) clip_parameter ( config , sgd_minibatch_size_str , ParametersPPO . SGD_MINIBATCH_SIZE_MAX , ParametersPPO . SGD_MINIBATCH_SIZE_MIN ) config [ sgd_minibatch_size_str ] = int ( config [ sgd_minibatch_size_str ]) clip_parameter ( config , train_batch_size_str , ParametersPPO . TRAIN_BATCH_SIZE_MAX , ParametersPPO . TRAIN_BATCH_SIZE_MIN ) if config [ train_batch_size_str ] < config [ sgd_minibatch_size_str ] * 2 : config [ train_batch_size_str ] = config [ sgd_minibatch_size_str ] * 2 config [ train_batch_size_str ] = int ( config [ train_batch_size_str ]) return config CLIP_PARAM_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM Clipping Range: 0.1, 0.2, 0.3 Clipping also known as: Clipping parameter epsilon (PPO Paper), clip_param (RLlib), cliprange (ppo2 baselines), clip_param (ppo baselines), epsilon (Unity ML), likelihood_ratio_clipping (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def CLIP_PARAM_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Clipping Range: 0.1, 0.2, 0.3 Clipping also known as: Clipping parameter epsilon (PPO Paper), clip_param (RLlib), cliprange (ppo2 baselines), clip_param (ppo baselines), epsilon (Unity ML), likelihood_ratio_clipping (TensorForce) \"\"\" return ParametersPPO . CLIP_PARAM_DIST ENTROPY_COEFF_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM Entropy Coefficient Range: 0 to 0.01 Entropy Coefficient also known as: Entropy coeff. (PPO Paper), entropy_coeff (RLlib), ent_coeff (ppo2 baselines), entcoeff (ppo baselines), beta (Unity ML), entropy_regularization (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def ENTROPY_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Entropy Coefficient Range: 0 to 0.01 Entropy Coefficient also known as: Entropy coeff. (PPO Paper), entropy_coeff (RLlib), ent_coeff (ppo2 baselines), entcoeff (ppo baselines), beta (Unity ML), entropy_regularization (TensorForce) \"\"\" return ParametersPPO . ENTROPY_COEFF_DIST GAMMA_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM Discount Factor Gamma Range: 0.99 (most common), 0.8 to 0.9997 Discount Factor Gamma also known as: Discount (gamma) (PPO Paper), gamma (RLlib), gamma (ppo2 baselines), gamma (ppo baselines), gamma (Unity ML), discount (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def GAMMA_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Discount Factor Gamma Range: 0.99 (most common), 0.8 to 0.9997 Discount Factor Gamma also known as: Discount (gamma) (PPO Paper), gamma (RLlib), gamma (ppo2 baselines), gamma (ppo baselines), gamma (Unity ML), discount (TensorForce) \"\"\" return ParametersPPO . GAMMA_DIST KL_COEFF_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB Source code in corl/libraries/hparam_search_util.py @staticmethod def KL_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB \"\"\" return ParametersPPO . KL_COEFF_DIST KL_TARGET_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB Source code in corl/libraries/hparam_search_util.py @staticmethod def KL_TARGET_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB \"\"\" return ParametersPPO . KL_COEFF_DIST LAMBDA_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM GAE Parameter Lambda Range: 0.9 to 1 GAE Parameter Lambda also known as: GAE Parameter (lambda) (PPO Paper), lambda (RLlib), lambda (ppo2 baselines), lambda (ppo baselines), lambda (Unity ML), gae_lambda (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def LAMBDA_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM GAE Parameter Lambda Range: 0.9 to 1 GAE Parameter Lambda also known as: GAE Parameter (lambda) (PPO Paper), lambda (RLlib), lambda (ppo2 baselines), lambda (ppo baselines), lambda (Unity ML), gae_lambda (TensorForce) \"\"\" return ParametersPPO . LAMBDA_DIST LR_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM Learning Rate Range: 0.003 to 5e-6 Learning Rate also known as: Adam stepsize (PPO Paper), sgd_stepsize (RLlib), lr (ppo2 baselines), (ppo baselines: unclear), learning_rate (Unity ML), learning_rate (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def LR_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Learning Rate Range: 0.003 to 5e-6 Learning Rate also known as: Adam stepsize (PPO Paper), sgd_stepsize (RLlib), lr (ppo2 baselines), (ppo baselines: unclear), learning_rate (Unity ML), learning_rate (TensorForce) \"\"\" return ParametersPPO . LR_DIST NSGD_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Epoch Range: 3 to 30 Epoch also known as: Num. epochs (PPO paper), num_sgd_iter (RLlib), epochs (ppo2 baselines), optim_epochs (ppo baselines), num_epoch (Unity ML), (TensorForce: unclear) Source code in corl/libraries/hparam_search_util.py @staticmethod def NSGD_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Epoch Range: 3 to 30 Epoch also known as: Num. epochs (PPO paper), num_sgd_iter (RLlib), epochs (ppo2 baselines), optim_epochs (ppo baselines), num_epoch (Unity ML), (TensorForce: unclear) \"\"\" return ParametersPPO . NSGD_DIST SGD_MINIBATCH_SIZE_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM Source code in corl/libraries/hparam_search_util.py @staticmethod def SGD_MINIBATCH_SIZE_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM\"\"\" return ParametersPPO . SGD_MINIBATCH_SIZE_DIST TRAIN_BATCH_SIZE_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM Source code in corl/libraries/hparam_search_util.py @staticmethod def TRAIN_BATCH_SIZE_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM\"\"\" return ParametersPPO . TRAIN_BATCH_SIZE_DIST VF_LOSS_COEFF_RANGE ( spec ) staticmethod \u00a4 Sets the default search space for HPARAM Value Function Coefficient Range: 0.5, 1 Value Function Coefficient also known as: VF coeff. (PPO Paper), vf_loss_coef (RLlib), vf_coef (ppo2 baselines), (ppo baselines: unclear), (Unity ML: unclear), (TensorForce: unclear) Source code in corl/libraries/hparam_search_util.py @staticmethod def VF_LOSS_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Value Function Coefficient Range: 0.5, 1 Value Function Coefficient also known as: VF coeff. (PPO Paper), vf_loss_coef (RLlib), vf_coef (ppo2 baselines), (ppo baselines: unclear), (Unity ML: unclear), (TensorForce: unclear) \"\"\" return ParametersPPO . VF_LOSS_COEFF_DIST pbt_ppo_explore ( config ) staticmethod \u00a4 The following function links to the companion function above. Sets the clipping needed by PBT https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: Type Description dict dict -- clipped config Source code in corl/libraries/hparam_search_util.py @staticmethod def pbt_ppo_explore ( config : dict ) -> dict : \"\"\"The following function links to the companion function above. Sets the clipping needed by PBT https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Arguments: config {dict} -- input config Returns: dict -- clipped config \"\"\" def clip_parameter ( config , parameter , parameter_max , parameter_min ): if config [ parameter ] > parameter_max : config [ parameter ] = parameter_max elif config [ parameter ] < parameter_min : config [ parameter ] = parameter_min clip_parameter ( config , \"lambda\" , ParametersPPO . LAMBDA_MAX , ParametersPPO . LAMBDA_MIN ) clip_parameter ( config , \"vf_loss_coeff\" , ParametersPPO . VF_LOSS_COEFF_MAX , ParametersPPO . VF_LOSS_COEFF_MIN ) clip_parameter ( config , \"entropy_coeff\" , ParametersPPO . ENTROPY_COEFF_MAX , ParametersPPO . ENTROPY_COEFF_MIN ) clip_parameter ( config , \"gamma\" , ParametersPPO . GAMMA_MAX , ParametersPPO . GAMMA_MIN ) clip_parameter ( config , \"clip_param\" , ParametersPPO . CLIP_PARAM_MAX , ParametersPPO . CLIP_PARAM_MIN ) clip_parameter ( config , \"lr\" , ParametersPPO . LR_MIN , ParametersPPO . LR_MAX ) clip_parameter ( config , \"kl_coeff\" , ParametersPPO . KL_COEFF_MIN , ParametersPPO . KL_COEFF_MAX ) clip_parameter ( config , \"kl_target\" , ParametersPPO . KL_TARGET_MIN , ParametersPPO . KL_TARGET_MAX ) sgd_minibatch_size_str = \"sgd_minibatch_size\" train_batch_size_str = \"train_batch_size\" num_sgd_iter_str = \"num_sgd_iter\" clip_parameter ( config , num_sgd_iter_str , ParametersPPO . NSGD_MAX , ParametersPPO . NSGD_MIN ) config [ num_sgd_iter_str ] = int ( config [ num_sgd_iter_str ]) clip_parameter ( config , sgd_minibatch_size_str , ParametersPPO . SGD_MINIBATCH_SIZE_MAX , ParametersPPO . SGD_MINIBATCH_SIZE_MIN ) config [ sgd_minibatch_size_str ] = int ( config [ sgd_minibatch_size_str ]) clip_parameter ( config , train_batch_size_str , ParametersPPO . TRAIN_BATCH_SIZE_MAX , ParametersPPO . TRAIN_BATCH_SIZE_MIN ) if config [ train_batch_size_str ] < config [ sgd_minibatch_size_str ] * 2 : config [ train_batch_size_str ] = config [ sgd_minibatch_size_str ] * 2 config [ train_batch_size_str ] = int ( config [ train_batch_size_str ]) return config ppo_hyperparameters () staticmethod \u00a4 PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: Type Description dict dict -- model configuration Source code in corl/libraries/hparam_search_util.py @staticmethod def ppo_hyperparameters () -> dict : \"\"\"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: dict -- model configuration \"\"\" ppo_hparams = {} ppo_hparams [ \"lambda\" ] = tune . sample_from ( ParametersPPO . LAMBDA_RANGE ) ppo_hparams [ \"vf_loss_coeff\" ] = tune . sample_from ( ParametersPPO . VF_LOSS_COEFF_RANGE ) ppo_hparams [ \"entropy_coeff\" ] = tune . sample_from ( ParametersPPO . ENTROPY_COEFF_RANGE ) ppo_hparams [ \"clip_param\" ] = tune . sample_from ( ParametersPPO . CLIP_PARAM_RANGE ) ppo_hparams [ \"gamma\" ] = tune . sample_from ( ParametersPPO . GAMMA_RANGE ) ppo_hparams [ \"lr\" ] = tune . sample_from ( ParametersPPO . LR_RANGE ) ppo_hparams [ \"num_sgd_iter\" ] = tune . sample_from ( ParametersPPO . NSGD_RANGE ) ppo_hparams [ \"sgd_minibatch_size\" ] = tune . sample_from ( ParametersPPO . SGD_MINIBATCH_SIZE_RANGE ) ppo_hparams [ \"train_batch_size\" ] = tune . sample_from ( ParametersPPO . TRAIN_BATCH_SIZE_RANGE ) ppo_hparams [ \"kl_coeff\" ] = tune . sample_from ( ParametersPPO . KL_COEFF_RANGE ) ppo_hparams [ \"kl_target\" ] = tune . sample_from ( ParametersPPO . KL_TARGET_RANGE ) return ppo_hparams sample_ppo_hyperparameters () staticmethod \u00a4 PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: Type Description dict dict -- model configuration Source code in corl/libraries/hparam_search_util.py @staticmethod def sample_ppo_hyperparameters () -> dict : \"\"\"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: dict -- model configuration \"\"\" ppo_hparams = ParametersPPO . ppo_hyperparameters () for k , v in ppo_hparams . items (): ppo_hparams [ k ] = v . sample () return ppo_hparams","title":"Hparam search util"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.BaseHparamSearch","text":"Experiment provides an anstract class to run specific types of experiments this allows users to do specific setup steps or to run some sort of custom training loop Source code in corl/libraries/hparam_search_util.py class BaseHparamSearch ( abc . ABC ): \"\"\" Experiment provides an anstract class to run specific types of experiments this allows users to do specific setup steps or to run some sort of custom training loop \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseHparamSearchValidator = self . get_validator ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseHparamSearchValidator ]: \"\"\"Gets the validator Returns ------- typing.Type[BaseHparamSearchValidator] The validator class \"\"\" return BaseHparamSearchValidator @abc . abstractmethod def add_algorithm_hparams ( self , rllib_config : dict , tune_config : dict ) -> None : \"\"\"Updates the configurations based on search alg Parameters ---------- rllib_config : dict The rllib config tune_config : dict The tune config \"\"\" ...","title":"BaseHparamSearch"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.BaseHparamSearch.get_validator","text":"Gets the validator","title":"get_validator"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.BaseHparamSearch.get_validator--returns","text":"typing.Type[BaseHparamSearchValidator] The validator class","title":"Returns"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.BaseHparamSearch.add_algorithm_hparams","text":"Updates the configurations based on search alg","title":"add_algorithm_hparams()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.BaseHparamSearch.add_algorithm_hparams--parameters","text":"rllib_config : dict The rllib config tune_config : dict The tune config Source code in corl/libraries/hparam_search_util.py @abc . abstractmethod def add_algorithm_hparams ( self , rllib_config : dict , tune_config : dict ) -> None : \"\"\"Updates the configurations based on search alg Parameters ---------- rllib_config : dict The rllib config tune_config : dict The tune config \"\"\" ...","title":"Parameters"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.BaseHparamSearchValidator","text":"Base Validator to subclass for Experiments subclassing BaseExperiment Source code in corl/libraries/hparam_search_util.py class BaseHparamSearchValidator ( BaseModel ): \"\"\" Base Validator to subclass for Experiments subclassing BaseExperiment \"\"\" ...","title":"BaseHparamSearchValidator"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_AHBS","text":"Asynchronous Hyper Band Example https://docs.ray.io/en/master/tune/examples/includes/async_hyperband_example.html Source code in corl/libraries/hparam_search_util.py class HparamSearchPPO_AHBS ( BaseHparamSearch ): \"\"\"Asynchronous Hyper Band Example https://docs.ray.io/en/master/tune/examples/includes/async_hyperband_example.html \"\"\" def __init__ ( self , ** kwargs ) -> None : super () . __init__ ( kwargs = kwargs ) self . _model_choices = [] if self . config . include_lstm_search : # type: ignore self . _model_choices . append ( ParametersModel . select_lstm_model ) if self . config . inclue_fully_connected_search : # type: ignore self . _model_choices . append ( ParametersModel . select_fully_connected_model ) if self . config . include_frame_stacking_search : # type: ignore self . _model_choices . append ( ParametersModel . select_framestacking_model ) if self . config . include_gtrxl_search : # type: ignore self . _model_choices . append ( ParametersModel . select_gtrxl_model ) @property def get_validator ( self ) -> typing . Type [ HparamSearchValidator_AHBS ]: \"\"\"gets the configuration for AHBS Returns ------- typing.Type[HparamSearchValidator_AHBS] validator \"\"\" return HparamSearchValidator_AHBS def add_algorithm_hparams ( self , rllib_config , tune_config ) -> None : \"\"\"[summary] Parameters ---------- run_or_experiment_config : [type] [description] tune_config : [type] [description] args : [type] [description] \"\"\" ahbs = tune . schedulers . AsyncHyperBandScheduler ( time_attr = self . config . time_attr , # type: ignore metric = self . config . metric , # type: ignore mode = self . config . mode , # type: ignore max_t = self . config . max_t , # type: ignore grace_period = self . config . grace_period , # type: ignore brackets = self . config . brackets # type: ignore ) tune_config [ \"num_samples\" ] = self . config . samples # type: ignore tune_config [ \"scheduler\" ] = ahbs rllib_config . update ( ParametersPPO . ppo_hyperparameters ()) rllib_config [ \"model\" ] = tune . sample_from ( partial ( ParametersModel . select_model , self . _model_choices ))","title":"HparamSearchPPO_AHBS"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_AHBS.get_validator","text":"gets the configuration for AHBS","title":"get_validator"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_AHBS.get_validator--returns","text":"typing.Type[HparamSearchValidator_AHBS] validator","title":"Returns"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_AHBS.add_algorithm_hparams","text":"[summary]","title":"add_algorithm_hparams()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_AHBS.add_algorithm_hparams--parameters","text":"run_or_experiment_config : [type] [description] tune_config : [type] [description] args : [type] [description] Source code in corl/libraries/hparam_search_util.py def add_algorithm_hparams ( self , rllib_config , tune_config ) -> None : \"\"\"[summary] Parameters ---------- run_or_experiment_config : [type] [description] tune_config : [type] [description] args : [type] [description] \"\"\" ahbs = tune . schedulers . AsyncHyperBandScheduler ( time_attr = self . config . time_attr , # type: ignore metric = self . config . metric , # type: ignore mode = self . config . mode , # type: ignore max_t = self . config . max_t , # type: ignore grace_period = self . config . grace_period , # type: ignore brackets = self . config . brackets # type: ignore ) tune_config [ \"num_samples\" ] = self . config . samples # type: ignore tune_config [ \"scheduler\" ] = ahbs rllib_config . update ( ParametersPPO . ppo_hyperparameters ()) rllib_config [ \"model\" ] = tune . sample_from ( partial ( ParametersModel . select_model , self . _model_choices ))","title":"Parameters"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_PBT","text":"PPO PBT Search Space https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Source code in corl/libraries/hparam_search_util.py class HparamSearchPPO_PBT ( BaseHparamSearch ): \"\"\" PPO PBT Search Space https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe \"\"\" @property def get_validator ( self ) -> typing . Type [ HparamSearchValidator_PBT ]: \"\"\"gets the configuration for AHBS Returns ------- typing.Type[HparamSearchValidator_AHBS] validator \"\"\" return HparamSearchValidator_PBT def add_algorithm_hparams ( self , rllib_config , tune_config ) -> None : \"\"\"Adds population based training to the configuration (TBD items to be added - default never add) Parameters ---------- rllib_config : dict The experiment configuration tune_config : dict The tune configuration \"\"\" # Postprocess the perturbed config to ensure it's still valid pbt = tune . schedulers . PopulationBasedTraining ( time_attr = self . config . time_attr , # type: ignore metric = self . config . metric , # type: ignore mode = self . config . mode , # type: ignore perturbation_interval = self . config . perturbation_interval , # type: ignore resample_probability = self . config . resample_probability , # type: ignore burn_in_period = self . config . burn_in_period , # type: ignore log_config = True , # Specifies the mutations of these hyper params hyperparam_mutations = { \"lambda\" : ParametersPPO . LAMBDA_DIST , \"clip_param\" : ParametersPPO . CLIP_PARAM_DIST , \"lr\" : ParametersPPO . LR_DIST , \"num_sgd_iter\" : ParametersPPO . NSGD_DIST , \"sgd_minibatch_size\" : ParametersPPO . SGD_MINIBATCH_SIZE_DIST , \"train_batch_size\" : ParametersPPO . TRAIN_BATCH_SIZE_DIST , \"vf_loss_coeff\" : ParametersPPO . VF_LOSS_COEFF_DIST , \"entropy_coeff\" : ParametersPPO . ENTROPY_COEFF_DIST , \"gamma\" : ParametersPPO . GAMMA_DIST , \"kl_coeff\" : ParametersPPO . KL_COEFF_DIST , \"kl_target\" : ParametersPPO . KL_TARGET_DIST }, custom_explore_fn = ParametersPPO . pbt_ppo_explore ) # These params start off randomly drawn from a set. tune_config [ \"scheduler\" ] = pbt tune_config [ \"num_samples\" ] = self . config . samples # type: ignore rllib_config . update ( ParametersPPO . ppo_hyperparameters ())","title":"HparamSearchPPO_PBT"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_PBT.get_validator","text":"gets the configuration for AHBS","title":"get_validator"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_PBT.get_validator--returns","text":"typing.Type[HparamSearchValidator_AHBS] validator","title":"Returns"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_PBT.add_algorithm_hparams","text":"Adds population based training to the configuration (TBD items to be added - default never add)","title":"add_algorithm_hparams()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchPPO_PBT.add_algorithm_hparams--parameters","text":"rllib_config : dict The experiment configuration tune_config : dict The tune configuration Source code in corl/libraries/hparam_search_util.py def add_algorithm_hparams ( self , rllib_config , tune_config ) -> None : \"\"\"Adds population based training to the configuration (TBD items to be added - default never add) Parameters ---------- rllib_config : dict The experiment configuration tune_config : dict The tune configuration \"\"\" # Postprocess the perturbed config to ensure it's still valid pbt = tune . schedulers . PopulationBasedTraining ( time_attr = self . config . time_attr , # type: ignore metric = self . config . metric , # type: ignore mode = self . config . mode , # type: ignore perturbation_interval = self . config . perturbation_interval , # type: ignore resample_probability = self . config . resample_probability , # type: ignore burn_in_period = self . config . burn_in_period , # type: ignore log_config = True , # Specifies the mutations of these hyper params hyperparam_mutations = { \"lambda\" : ParametersPPO . LAMBDA_DIST , \"clip_param\" : ParametersPPO . CLIP_PARAM_DIST , \"lr\" : ParametersPPO . LR_DIST , \"num_sgd_iter\" : ParametersPPO . NSGD_DIST , \"sgd_minibatch_size\" : ParametersPPO . SGD_MINIBATCH_SIZE_DIST , \"train_batch_size\" : ParametersPPO . TRAIN_BATCH_SIZE_DIST , \"vf_loss_coeff\" : ParametersPPO . VF_LOSS_COEFF_DIST , \"entropy_coeff\" : ParametersPPO . ENTROPY_COEFF_DIST , \"gamma\" : ParametersPPO . GAMMA_DIST , \"kl_coeff\" : ParametersPPO . KL_COEFF_DIST , \"kl_target\" : ParametersPPO . KL_TARGET_DIST }, custom_explore_fn = ParametersPPO . pbt_ppo_explore ) # These params start off randomly drawn from a set. tune_config [ \"scheduler\" ] = pbt tune_config [ \"num_samples\" ] = self . config . samples # type: ignore rllib_config . update ( ParametersPPO . ppo_hyperparameters ())","title":"Parameters"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchValidator_AHBS","text":"Base Validator to subclass for search subclassing Source code in corl/libraries/hparam_search_util.py class HparamSearchValidator_AHBS ( HparamSearchValidator_Shared ): \"\"\" Base Validator to subclass for search subclassing \"\"\" # max time units per trial. Trials will be stopped after max_t time units (determined # by time_attr) have passed. max_t : float = 1e7 # Brackets brackets : float = 1 # Only stop trials at least this old in time. The units are the same as the attribute # named by time-attr. grace_period : float = 5e6 include_lstm_search : bool = False inclue_fully_connected_search : bool = True include_frame_stacking_search : bool = False include_gtrxl_search : bool = False","title":"HparamSearchValidator_AHBS"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchValidator_PBT","text":"Base Validator to subclass for search subclassing Source code in corl/libraries/hparam_search_util.py class HparamSearchValidator_PBT ( HparamSearchValidator_Shared ): \"\"\" Base Validator to subclass for search subclassing \"\"\" # The probability of resampling from the original distribution when applying hyperparam_mutations. # If not resampled, the value will be perturbed by a factor of 1.2 or 0.8 if continuous, or changed # to an adjacent value if discrete. Note that resample_probability by default is 0.25, thus # hyperparameter with a distribution may go out of the specific range. resample_probability : float = 0.25 # (float) \u2013 Models will be considered for perturbation at this interval of time_attr. Note that # perturbation incurs checkpoint overhead, so you shouldn\u2019t set this to be too frequent. perturbation_interval : float = 4 # (float) \u2013 Models will not be considered for perturbation before this interval of time_attr has # passed. This guarantees that models are trained for at least a certain amount of time or timesteps # before being perturbed. burn_in_period : float = 10","title":"HparamSearchValidator_PBT"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.HparamSearchValidator_Shared","text":"Base Validator to subclass for search subclassing Source code in corl/libraries/hparam_search_util.py class HparamSearchValidator_Shared ( BaseHparamSearchValidator ): \"\"\" Base Validator to subclass for search subclassing \"\"\" # \"The training result objective value attribute. Stopping procedures will use this attribute.\" metric : str = \"episode_reward_mean\" # One of {min, max}. Determines whether objective is minimizing or maximizing the metric attribute. mode : str = \"max\" # A training result attr to use for comparing time. Note that you can pass in something # non-temporal such as training_iteration as a measure of progress, the only requirement is # that the attribute should increase monotonically. time_attr : str = \"timesteps_total\" # The number of samples to collect during HPARAM search (trials) samples : int = 4","title":"HparamSearchValidator_Shared"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel","text":"Holds the model parameters Source code in corl/libraries/hparam_search_util.py class ParametersModel : \"\"\"Holds the model parameters \"\"\" FC_LAYER_CHOICES = [ 32 , 64 , 128 , 256 , 512 ] FC_LAYER_COUNT = [ 2 , 3 , 4 , 5 , 6 ] @staticmethod def __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ): model_layers : typing . List = [] for _ in range ( 0 , layer_count ): temp_layers = layer_choices [ MIN_LAYER_INDEX :] if len ( model_layers ) < FC_FILTER_LOWER_VALUES_THRESHOLD else layer_choices if model_layers : model_layers . append ( random . choice ([ x for x in temp_layers if x <= model_layers [ - 1 ]])) else : model_layers . append ( random . choice ( temp_layers )) return model_layers @staticmethod def select_lstm_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"use_lstm\" ] = True model_config [ \"max_seq_len\" ] = random . choice ([ 2 , 3 , 5 , 10 ]) model_config [ \"lstm_cell_size\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"vf_share_layers\" ] = True # model_config[\"vf_share_layers\"] = random.choice([True, False]) # model_config[\"lstm_use_prev_action\"] = random.choice([True, False]) # model_config[\"lstm_use_prev_reward\"] = random.choice([True, False]) # model_config[\"_time_major\"] = random.choice([True, False]) return model_config @staticmethod def select_fully_connected_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" layer_count = random . choice ( ParametersModel . FC_LAYER_COUNT ) layer_choices = ParametersModel . FC_LAYER_CHOICES FC_FILTER_LOWER_VALUES_THRESHOLD = 2 MIN_LAYER_INDEX = 2 model_layers = ParametersModel . __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ) model_config : dict = {} model_config [ \"fcnet_hiddens\" ] = model_layers model_config [ \"fcnet_activation\" ] = random . choice ([ \"relu\" , \"tanh\" ]) return model_config @staticmethod def select_framestacking_model () -> dict : \"\"\"[summary] Returns: dict -- [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"custom_model\" ] = \"TorchFrameStack\" model_config [ \"custom_model_config\" ] = {} # type: ignore model_config [ \"custom_model_config\" ][ \"num_frames\" ] = random . choice ( list ( range ( 1 , 11 ))) model_config [ \"custom_model_config\" ][ \"include_actions\" ] = random . choice ([ True , False ]) model_config [ \"custom_model_config\" ][ \"include_rewards\" ] = random . choice ([ True , False ]) layer_count = random . choice ( ParametersModel . FC_LAYER_COUNT ) layer_choices = ParametersModel . FC_LAYER_CHOICES FC_FILTER_LOWER_VALUES_THRESHOLD = 2 MIN_LAYER_INDEX = 2 model_config [ \"custom_model_config\" ][ \"post_fcnet_hiddens\" ] = ParametersModel . __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ) return model_config @staticmethod def select_gtrxl_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"use_attention\" ] = False model_config [ \"attention_num_transformer_units\" ] = random . choice ( list ( range ( 1 , 6 ))) model_config [ \"attention_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"attention_num_heads\" ] = random . choice ( list ( range ( 1 , 6 ))) model_config [ \"attention_head_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"attention_memory_inference\" ] = 50 model_config [ \"attention_memory_training\" ] = 50 model_config [ \"attention_position_wise_mlp_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) # model_config[\"attention_init_gru_gate_bias\"] = 2.0 model_config [ \"attention_use_n_prev_actions\" ] = random . choice ( list ( range ( 1 , 11 ))) model_config [ \"attention_use_n_prev_rewards\" ] = random . choice ( list ( range ( 1 , 11 ))) return model_config @staticmethod def select_model ( model_choices ) -> dict : \"\"\"The following function provides the start to exploring model configurations. \"\"\" model_config_func = random . choice ( model_choices ) return model_config_func ()","title":"ParametersModel"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel.select_framestacking_model","text":"[summary] Returns: Type Description dict dict -- [description] Source code in corl/libraries/hparam_search_util.py @staticmethod def select_framestacking_model () -> dict : \"\"\"[summary] Returns: dict -- [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"custom_model\" ] = \"TorchFrameStack\" model_config [ \"custom_model_config\" ] = {} # type: ignore model_config [ \"custom_model_config\" ][ \"num_frames\" ] = random . choice ( list ( range ( 1 , 11 ))) model_config [ \"custom_model_config\" ][ \"include_actions\" ] = random . choice ([ True , False ]) model_config [ \"custom_model_config\" ][ \"include_rewards\" ] = random . choice ([ True , False ]) layer_count = random . choice ( ParametersModel . FC_LAYER_COUNT ) layer_choices = ParametersModel . FC_LAYER_CHOICES FC_FILTER_LOWER_VALUES_THRESHOLD = 2 MIN_LAYER_INDEX = 2 model_config [ \"custom_model_config\" ][ \"post_fcnet_hiddens\" ] = ParametersModel . __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ) return model_config","title":"select_framestacking_model()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel.select_fully_connected_model","text":"[summary]","title":"select_fully_connected_model()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel.select_fully_connected_model--returns","text":"dict [description] Source code in corl/libraries/hparam_search_util.py @staticmethod def select_fully_connected_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" layer_count = random . choice ( ParametersModel . FC_LAYER_COUNT ) layer_choices = ParametersModel . FC_LAYER_CHOICES FC_FILTER_LOWER_VALUES_THRESHOLD = 2 MIN_LAYER_INDEX = 2 model_layers = ParametersModel . __get_layers ( layer_count , FC_FILTER_LOWER_VALUES_THRESHOLD , layer_choices , MIN_LAYER_INDEX ) model_config : dict = {} model_config [ \"fcnet_hiddens\" ] = model_layers model_config [ \"fcnet_activation\" ] = random . choice ([ \"relu\" , \"tanh\" ]) return model_config","title":"Returns"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel.select_gtrxl_model","text":"[summary]","title":"select_gtrxl_model()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel.select_gtrxl_model--returns","text":"dict [description] Source code in corl/libraries/hparam_search_util.py @staticmethod def select_gtrxl_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"use_attention\" ] = False model_config [ \"attention_num_transformer_units\" ] = random . choice ( list ( range ( 1 , 6 ))) model_config [ \"attention_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"attention_num_heads\" ] = random . choice ( list ( range ( 1 , 6 ))) model_config [ \"attention_head_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"attention_memory_inference\" ] = 50 model_config [ \"attention_memory_training\" ] = 50 model_config [ \"attention_position_wise_mlp_dim\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) # model_config[\"attention_init_gru_gate_bias\"] = 2.0 model_config [ \"attention_use_n_prev_actions\" ] = random . choice ( list ( range ( 1 , 11 ))) model_config [ \"attention_use_n_prev_rewards\" ] = random . choice ( list ( range ( 1 , 11 ))) return model_config","title":"Returns"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel.select_lstm_model","text":"[summary]","title":"select_lstm_model()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel.select_lstm_model--returns","text":"dict [description] Source code in corl/libraries/hparam_search_util.py @staticmethod def select_lstm_model () -> dict : \"\"\"[summary] Returns ------- dict [description] \"\"\" model_config = ParametersModel . select_fully_connected_model () model_config [ \"use_lstm\" ] = True model_config [ \"max_seq_len\" ] = random . choice ([ 2 , 3 , 5 , 10 ]) model_config [ \"lstm_cell_size\" ] = random . choice ([ 64 , 128 , 256 , 512 , 1024 , 2048 ]) model_config [ \"vf_share_layers\" ] = True # model_config[\"vf_share_layers\"] = random.choice([True, False]) # model_config[\"lstm_use_prev_action\"] = random.choice([True, False]) # model_config[\"lstm_use_prev_reward\"] = random.choice([True, False]) # model_config[\"_time_major\"] = random.choice([True, False]) return model_config","title":"Returns"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersModel.select_model","text":"The following function provides the start to exploring model configurations. Source code in corl/libraries/hparam_search_util.py @staticmethod def select_model ( model_choices ) -> dict : \"\"\"The following function provides the start to exploring model configurations. \"\"\" model_config_func = random . choice ( model_choices ) return model_config_func ()","title":"select_model()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO","text":"Utility functions for processing hparam searches in the framework for PPO algorithm https://github.com/ray-project/ray/blob/00922817b66ee14ba215972a98f416f3d6fef1ba/rllib/agents/ppo/ppo.py https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe https://blog.openai.com/openai-five/ https://docs.ray.io/en/master/tune/api_docs/trainable.html#advanced-reusing-actors Source code in corl/libraries/hparam_search_util.py class ParametersPPO : \"\"\"Utility functions for processing hparam searches in the framework for PPO algorithm https://github.com/ray-project/ray/blob/00922817b66ee14ba215972a98f416f3d6fef1ba/rllib/agents/ppo/ppo.py https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe https://blog.openai.com/openai-five/ https://docs.ray.io/en/master/tune/api_docs/trainable.html#advanced-reusing-actors \"\"\" LAMBDA_MIN = 0.9 LAMBDA_MAX = 1.0 LAMBDA_DIST = tune . uniform ( LAMBDA_MIN , LAMBDA_MAX ) @staticmethod def LAMBDA_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM GAE Parameter Lambda Range: 0.9 to 1 GAE Parameter Lambda also known as: GAE Parameter (lambda) (PPO Paper), lambda (RLlib), lambda (ppo2 baselines), lambda (ppo baselines), lambda (Unity ML), gae_lambda (TensorForce) \"\"\" return ParametersPPO . LAMBDA_DIST VF_LOSS_COEFF_MIN = 0.5 VF_LOSS_COEFF_MAX = 1.0 VF_LOSS_COEFF_DIST = tune . uniform ( VF_LOSS_COEFF_MIN , VF_LOSS_COEFF_MAX ) @staticmethod def VF_LOSS_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Value Function Coefficient Range: 0.5, 1 Value Function Coefficient also known as: VF coeff. (PPO Paper), vf_loss_coef (RLlib), vf_coef (ppo2 baselines), (ppo baselines: unclear), (Unity ML: unclear), (TensorForce: unclear) \"\"\" return ParametersPPO . VF_LOSS_COEFF_DIST ENTROPY_COEFF_MIN = 0.00 ENTROPY_COEFF_MAX = 0.01 ENTROPY_COEFF_DIST = tune . uniform ( ENTROPY_COEFF_MIN , ENTROPY_COEFF_MAX ) @staticmethod def ENTROPY_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Entropy Coefficient Range: 0 to 0.01 Entropy Coefficient also known as: Entropy coeff. (PPO Paper), entropy_coeff (RLlib), ent_coeff (ppo2 baselines), entcoeff (ppo baselines), beta (Unity ML), entropy_regularization (TensorForce) \"\"\" return ParametersPPO . ENTROPY_COEFF_DIST CLIP_PARAM_MIN = 0.1 CLIP_PARAM_MAX = 0.3 CLIP_PARAM_DIST = tune . choice ([ 0.1 , 0.2 , 0.3 ]) @staticmethod def CLIP_PARAM_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Clipping Range: 0.1, 0.2, 0.3 Clipping also known as: Clipping parameter epsilon (PPO Paper), clip_param (RLlib), cliprange (ppo2 baselines), clip_param (ppo baselines), epsilon (Unity ML), likelihood_ratio_clipping (TensorForce) \"\"\" return ParametersPPO . CLIP_PARAM_DIST KL_TARGET_MIN = 0.003 KL_TARGET_MAX = 0.03 KL_TARGET_DIST = tune . uniform ( KL_TARGET_MIN , KL_TARGET_MAX ) @staticmethod def KL_TARGET_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB \"\"\" return ParametersPPO . KL_COEFF_DIST KL_COEFF_MIN = 0.2 # RLLIB Default KL_COEFF_MAX = 1.0 # https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe KL_COEFF_DIST = tune . uniform ( KL_COEFF_MIN , KL_COEFF_MAX ) @staticmethod def KL_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB \"\"\" return ParametersPPO . KL_COEFF_DIST GAMMA_MIN = 0.8000 GAMMA_MAX = 0.9997 GAMMA_DIST = tune . uniform ( GAMMA_MIN , GAMMA_MAX ) @staticmethod def GAMMA_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Discount Factor Gamma Range: 0.99 (most common), 0.8 to 0.9997 Discount Factor Gamma also known as: Discount (gamma) (PPO Paper), gamma (RLlib), gamma (ppo2 baselines), gamma (ppo baselines), gamma (Unity ML), discount (TensorForce) \"\"\" return ParametersPPO . GAMMA_DIST LR_MIN = 5e-6 LR_MAX = 0.003 LR_DIST = tune . uniform ( LR_MIN , LR_MAX ) @staticmethod def LR_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Learning Rate Range: 0.003 to 5e-6 Learning Rate also known as: Adam stepsize (PPO Paper), sgd_stepsize (RLlib), lr (ppo2 baselines), (ppo baselines: unclear), learning_rate (Unity ML), learning_rate (TensorForce) \"\"\" return ParametersPPO . LR_DIST NSGD_MIN = 3 NSGD_MAX = 30 NSGD_DIST = tune . choice ( list ( range ( NSGD_MIN , NSGD_MAX + 1 ))) @staticmethod def NSGD_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Epoch Range: 3 to 30 Epoch also known as: Num. epochs (PPO paper), num_sgd_iter (RLlib), epochs (ppo2 baselines), optim_epochs (ppo baselines), num_epoch (Unity ML), (TensorForce: unclear) \"\"\" return ParametersPPO . NSGD_DIST SGD_MINIBATCH_SIZE_MIN = 128 SGD_MINIBATCH_SIZE_MAX = 4096 SGD_MINIBATCH_SIZE_DIST = tune . choice ([ 128 , 256 , 512 , 1024 , 2048 , 4096 ]) @staticmethod def SGD_MINIBATCH_SIZE_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM\"\"\" return ParametersPPO . SGD_MINIBATCH_SIZE_DIST TRAIN_BATCH_SIZE_MIN = 4096 TRAIN_BATCH_SIZE_MAX = 160000 TRAIN_BATCH_SIZE_INC = 256 # [4096, 4352, 4608, 4864, 5120, 5376, 5632, 5888, 6144, 6400, 6656, 6912, 7168, 7424, 7680, 7936, 8192, 8448, # 8704, 8960, 9216, 9472, 9728, 9984, 10240, 10496, 10752, 11008, 11264, 11520, 11776, 12032, 12288, 12544, # 12800, 13056, 13312, 13568, 13824, 14080, 14336, 14592, 14848, 15104, 15360, 15616, 15872, 16128, 16384, # 16640, 16896, 17152, 17408, 17664, 17920, 18176, 18432, 18688, 18944, 19200, 19456, 19712, 19968, 20224, # 20480, 20736, 20992, 21248, 21504, 21760, 22016, 22272, 22528, 22784, 23040, 23296, 23552, 23808, 24064, # 24320, 24576, 24832, 25088, 25344, 25600, 25856, 26112, 26368, 26624, 26880, 27136, 27392, 27648, 27904, # 28160, 28416, 28672, 28928, 29184, 29440, 29696, 29952, 30208, 30464, 30720, 30976, 31232, 31488, 31744, # 32000, 32256, 32512, 32768, 33024, 33280, 33536, 33792, 34048, 34304, 34560, 34816, 35072, 35328, 35584, # 35840, 36096, 36352, 36608, 36864, 37120, 37376, 37632, 37888, 38144, 38400, 38656, 38912, 39168, 39424, # 39680, 39936, 40192, 40448, 40704, 40960, 41216, 41472, 41728, 41984, 42240, 42496, 42752, 43008, 43264, # 43520, 43776, 44032, 44288, 44544, 44800, 45056, 45312, 45568, 45824, 46080, 46336, 46592, 46848, 47104, # 47360, 47616, 47872, 48128, 48384, 48640, 48896, 49152, 49408, 49664, 49920, 50176, 50432, 50688, 50944, # 51200, 51456, 51712, 51968, 52224, 52480, 52736, 52992, 53248, 53504, 53760, 54016, 54272, 54528, 54784, # 55040, 55296, 55552, 55808, 56064, 56320, 56576, 56832, 57088, 57344, 57600, 57856, 58112, 58368, 58624, # 58880, 59136, 59392, 59648, 59904, 60160, 60416, 60672, 60928, 61184, 61440, 61696, 61952, 62208, 62464, # 62720, 62976, 63232, 63488, 63744, 64000, 64256, 64512, 64768, 65024, 65280, 65536, 65792, 66048, 66304, # 66560, 66816, 67072, 67328, 67584, 67840, 68096, 68352, 68608, 68864, 69120, 69376, 69632, 69888, 70144, # 70400, 70656, 70912, 71168, 71424, 71680, 71936, 72192, 72448, 72704, 72960, 73216, 73472, 73728, 73984, # 74240, 74496, 74752, 75008, 75264, 75520, 75776, 76032, 76288, 76544, 76800, 77056, 77312, 77568, 77824, # 78080, 78336, 78592, 78848, 79104, 79360, 79616, 79872, 80128, 80384, 80640, 80896, 81152, 81408, 81664, # 81920, 82176, 82432, 82688, 82944, 83200, 83456, 83712, 83968, 84224, 84480, 84736, 84992, 85248, 85504, # 85760, 86016, 86272, 86528, 86784, 87040, 87296, 87552, 87808, 88064, 88320, 88576, 88832, 89088, 89344, # 89600, 89856, 90112, 90368, 90624, 90880, 91136, 91392, 91648, 91904, 92160, 92416, 92672, 92928, 93184, # 93440, 93696, 93952, 94208, 94464, 94720, 94976, 95232, 95488, 95744, 96000, 96256, 96512, 96768, 97024, # 97280, 97536, 97792, 98048, 98304, 98560, 98816, 99072, 99328, 99584, 99840, 100096, 100352, 100608, 100864, # 101120, 101376, 101632, 101888, 102144, 102400, 102656, 102912, 103168, 103424, 103680, 103936, 104192, # 104448, 104704, 104960, 105216, 105472, 105728, 105984, 106240, 106496, 106752, 107008, 107264, 107520, # 107776, 108032, 108288, 108544, 108800, 109056, 109312, 109568, 109824, 110080, 110336, 110592, 110848, # 111104, 111360, 111616, 111872, 112128, 112384, 112640, 112896, 113152, 113408, 113664, 113920, 114176, # 114432, 114688, 114944, 115200, 115456, 115712, 115968, 116224, 116480, 116736, 116992, 117248, 117504, # 117760, 118016, 118272, 118528, 118784, 119040, 119296, 119552, 119808, 120064, 120320, 120576, 120832, # 121088, 121344, 121600, 121856, 122112, 122368, 122624, 122880, 123136, 123392, 123648, 123904, 124160, # 124416, 124672, 124928, 125184, 125440, 125696, 125952, 126208, 126464, 126720, 126976, 127232, 127488, # 127744, 128000, 128256, 128512, 128768, 129024, 129280, 129536, 129792, 130048, 130304, 130560, 130816, # 131072, 131328, 131584, 131840, 132096, 132352, 132608, 132864, 133120, 133376, 133632, 133888, 134144, # 134400, 134656, 134912, 135168, 135424, 135680, 135936, 136192, 136448, 136704, 136960, 137216, 137472, # 137728, 137984, 138240, 138496, 138752, 139008, 139264, 139520, 139776, 140032, 140288, 140544, 140800, # 141056, 141312, 141568, 141824, 142080, 142336, 142592, 142848, 143104, 143360, 143616, 143872, 144128, # 144384, 144640, 144896, 145152, 145408, 145664, 145920, 146176, 146432, 146688, 146944, 147200, 147456, # 147712, 147968, 148224, 148480, 148736, 148992, 149248, 149504, 149760, 150016, 150272, 150528, 150784, # 151040, 151296, 151552, 151808, 152064, 152320, 152576, 152832, 153088, 153344, 153600, 153856, 154112, # 154368, 154624, 154880, 155136, 155392, 155648, 155904, 156160, 156416, 156672, 156928, 157184, 157440, # 157696, 157952, 158208, 158464, 158720, 158976, 159232, 159488, 159744, 160000] TRAIN_BATCH_SIZE_DIST = tune . choice ( list ( range ( 2 ** 12 , 160000 + TRAIN_BATCH_SIZE_INC , TRAIN_BATCH_SIZE_INC ))) @staticmethod def TRAIN_BATCH_SIZE_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM\"\"\" return ParametersPPO . TRAIN_BATCH_SIZE_DIST @staticmethod def ppo_hyperparameters () -> dict : \"\"\"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: dict -- model configuration \"\"\" ppo_hparams = {} ppo_hparams [ \"lambda\" ] = tune . sample_from ( ParametersPPO . LAMBDA_RANGE ) ppo_hparams [ \"vf_loss_coeff\" ] = tune . sample_from ( ParametersPPO . VF_LOSS_COEFF_RANGE ) ppo_hparams [ \"entropy_coeff\" ] = tune . sample_from ( ParametersPPO . ENTROPY_COEFF_RANGE ) ppo_hparams [ \"clip_param\" ] = tune . sample_from ( ParametersPPO . CLIP_PARAM_RANGE ) ppo_hparams [ \"gamma\" ] = tune . sample_from ( ParametersPPO . GAMMA_RANGE ) ppo_hparams [ \"lr\" ] = tune . sample_from ( ParametersPPO . LR_RANGE ) ppo_hparams [ \"num_sgd_iter\" ] = tune . sample_from ( ParametersPPO . NSGD_RANGE ) ppo_hparams [ \"sgd_minibatch_size\" ] = tune . sample_from ( ParametersPPO . SGD_MINIBATCH_SIZE_RANGE ) ppo_hparams [ \"train_batch_size\" ] = tune . sample_from ( ParametersPPO . TRAIN_BATCH_SIZE_RANGE ) ppo_hparams [ \"kl_coeff\" ] = tune . sample_from ( ParametersPPO . KL_COEFF_RANGE ) ppo_hparams [ \"kl_target\" ] = tune . sample_from ( ParametersPPO . KL_TARGET_RANGE ) return ppo_hparams @staticmethod def sample_ppo_hyperparameters () -> dict : \"\"\"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: dict -- model configuration \"\"\" ppo_hparams = ParametersPPO . ppo_hyperparameters () for k , v in ppo_hparams . items (): ppo_hparams [ k ] = v . sample () return ppo_hparams @staticmethod def pbt_ppo_explore ( config : dict ) -> dict : \"\"\"The following function links to the companion function above. Sets the clipping needed by PBT https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Arguments: config {dict} -- input config Returns: dict -- clipped config \"\"\" def clip_parameter ( config , parameter , parameter_max , parameter_min ): if config [ parameter ] > parameter_max : config [ parameter ] = parameter_max elif config [ parameter ] < parameter_min : config [ parameter ] = parameter_min clip_parameter ( config , \"lambda\" , ParametersPPO . LAMBDA_MAX , ParametersPPO . LAMBDA_MIN ) clip_parameter ( config , \"vf_loss_coeff\" , ParametersPPO . VF_LOSS_COEFF_MAX , ParametersPPO . VF_LOSS_COEFF_MIN ) clip_parameter ( config , \"entropy_coeff\" , ParametersPPO . ENTROPY_COEFF_MAX , ParametersPPO . ENTROPY_COEFF_MIN ) clip_parameter ( config , \"gamma\" , ParametersPPO . GAMMA_MAX , ParametersPPO . GAMMA_MIN ) clip_parameter ( config , \"clip_param\" , ParametersPPO . CLIP_PARAM_MAX , ParametersPPO . CLIP_PARAM_MIN ) clip_parameter ( config , \"lr\" , ParametersPPO . LR_MIN , ParametersPPO . LR_MAX ) clip_parameter ( config , \"kl_coeff\" , ParametersPPO . KL_COEFF_MIN , ParametersPPO . KL_COEFF_MAX ) clip_parameter ( config , \"kl_target\" , ParametersPPO . KL_TARGET_MIN , ParametersPPO . KL_TARGET_MAX ) sgd_minibatch_size_str = \"sgd_minibatch_size\" train_batch_size_str = \"train_batch_size\" num_sgd_iter_str = \"num_sgd_iter\" clip_parameter ( config , num_sgd_iter_str , ParametersPPO . NSGD_MAX , ParametersPPO . NSGD_MIN ) config [ num_sgd_iter_str ] = int ( config [ num_sgd_iter_str ]) clip_parameter ( config , sgd_minibatch_size_str , ParametersPPO . SGD_MINIBATCH_SIZE_MAX , ParametersPPO . SGD_MINIBATCH_SIZE_MIN ) config [ sgd_minibatch_size_str ] = int ( config [ sgd_minibatch_size_str ]) clip_parameter ( config , train_batch_size_str , ParametersPPO . TRAIN_BATCH_SIZE_MAX , ParametersPPO . TRAIN_BATCH_SIZE_MIN ) if config [ train_batch_size_str ] < config [ sgd_minibatch_size_str ] * 2 : config [ train_batch_size_str ] = config [ sgd_minibatch_size_str ] * 2 config [ train_batch_size_str ] = int ( config [ train_batch_size_str ]) return config","title":"ParametersPPO"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.CLIP_PARAM_RANGE","text":"Sets the default search space for HPARAM Clipping Range: 0.1, 0.2, 0.3 Clipping also known as: Clipping parameter epsilon (PPO Paper), clip_param (RLlib), cliprange (ppo2 baselines), clip_param (ppo baselines), epsilon (Unity ML), likelihood_ratio_clipping (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def CLIP_PARAM_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Clipping Range: 0.1, 0.2, 0.3 Clipping also known as: Clipping parameter epsilon (PPO Paper), clip_param (RLlib), cliprange (ppo2 baselines), clip_param (ppo baselines), epsilon (Unity ML), likelihood_ratio_clipping (TensorForce) \"\"\" return ParametersPPO . CLIP_PARAM_DIST","title":"CLIP_PARAM_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.ENTROPY_COEFF_RANGE","text":"Sets the default search space for HPARAM Entropy Coefficient Range: 0 to 0.01 Entropy Coefficient also known as: Entropy coeff. (PPO Paper), entropy_coeff (RLlib), ent_coeff (ppo2 baselines), entcoeff (ppo baselines), beta (Unity ML), entropy_regularization (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def ENTROPY_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Entropy Coefficient Range: 0 to 0.01 Entropy Coefficient also known as: Entropy coeff. (PPO Paper), entropy_coeff (RLlib), ent_coeff (ppo2 baselines), entcoeff (ppo baselines), beta (Unity ML), entropy_regularization (TensorForce) \"\"\" return ParametersPPO . ENTROPY_COEFF_DIST","title":"ENTROPY_COEFF_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.GAMMA_RANGE","text":"Sets the default search space for HPARAM Discount Factor Gamma Range: 0.99 (most common), 0.8 to 0.9997 Discount Factor Gamma also known as: Discount (gamma) (PPO Paper), gamma (RLlib), gamma (ppo2 baselines), gamma (ppo baselines), gamma (Unity ML), discount (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def GAMMA_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Discount Factor Gamma Range: 0.99 (most common), 0.8 to 0.9997 Discount Factor Gamma also known as: Discount (gamma) (PPO Paper), gamma (RLlib), gamma (ppo2 baselines), gamma (ppo baselines), gamma (Unity ML), discount (TensorForce) \"\"\" return ParametersPPO . GAMMA_DIST","title":"GAMMA_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.KL_COEFF_RANGE","text":"Sets the default search space for HPARAM The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB Source code in corl/libraries/hparam_search_util.py @staticmethod def KL_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB \"\"\" return ParametersPPO . KL_COEFF_DIST","title":"KL_COEFF_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.KL_TARGET_RANGE","text":"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB Source code in corl/libraries/hparam_search_util.py @staticmethod def KL_TARGET_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe The KL penalty implementation (third line in the above picture) is available in RLlib\u2019s PPO implementation. The parameters kl_coeff (initial coefficient for KL divergence) and kl_target can be used for the KL implementation. KL Target Range: 0.003 to 0.03 KL Initialization Range: 0.3 to 1 --- KL_COEFF IN RLLIB \"\"\" return ParametersPPO . KL_COEFF_DIST","title":"KL_TARGET_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.LAMBDA_RANGE","text":"Sets the default search space for HPARAM GAE Parameter Lambda Range: 0.9 to 1 GAE Parameter Lambda also known as: GAE Parameter (lambda) (PPO Paper), lambda (RLlib), lambda (ppo2 baselines), lambda (ppo baselines), lambda (Unity ML), gae_lambda (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def LAMBDA_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM GAE Parameter Lambda Range: 0.9 to 1 GAE Parameter Lambda also known as: GAE Parameter (lambda) (PPO Paper), lambda (RLlib), lambda (ppo2 baselines), lambda (ppo baselines), lambda (Unity ML), gae_lambda (TensorForce) \"\"\" return ParametersPPO . LAMBDA_DIST","title":"LAMBDA_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.LR_RANGE","text":"Sets the default search space for HPARAM Learning Rate Range: 0.003 to 5e-6 Learning Rate also known as: Adam stepsize (PPO Paper), sgd_stepsize (RLlib), lr (ppo2 baselines), (ppo baselines: unclear), learning_rate (Unity ML), learning_rate (TensorForce) Source code in corl/libraries/hparam_search_util.py @staticmethod def LR_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Learning Rate Range: 0.003 to 5e-6 Learning Rate also known as: Adam stepsize (PPO Paper), sgd_stepsize (RLlib), lr (ppo2 baselines), (ppo baselines: unclear), learning_rate (Unity ML), learning_rate (TensorForce) \"\"\" return ParametersPPO . LR_DIST","title":"LR_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.NSGD_RANGE","text":"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Epoch Range: 3 to 30 Epoch also known as: Num. epochs (PPO paper), num_sgd_iter (RLlib), epochs (ppo2 baselines), optim_epochs (ppo baselines), num_epoch (Unity ML), (TensorForce: unclear) Source code in corl/libraries/hparam_search_util.py @staticmethod def NSGD_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Epoch Range: 3 to 30 Epoch also known as: Num. epochs (PPO paper), num_sgd_iter (RLlib), epochs (ppo2 baselines), optim_epochs (ppo baselines), num_epoch (Unity ML), (TensorForce: unclear) \"\"\" return ParametersPPO . NSGD_DIST","title":"NSGD_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.SGD_MINIBATCH_SIZE_RANGE","text":"Sets the default search space for HPARAM Source code in corl/libraries/hparam_search_util.py @staticmethod def SGD_MINIBATCH_SIZE_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM\"\"\" return ParametersPPO . SGD_MINIBATCH_SIZE_DIST","title":"SGD_MINIBATCH_SIZE_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.TRAIN_BATCH_SIZE_RANGE","text":"Sets the default search space for HPARAM Source code in corl/libraries/hparam_search_util.py @staticmethod def TRAIN_BATCH_SIZE_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM\"\"\" return ParametersPPO . TRAIN_BATCH_SIZE_DIST","title":"TRAIN_BATCH_SIZE_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.VF_LOSS_COEFF_RANGE","text":"Sets the default search space for HPARAM Value Function Coefficient Range: 0.5, 1 Value Function Coefficient also known as: VF coeff. (PPO Paper), vf_loss_coef (RLlib), vf_coef (ppo2 baselines), (ppo baselines: unclear), (Unity ML: unclear), (TensorForce: unclear) Source code in corl/libraries/hparam_search_util.py @staticmethod def VF_LOSS_COEFF_RANGE ( spec ): # pylint: disable=W0613 \"\"\"Sets the default search space for HPARAM Value Function Coefficient Range: 0.5, 1 Value Function Coefficient also known as: VF coeff. (PPO Paper), vf_loss_coef (RLlib), vf_coef (ppo2 baselines), (ppo baselines: unclear), (Unity ML: unclear), (TensorForce: unclear) \"\"\" return ParametersPPO . VF_LOSS_COEFF_DIST","title":"VF_LOSS_COEFF_RANGE()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.pbt_ppo_explore","text":"The following function links to the companion function above. Sets the clipping needed by PBT https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: Type Description dict dict -- clipped config Source code in corl/libraries/hparam_search_util.py @staticmethod def pbt_ppo_explore ( config : dict ) -> dict : \"\"\"The following function links to the companion function above. Sets the clipping needed by PBT https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Arguments: config {dict} -- input config Returns: dict -- clipped config \"\"\" def clip_parameter ( config , parameter , parameter_max , parameter_min ): if config [ parameter ] > parameter_max : config [ parameter ] = parameter_max elif config [ parameter ] < parameter_min : config [ parameter ] = parameter_min clip_parameter ( config , \"lambda\" , ParametersPPO . LAMBDA_MAX , ParametersPPO . LAMBDA_MIN ) clip_parameter ( config , \"vf_loss_coeff\" , ParametersPPO . VF_LOSS_COEFF_MAX , ParametersPPO . VF_LOSS_COEFF_MIN ) clip_parameter ( config , \"entropy_coeff\" , ParametersPPO . ENTROPY_COEFF_MAX , ParametersPPO . ENTROPY_COEFF_MIN ) clip_parameter ( config , \"gamma\" , ParametersPPO . GAMMA_MAX , ParametersPPO . GAMMA_MIN ) clip_parameter ( config , \"clip_param\" , ParametersPPO . CLIP_PARAM_MAX , ParametersPPO . CLIP_PARAM_MIN ) clip_parameter ( config , \"lr\" , ParametersPPO . LR_MIN , ParametersPPO . LR_MAX ) clip_parameter ( config , \"kl_coeff\" , ParametersPPO . KL_COEFF_MIN , ParametersPPO . KL_COEFF_MAX ) clip_parameter ( config , \"kl_target\" , ParametersPPO . KL_TARGET_MIN , ParametersPPO . KL_TARGET_MAX ) sgd_minibatch_size_str = \"sgd_minibatch_size\" train_batch_size_str = \"train_batch_size\" num_sgd_iter_str = \"num_sgd_iter\" clip_parameter ( config , num_sgd_iter_str , ParametersPPO . NSGD_MAX , ParametersPPO . NSGD_MIN ) config [ num_sgd_iter_str ] = int ( config [ num_sgd_iter_str ]) clip_parameter ( config , sgd_minibatch_size_str , ParametersPPO . SGD_MINIBATCH_SIZE_MAX , ParametersPPO . SGD_MINIBATCH_SIZE_MIN ) config [ sgd_minibatch_size_str ] = int ( config [ sgd_minibatch_size_str ]) clip_parameter ( config , train_batch_size_str , ParametersPPO . TRAIN_BATCH_SIZE_MAX , ParametersPPO . TRAIN_BATCH_SIZE_MIN ) if config [ train_batch_size_str ] < config [ sgd_minibatch_size_str ] * 2 : config [ train_batch_size_str ] = config [ sgd_minibatch_size_str ] * 2 config [ train_batch_size_str ] = int ( config [ train_batch_size_str ]) return config","title":"pbt_ppo_explore()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.ppo_hyperparameters","text":"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: Type Description dict dict -- model configuration Source code in corl/libraries/hparam_search_util.py @staticmethod def ppo_hyperparameters () -> dict : \"\"\"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: dict -- model configuration \"\"\" ppo_hparams = {} ppo_hparams [ \"lambda\" ] = tune . sample_from ( ParametersPPO . LAMBDA_RANGE ) ppo_hparams [ \"vf_loss_coeff\" ] = tune . sample_from ( ParametersPPO . VF_LOSS_COEFF_RANGE ) ppo_hparams [ \"entropy_coeff\" ] = tune . sample_from ( ParametersPPO . ENTROPY_COEFF_RANGE ) ppo_hparams [ \"clip_param\" ] = tune . sample_from ( ParametersPPO . CLIP_PARAM_RANGE ) ppo_hparams [ \"gamma\" ] = tune . sample_from ( ParametersPPO . GAMMA_RANGE ) ppo_hparams [ \"lr\" ] = tune . sample_from ( ParametersPPO . LR_RANGE ) ppo_hparams [ \"num_sgd_iter\" ] = tune . sample_from ( ParametersPPO . NSGD_RANGE ) ppo_hparams [ \"sgd_minibatch_size\" ] = tune . sample_from ( ParametersPPO . SGD_MINIBATCH_SIZE_RANGE ) ppo_hparams [ \"train_batch_size\" ] = tune . sample_from ( ParametersPPO . TRAIN_BATCH_SIZE_RANGE ) ppo_hparams [ \"kl_coeff\" ] = tune . sample_from ( ParametersPPO . KL_COEFF_RANGE ) ppo_hparams [ \"kl_target\" ] = tune . sample_from ( ParametersPPO . KL_TARGET_RANGE ) return ppo_hparams","title":"ppo_hyperparameters()"},{"location":"reference/libraries/hparam_search_util/#corl.libraries.hparam_search_util.ParametersPPO.sample_ppo_hyperparameters","text":"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: Type Description dict dict -- model configuration Source code in corl/libraries/hparam_search_util.py @staticmethod def sample_ppo_hyperparameters () -> dict : \"\"\"PPO hyper parameters for hparam search https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe Returns: dict -- model configuration \"\"\" ppo_hparams = ParametersPPO . ppo_hyperparameters () for k , v in ppo_hparams . items (): ppo_hparams [ k ] = v . sample () return ppo_hparams","title":"sample_ppo_hyperparameters()"},{"location":"reference/libraries/nan_check/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. NaN check module nan_check_result ( data , skip_trace = False ) \u00a4 Checks for nan in np array Source code in corl/libraries/nan_check.py def nan_check_result ( data , skip_trace = False ): \"\"\" Checks for nan in np array \"\"\" if np . isscalar ( data ): if np . isnan ( data ): if not skip_trace : print_trace () raise ValueError ( \"Data contains nan\" ) # special case for repeated space elif isinstance ( data , dict ): list ( map ( nan_check_result , data . values ())) elif isinstance ( data , list ): list ( map ( nan_check_result , data )) elif data is None : if not skip_trace : print_trace () raise ValueError ( \"Data contains nan/None\" ) elif data . shape == (): if np . isnan ( data ): if not skip_trace : print_trace () raise ValueError ( \"Data contains nan\" ) elif len ( data . shape ) == 1 : if any ( np . isnan ( data )): if not skip_trace : print_trace () raise ValueError ( \"Data contains nan\" ) else : if np . isnan ( data ) . any (): if not skip_trace : print_trace () raise ValueError ( \"Data contains nan\" ) return data print_trace () \u00a4 The following function adds the trace back for the NaN check below. Ensures that the user knows which calling function caused the NaN - Typical that NaN is caused by damaged/broken platform Source code in corl/libraries/nan_check.py def print_trace (): \"\"\" The following function adds the trace back for the `NaN` check below. Ensures that the user knows which calling function caused the NaN - Typical that NaN is caused by damaged/broken platform \"\"\" for line in traceback . format_stack (): print ( line . strip ())","title":"Nan check"},{"location":"reference/libraries/nan_check/#corl.libraries.nan_check.nan_check_result","text":"Checks for nan in np array Source code in corl/libraries/nan_check.py def nan_check_result ( data , skip_trace = False ): \"\"\" Checks for nan in np array \"\"\" if np . isscalar ( data ): if np . isnan ( data ): if not skip_trace : print_trace () raise ValueError ( \"Data contains nan\" ) # special case for repeated space elif isinstance ( data , dict ): list ( map ( nan_check_result , data . values ())) elif isinstance ( data , list ): list ( map ( nan_check_result , data )) elif data is None : if not skip_trace : print_trace () raise ValueError ( \"Data contains nan/None\" ) elif data . shape == (): if np . isnan ( data ): if not skip_trace : print_trace () raise ValueError ( \"Data contains nan\" ) elif len ( data . shape ) == 1 : if any ( np . isnan ( data )): if not skip_trace : print_trace () raise ValueError ( \"Data contains nan\" ) else : if np . isnan ( data ) . any (): if not skip_trace : print_trace () raise ValueError ( \"Data contains nan\" ) return data","title":"nan_check_result()"},{"location":"reference/libraries/nan_check/#corl.libraries.nan_check.print_trace","text":"The following function adds the trace back for the NaN check below. Ensures that the user knows which calling function caused the NaN - Typical that NaN is caused by damaged/broken platform Source code in corl/libraries/nan_check.py def print_trace (): \"\"\" The following function adds the trace back for the `NaN` check below. Ensures that the user knows which calling function caused the NaN - Typical that NaN is caused by damaged/broken platform \"\"\" for line in traceback . format_stack (): print ( line . strip ())","title":"print_trace()"},{"location":"reference/libraries/observation_extractor/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Observation Extractor ExtractorSet ( tuple ) \u00a4 Class defining the set of extractors to pull information about a specific observation Source code in corl/libraries/observation_extractor.py class ExtractorSet ( typing . NamedTuple ): \"\"\"Class defining the set of extractors to pull information about a specific observation \"\"\" value : typing . Callable space : typing . Callable unit : typing . Callable __getnewargs__ ( self ) special \u00a4 Return self as a plain tuple. Used by copy and pickle. Source code in corl/libraries/observation_extractor.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , value , space , unit ) special staticmethod \u00a4 Create new instance of ExtractorSet(value, space, unit) __repr__ ( self ) special \u00a4 Return a nicely formatted representation string Source code in corl/libraries/observation_extractor.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self ObservationExtractor ( observation , fields , indices = None ) \u00a4 This function extracts a metric produced by a specific glue in observation space of an agent ---- Arguments ---- observation - A dict of an agent's observations platforms - The platforms the glue is observing, needed to compute the glue's prefix fields - Fields the extractor will attempt to walk through indicies - These will be accessed after the fields have been accessed, allowing users to reduce arrays to single values ---- Raises ---- RuntimeError - Thrown when the fields do not exists in the glue's measurments Source code in corl/libraries/observation_extractor.py def ObservationExtractor ( observation , fields : typing . List [ str ], indices : typing . Union [ int , typing . List [ int ]] = None , ): \"\"\" This function extracts a metric produced by a specific glue in observation space of an agent ---- Arguments ---- observation - A dict of an agent's observations platforms - The platforms the glue is observing, needed to compute the glue's prefix fields - Fields the extractor will attempt to walk through indicies - These will be accessed after the fields have been accessed, allowing users to reduce arrays to single values ---- Raises ---- RuntimeError - Thrown when the fields do not exists in the glue's measurments \"\"\" if indices is None : indices = [] if not isinstance ( indices , typing . List ): indices = [ indices ] value = observation for field in fields : if field not in value : raise RuntimeError ( f \"The field { field } is not present in the observation, the requested fields were { fields } , \" ) value = value [ field ] for index in indices : value = value [ index ] return value ObservationSpaceExtractor ( observation_space , fields ) \u00a4 Extract the observation space from a glue ---- Arguments ---- observation - A dict of an agent's observations fields - Fields the extractor will attempt to walk through ---- Raises ---- RuntimeError - Thrown when the field does not exists in the glue's obs space Source code in corl/libraries/observation_extractor.py def ObservationSpaceExtractor ( observation_space , fields : typing . List [ str ], ): \"\"\"Extract the observation space from a glue ---- Arguments ---- observation - A dict of an agent's observations fields - Fields the extractor will attempt to walk through ---- Raises ---- RuntimeError - Thrown when the field does not exists in the glue's obs space \"\"\" space = observation_space for field in fields : if field not in space . spaces : raise RuntimeError ( f \"The field { field } is not present in the observation space,\" f \"the requested fields were { fields } , space is { observation_space } \" ) space = space [ field ] return space","title":"Observation extractor"},{"location":"reference/libraries/observation_extractor/#corl.libraries.observation_extractor.ExtractorSet","text":"Class defining the set of extractors to pull information about a specific observation Source code in corl/libraries/observation_extractor.py class ExtractorSet ( typing . NamedTuple ): \"\"\"Class defining the set of extractors to pull information about a specific observation \"\"\" value : typing . Callable space : typing . Callable unit : typing . Callable","title":"ExtractorSet"},{"location":"reference/libraries/observation_extractor/#corl.libraries.observation_extractor.ExtractorSet.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in corl/libraries/observation_extractor.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"reference/libraries/observation_extractor/#corl.libraries.observation_extractor.ExtractorSet.__new__","text":"Create new instance of ExtractorSet(value, space, unit)","title":"__new__()"},{"location":"reference/libraries/observation_extractor/#corl.libraries.observation_extractor.ExtractorSet.__repr__","text":"Return a nicely formatted representation string Source code in corl/libraries/observation_extractor.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"reference/libraries/observation_extractor/#corl.libraries.observation_extractor.ObservationExtractor","text":"This function extracts a metric produced by a specific glue in observation space of an agent ---- Arguments ---- observation - A dict of an agent's observations platforms - The platforms the glue is observing, needed to compute the glue's prefix fields - Fields the extractor will attempt to walk through indicies - These will be accessed after the fields have been accessed, allowing users to reduce arrays to single values ---- Raises ---- RuntimeError - Thrown when the fields do not exists in the glue's measurments Source code in corl/libraries/observation_extractor.py def ObservationExtractor ( observation , fields : typing . List [ str ], indices : typing . Union [ int , typing . List [ int ]] = None , ): \"\"\" This function extracts a metric produced by a specific glue in observation space of an agent ---- Arguments ---- observation - A dict of an agent's observations platforms - The platforms the glue is observing, needed to compute the glue's prefix fields - Fields the extractor will attempt to walk through indicies - These will be accessed after the fields have been accessed, allowing users to reduce arrays to single values ---- Raises ---- RuntimeError - Thrown when the fields do not exists in the glue's measurments \"\"\" if indices is None : indices = [] if not isinstance ( indices , typing . List ): indices = [ indices ] value = observation for field in fields : if field not in value : raise RuntimeError ( f \"The field { field } is not present in the observation, the requested fields were { fields } , \" ) value = value [ field ] for index in indices : value = value [ index ] return value","title":"ObservationExtractor()"},{"location":"reference/libraries/observation_extractor/#corl.libraries.observation_extractor.ObservationSpaceExtractor","text":"Extract the observation space from a glue ---- Arguments ---- observation - A dict of an agent's observations fields - Fields the extractor will attempt to walk through ---- Raises ---- RuntimeError - Thrown when the field does not exists in the glue's obs space Source code in corl/libraries/observation_extractor.py def ObservationSpaceExtractor ( observation_space , fields : typing . List [ str ], ): \"\"\"Extract the observation space from a glue ---- Arguments ---- observation - A dict of an agent's observations fields - Fields the extractor will attempt to walk through ---- Raises ---- RuntimeError - Thrown when the field does not exists in the glue's obs space \"\"\" space = observation_space for field in fields : if field not in space . spaces : raise RuntimeError ( f \"The field { field } is not present in the observation space,\" f \"the requested fields were { fields } , space is { observation_space } \" ) space = space [ field ] return space","title":"ObservationSpaceExtractor()"},{"location":"reference/libraries/observation_util/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. filter_observations ( observations , filter_fn ) \u00a4 Filters observations that don't match filter_fn Parameters \u00a4 Observations An nested dictionary: observations AGENT_ID -> OBSERVATION filter_fn(AGENT_ID: str, OBSERVATION_NAME: str, OBSERVATION: ObsType): bool Returns ------- true: the observation will be included in the output false: the observation will not be included in the output Returns \u00a4 Ordereddict the filtered observation samples Source code in corl/libraries/observation_util.py def filter_observations ( observations : OrderedDict , filter_fn : Callable [[ str , str , ObsType ], bool ]) -> OrderedDict : \"\"\" Filters observations that don't match filter_fn Parameters ---------- observations: An nested dictionary: observations[AGENT_ID][OBSERVATION_NAME] -> OBSERVATION filter_fn(AGENT_ID: str, OBSERVATION_NAME: str, OBSERVATION: ObsType): bool Returns ------- true: the observation will be included in the output false: the observation will not be included in the output Returns ------- OrderedDict: the filtered observation samples \"\"\" return mutate_observations ( observations , lambda agent_id , obs_name , obs : obs if filter_fn ( agent_id , obs_name , obs ) else None # type: ignore ) # type: ignore mutate_observations ( observations , mutate_fn ) \u00a4 Mutates observations according to mutate_fn if the MUTATED_OBSERVATION is not None, it will be included in the output Parameters \u00a4 Observations An nested dictionary: observations AGENT_ID -> OBSERVATION mutate_fn(AGENT_ID: str, OBSERVATION_NAME: str, OBSERVATION: ObsType): MUTATED_OBSERVATION: ObsType Returns ------- The mutated observation Returns \u00a4 Ordereddict the mutated observation samples Source code in corl/libraries/observation_util.py def mutate_observations ( observations : OrderedDict , mutate_fn : Callable [[ str , str , ObsType ], ObsType ]) -> OrderedDict : \"\"\" Mutates observations according to mutate_fn if the MUTATED_OBSERVATION is not None, it will be included in the output Parameters ---------- observations: An nested dictionary: observations[AGENT_ID][OBSERVATION_NAME] -> OBSERVATION mutate_fn(AGENT_ID: str, OBSERVATION_NAME: str, OBSERVATION: ObsType): MUTATED_OBSERVATION: ObsType Returns ------- The mutated observation Returns ------- OrderedDict: the mutated observation samples \"\"\" mutated_observation_dict : OrderedDict = OrderedDict () for agent_id , obs_dict in observations . items (): for obs_name , obs in obs_dict . items (): mutated_obs = mutate_fn ( agent_id , obs_name , obs ) if mutated_obs is not None : if agent_id not in mutated_observation_dict : mutated_observation_dict [ agent_id ] = OrderedDict () mutated_observation_dict [ agent_id ][ obs_name ] = mutated_obs return mutated_observation_dict","title":"Observation util"},{"location":"reference/libraries/observation_util/#corl.libraries.observation_util.filter_observations","text":"Filters observations that don't match filter_fn","title":"filter_observations()"},{"location":"reference/libraries/observation_util/#corl.libraries.observation_util.filter_observations--parameters","text":"Observations An nested dictionary: observations AGENT_ID -> OBSERVATION filter_fn(AGENT_ID: str, OBSERVATION_NAME: str, OBSERVATION: ObsType): bool Returns ------- true: the observation will be included in the output false: the observation will not be included in the output","title":"Parameters"},{"location":"reference/libraries/observation_util/#corl.libraries.observation_util.filter_observations--returns","text":"Ordereddict the filtered observation samples Source code in corl/libraries/observation_util.py def filter_observations ( observations : OrderedDict , filter_fn : Callable [[ str , str , ObsType ], bool ]) -> OrderedDict : \"\"\" Filters observations that don't match filter_fn Parameters ---------- observations: An nested dictionary: observations[AGENT_ID][OBSERVATION_NAME] -> OBSERVATION filter_fn(AGENT_ID: str, OBSERVATION_NAME: str, OBSERVATION: ObsType): bool Returns ------- true: the observation will be included in the output false: the observation will not be included in the output Returns ------- OrderedDict: the filtered observation samples \"\"\" return mutate_observations ( observations , lambda agent_id , obs_name , obs : obs if filter_fn ( agent_id , obs_name , obs ) else None # type: ignore ) # type: ignore","title":"Returns"},{"location":"reference/libraries/observation_util/#corl.libraries.observation_util.mutate_observations","text":"Mutates observations according to mutate_fn if the MUTATED_OBSERVATION is not None, it will be included in the output","title":"mutate_observations()"},{"location":"reference/libraries/observation_util/#corl.libraries.observation_util.mutate_observations--parameters","text":"Observations An nested dictionary: observations AGENT_ID -> OBSERVATION mutate_fn(AGENT_ID: str, OBSERVATION_NAME: str, OBSERVATION: ObsType): MUTATED_OBSERVATION: ObsType Returns ------- The mutated observation","title":"Parameters"},{"location":"reference/libraries/observation_util/#corl.libraries.observation_util.mutate_observations--returns","text":"Ordereddict the mutated observation samples Source code in corl/libraries/observation_util.py def mutate_observations ( observations : OrderedDict , mutate_fn : Callable [[ str , str , ObsType ], ObsType ]) -> OrderedDict : \"\"\" Mutates observations according to mutate_fn if the MUTATED_OBSERVATION is not None, it will be included in the output Parameters ---------- observations: An nested dictionary: observations[AGENT_ID][OBSERVATION_NAME] -> OBSERVATION mutate_fn(AGENT_ID: str, OBSERVATION_NAME: str, OBSERVATION: ObsType): MUTATED_OBSERVATION: ObsType Returns ------- The mutated observation Returns ------- OrderedDict: the mutated observation samples \"\"\" mutated_observation_dict : OrderedDict = OrderedDict () for agent_id , obs_dict in observations . items (): for obs_name , obs in obs_dict . items (): mutated_obs = mutate_fn ( agent_id , obs_name , obs ) if mutated_obs is not None : if agent_id not in mutated_observation_dict : mutated_observation_dict [ agent_id ] = OrderedDict () mutated_observation_dict [ agent_id ][ obs_name ] = mutated_obs return mutated_observation_dict","title":"Returns"},{"location":"reference/libraries/parameters/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Structures that hold parameters and the ability to update them. BoundStepUpdater ( Updater ) \u00a4 An Updater that advances by a constant step, limited by a bound. On each update, the provided value is incremented by a step size. If that increment causes it to violate the provided bound, the value is given the value of that bound. Reverse updates are supported. They are bounded at the initial value of the provided parameter. Source code in corl/libraries/parameters.py class BoundStepUpdater ( Updater ): \"\"\"An `Updater` that advances by a constant step, limited by a bound. On each update, the provided value is incremented by a step size. If that increment causes it to violate the provided bound, the value is given the value of that bound. Reverse updates are supported. They are bounded at the initial value of the provided parameter. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BoundStepUpdaterValidator super () . __init__ ( ** kwargs ) self . _reverse_bound = self . get_current_extent () self . _bound_func : typing . Callable self . _reverse_bound_func : typing . Callable if self . config . bound_type == 'min' : self . _bound_func = max self . _reverse_bound_func = min elif self . config . bound_type == 'max' : self . _bound_func = min self . _reverse_bound_func = max else : raise ValueError ( f 'Unknown bound type { self . config [ \"bound_type\" ] } ' ) self . _at_bound : bool = False @property def get_validator ( self ) -> typing . Type [ BoundStepUpdaterValidator ]: \"\"\"Get validator for BoundStepUpdater\"\"\" return BoundStepUpdaterValidator def do_call ( self , arg : Number , * , reverse : bool = False ) -> Number : if reverse : arg -= self . config . step output = self . _reverse_bound_func ( arg , self . _reverse_bound ) else : arg += self . config . step output = self . _bound_func ( arg , self . config . bound ) self . _at_bound = np . isclose ( output , self . config . bound ) return output def supports_reverse_update ( self ) -> bool : return True def update_to_bound ( self ) -> None : super () . update_to_bound () self . _at_bound = True def at_bound ( self ) -> bool : return self . _at_bound def get_bound ( self ) -> Number : return self . config . bound def create_config ( self ) -> dict : return { 'bound' : self . config . bound , 'step' : self . config . step , 'bound_type' : self . config . bound_type } get_validator : Type [ corl . libraries . parameters . BoundStepUpdaterValidator ] property readonly \u00a4 Get validator for BoundStepUpdater at_bound ( self ) \u00a4 Indicator whether this updater is at its bound. Source code in corl/libraries/parameters.py def at_bound ( self ) -> bool : return self . _at_bound create_config ( self ) \u00a4 Create the configuration file that would generate this object in its current state. Returns \u00a4 dict Configuration file of the current state of the object. This object can be passed to the constructor of the Parameter to regenerate it. Source code in corl/libraries/parameters.py def create_config ( self ) -> dict : return { 'bound' : self . config . bound , 'step' : self . config . step , 'bound_type' : self . config . bound_type } do_call ( self , arg , * , reverse = False ) \u00a4 Perform the update of the provided hyperparameter. Parameters \u00a4 arg : Union[int, float] The current value of the hyperparameter. bool, optional Perform a reverse update. If allowed, performing a sequence with a normal update followed by a reverse update should return the original value. Not all Updater subclasses allow reverse updates. The default is False. Source code in corl/libraries/parameters.py def do_call ( self , arg : Number , * , reverse : bool = False ) -> Number : if reverse : arg -= self . config . step output = self . _reverse_bound_func ( arg , self . _reverse_bound ) else : arg += self . config . step output = self . _bound_func ( arg , self . config . bound ) self . _at_bound = np . isclose ( output , self . config . bound ) return output get_bound ( self ) \u00a4 Get the bound of this updater. Source code in corl/libraries/parameters.py def get_bound ( self ) -> Number : return self . config . bound supports_reverse_update ( self ) \u00a4 Indicator whether this updater supports reverse updates. Source code in corl/libraries/parameters.py def supports_reverse_update ( self ) -> bool : return True update_to_bound ( self ) \u00a4 Update all the way to the bound. Source code in corl/libraries/parameters.py def update_to_bound ( self ) -> None : super () . update_to_bound () self . _at_bound = True BoundStepUpdaterValidator ( UpdaterValidator ) pydantic-model \u00a4 Validator class for BoundStepUpdater Source code in corl/libraries/parameters.py class BoundStepUpdaterValidator ( UpdaterValidator ): \"\"\"Validator class for BoundStepUpdater\"\"\" bound_type : typing . Literal [ \"min\" , \"max\" ] bound : Number step : Number @validator ( 'step' ) def step_validator ( cls , v , values ): \"\"\"Validator for step field\"\"\" if v >= 0 and values [ 'bound_type' ] == 'min' : raise ValueError ( 'Step must be negative for minimum bound' ) if v <= 0 and values [ 'bound_type' ] == 'max' : raise ValueError ( 'Step must be positive for maximum bound' ) return v step_validator ( v , values ) classmethod \u00a4 Validator for step field Source code in corl/libraries/parameters.py @validator ( 'step' ) def step_validator ( cls , v , values ): \"\"\"Validator for step field\"\"\" if v >= 0 and values [ 'bound_type' ] == 'min' : raise ValueError ( 'Step must be negative for minimum bound' ) if v <= 0 and values [ 'bound_type' ] == 'max' : raise ValueError ( 'Step must be positive for maximum bound' ) return v ChoiceParameter ( Parameter ) \u00a4 A parameter drawn uniformly from a collection of discrete values. This parameter does not support updaters. Parameters \u00a4 hparams : dict The hyperparameters that define this parameter. In addition to the structure specified by the base class Parameter, there needs to be the following fields, expressed as YAML: ``` yaml choices : Sequence [ Any ] ``` Source code in corl/libraries/parameters.py class ChoiceParameter ( Parameter ): \"\"\"A parameter drawn uniformly from a collection of discrete values. This parameter does not support updaters. Parameters ---------- hparams : dict The hyperparameters that define this parameter. In addition to the structure specified by the base class Parameter, there needs to be the following fields, expressed as YAML: ```yaml choices: Sequence[Any] ``` \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ChoiceParameterValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ ChoiceParameterValidator ]: return ChoiceParameterValidator def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = rng . choice ( self . config . choices ), units = self . config . units ) get_validator : Type [ corl . libraries . parameters . ChoiceParameterValidator ] property readonly \u00a4 Get the validator class for this Parameter get_value ( self , rng ) \u00a4 Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept. Parameters \u00a4 rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = rng . choice ( self . config . choices ), units = self . config . units ) ChoiceParameterValidator ( ParameterValidator ) pydantic-model \u00a4 Validator for ChoiceParameter Source code in corl/libraries/parameters.py class ChoiceParameterValidator ( ParameterValidator ): \"\"\"Validator for ChoiceParameter\"\"\" choices : typing . Sequence [ typing . Any ] ConstantParameter ( Parameter ) \u00a4 A parameter that always has a constant value. Source code in corl/libraries/parameters.py class ConstantParameter ( Parameter ): \"\"\"A parameter that always has a constant value.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ConstantParameterValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ ConstantParameterValidator ]: return ConstantParameterValidator def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = self . config . value , units = self . config . units ) get_validator : Type [ corl . libraries . parameters . ConstantParameterValidator ] property readonly \u00a4 Get the validator class for this Parameter get_value ( self , rng ) \u00a4 Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept. Parameters \u00a4 rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = self . config . value , units = self . config . units ) ConstantParameterValidator ( ParameterValidator ) pydantic-model \u00a4 Validator class for ConstantParameter Source code in corl/libraries/parameters.py class ConstantParameterValidator ( ParameterValidator ): \"\"\"Validator class for ConstantParameter\"\"\" value : typing . Union [ Number , str ] OverridableParameterWrapper ( Parameter ) \u00a4 A Parameter that wraps another parameter and can override its output. Source code in corl/libraries/parameters.py class OverridableParameterWrapper ( Parameter ): \"\"\"A Parameter that wraps another parameter and can override its output.\"\"\" def __init__ ( self , base : Parameter ) -> None : # pylint: disable=super-init-not-called # Base class API self . config = base . config self . updaters = base . updaters # Other attributes self . base = base self . override_value : typing . Any = None @property def get_validator ( self ) -> typing . NoReturn : \"\"\"OverridableParameterWrapper is not parsed using validators.\"\"\" raise NotImplementedError () def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: return self . base . get_constraint ( name = name ) def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : if self . override_value is not None : return ValueWithUnits ( value = self . override_value , units = self . config . units ) return self . base . get_value ( rng ) get_validator : NoReturn property readonly \u00a4 OverridableParameterWrapper is not parsed using validators. get_constraint ( self , name ) \u00a4 Get the constraint function for this Parameter's updater config Source code in corl/libraries/parameters.py def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: return self . base . get_constraint ( name = name ) get_value ( self , rng ) \u00a4 Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept. Parameters \u00a4 rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : if self . override_value is not None : return ValueWithUnits ( value = self . override_value , units = self . config . units ) return self . base . get_value ( rng ) Parameter ( ABC ) \u00a4 Parameter class Source code in corl/libraries/parameters.py class Parameter ( abc . ABC ): \"\"\"Parameter class\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ParameterValidator = self . get_validator ( ** kwargs ) # Create and save updaters self . updaters : typing . Dict [ str , typing . Any ] = {} for name , val in self . config . update . items (): factory = Factory ( ** val ) self . updaters [ name ] = factory . build ( param = self , name = name , constraint = self . get_constraint ( name = name )) @property def get_validator ( self ) -> typing . Type [ ParameterValidator ]: \"\"\"Get the validator class for this Parameter\"\"\" return ParameterValidator def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: # pylint: disable=unused-argument \"\"\"Get the constraint function for this Parameter's updater config\"\"\" return None @abc . abstractmethod def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : \"\"\"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of `self`. This is the equivalent of a C++ `const` method for developers familiar with that concept. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. \"\"\" ... get_validator : Type [ corl . libraries . parameters . ParameterValidator ] property readonly \u00a4 Get the validator class for this Parameter get_constraint ( self , name ) \u00a4 Get the constraint function for this Parameter's updater config Source code in corl/libraries/parameters.py def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: # pylint: disable=unused-argument \"\"\"Get the constraint function for this Parameter's updater config\"\"\" return None get_value ( self , rng ) \u00a4 Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept. Parameters \u00a4 rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py @abc . abstractmethod def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : \"\"\"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of `self`. This is the equivalent of a C++ `const` method for developers familiar with that concept. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. \"\"\" ... ParameterValidator ( BaseModel ) pydantic-model \u00a4 Validator class for Parameter Source code in corl/libraries/parameters.py class ParameterValidator ( BaseModel ): \"\"\"Validator class for Parameter\"\"\" units : typing . Optional [ enum . Enum ] update : typing . Dict [ str , typing . Any ] = {} simulator : typing . Dict [ str , typing . Any ] = {} episode_parameter_provider : typing . Dict [ str , typing . Any ] = {} @validator ( 'units' , pre = True ) def units_validator ( cls , v ): \"\"\"Validate the units field\"\"\" return units . GetUnitFromStr ( v ) if v is not None else None units_validator ( v ) classmethod \u00a4 Validate the units field Source code in corl/libraries/parameters.py @validator ( 'units' , pre = True ) def units_validator ( cls , v ): \"\"\"Validate the units field\"\"\" return units . GetUnitFromStr ( v ) if v is not None else None TruncatedNormalParameter ( Parameter ) \u00a4 A parameter that draws from a truncated normal distribution. Source code in corl/libraries/parameters.py class TruncatedNormalParameter ( Parameter ): \"\"\"A parameter that draws from a truncated normal distribution. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : TruncatedNormalParameterValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ TruncatedNormalParameterValidator ]: return TruncatedNormalParameterValidator def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : low = self . config . mu - self . config . half_width_factor * self . config . std high = self . config . mu + self . config . half_width_factor * self . config . std value = stats . truncnorm . rvs ( ( low - self . config . mu ) / self . config . std , ( high - self . config . mu ) / self . config . std , loc = self . config . mu , scale = self . config . std , size = 1 , random_state = rng )[ 0 ] return units . ValueWithUnits ( value = value , units = self . config . units ) def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: if name == 'std' : return self . _std_positive if name == 'half_width_factor' : return self . _half_width_factor_positive raise ValueError ( \"Unknown contraint name\" ) @staticmethod def _generic_positive ( variable : str , old_arg : Number , new_arg : Number ) -> Number : if new_arg < 0 : warnings . warn ( f 'Could not update TruncatedNormalParameter { variable } because it is not strictly positive' ) return old_arg return new_arg def _std_positive ( self , old_arg : Number , new_arg : Number ) -> Number : return self . _generic_positive ( variable = 'standard deviation' , old_arg = old_arg , new_arg = new_arg ) def _half_width_factor_positive ( self , old_arg : Number , new_arg : Number ) -> Number : return self . _generic_positive ( variable = 'half width factor' , old_arg = old_arg , new_arg = new_arg ) get_validator : Type [ corl . libraries . parameters . TruncatedNormalParameterValidator ] property readonly \u00a4 Get the validator class for this Parameter get_constraint ( self , name ) \u00a4 Get the constraint function for this Parameter's updater config Source code in corl/libraries/parameters.py def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: if name == 'std' : return self . _std_positive if name == 'half_width_factor' : return self . _half_width_factor_positive raise ValueError ( \"Unknown contraint name\" ) get_value ( self , rng ) \u00a4 Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept. Parameters \u00a4 rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : low = self . config . mu - self . config . half_width_factor * self . config . std high = self . config . mu + self . config . half_width_factor * self . config . std value = stats . truncnorm . rvs ( ( low - self . config . mu ) / self . config . std , ( high - self . config . mu ) / self . config . std , loc = self . config . mu , scale = self . config . std , size = 1 , random_state = rng )[ 0 ] return units . ValueWithUnits ( value = value , units = self . config . units ) TruncatedNormalParameterValidator ( ParameterValidator ) pydantic-model \u00a4 Validator class for TruncatedNormalParameter Source code in corl/libraries/parameters.py class TruncatedNormalParameterValidator ( ParameterValidator ): \"\"\"Validator class for TruncatedNormalParameter\"\"\" mu : Number std : PositiveFloat half_width_factor : PositiveFloat UniformParameter ( Parameter ) \u00a4 A parameter that draws from a uniform distribution. Source code in corl/libraries/parameters.py class UniformParameter ( Parameter ): \"\"\"A parameter that draws from a uniform distribution.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : UniformParameterValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ UniformParameterValidator ]: return UniformParameterValidator def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = rng . uniform ( self . config . low , self . config . high ), units = self . config . units ) def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: if name == 'low' : return self . _min_with_high if name == 'high' : return self . _max_with_low raise ValueError ( \"Unknown contraint name\" ) def _min_with_high ( self , old_arg : Number , new_arg : Number ) -> Number : # pylint: disable=unused-argument if new_arg > self . config . high : warnings . warn ( 'Could not fully update UniformParameter lower bound as it exceeds higher bound' ) return self . config . high return new_arg def _max_with_low ( self , old_arg : Number , new_arg : Number ) -> Number : # pylint: disable=unused-argument if new_arg < self . config . low : warnings . warn ( 'Could not fully update UniformParameter lower bound as it goes below the lower bound' ) return self . config . low return new_arg get_validator : Type [ corl . libraries . parameters . UniformParameterValidator ] property readonly \u00a4 Get the validator class for this Parameter get_constraint ( self , name ) \u00a4 Get the constraint function for this Parameter's updater config Source code in corl/libraries/parameters.py def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: if name == 'low' : return self . _min_with_high if name == 'high' : return self . _max_with_low raise ValueError ( \"Unknown contraint name\" ) get_value ( self , rng ) \u00a4 Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept. Parameters \u00a4 rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = rng . uniform ( self . config . low , self . config . high ), units = self . config . units ) UniformParameterValidator ( ParameterValidator ) pydantic-model \u00a4 Validator class for UniformParameter Source code in corl/libraries/parameters.py class UniformParameterValidator ( ParameterValidator ): \"\"\"Validator class for UniformParameter\"\"\" low : Number high : Number @validator ( \"high\" ) def high_validator ( cls , v , values ): \"\"\"Validate the high field\"\"\" if v < values [ 'low' ]: raise ValueError ( 'Upper bound must not be smaller than lower bound' ) return v high_validator ( v , values ) classmethod \u00a4 Validate the high field Source code in corl/libraries/parameters.py @validator ( \"high\" ) def high_validator ( cls , v , values ): \"\"\"Validate the high field\"\"\" if v < values [ 'low' ]: raise ValueError ( 'Upper bound must not be smaller than lower bound' ) return v Updater ( ABC ) \u00a4 Generic structure to define the method of updating a hyperparameter of a Parameter . Source code in corl/libraries/parameters.py class Updater ( abc . ABC ): \"\"\"Generic structure to define the method of updating a hyperparameter of a `Parameter`.\"\"\" def __init__ ( self , ** kwargs ) -> None : # Validate and save updater configuration data self . config : UpdaterValidator = self . get_validator ( ** kwargs ) def __call__ ( self , * , reverse : bool = False ) -> None : \"\"\"Perform an update of the hyperparameter according to the functionality of the updater. The generic functionality of an update has four steps: 1. Get the current value of the hyperparameter by extracting the named attribute from the connected parameter. 2. Perform the update according to the `do_call` method within the updater. The current value of the hyperparameter is provided, along with the flag to specify reverse updates. 3. Apply the constraint function. The primary purpose of this function is to allow the `Parameter` to provide a function to ensure that the updated hyperparameter value leaves the `Parameter` in a consistent state. It receives the old argument and the new argument as keyword arguments (with names `old_arg` and `new_arg` respectively). Common uses are to ensure that multiple hyperparameters within an `Parameter` maintain a consistent state (i.e., lower bound is less than upper bound) or that some invariant property is maintained (i.e., standard deviation is positive). The constraint function must return the value that should be set into the hyperparameter. 4. The output of the constraint function is set back to the named attribute in the connected parameter. Subclasses should not override `__call__`, but should implement `do_call`. Parameters ---------- reverse : bool, optional Perform a reverse update, by default False \"\"\" old_arg = self . get_current_extent () new_arg = self . do_call ( old_arg , reverse = reverse ) constrained_arg = self . config . constraint ( old_arg = old_arg , new_arg = new_arg ) setattr ( self . config . param . config , self . config . name , constrained_arg ) def update_to_bound ( self ) -> None : \"\"\"Update all the way to the bound.\"\"\" old_arg = self . get_current_extent () new_arg = self . get_bound () constrained_arg = self . config . constraint ( old_arg = old_arg , new_arg = new_arg ) setattr ( self . config . param . config , self . config . name , constrained_arg ) @abc . abstractmethod def do_call ( self , arg : Number , * , reverse : bool = False ) -> Number : \"\"\"Perform the update of the provided hyperparameter. Parameters ---------- arg : Union[int, float] The current value of the hyperparameter. reverse: bool, optional Perform a reverse update. If allowed, performing a sequence with a normal update followed by a reverse update should return the original value. Not all `Updater` subclasses allow reverse updates. The default is False. \"\"\" ... @property def get_validator ( self ) -> typing . Type [ UpdaterValidator ]: \"\"\"Get the validator for this class\"\"\" return UpdaterValidator @abc . abstractmethod def supports_reverse_update ( self ) -> bool : \"\"\"Indicator whether this updater supports reverse updates.\"\"\" ... @abc . abstractmethod def at_bound ( self ) -> bool : \"\"\"Indicator whether this updater is at its bound.\"\"\" ... @abc . abstractmethod def get_bound ( self ) -> Number : \"\"\"Get the bound of this updater.\"\"\" ... def get_current_extent ( self ) -> Number : \"\"\"returns the current extent of the parameter controlled by this updater Returns: Number -- The current extent of the parameter controlled by this updater \"\"\" return getattr ( self . config . param . config , self . config . name ) @abc . abstractmethod def create_config ( self ) -> dict : \"\"\"Create the configuration file that would generate this object in its current state. Returns ------- dict Configuration file of the current state of the object. This object can be passed to the constructor of the Parameter to regenerate it. \"\"\" raise NotImplementedError () get_validator : Type [ corl . libraries . parameters . UpdaterValidator ] property readonly \u00a4 Get the validator for this class __call__ ( self , * , reverse = False ) special \u00a4 Perform an update of the hyperparameter according to the functionality of the updater. The generic functionality of an update has four steps: 1. Get the current value of the hyperparameter by extracting the named attribute from the connected parameter. 2. Perform the update according to the do_call method within the updater. The current value of the hyperparameter is provided, along with the flag to specify reverse updates. 3. Apply the constraint function. The primary purpose of this function is to allow the Parameter to provide a function to ensure that the updated hyperparameter value leaves the Parameter in a consistent state. It receives the old argument and the new argument as keyword arguments (with names old_arg and new_arg respectively). Common uses are to ensure that multiple hyperparameters within an Parameter maintain a consistent state (i.e., lower bound is less than upper bound) or that some invariant property is maintained (i.e., standard deviation is positive). The constraint function must return the value that should be set into the hyperparameter. 4. The output of the constraint function is set back to the named attribute in the connected parameter. Subclasses should not override __call__ , but should implement do_call . Parameters \u00a4 reverse : bool, optional Perform a reverse update, by default False Source code in corl/libraries/parameters.py def __call__ ( self , * , reverse : bool = False ) -> None : \"\"\"Perform an update of the hyperparameter according to the functionality of the updater. The generic functionality of an update has four steps: 1. Get the current value of the hyperparameter by extracting the named attribute from the connected parameter. 2. Perform the update according to the `do_call` method within the updater. The current value of the hyperparameter is provided, along with the flag to specify reverse updates. 3. Apply the constraint function. The primary purpose of this function is to allow the `Parameter` to provide a function to ensure that the updated hyperparameter value leaves the `Parameter` in a consistent state. It receives the old argument and the new argument as keyword arguments (with names `old_arg` and `new_arg` respectively). Common uses are to ensure that multiple hyperparameters within an `Parameter` maintain a consistent state (i.e., lower bound is less than upper bound) or that some invariant property is maintained (i.e., standard deviation is positive). The constraint function must return the value that should be set into the hyperparameter. 4. The output of the constraint function is set back to the named attribute in the connected parameter. Subclasses should not override `__call__`, but should implement `do_call`. Parameters ---------- reverse : bool, optional Perform a reverse update, by default False \"\"\" old_arg = self . get_current_extent () new_arg = self . do_call ( old_arg , reverse = reverse ) constrained_arg = self . config . constraint ( old_arg = old_arg , new_arg = new_arg ) setattr ( self . config . param . config , self . config . name , constrained_arg ) at_bound ( self ) \u00a4 Indicator whether this updater is at its bound. Source code in corl/libraries/parameters.py @abc . abstractmethod def at_bound ( self ) -> bool : \"\"\"Indicator whether this updater is at its bound.\"\"\" ... create_config ( self ) \u00a4 Create the configuration file that would generate this object in its current state. Returns \u00a4 dict Configuration file of the current state of the object. This object can be passed to the constructor of the Parameter to regenerate it. Source code in corl/libraries/parameters.py @abc . abstractmethod def create_config ( self ) -> dict : \"\"\"Create the configuration file that would generate this object in its current state. Returns ------- dict Configuration file of the current state of the object. This object can be passed to the constructor of the Parameter to regenerate it. \"\"\" raise NotImplementedError () do_call ( self , arg , * , reverse = False ) \u00a4 Perform the update of the provided hyperparameter. Parameters \u00a4 arg : Union[int, float] The current value of the hyperparameter. bool, optional Perform a reverse update. If allowed, performing a sequence with a normal update followed by a reverse update should return the original value. Not all Updater subclasses allow reverse updates. The default is False. Source code in corl/libraries/parameters.py @abc . abstractmethod def do_call ( self , arg : Number , * , reverse : bool = False ) -> Number : \"\"\"Perform the update of the provided hyperparameter. Parameters ---------- arg : Union[int, float] The current value of the hyperparameter. reverse: bool, optional Perform a reverse update. If allowed, performing a sequence with a normal update followed by a reverse update should return the original value. Not all `Updater` subclasses allow reverse updates. The default is False. \"\"\" ... get_bound ( self ) \u00a4 Get the bound of this updater. Source code in corl/libraries/parameters.py @abc . abstractmethod def get_bound ( self ) -> Number : \"\"\"Get the bound of this updater.\"\"\" ... get_current_extent ( self ) \u00a4 returns the current extent of the parameter controlled by this updater Returns: Type Description Union[pydantic.types.StrictInt, float] Number -- The current extent of the parameter controlled by this updater Source code in corl/libraries/parameters.py def get_current_extent ( self ) -> Number : \"\"\"returns the current extent of the parameter controlled by this updater Returns: Number -- The current extent of the parameter controlled by this updater \"\"\" return getattr ( self . config . param . config , self . config . name ) supports_reverse_update ( self ) \u00a4 Indicator whether this updater supports reverse updates. Source code in corl/libraries/parameters.py @abc . abstractmethod def supports_reverse_update ( self ) -> bool : \"\"\"Indicator whether this updater supports reverse updates.\"\"\" ... update_to_bound ( self ) \u00a4 Update all the way to the bound. Source code in corl/libraries/parameters.py def update_to_bound ( self ) -> None : \"\"\"Update all the way to the bound.\"\"\" old_arg = self . get_current_extent () new_arg = self . get_bound () constrained_arg = self . config . constraint ( old_arg = old_arg , new_arg = new_arg ) setattr ( self . config . param . config , self . config . name , constrained_arg ) UpdaterValidator ( BaseModel ) pydantic-model \u00a4 Validator class for Updater Source code in corl/libraries/parameters.py class UpdaterValidator ( BaseModel ): \"\"\"Validator class for Updater\"\"\" name : str param : Parameter constraint : _ConstraintCallbackType = lambda old_arg , new_arg : new_arg class Config : \"\"\"pydantic Config class\"\"\" arbitrary_types_allowed = True @validator ( 'param' ) def param_hasattr ( cls , v , values ): \"\"\"Validator for param field\"\"\" assert hasattr ( v . config , values [ 'name' ]), f 'Attribute { values [ \"name\" ] } does not exist' return v @validator ( 'constraint' , pre = True ) def constraint_not_none ( cls , v ): \"\"\"Conversion for constraint of None to identity\"\"\" if v is None : return lambda old_arg , new_arg : new_arg return v Config \u00a4 pydantic Config class Source code in corl/libraries/parameters.py class Config : \"\"\"pydantic Config class\"\"\" arbitrary_types_allowed = True constraint_not_none ( v ) classmethod \u00a4 Conversion for constraint of None to identity Source code in corl/libraries/parameters.py @validator ( 'constraint' , pre = True ) def constraint_not_none ( cls , v ): \"\"\"Conversion for constraint of None to identity\"\"\" if v is None : return lambda old_arg , new_arg : new_arg return v param_hasattr ( v , values ) classmethod \u00a4 Validator for param field Source code in corl/libraries/parameters.py @validator ( 'param' ) def param_hasattr ( cls , v , values ): \"\"\"Validator for param field\"\"\" assert hasattr ( v . config , values [ 'name' ]), f 'Attribute { values [ \"name\" ] } does not exist' return v","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater","text":"An Updater that advances by a constant step, limited by a bound. On each update, the provided value is incremented by a step size. If that increment causes it to violate the provided bound, the value is given the value of that bound. Reverse updates are supported. They are bounded at the initial value of the provided parameter. Source code in corl/libraries/parameters.py class BoundStepUpdater ( Updater ): \"\"\"An `Updater` that advances by a constant step, limited by a bound. On each update, the provided value is incremented by a step size. If that increment causes it to violate the provided bound, the value is given the value of that bound. Reverse updates are supported. They are bounded at the initial value of the provided parameter. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BoundStepUpdaterValidator super () . __init__ ( ** kwargs ) self . _reverse_bound = self . get_current_extent () self . _bound_func : typing . Callable self . _reverse_bound_func : typing . Callable if self . config . bound_type == 'min' : self . _bound_func = max self . _reverse_bound_func = min elif self . config . bound_type == 'max' : self . _bound_func = min self . _reverse_bound_func = max else : raise ValueError ( f 'Unknown bound type { self . config [ \"bound_type\" ] } ' ) self . _at_bound : bool = False @property def get_validator ( self ) -> typing . Type [ BoundStepUpdaterValidator ]: \"\"\"Get validator for BoundStepUpdater\"\"\" return BoundStepUpdaterValidator def do_call ( self , arg : Number , * , reverse : bool = False ) -> Number : if reverse : arg -= self . config . step output = self . _reverse_bound_func ( arg , self . _reverse_bound ) else : arg += self . config . step output = self . _bound_func ( arg , self . config . bound ) self . _at_bound = np . isclose ( output , self . config . bound ) return output def supports_reverse_update ( self ) -> bool : return True def update_to_bound ( self ) -> None : super () . update_to_bound () self . _at_bound = True def at_bound ( self ) -> bool : return self . _at_bound def get_bound ( self ) -> Number : return self . config . bound def create_config ( self ) -> dict : return { 'bound' : self . config . bound , 'step' : self . config . step , 'bound_type' : self . config . bound_type }","title":"BoundStepUpdater"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.get_validator","text":"Get validator for BoundStepUpdater","title":"get_validator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.at_bound","text":"Indicator whether this updater is at its bound. Source code in corl/libraries/parameters.py def at_bound ( self ) -> bool : return self . _at_bound","title":"at_bound()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.create_config","text":"Create the configuration file that would generate this object in its current state.","title":"create_config()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.create_config--returns","text":"dict Configuration file of the current state of the object. This object can be passed to the constructor of the Parameter to regenerate it. Source code in corl/libraries/parameters.py def create_config ( self ) -> dict : return { 'bound' : self . config . bound , 'step' : self . config . step , 'bound_type' : self . config . bound_type }","title":"Returns"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.do_call","text":"Perform the update of the provided hyperparameter.","title":"do_call()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.do_call--parameters","text":"arg : Union[int, float] The current value of the hyperparameter. bool, optional Perform a reverse update. If allowed, performing a sequence with a normal update followed by a reverse update should return the original value. Not all Updater subclasses allow reverse updates. The default is False. Source code in corl/libraries/parameters.py def do_call ( self , arg : Number , * , reverse : bool = False ) -> Number : if reverse : arg -= self . config . step output = self . _reverse_bound_func ( arg , self . _reverse_bound ) else : arg += self . config . step output = self . _bound_func ( arg , self . config . bound ) self . _at_bound = np . isclose ( output , self . config . bound ) return output","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.get_bound","text":"Get the bound of this updater. Source code in corl/libraries/parameters.py def get_bound ( self ) -> Number : return self . config . bound","title":"get_bound()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.supports_reverse_update","text":"Indicator whether this updater supports reverse updates. Source code in corl/libraries/parameters.py def supports_reverse_update ( self ) -> bool : return True","title":"supports_reverse_update()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdater.update_to_bound","text":"Update all the way to the bound. Source code in corl/libraries/parameters.py def update_to_bound ( self ) -> None : super () . update_to_bound () self . _at_bound = True","title":"update_to_bound()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdaterValidator","text":"Validator class for BoundStepUpdater Source code in corl/libraries/parameters.py class BoundStepUpdaterValidator ( UpdaterValidator ): \"\"\"Validator class for BoundStepUpdater\"\"\" bound_type : typing . Literal [ \"min\" , \"max\" ] bound : Number step : Number @validator ( 'step' ) def step_validator ( cls , v , values ): \"\"\"Validator for step field\"\"\" if v >= 0 and values [ 'bound_type' ] == 'min' : raise ValueError ( 'Step must be negative for minimum bound' ) if v <= 0 and values [ 'bound_type' ] == 'max' : raise ValueError ( 'Step must be positive for maximum bound' ) return v","title":"BoundStepUpdaterValidator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.BoundStepUpdaterValidator.step_validator","text":"Validator for step field Source code in corl/libraries/parameters.py @validator ( 'step' ) def step_validator ( cls , v , values ): \"\"\"Validator for step field\"\"\" if v >= 0 and values [ 'bound_type' ] == 'min' : raise ValueError ( 'Step must be negative for minimum bound' ) if v <= 0 and values [ 'bound_type' ] == 'max' : raise ValueError ( 'Step must be positive for maximum bound' ) return v","title":"step_validator()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ChoiceParameter","text":"A parameter drawn uniformly from a collection of discrete values. This parameter does not support updaters.","title":"ChoiceParameter"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ChoiceParameter--parameters","text":"hparams : dict The hyperparameters that define this parameter. In addition to the structure specified by the base class Parameter, there needs to be the following fields, expressed as YAML: ``` yaml choices : Sequence [ Any ] ``` Source code in corl/libraries/parameters.py class ChoiceParameter ( Parameter ): \"\"\"A parameter drawn uniformly from a collection of discrete values. This parameter does not support updaters. Parameters ---------- hparams : dict The hyperparameters that define this parameter. In addition to the structure specified by the base class Parameter, there needs to be the following fields, expressed as YAML: ```yaml choices: Sequence[Any] ``` \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ChoiceParameterValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ ChoiceParameterValidator ]: return ChoiceParameterValidator def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = rng . choice ( self . config . choices ), units = self . config . units )","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ChoiceParameter.get_validator","text":"Get the validator class for this Parameter","title":"get_validator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ChoiceParameter.get_value","text":"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept.","title":"get_value()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ChoiceParameter.get_value--parameters","text":"rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = rng . choice ( self . config . choices ), units = self . config . units )","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ChoiceParameterValidator","text":"Validator for ChoiceParameter Source code in corl/libraries/parameters.py class ChoiceParameterValidator ( ParameterValidator ): \"\"\"Validator for ChoiceParameter\"\"\" choices : typing . Sequence [ typing . Any ]","title":"ChoiceParameterValidator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ConstantParameter","text":"A parameter that always has a constant value. Source code in corl/libraries/parameters.py class ConstantParameter ( Parameter ): \"\"\"A parameter that always has a constant value.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ConstantParameterValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ ConstantParameterValidator ]: return ConstantParameterValidator def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = self . config . value , units = self . config . units )","title":"ConstantParameter"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ConstantParameter.get_validator","text":"Get the validator class for this Parameter","title":"get_validator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ConstantParameter.get_value","text":"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept.","title":"get_value()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ConstantParameter.get_value--parameters","text":"rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = self . config . value , units = self . config . units )","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ConstantParameterValidator","text":"Validator class for ConstantParameter Source code in corl/libraries/parameters.py class ConstantParameterValidator ( ParameterValidator ): \"\"\"Validator class for ConstantParameter\"\"\" value : typing . Union [ Number , str ]","title":"ConstantParameterValidator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.OverridableParameterWrapper","text":"A Parameter that wraps another parameter and can override its output. Source code in corl/libraries/parameters.py class OverridableParameterWrapper ( Parameter ): \"\"\"A Parameter that wraps another parameter and can override its output.\"\"\" def __init__ ( self , base : Parameter ) -> None : # pylint: disable=super-init-not-called # Base class API self . config = base . config self . updaters = base . updaters # Other attributes self . base = base self . override_value : typing . Any = None @property def get_validator ( self ) -> typing . NoReturn : \"\"\"OverridableParameterWrapper is not parsed using validators.\"\"\" raise NotImplementedError () def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: return self . base . get_constraint ( name = name ) def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : if self . override_value is not None : return ValueWithUnits ( value = self . override_value , units = self . config . units ) return self . base . get_value ( rng )","title":"OverridableParameterWrapper"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.OverridableParameterWrapper.get_validator","text":"OverridableParameterWrapper is not parsed using validators.","title":"get_validator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.OverridableParameterWrapper.get_constraint","text":"Get the constraint function for this Parameter's updater config Source code in corl/libraries/parameters.py def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: return self . base . get_constraint ( name = name )","title":"get_constraint()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.OverridableParameterWrapper.get_value","text":"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept.","title":"get_value()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.OverridableParameterWrapper.get_value--parameters","text":"rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : if self . override_value is not None : return ValueWithUnits ( value = self . override_value , units = self . config . units ) return self . base . get_value ( rng )","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Parameter","text":"Parameter class Source code in corl/libraries/parameters.py class Parameter ( abc . ABC ): \"\"\"Parameter class\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : ParameterValidator = self . get_validator ( ** kwargs ) # Create and save updaters self . updaters : typing . Dict [ str , typing . Any ] = {} for name , val in self . config . update . items (): factory = Factory ( ** val ) self . updaters [ name ] = factory . build ( param = self , name = name , constraint = self . get_constraint ( name = name )) @property def get_validator ( self ) -> typing . Type [ ParameterValidator ]: \"\"\"Get the validator class for this Parameter\"\"\" return ParameterValidator def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: # pylint: disable=unused-argument \"\"\"Get the constraint function for this Parameter's updater config\"\"\" return None @abc . abstractmethod def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : \"\"\"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of `self`. This is the equivalent of a C++ `const` method for developers familiar with that concept. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. \"\"\" ...","title":"Parameter"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Parameter.get_validator","text":"Get the validator class for this Parameter","title":"get_validator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Parameter.get_constraint","text":"Get the constraint function for this Parameter's updater config Source code in corl/libraries/parameters.py def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: # pylint: disable=unused-argument \"\"\"Get the constraint function for this Parameter's updater config\"\"\" return None","title":"get_constraint()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Parameter.get_value","text":"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept.","title":"get_value()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Parameter.get_value--parameters","text":"rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py @abc . abstractmethod def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : \"\"\"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of `self`. This is the equivalent of a C++ `const` method for developers familiar with that concept. Parameters ---------- rng : Union[Generator, RandomState] Random number generator from which to draw random values. \"\"\" ...","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ParameterValidator","text":"Validator class for Parameter Source code in corl/libraries/parameters.py class ParameterValidator ( BaseModel ): \"\"\"Validator class for Parameter\"\"\" units : typing . Optional [ enum . Enum ] update : typing . Dict [ str , typing . Any ] = {} simulator : typing . Dict [ str , typing . Any ] = {} episode_parameter_provider : typing . Dict [ str , typing . Any ] = {} @validator ( 'units' , pre = True ) def units_validator ( cls , v ): \"\"\"Validate the units field\"\"\" return units . GetUnitFromStr ( v ) if v is not None else None","title":"ParameterValidator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.ParameterValidator.units_validator","text":"Validate the units field Source code in corl/libraries/parameters.py @validator ( 'units' , pre = True ) def units_validator ( cls , v ): \"\"\"Validate the units field\"\"\" return units . GetUnitFromStr ( v ) if v is not None else None","title":"units_validator()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.TruncatedNormalParameter","text":"A parameter that draws from a truncated normal distribution. Source code in corl/libraries/parameters.py class TruncatedNormalParameter ( Parameter ): \"\"\"A parameter that draws from a truncated normal distribution. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : TruncatedNormalParameterValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ TruncatedNormalParameterValidator ]: return TruncatedNormalParameterValidator def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : low = self . config . mu - self . config . half_width_factor * self . config . std high = self . config . mu + self . config . half_width_factor * self . config . std value = stats . truncnorm . rvs ( ( low - self . config . mu ) / self . config . std , ( high - self . config . mu ) / self . config . std , loc = self . config . mu , scale = self . config . std , size = 1 , random_state = rng )[ 0 ] return units . ValueWithUnits ( value = value , units = self . config . units ) def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: if name == 'std' : return self . _std_positive if name == 'half_width_factor' : return self . _half_width_factor_positive raise ValueError ( \"Unknown contraint name\" ) @staticmethod def _generic_positive ( variable : str , old_arg : Number , new_arg : Number ) -> Number : if new_arg < 0 : warnings . warn ( f 'Could not update TruncatedNormalParameter { variable } because it is not strictly positive' ) return old_arg return new_arg def _std_positive ( self , old_arg : Number , new_arg : Number ) -> Number : return self . _generic_positive ( variable = 'standard deviation' , old_arg = old_arg , new_arg = new_arg ) def _half_width_factor_positive ( self , old_arg : Number , new_arg : Number ) -> Number : return self . _generic_positive ( variable = 'half width factor' , old_arg = old_arg , new_arg = new_arg )","title":"TruncatedNormalParameter"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.TruncatedNormalParameter.get_validator","text":"Get the validator class for this Parameter","title":"get_validator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.TruncatedNormalParameter.get_constraint","text":"Get the constraint function for this Parameter's updater config Source code in corl/libraries/parameters.py def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: if name == 'std' : return self . _std_positive if name == 'half_width_factor' : return self . _half_width_factor_positive raise ValueError ( \"Unknown contraint name\" )","title":"get_constraint()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.TruncatedNormalParameter.get_value","text":"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept.","title":"get_value()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.TruncatedNormalParameter.get_value--parameters","text":"rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : low = self . config . mu - self . config . half_width_factor * self . config . std high = self . config . mu + self . config . half_width_factor * self . config . std value = stats . truncnorm . rvs ( ( low - self . config . mu ) / self . config . std , ( high - self . config . mu ) / self . config . std , loc = self . config . mu , scale = self . config . std , size = 1 , random_state = rng )[ 0 ] return units . ValueWithUnits ( value = value , units = self . config . units )","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.TruncatedNormalParameterValidator","text":"Validator class for TruncatedNormalParameter Source code in corl/libraries/parameters.py class TruncatedNormalParameterValidator ( ParameterValidator ): \"\"\"Validator class for TruncatedNormalParameter\"\"\" mu : Number std : PositiveFloat half_width_factor : PositiveFloat","title":"TruncatedNormalParameterValidator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UniformParameter","text":"A parameter that draws from a uniform distribution. Source code in corl/libraries/parameters.py class UniformParameter ( Parameter ): \"\"\"A parameter that draws from a uniform distribution.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : UniformParameterValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ UniformParameterValidator ]: return UniformParameterValidator def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = rng . uniform ( self . config . low , self . config . high ), units = self . config . units ) def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: if name == 'low' : return self . _min_with_high if name == 'high' : return self . _max_with_low raise ValueError ( \"Unknown contraint name\" ) def _min_with_high ( self , old_arg : Number , new_arg : Number ) -> Number : # pylint: disable=unused-argument if new_arg > self . config . high : warnings . warn ( 'Could not fully update UniformParameter lower bound as it exceeds higher bound' ) return self . config . high return new_arg def _max_with_low ( self , old_arg : Number , new_arg : Number ) -> Number : # pylint: disable=unused-argument if new_arg < self . config . low : warnings . warn ( 'Could not fully update UniformParameter lower bound as it goes below the lower bound' ) return self . config . low return new_arg","title":"UniformParameter"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UniformParameter.get_validator","text":"Get the validator class for this Parameter","title":"get_validator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UniformParameter.get_constraint","text":"Get the constraint function for this Parameter's updater config Source code in corl/libraries/parameters.py def get_constraint ( self , name : str ) -> typing . Optional [ _ConstraintCallbackType ]: if name == 'low' : return self . _min_with_high if name == 'high' : return self . _max_with_low raise ValueError ( \"Unknown contraint name\" )","title":"get_constraint()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UniformParameter.get_value","text":"Get the value of the parameter. In order to avoid inconsistent operation between cases where the value is serialized (such as from the ray object store) or not, this method should not modify the attributes of self . This is the equivalent of a C++ const method for developers familiar with that concept.","title":"get_value()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UniformParameter.get_value--parameters","text":"rng : Union[Generator, RandomState] Random number generator from which to draw random values. Source code in corl/libraries/parameters.py def get_value ( self , rng : Randomness ) -> units . ValueWithUnits : return units . ValueWithUnits ( value = rng . uniform ( self . config . low , self . config . high ), units = self . config . units )","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UniformParameterValidator","text":"Validator class for UniformParameter Source code in corl/libraries/parameters.py class UniformParameterValidator ( ParameterValidator ): \"\"\"Validator class for UniformParameter\"\"\" low : Number high : Number @validator ( \"high\" ) def high_validator ( cls , v , values ): \"\"\"Validate the high field\"\"\" if v < values [ 'low' ]: raise ValueError ( 'Upper bound must not be smaller than lower bound' ) return v","title":"UniformParameterValidator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UniformParameterValidator.high_validator","text":"Validate the high field Source code in corl/libraries/parameters.py @validator ( \"high\" ) def high_validator ( cls , v , values ): \"\"\"Validate the high field\"\"\" if v < values [ 'low' ]: raise ValueError ( 'Upper bound must not be smaller than lower bound' ) return v","title":"high_validator()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater","text":"Generic structure to define the method of updating a hyperparameter of a Parameter . Source code in corl/libraries/parameters.py class Updater ( abc . ABC ): \"\"\"Generic structure to define the method of updating a hyperparameter of a `Parameter`.\"\"\" def __init__ ( self , ** kwargs ) -> None : # Validate and save updater configuration data self . config : UpdaterValidator = self . get_validator ( ** kwargs ) def __call__ ( self , * , reverse : bool = False ) -> None : \"\"\"Perform an update of the hyperparameter according to the functionality of the updater. The generic functionality of an update has four steps: 1. Get the current value of the hyperparameter by extracting the named attribute from the connected parameter. 2. Perform the update according to the `do_call` method within the updater. The current value of the hyperparameter is provided, along with the flag to specify reverse updates. 3. Apply the constraint function. The primary purpose of this function is to allow the `Parameter` to provide a function to ensure that the updated hyperparameter value leaves the `Parameter` in a consistent state. It receives the old argument and the new argument as keyword arguments (with names `old_arg` and `new_arg` respectively). Common uses are to ensure that multiple hyperparameters within an `Parameter` maintain a consistent state (i.e., lower bound is less than upper bound) or that some invariant property is maintained (i.e., standard deviation is positive). The constraint function must return the value that should be set into the hyperparameter. 4. The output of the constraint function is set back to the named attribute in the connected parameter. Subclasses should not override `__call__`, but should implement `do_call`. Parameters ---------- reverse : bool, optional Perform a reverse update, by default False \"\"\" old_arg = self . get_current_extent () new_arg = self . do_call ( old_arg , reverse = reverse ) constrained_arg = self . config . constraint ( old_arg = old_arg , new_arg = new_arg ) setattr ( self . config . param . config , self . config . name , constrained_arg ) def update_to_bound ( self ) -> None : \"\"\"Update all the way to the bound.\"\"\" old_arg = self . get_current_extent () new_arg = self . get_bound () constrained_arg = self . config . constraint ( old_arg = old_arg , new_arg = new_arg ) setattr ( self . config . param . config , self . config . name , constrained_arg ) @abc . abstractmethod def do_call ( self , arg : Number , * , reverse : bool = False ) -> Number : \"\"\"Perform the update of the provided hyperparameter. Parameters ---------- arg : Union[int, float] The current value of the hyperparameter. reverse: bool, optional Perform a reverse update. If allowed, performing a sequence with a normal update followed by a reverse update should return the original value. Not all `Updater` subclasses allow reverse updates. The default is False. \"\"\" ... @property def get_validator ( self ) -> typing . Type [ UpdaterValidator ]: \"\"\"Get the validator for this class\"\"\" return UpdaterValidator @abc . abstractmethod def supports_reverse_update ( self ) -> bool : \"\"\"Indicator whether this updater supports reverse updates.\"\"\" ... @abc . abstractmethod def at_bound ( self ) -> bool : \"\"\"Indicator whether this updater is at its bound.\"\"\" ... @abc . abstractmethod def get_bound ( self ) -> Number : \"\"\"Get the bound of this updater.\"\"\" ... def get_current_extent ( self ) -> Number : \"\"\"returns the current extent of the parameter controlled by this updater Returns: Number -- The current extent of the parameter controlled by this updater \"\"\" return getattr ( self . config . param . config , self . config . name ) @abc . abstractmethod def create_config ( self ) -> dict : \"\"\"Create the configuration file that would generate this object in its current state. Returns ------- dict Configuration file of the current state of the object. This object can be passed to the constructor of the Parameter to regenerate it. \"\"\" raise NotImplementedError ()","title":"Updater"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.get_validator","text":"Get the validator for this class","title":"get_validator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.__call__","text":"Perform an update of the hyperparameter according to the functionality of the updater. The generic functionality of an update has four steps: 1. Get the current value of the hyperparameter by extracting the named attribute from the connected parameter. 2. Perform the update according to the do_call method within the updater. The current value of the hyperparameter is provided, along with the flag to specify reverse updates. 3. Apply the constraint function. The primary purpose of this function is to allow the Parameter to provide a function to ensure that the updated hyperparameter value leaves the Parameter in a consistent state. It receives the old argument and the new argument as keyword arguments (with names old_arg and new_arg respectively). Common uses are to ensure that multiple hyperparameters within an Parameter maintain a consistent state (i.e., lower bound is less than upper bound) or that some invariant property is maintained (i.e., standard deviation is positive). The constraint function must return the value that should be set into the hyperparameter. 4. The output of the constraint function is set back to the named attribute in the connected parameter. Subclasses should not override __call__ , but should implement do_call .","title":"__call__()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.__call__--parameters","text":"reverse : bool, optional Perform a reverse update, by default False Source code in corl/libraries/parameters.py def __call__ ( self , * , reverse : bool = False ) -> None : \"\"\"Perform an update of the hyperparameter according to the functionality of the updater. The generic functionality of an update has four steps: 1. Get the current value of the hyperparameter by extracting the named attribute from the connected parameter. 2. Perform the update according to the `do_call` method within the updater. The current value of the hyperparameter is provided, along with the flag to specify reverse updates. 3. Apply the constraint function. The primary purpose of this function is to allow the `Parameter` to provide a function to ensure that the updated hyperparameter value leaves the `Parameter` in a consistent state. It receives the old argument and the new argument as keyword arguments (with names `old_arg` and `new_arg` respectively). Common uses are to ensure that multiple hyperparameters within an `Parameter` maintain a consistent state (i.e., lower bound is less than upper bound) or that some invariant property is maintained (i.e., standard deviation is positive). The constraint function must return the value that should be set into the hyperparameter. 4. The output of the constraint function is set back to the named attribute in the connected parameter. Subclasses should not override `__call__`, but should implement `do_call`. Parameters ---------- reverse : bool, optional Perform a reverse update, by default False \"\"\" old_arg = self . get_current_extent () new_arg = self . do_call ( old_arg , reverse = reverse ) constrained_arg = self . config . constraint ( old_arg = old_arg , new_arg = new_arg ) setattr ( self . config . param . config , self . config . name , constrained_arg )","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.at_bound","text":"Indicator whether this updater is at its bound. Source code in corl/libraries/parameters.py @abc . abstractmethod def at_bound ( self ) -> bool : \"\"\"Indicator whether this updater is at its bound.\"\"\" ...","title":"at_bound()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.create_config","text":"Create the configuration file that would generate this object in its current state.","title":"create_config()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.create_config--returns","text":"dict Configuration file of the current state of the object. This object can be passed to the constructor of the Parameter to regenerate it. Source code in corl/libraries/parameters.py @abc . abstractmethod def create_config ( self ) -> dict : \"\"\"Create the configuration file that would generate this object in its current state. Returns ------- dict Configuration file of the current state of the object. This object can be passed to the constructor of the Parameter to regenerate it. \"\"\" raise NotImplementedError ()","title":"Returns"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.do_call","text":"Perform the update of the provided hyperparameter.","title":"do_call()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.do_call--parameters","text":"arg : Union[int, float] The current value of the hyperparameter. bool, optional Perform a reverse update. If allowed, performing a sequence with a normal update followed by a reverse update should return the original value. Not all Updater subclasses allow reverse updates. The default is False. Source code in corl/libraries/parameters.py @abc . abstractmethod def do_call ( self , arg : Number , * , reverse : bool = False ) -> Number : \"\"\"Perform the update of the provided hyperparameter. Parameters ---------- arg : Union[int, float] The current value of the hyperparameter. reverse: bool, optional Perform a reverse update. If allowed, performing a sequence with a normal update followed by a reverse update should return the original value. Not all `Updater` subclasses allow reverse updates. The default is False. \"\"\" ...","title":"Parameters"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.get_bound","text":"Get the bound of this updater. Source code in corl/libraries/parameters.py @abc . abstractmethod def get_bound ( self ) -> Number : \"\"\"Get the bound of this updater.\"\"\" ...","title":"get_bound()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.get_current_extent","text":"returns the current extent of the parameter controlled by this updater Returns: Type Description Union[pydantic.types.StrictInt, float] Number -- The current extent of the parameter controlled by this updater Source code in corl/libraries/parameters.py def get_current_extent ( self ) -> Number : \"\"\"returns the current extent of the parameter controlled by this updater Returns: Number -- The current extent of the parameter controlled by this updater \"\"\" return getattr ( self . config . param . config , self . config . name )","title":"get_current_extent()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.supports_reverse_update","text":"Indicator whether this updater supports reverse updates. Source code in corl/libraries/parameters.py @abc . abstractmethod def supports_reverse_update ( self ) -> bool : \"\"\"Indicator whether this updater supports reverse updates.\"\"\" ...","title":"supports_reverse_update()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.Updater.update_to_bound","text":"Update all the way to the bound. Source code in corl/libraries/parameters.py def update_to_bound ( self ) -> None : \"\"\"Update all the way to the bound.\"\"\" old_arg = self . get_current_extent () new_arg = self . get_bound () constrained_arg = self . config . constraint ( old_arg = old_arg , new_arg = new_arg ) setattr ( self . config . param . config , self . config . name , constrained_arg )","title":"update_to_bound()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UpdaterValidator","text":"Validator class for Updater Source code in corl/libraries/parameters.py class UpdaterValidator ( BaseModel ): \"\"\"Validator class for Updater\"\"\" name : str param : Parameter constraint : _ConstraintCallbackType = lambda old_arg , new_arg : new_arg class Config : \"\"\"pydantic Config class\"\"\" arbitrary_types_allowed = True @validator ( 'param' ) def param_hasattr ( cls , v , values ): \"\"\"Validator for param field\"\"\" assert hasattr ( v . config , values [ 'name' ]), f 'Attribute { values [ \"name\" ] } does not exist' return v @validator ( 'constraint' , pre = True ) def constraint_not_none ( cls , v ): \"\"\"Conversion for constraint of None to identity\"\"\" if v is None : return lambda old_arg , new_arg : new_arg return v","title":"UpdaterValidator"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UpdaterValidator.Config","text":"pydantic Config class Source code in corl/libraries/parameters.py class Config : \"\"\"pydantic Config class\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UpdaterValidator.constraint_not_none","text":"Conversion for constraint of None to identity Source code in corl/libraries/parameters.py @validator ( 'constraint' , pre = True ) def constraint_not_none ( cls , v ): \"\"\"Conversion for constraint of None to identity\"\"\" if v is None : return lambda old_arg , new_arg : new_arg return v","title":"constraint_not_none()"},{"location":"reference/libraries/parameters/#corl.libraries.parameters.UpdaterValidator.param_hasattr","text":"Validator for param field Source code in corl/libraries/parameters.py @validator ( 'param' ) def param_hasattr ( cls , v , values ): \"\"\"Validator for param field\"\"\" assert hasattr ( v . config , values [ 'name' ]), f 'Attribute { values [ \"name\" ] } does not exist' return v","title":"param_hasattr()"},{"location":"reference/libraries/plugin_library/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. PluginLibrary.AddClassToGroup(CLASS, \"NAME\", {\"simulator\": INTEGRATION_CLASS, \"platform_type\": AvailablePlatformTypes.XXXX}) difference_metric ( x1 , x2 ) \u00a4 Heuristic for determining how much of a match 2 dictionaries are Dictionaries may only be a single layer or this may not work correctly This heuristic is calculated as follows number of conditions matches / (total number of keys in both dicts /2) this allows any match to get selected, and prioritize dictionaries with more matches in the event that x2 is empty the match score is set to 0.5 / (total number of keys in both dicts /2). This essentially is a 0.5 match of a key and allows dictionaries with a single match to have a higher score while empty dicts to result in a non-zero match. Specifically, this is used for plugins with no conditions. Furthermore, all keys that exist in both x1 and x2 must match or a metric of 0 is returned. Returns: Type Description float -- The heuristic value of the comparison between the two 1 is max - every single field matched) 0 is minimum - no field matches) 0 is minimum - field present in both dictionaries does not match Source code in corl/libraries/plugin_library.py def difference_metric ( x1 : typing . Dict [ str , typing . Any ], x2 : typing . Dict [ str , typing . Any ]): \"\"\" Heuristic for determining how much of a match 2 dictionaries are Dictionaries may only be a single layer or this may not work correctly This heuristic is calculated as follows number of conditions matches / (total number of keys in both dicts /2) this allows any match to get selected, and prioritize dictionaries with more matches in the event that x2 is empty the match score is set to 0.5 / (total number of keys in both dicts /2). This essentially is a 0.5 match of a key and allows dictionaries with a single match to have a higher score while empty dicts to result in a non-zero match. Specifically, this is used for plugins with no conditions. Furthermore, all keys that exist in both x1 and x2 must match or a metric of 0 is returned. Arguments: x1 {dict} -- The dictionary used to query the list of dictionaries x2 {dict} -- The dictionary being compared to x1 Returns: float -- The heuristic value of the comparison between the two 1 is max - every single field matched) 0 is minimum - no field matches) 0 is minimum - field present in both dictionaries does not match \"\"\" match_score = 0 total = ( len ( x1 ) + len ( x2 )) / 2 if not x2 : return 0.5 / total for key , value in x2 . items (): if key not in x1 : continue if value == x1 [ key ]: match_score += 1 else : return 0 return match_score / total","title":"Plugin library"},{"location":"reference/libraries/plugin_library/#corl.libraries.plugin_library.difference_metric","text":"Heuristic for determining how much of a match 2 dictionaries are Dictionaries may only be a single layer or this may not work correctly This heuristic is calculated as follows number of conditions matches / (total number of keys in both dicts /2) this allows any match to get selected, and prioritize dictionaries with more matches in the event that x2 is empty the match score is set to 0.5 / (total number of keys in both dicts /2). This essentially is a 0.5 match of a key and allows dictionaries with a single match to have a higher score while empty dicts to result in a non-zero match. Specifically, this is used for plugins with no conditions. Furthermore, all keys that exist in both x1 and x2 must match or a metric of 0 is returned. Returns: Type Description float -- The heuristic value of the comparison between the two 1 is max - every single field matched) 0 is minimum - no field matches) 0 is minimum - field present in both dictionaries does not match Source code in corl/libraries/plugin_library.py def difference_metric ( x1 : typing . Dict [ str , typing . Any ], x2 : typing . Dict [ str , typing . Any ]): \"\"\" Heuristic for determining how much of a match 2 dictionaries are Dictionaries may only be a single layer or this may not work correctly This heuristic is calculated as follows number of conditions matches / (total number of keys in both dicts /2) this allows any match to get selected, and prioritize dictionaries with more matches in the event that x2 is empty the match score is set to 0.5 / (total number of keys in both dicts /2). This essentially is a 0.5 match of a key and allows dictionaries with a single match to have a higher score while empty dicts to result in a non-zero match. Specifically, this is used for plugins with no conditions. Furthermore, all keys that exist in both x1 and x2 must match or a metric of 0 is returned. Arguments: x1 {dict} -- The dictionary used to query the list of dictionaries x2 {dict} -- The dictionary being compared to x1 Returns: float -- The heuristic value of the comparison between the two 1 is max - every single field matched) 0 is minimum - no field matches) 0 is minimum - field present in both dictionaries does not match \"\"\" match_score = 0 total = ( len ( x1 ) + len ( x2 )) / 2 if not x2 : return 0.5 / total for key , value in x2 . items (): if key not in x1 : continue if value == x1 [ key ]: match_score += 1 else : return 0 return match_score / total","title":"difference_metric()"},{"location":"reference/libraries/property/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Property Module BoxProp ( Prop ) pydantic-model \u00a4 Represents the multi box outside of RLLIB Source code in corl/libraries/property.py class BoxProp ( Prop ): \"\"\"Represents the multi box outside of RLLIB \"\"\" dtype : typing . Optional [ np . dtype ] = np . dtype ( np . float32 ) low : typing . Union [ typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]] high : typing . Union [ typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]] unit : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]]] shape : typing . Optional [ typing . Union [ typing . Tuple [ int ], typing . Tuple [ int , int ]]] @validator ( 'unit' ) def unit_validator ( cls , v ): \"\"\"Validate unit\"\"\" for item in v : if not isinstance ( item , str ) and isinstance ( item , collections . abc . Sequence ): # list of lists 2D case for value in item : assert isinstance ( GetUnitFromStr ( value ), enum . Enum ), f ' { value } is not valid unit' else : # 1D case assert isinstance ( GetUnitFromStr ( item ), enum . Enum ), f ' { value } is not valid unit' return v @root_validator def prop_validator ( cls , values ): \"\"\"Validate\"\"\" if not all ( k in values for k in ( 'low' , 'high' , 'unit' )): # Something went wrong, return return values # Broadcast space as needed space = gym . spaces . Box ( low = np . array ( values [ 'low' ]) . astype ( values [ 'dtype' ]), high = np . array ( values [ 'high' ]) . astype ( values [ 'dtype' ]), dtype = values [ 'dtype' ], shape = values [ 'shape' ] ) values [ 'low' ] = space . low . tolist () values [ 'high' ] = space . high . tolist () # Validate dimensions # 1D case assert len ( values [ 'low' ]) == len ( values [ 'high' ]), \"low and high different length\" assert len ( values [ 'low' ]) == len ( values [ 'unit' ]), \"low and unit different length\" if len ( values [ 'low' ]) > 0 : if isinstance ( values [ 'low' ][ 0 ], list ): # list of lists 2D case for i , _ in enumerate ( values [ 'low' ]): assert len ( values [ 'low' ][ i ]) == len ( values [ 'high' ][ i ]), \"low and high list elements different length\" assert len ( values [ 'low' ][ i ]) == len ( values [ 'unit' ][ i ]), \"low and unit list elements different length\" return values class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True validate_all = True def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Box space \"\"\" return gym . spaces . Box ( low = np . array ( self . low ) . astype ( self . dtype ), high = np . array ( self . high ) . astype ( self . dtype ), dtype = self . dtype , shape = self . shape ) def min ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> typing . Union [ float , typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]]: \"\"\"Min are pulled from sensor properties, converted to output units when necessary \"\"\" if isinstance ( self . low [ 0 ], collections . abc . Sequence ): # list of lists 2D case arr2d = [] for i , value in enumerate ( self . low ): assert isinstance ( value , collections . abc . Sequence ) unit_array = convert [ i ] assert isinstance ( unit_array , collections . abc . Sequence ) out = [] for j , val in enumerate ( value ): tmp = Convert ( val , self . unit [ i ][ j ], unit_array [ j ]) out . append ( tmp ) arr2d . append ( out ) return arr2d # 1D case arr = [] for i , value in enumerate ( self . low ): assert isinstance ( value , float ) in_units = self . unit [ i ] assert isinstance ( in_units , ( str , enum . Enum )) out_units = convert [ i ] assert isinstance ( out_units , ( str , enum . Enum )) tmp = Convert ( value , in_units , out_units ) arr . append ( tmp ) return arr def max ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> typing . Union [ float , typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]]: \"\"\"Max are pulled from sensor properties, converted to output units when necessary \"\"\" if isinstance ( self . high [ 0 ], collections . abc . Sequence ): # list of lists 2D case arr2d = [] for i , value in enumerate ( self . high ): assert isinstance ( value , collections . abc . Sequence ) unit_array = convert [ i ] assert isinstance ( unit_array , collections . abc . Sequence ) out = [] for j , val in enumerate ( value ): tmp = Convert ( val , self . unit [ i ][ j ], unit_array [ j ]) out . append ( tmp ) arr2d . append ( out ) return arr2d # 1D case arr = [] for i , value in enumerate ( self . high ): assert isinstance ( value , float ) in_units = self . unit [ i ] assert isinstance ( in_units , ( str , enum . Enum )) out_units = convert [ i ] assert isinstance ( out_units , ( str , enum . Enum )) tmp = Convert ( value , in_units , out_units ) arr . append ( tmp ) return arr def create_converted_space ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> gym . spaces . Space : \"\"\" Creates RLLIB Box space \"\"\" return gym . spaces . Box ( low = np . array ( self . min ( convert )) . astype ( self . dtype ), high = np . array ( self . max ( convert )) . astype ( self . dtype ), dtype = self . dtype , shape = self . shape ) create_converted_space ( self , convert ) \u00a4 Creates RLLIB Box space Source code in corl/libraries/property.py def create_converted_space ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> gym . spaces . Space : \"\"\" Creates RLLIB Box space \"\"\" return gym . spaces . Box ( low = np . array ( self . min ( convert )) . astype ( self . dtype ), high = np . array ( self . max ( convert )) . astype ( self . dtype ), dtype = self . dtype , shape = self . shape ) create_space ( self ) \u00a4 Creates RLLIB Box space Source code in corl/libraries/property.py def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Box space \"\"\" return gym . spaces . Box ( low = np . array ( self . low ) . astype ( self . dtype ), high = np . array ( self . high ) . astype ( self . dtype ), dtype = self . dtype , shape = self . shape ) max ( self , convert ) \u00a4 Max are pulled from sensor properties, converted to output units when necessary Source code in corl/libraries/property.py def max ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> typing . Union [ float , typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]]: \"\"\"Max are pulled from sensor properties, converted to output units when necessary \"\"\" if isinstance ( self . high [ 0 ], collections . abc . Sequence ): # list of lists 2D case arr2d = [] for i , value in enumerate ( self . high ): assert isinstance ( value , collections . abc . Sequence ) unit_array = convert [ i ] assert isinstance ( unit_array , collections . abc . Sequence ) out = [] for j , val in enumerate ( value ): tmp = Convert ( val , self . unit [ i ][ j ], unit_array [ j ]) out . append ( tmp ) arr2d . append ( out ) return arr2d # 1D case arr = [] for i , value in enumerate ( self . high ): assert isinstance ( value , float ) in_units = self . unit [ i ] assert isinstance ( in_units , ( str , enum . Enum )) out_units = convert [ i ] assert isinstance ( out_units , ( str , enum . Enum )) tmp = Convert ( value , in_units , out_units ) arr . append ( tmp ) return arr min ( self , convert ) \u00a4 Min are pulled from sensor properties, converted to output units when necessary Source code in corl/libraries/property.py def min ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> typing . Union [ float , typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]]: \"\"\"Min are pulled from sensor properties, converted to output units when necessary \"\"\" if isinstance ( self . low [ 0 ], collections . abc . Sequence ): # list of lists 2D case arr2d = [] for i , value in enumerate ( self . low ): assert isinstance ( value , collections . abc . Sequence ) unit_array = convert [ i ] assert isinstance ( unit_array , collections . abc . Sequence ) out = [] for j , val in enumerate ( value ): tmp = Convert ( val , self . unit [ i ][ j ], unit_array [ j ]) out . append ( tmp ) arr2d . append ( out ) return arr2d # 1D case arr = [] for i , value in enumerate ( self . low ): assert isinstance ( value , float ) in_units = self . unit [ i ] assert isinstance ( in_units , ( str , enum . Enum )) out_units = convert [ i ] assert isinstance ( out_units , ( str , enum . Enum )) tmp = Convert ( value , in_units , out_units ) arr . append ( tmp ) return arr prop_validator ( values ) classmethod \u00a4 Validate Source code in corl/libraries/property.py @root_validator def prop_validator ( cls , values ): \"\"\"Validate\"\"\" if not all ( k in values for k in ( 'low' , 'high' , 'unit' )): # Something went wrong, return return values # Broadcast space as needed space = gym . spaces . Box ( low = np . array ( values [ 'low' ]) . astype ( values [ 'dtype' ]), high = np . array ( values [ 'high' ]) . astype ( values [ 'dtype' ]), dtype = values [ 'dtype' ], shape = values [ 'shape' ] ) values [ 'low' ] = space . low . tolist () values [ 'high' ] = space . high . tolist () # Validate dimensions # 1D case assert len ( values [ 'low' ]) == len ( values [ 'high' ]), \"low and high different length\" assert len ( values [ 'low' ]) == len ( values [ 'unit' ]), \"low and unit different length\" if len ( values [ 'low' ]) > 0 : if isinstance ( values [ 'low' ][ 0 ], list ): # list of lists 2D case for i , _ in enumerate ( values [ 'low' ]): assert len ( values [ 'low' ][ i ]) == len ( values [ 'high' ][ i ]), \"low and high list elements different length\" assert len ( values [ 'low' ][ i ]) == len ( values [ 'unit' ][ i ]), \"low and unit list elements different length\" return values unit_validator ( v ) classmethod \u00a4 Validate unit Source code in corl/libraries/property.py @validator ( 'unit' ) def unit_validator ( cls , v ): \"\"\"Validate unit\"\"\" for item in v : if not isinstance ( item , str ) and isinstance ( item , collections . abc . Sequence ): # list of lists 2D case for value in item : assert isinstance ( GetUnitFromStr ( value ), enum . Enum ), f ' { value } is not valid unit' else : # 1D case assert isinstance ( GetUnitFromStr ( item ), enum . Enum ), f ' { value } is not valid unit' return v DiscreteProp ( Prop ) pydantic-model \u00a4 Represents the Discrete outside of RLLIB Source code in corl/libraries/property.py class DiscreteProp ( Prop ): \"\"\"Represents the Discrete outside of RLLIB \"\"\" n : int def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Discrete space \"\"\" return gym . spaces . Discrete ( self . n ) create_space ( self ) \u00a4 Creates RLLIB Discrete space Source code in corl/libraries/property.py def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Discrete space \"\"\" return gym . spaces . Discrete ( self . n ) MultiBinary ( Prop ) pydantic-model \u00a4 Represents the multi binary outside of RLLIB Source code in corl/libraries/property.py class MultiBinary ( Prop ): \"\"\"Represents the multi binary outside of RLLIB \"\"\" n : int def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB MultiBinary space \"\"\" return gym . spaces . MultiBinary ( self . n ) create_space ( self ) \u00a4 Creates RLLIB MultiBinary space Source code in corl/libraries/property.py def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB MultiBinary space \"\"\" return gym . spaces . MultiBinary ( self . n ) Prop ( BaseModel , ABC ) pydantic-model \u00a4 Represents the space prop outside of RLLIB Source code in corl/libraries/property.py class Prop ( BaseModel , abc . ABC ): \"\"\"Represents the space prop outside of RLLIB \"\"\" name : str description : str class Config : # pylint: disable=C0115, R0903 validate_all = True @abc . abstractclassmethod def create_space ( cls ) -> gym . spaces . Space : \"\"\" Creates RLLIB space \"\"\" ... create_space () classmethod \u00a4 Creates RLLIB space Source code in corl/libraries/property.py @abc . abstractclassmethod def create_space ( cls ) -> gym . spaces . Space : \"\"\" Creates RLLIB space \"\"\" ... RepeatedProp ( Prop ) pydantic-model \u00a4 Represents the multi binary outside of RLLIB Source code in corl/libraries/property.py class RepeatedProp ( Prop ): \"\"\"Represents the multi binary outside of RLLIB \"\"\" max_len : int child_space : typing . Dict [ str , Prop ] def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Repeated space \"\"\" gym_child_space = gym . spaces . Dict ({ key : value . create_space () for key , value in self . child_space . items ()}) return Repeated ( child_space = gym_child_space , max_len = self . max_len ) create_space ( self ) \u00a4 Creates RLLIB Repeated space Source code in corl/libraries/property.py def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Repeated space \"\"\" gym_child_space = gym . spaces . Dict ({ key : value . create_space () for key , value in self . child_space . items ()}) return Repeated ( child_space = gym_child_space , max_len = self . max_len )","title":"Property"},{"location":"reference/libraries/property/#corl.libraries.property.BoxProp","text":"Represents the multi box outside of RLLIB Source code in corl/libraries/property.py class BoxProp ( Prop ): \"\"\"Represents the multi box outside of RLLIB \"\"\" dtype : typing . Optional [ np . dtype ] = np . dtype ( np . float32 ) low : typing . Union [ typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]] high : typing . Union [ typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]] unit : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]]] shape : typing . Optional [ typing . Union [ typing . Tuple [ int ], typing . Tuple [ int , int ]]] @validator ( 'unit' ) def unit_validator ( cls , v ): \"\"\"Validate unit\"\"\" for item in v : if not isinstance ( item , str ) and isinstance ( item , collections . abc . Sequence ): # list of lists 2D case for value in item : assert isinstance ( GetUnitFromStr ( value ), enum . Enum ), f ' { value } is not valid unit' else : # 1D case assert isinstance ( GetUnitFromStr ( item ), enum . Enum ), f ' { value } is not valid unit' return v @root_validator def prop_validator ( cls , values ): \"\"\"Validate\"\"\" if not all ( k in values for k in ( 'low' , 'high' , 'unit' )): # Something went wrong, return return values # Broadcast space as needed space = gym . spaces . Box ( low = np . array ( values [ 'low' ]) . astype ( values [ 'dtype' ]), high = np . array ( values [ 'high' ]) . astype ( values [ 'dtype' ]), dtype = values [ 'dtype' ], shape = values [ 'shape' ] ) values [ 'low' ] = space . low . tolist () values [ 'high' ] = space . high . tolist () # Validate dimensions # 1D case assert len ( values [ 'low' ]) == len ( values [ 'high' ]), \"low and high different length\" assert len ( values [ 'low' ]) == len ( values [ 'unit' ]), \"low and unit different length\" if len ( values [ 'low' ]) > 0 : if isinstance ( values [ 'low' ][ 0 ], list ): # list of lists 2D case for i , _ in enumerate ( values [ 'low' ]): assert len ( values [ 'low' ][ i ]) == len ( values [ 'high' ][ i ]), \"low and high list elements different length\" assert len ( values [ 'low' ][ i ]) == len ( values [ 'unit' ][ i ]), \"low and unit list elements different length\" return values class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True validate_all = True def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Box space \"\"\" return gym . spaces . Box ( low = np . array ( self . low ) . astype ( self . dtype ), high = np . array ( self . high ) . astype ( self . dtype ), dtype = self . dtype , shape = self . shape ) def min ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> typing . Union [ float , typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]]: \"\"\"Min are pulled from sensor properties, converted to output units when necessary \"\"\" if isinstance ( self . low [ 0 ], collections . abc . Sequence ): # list of lists 2D case arr2d = [] for i , value in enumerate ( self . low ): assert isinstance ( value , collections . abc . Sequence ) unit_array = convert [ i ] assert isinstance ( unit_array , collections . abc . Sequence ) out = [] for j , val in enumerate ( value ): tmp = Convert ( val , self . unit [ i ][ j ], unit_array [ j ]) out . append ( tmp ) arr2d . append ( out ) return arr2d # 1D case arr = [] for i , value in enumerate ( self . low ): assert isinstance ( value , float ) in_units = self . unit [ i ] assert isinstance ( in_units , ( str , enum . Enum )) out_units = convert [ i ] assert isinstance ( out_units , ( str , enum . Enum )) tmp = Convert ( value , in_units , out_units ) arr . append ( tmp ) return arr def max ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> typing . Union [ float , typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]]: \"\"\"Max are pulled from sensor properties, converted to output units when necessary \"\"\" if isinstance ( self . high [ 0 ], collections . abc . Sequence ): # list of lists 2D case arr2d = [] for i , value in enumerate ( self . high ): assert isinstance ( value , collections . abc . Sequence ) unit_array = convert [ i ] assert isinstance ( unit_array , collections . abc . Sequence ) out = [] for j , val in enumerate ( value ): tmp = Convert ( val , self . unit [ i ][ j ], unit_array [ j ]) out . append ( tmp ) arr2d . append ( out ) return arr2d # 1D case arr = [] for i , value in enumerate ( self . high ): assert isinstance ( value , float ) in_units = self . unit [ i ] assert isinstance ( in_units , ( str , enum . Enum )) out_units = convert [ i ] assert isinstance ( out_units , ( str , enum . Enum )) tmp = Convert ( value , in_units , out_units ) arr . append ( tmp ) return arr def create_converted_space ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> gym . spaces . Space : \"\"\" Creates RLLIB Box space \"\"\" return gym . spaces . Box ( low = np . array ( self . min ( convert )) . astype ( self . dtype ), high = np . array ( self . max ( convert )) . astype ( self . dtype ), dtype = self . dtype , shape = self . shape )","title":"BoxProp"},{"location":"reference/libraries/property/#corl.libraries.property.BoxProp.create_converted_space","text":"Creates RLLIB Box space Source code in corl/libraries/property.py def create_converted_space ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> gym . spaces . Space : \"\"\" Creates RLLIB Box space \"\"\" return gym . spaces . Box ( low = np . array ( self . min ( convert )) . astype ( self . dtype ), high = np . array ( self . max ( convert )) . astype ( self . dtype ), dtype = self . dtype , shape = self . shape )","title":"create_converted_space()"},{"location":"reference/libraries/property/#corl.libraries.property.BoxProp.create_space","text":"Creates RLLIB Box space Source code in corl/libraries/property.py def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Box space \"\"\" return gym . spaces . Box ( low = np . array ( self . low ) . astype ( self . dtype ), high = np . array ( self . high ) . astype ( self . dtype ), dtype = self . dtype , shape = self . shape )","title":"create_space()"},{"location":"reference/libraries/property/#corl.libraries.property.BoxProp.max","text":"Max are pulled from sensor properties, converted to output units when necessary Source code in corl/libraries/property.py def max ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> typing . Union [ float , typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]]: \"\"\"Max are pulled from sensor properties, converted to output units when necessary \"\"\" if isinstance ( self . high [ 0 ], collections . abc . Sequence ): # list of lists 2D case arr2d = [] for i , value in enumerate ( self . high ): assert isinstance ( value , collections . abc . Sequence ) unit_array = convert [ i ] assert isinstance ( unit_array , collections . abc . Sequence ) out = [] for j , val in enumerate ( value ): tmp = Convert ( val , self . unit [ i ][ j ], unit_array [ j ]) out . append ( tmp ) arr2d . append ( out ) return arr2d # 1D case arr = [] for i , value in enumerate ( self . high ): assert isinstance ( value , float ) in_units = self . unit [ i ] assert isinstance ( in_units , ( str , enum . Enum )) out_units = convert [ i ] assert isinstance ( out_units , ( str , enum . Enum )) tmp = Convert ( value , in_units , out_units ) arr . append ( tmp ) return arr","title":"max()"},{"location":"reference/libraries/property/#corl.libraries.property.BoxProp.min","text":"Min are pulled from sensor properties, converted to output units when necessary Source code in corl/libraries/property.py def min ( self , convert : typing . Union [ typing . Sequence [ str ], typing . Sequence [ typing . Sequence [ str ]], typing . Sequence [ enum . Enum ], typing . Sequence [ typing . Sequence [ enum . Enum ]]] ) -> typing . Union [ float , typing . Sequence [ float ], typing . Sequence [ typing . Sequence [ float ]]]: \"\"\"Min are pulled from sensor properties, converted to output units when necessary \"\"\" if isinstance ( self . low [ 0 ], collections . abc . Sequence ): # list of lists 2D case arr2d = [] for i , value in enumerate ( self . low ): assert isinstance ( value , collections . abc . Sequence ) unit_array = convert [ i ] assert isinstance ( unit_array , collections . abc . Sequence ) out = [] for j , val in enumerate ( value ): tmp = Convert ( val , self . unit [ i ][ j ], unit_array [ j ]) out . append ( tmp ) arr2d . append ( out ) return arr2d # 1D case arr = [] for i , value in enumerate ( self . low ): assert isinstance ( value , float ) in_units = self . unit [ i ] assert isinstance ( in_units , ( str , enum . Enum )) out_units = convert [ i ] assert isinstance ( out_units , ( str , enum . Enum )) tmp = Convert ( value , in_units , out_units ) arr . append ( tmp ) return arr","title":"min()"},{"location":"reference/libraries/property/#corl.libraries.property.BoxProp.prop_validator","text":"Validate Source code in corl/libraries/property.py @root_validator def prop_validator ( cls , values ): \"\"\"Validate\"\"\" if not all ( k in values for k in ( 'low' , 'high' , 'unit' )): # Something went wrong, return return values # Broadcast space as needed space = gym . spaces . Box ( low = np . array ( values [ 'low' ]) . astype ( values [ 'dtype' ]), high = np . array ( values [ 'high' ]) . astype ( values [ 'dtype' ]), dtype = values [ 'dtype' ], shape = values [ 'shape' ] ) values [ 'low' ] = space . low . tolist () values [ 'high' ] = space . high . tolist () # Validate dimensions # 1D case assert len ( values [ 'low' ]) == len ( values [ 'high' ]), \"low and high different length\" assert len ( values [ 'low' ]) == len ( values [ 'unit' ]), \"low and unit different length\" if len ( values [ 'low' ]) > 0 : if isinstance ( values [ 'low' ][ 0 ], list ): # list of lists 2D case for i , _ in enumerate ( values [ 'low' ]): assert len ( values [ 'low' ][ i ]) == len ( values [ 'high' ][ i ]), \"low and high list elements different length\" assert len ( values [ 'low' ][ i ]) == len ( values [ 'unit' ][ i ]), \"low and unit list elements different length\" return values","title":"prop_validator()"},{"location":"reference/libraries/property/#corl.libraries.property.BoxProp.unit_validator","text":"Validate unit Source code in corl/libraries/property.py @validator ( 'unit' ) def unit_validator ( cls , v ): \"\"\"Validate unit\"\"\" for item in v : if not isinstance ( item , str ) and isinstance ( item , collections . abc . Sequence ): # list of lists 2D case for value in item : assert isinstance ( GetUnitFromStr ( value ), enum . Enum ), f ' { value } is not valid unit' else : # 1D case assert isinstance ( GetUnitFromStr ( item ), enum . Enum ), f ' { value } is not valid unit' return v","title":"unit_validator()"},{"location":"reference/libraries/property/#corl.libraries.property.DiscreteProp","text":"Represents the Discrete outside of RLLIB Source code in corl/libraries/property.py class DiscreteProp ( Prop ): \"\"\"Represents the Discrete outside of RLLIB \"\"\" n : int def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Discrete space \"\"\" return gym . spaces . Discrete ( self . n )","title":"DiscreteProp"},{"location":"reference/libraries/property/#corl.libraries.property.DiscreteProp.create_space","text":"Creates RLLIB Discrete space Source code in corl/libraries/property.py def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Discrete space \"\"\" return gym . spaces . Discrete ( self . n )","title":"create_space()"},{"location":"reference/libraries/property/#corl.libraries.property.MultiBinary","text":"Represents the multi binary outside of RLLIB Source code in corl/libraries/property.py class MultiBinary ( Prop ): \"\"\"Represents the multi binary outside of RLLIB \"\"\" n : int def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB MultiBinary space \"\"\" return gym . spaces . MultiBinary ( self . n )","title":"MultiBinary"},{"location":"reference/libraries/property/#corl.libraries.property.MultiBinary.create_space","text":"Creates RLLIB MultiBinary space Source code in corl/libraries/property.py def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB MultiBinary space \"\"\" return gym . spaces . MultiBinary ( self . n )","title":"create_space()"},{"location":"reference/libraries/property/#corl.libraries.property.Prop","text":"Represents the space prop outside of RLLIB Source code in corl/libraries/property.py class Prop ( BaseModel , abc . ABC ): \"\"\"Represents the space prop outside of RLLIB \"\"\" name : str description : str class Config : # pylint: disable=C0115, R0903 validate_all = True @abc . abstractclassmethod def create_space ( cls ) -> gym . spaces . Space : \"\"\" Creates RLLIB space \"\"\" ...","title":"Prop"},{"location":"reference/libraries/property/#corl.libraries.property.Prop.create_space","text":"Creates RLLIB space Source code in corl/libraries/property.py @abc . abstractclassmethod def create_space ( cls ) -> gym . spaces . Space : \"\"\" Creates RLLIB space \"\"\" ...","title":"create_space()"},{"location":"reference/libraries/property/#corl.libraries.property.RepeatedProp","text":"Represents the multi binary outside of RLLIB Source code in corl/libraries/property.py class RepeatedProp ( Prop ): \"\"\"Represents the multi binary outside of RLLIB \"\"\" max_len : int child_space : typing . Dict [ str , Prop ] def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Repeated space \"\"\" gym_child_space = gym . spaces . Dict ({ key : value . create_space () for key , value in self . child_space . items ()}) return Repeated ( child_space = gym_child_space , max_len = self . max_len )","title":"RepeatedProp"},{"location":"reference/libraries/property/#corl.libraries.property.RepeatedProp.create_space","text":"Creates RLLIB Repeated space Source code in corl/libraries/property.py def create_space ( self ) -> gym . spaces . Space : \"\"\" Creates RLLIB Repeated space \"\"\" gym_child_space = gym . spaces . Dict ({ key : value . create_space () for key , value in self . child_space . items ()}) return Repeated ( child_space = gym_child_space , max_len = self . max_len )","title":"create_space()"},{"location":"reference/libraries/rllib_setup_util/","text":"Auto code for rllib AutoRllibConfigSetup ( BaseModel ) pydantic-model \u00a4 Meta parameters for automatically defining rllib_config settings, these are used to override the following keys: num_cpus_for_driver num_workers num_cpus_per_worker num_gpus num_gpus_per_worker rollout_fragment_length train_batch_size sgd_minibatch_size Source code in corl/libraries/rllib_setup_util.py class AutoRllibConfigSetup ( BaseModel ): \"\"\"Meta parameters for automatically defining rllib_config settings, these are used to override the following keys: num_cpus_for_driver num_workers num_cpus_per_worker num_gpus num_gpus_per_worker rollout_fragment_length train_batch_size sgd_minibatch_size \"\"\" num_trials : int = 1 gpus_per_worker : bool = False sgd_minibatch_size_percentage : float = 0.1 ignore_hyper_threads : bool = True auto_configure_rllib_config ( rllib_config , auto_rllib_config_setup , ray_resources ) \u00a4 Optimize rllib_config parameters for trainer alg Source code in corl/libraries/rllib_setup_util.py def auto_configure_rllib_config ( rllib_config : typing . Dict [ str , typing . Any ], auto_rllib_config_setup : AutoRllibConfigSetup , ray_resources : dict ) -> None : \"\"\"Optimize rllib_config parameters for trainer alg\"\"\" print ( \"*\" * 50 + \"Auto Updates to RLLIB Settings (if not set)\" ) num_trials = auto_rllib_config_setup . num_trials sgd_minibatch_size_percentage = auto_rllib_config_setup . sgd_minibatch_size_percentage gpus_per_worker = auto_rllib_config_setup . gpus_per_worker # # note: Only allow gpu to be used if it is avail - This will override gpus_available = get_gpu_avail ( ray_resources ) cpus_available = get_cpu_avail ( auto_rllib_config_setup , ray_resources ) # # Note: this checks the cpus per trial on system if \"num_cpus_for_driver\" in rllib_config . keys (): num_cpus_for_driver = rllib_config [ \"num_cpus_for_driver\" ] else : num_cpus_for_driver = COMMON_CONFIG [ \"num_cpus_for_driver\" ] cpus_per_trial_available : float = int (( cpus_available - num_cpus_for_driver ) / num_trials ) if cpus_per_trial_available == 0 : raise RuntimeError ( f \"Not enough resourses for the number of trials (int(( { cpus_available } - { num_cpus_for_driver } ) / { num_trials } ))\" ) if \"num_cpus_per_worker\" in rllib_config . keys (): workers_per_arena = max ( 1 , int ( cpus_per_trial_available / rllib_config [ \"num_cpus_per_worker\" ])) else : workers_per_arena = max ( 1 , int ( cpus_per_trial_available / COMMON_CONFIG [ \"num_cpus_per_worker\" ])) if 'num_workers' not in rllib_config : rllib_config [ 'num_workers' ] = { \"grid_search\" : [ workers_per_arena ]} print ( f \"rllib_config['num_workers'] = { rllib_config [ 'num_workers' ] } \" ) else : rllib_config [ 'num_workers' ] = { \"grid_search\" : [ rllib_config [ 'num_workers' ]]} print ( f \"rllib_config['num_workers'] = { rllib_config [ 'num_workers' ] } -- no updates\" ) if gpus_per_worker : num_gpus = 0.0001 * gpus_available num_gpus_per_worker = ( gpus_available - num_gpus ) / workers_per_arena rllib_config [ 'num_gpus' ] = { \"grid_search\" : [ num_gpus ]} rllib_config [ 'num_gpus_per_worker' ] = { \"grid_search\" : [ num_gpus_per_worker ]} else : num_gpus = gpus_available / num_trials if 'num_gpus' not in rllib_config : rllib_config [ 'num_gpus' ] = { \"grid_search\" : [ num_gpus ]} print ( f \"rllib_config['num_gpus'] = { rllib_config [ 'num_gpus' ] } \" ) else : rllib_config [ 'num_gpus' ] = { \"grid_search\" : [ rllib_config [ 'num_gpus' ]]} print ( f \"rllib_config['num_gpus'] = { rllib_config [ 'num_gpus' ] } -- no updates\" ) update_rollout_fragment_length ( rllib_config ) update_train_batch_size ( rllib_config ) update_sgd_minibatch_size ( rllib_config , sgd_minibatch_size_percentage ) print ( \"*\" * 50 ) get_cpu_avail ( auto_rllib_config_setup , ray_resources ) \u00a4 get the cpu avail Parameters \u00a4 ray_resources : dict resources dict Returns \u00a4 float num cpu Source code in corl/libraries/rllib_setup_util.py def get_cpu_avail ( auto_rllib_config_setup , ray_resources : dict ) -> float : \"\"\"get the cpu avail Parameters ---------- ray_resources : dict resources dict Returns ------- float num cpu \"\"\" cpus_available : float = 1 if auto_rllib_config_setup . ignore_hyper_threads : cpus_available = ray_resources [ \"CPU\" ] / 2 else : cpus_available = ray_resources [ \"CPU\" ] return cpus_available get_gpu_avail ( ray_resources ) \u00a4 get the gpu avail Parameters \u00a4 ray_resources : dict resources dict Returns \u00a4 float num gpu Source code in corl/libraries/rllib_setup_util.py def get_gpu_avail ( ray_resources : dict ) -> float : \"\"\"get the gpu avail Parameters ---------- ray_resources : dict resources dict Returns ------- float num gpu \"\"\" if \"GPU\" in ray_resources . keys (): gpus_available = ray_resources [ \"GPU\" ] else : gpus_available = COMMON_CONFIG [ \"num_gpus\" ] return gpus_available update_rollout_fragment_length ( rllib_config ) \u00a4 Attempts to auto fill the rollout fragment length Parameters \u00a4 rllib_config : dict the current config Source code in corl/libraries/rllib_setup_util.py def update_rollout_fragment_length ( rllib_config : dict ) -> None : \"\"\"Attempts to auto fill the rollout fragment length Parameters ---------- rllib_config : dict the current config \"\"\" if \"horizon\" in rllib_config . keys (): temp_horizon = None if isinstance ( rllib_config [ 'horizon' ], dict ) and len ( rllib_config [ 'horizon' ][ \"grid_search\" ]) == 1 : temp_horizon = rllib_config [ 'horizon' ][ \"grid_search\" ] = 0 elif isinstance ( rllib_config [ 'horizon' ], int ): temp_horizon = rllib_config [ 'horizon' ] if \"rollout_fragment_length\" not in rllib_config . keys (): rllib_config [ 'rollout_fragment_length' ] = { \"grid_search\" : [ temp_horizon ]} print ( f \"rllib_config['rollout_fragment_length'] = { rllib_config [ 'rollout_fragment_length' ] } \" ) else : rllib_config [ 'rollout_fragment_length' ] = { \"grid_search\" : [ rllib_config [ 'rollout_fragment_length' ]]} print ( f \"rllib_config['rollout_fragment_length'] = { rllib_config [ 'rollout_fragment_length' ] } -- no updates\" ) update_sgd_minibatch_size ( rllib_config , sgd_minibatch_size_percentage ) \u00a4 Attempts to auto fill the sgd_minibatch_size Parameters \u00a4 rllib_config : dict the current config Source code in corl/libraries/rllib_setup_util.py def update_sgd_minibatch_size ( rllib_config : dict , sgd_minibatch_size_percentage : float ) -> None : \"\"\"Attempts to auto fill the sgd_minibatch_size Parameters ---------- rllib_config : dict the current config \"\"\" if \"sgd_minibatch_size\" not in rllib_config . keys (): rllib_config [ 'sgd_minibatch_size' ] = { \"grid_search\" : [ int ( rllib_config [ 'train_batch_size' ][ 'grid_search' ][ 0 ] * sgd_minibatch_size_percentage )] } print ( f \"rllib_config['sgd_minibatch_size'] = { rllib_config [ 'sgd_minibatch_size' ] } \" ) else : rllib_config [ 'sgd_minibatch_size' ] = { \"grid_search\" : [ rllib_config [ 'sgd_minibatch_size' ]]} print ( f \"rllib_config['sgd_minibatch_size'] = { rllib_config [ 'sgd_minibatch_size' ] } -- no updates\" ) update_train_batch_size ( rllib_config ) \u00a4 Attempts to auto fill the train batch size Parameters \u00a4 rllib_config : dict the current config Source code in corl/libraries/rllib_setup_util.py def update_train_batch_size ( rllib_config : dict ) -> None : \"\"\"Attempts to auto fill the train batch size Parameters ---------- rllib_config : dict the current config \"\"\" if \"train_batch_size\" not in rllib_config . keys (): rllib_config [ 'train_batch_size' ] = { \"grid_search\" : [ int ( rllib_config [ 'num_workers' ][ 'grid_search' ][ 0 ] * rllib_config [ 'rollout_fragment_length' ][ 'grid_search' ][ 0 ])] } print ( f \"rllib_config['train_batch_size'] = { rllib_config [ 'train_batch_size' ] } \" ) else : rllib_config [ 'train_batch_size' ] = { \"grid_search\" : [ rllib_config [ 'train_batch_size' ]]} print ( f \"rllib_config['train_batch_size'] = { rllib_config [ 'train_batch_size' ] } -- no updates\" )","title":"Rllib setup util"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.AutoRllibConfigSetup","text":"Meta parameters for automatically defining rllib_config settings, these are used to override the following keys: num_cpus_for_driver num_workers num_cpus_per_worker num_gpus num_gpus_per_worker rollout_fragment_length train_batch_size sgd_minibatch_size Source code in corl/libraries/rllib_setup_util.py class AutoRllibConfigSetup ( BaseModel ): \"\"\"Meta parameters for automatically defining rllib_config settings, these are used to override the following keys: num_cpus_for_driver num_workers num_cpus_per_worker num_gpus num_gpus_per_worker rollout_fragment_length train_batch_size sgd_minibatch_size \"\"\" num_trials : int = 1 gpus_per_worker : bool = False sgd_minibatch_size_percentage : float = 0.1 ignore_hyper_threads : bool = True","title":"AutoRllibConfigSetup"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.auto_configure_rllib_config","text":"Optimize rllib_config parameters for trainer alg Source code in corl/libraries/rllib_setup_util.py def auto_configure_rllib_config ( rllib_config : typing . Dict [ str , typing . Any ], auto_rllib_config_setup : AutoRllibConfigSetup , ray_resources : dict ) -> None : \"\"\"Optimize rllib_config parameters for trainer alg\"\"\" print ( \"*\" * 50 + \"Auto Updates to RLLIB Settings (if not set)\" ) num_trials = auto_rllib_config_setup . num_trials sgd_minibatch_size_percentage = auto_rllib_config_setup . sgd_minibatch_size_percentage gpus_per_worker = auto_rllib_config_setup . gpus_per_worker # # note: Only allow gpu to be used if it is avail - This will override gpus_available = get_gpu_avail ( ray_resources ) cpus_available = get_cpu_avail ( auto_rllib_config_setup , ray_resources ) # # Note: this checks the cpus per trial on system if \"num_cpus_for_driver\" in rllib_config . keys (): num_cpus_for_driver = rllib_config [ \"num_cpus_for_driver\" ] else : num_cpus_for_driver = COMMON_CONFIG [ \"num_cpus_for_driver\" ] cpus_per_trial_available : float = int (( cpus_available - num_cpus_for_driver ) / num_trials ) if cpus_per_trial_available == 0 : raise RuntimeError ( f \"Not enough resourses for the number of trials (int(( { cpus_available } - { num_cpus_for_driver } ) / { num_trials } ))\" ) if \"num_cpus_per_worker\" in rllib_config . keys (): workers_per_arena = max ( 1 , int ( cpus_per_trial_available / rllib_config [ \"num_cpus_per_worker\" ])) else : workers_per_arena = max ( 1 , int ( cpus_per_trial_available / COMMON_CONFIG [ \"num_cpus_per_worker\" ])) if 'num_workers' not in rllib_config : rllib_config [ 'num_workers' ] = { \"grid_search\" : [ workers_per_arena ]} print ( f \"rllib_config['num_workers'] = { rllib_config [ 'num_workers' ] } \" ) else : rllib_config [ 'num_workers' ] = { \"grid_search\" : [ rllib_config [ 'num_workers' ]]} print ( f \"rllib_config['num_workers'] = { rllib_config [ 'num_workers' ] } -- no updates\" ) if gpus_per_worker : num_gpus = 0.0001 * gpus_available num_gpus_per_worker = ( gpus_available - num_gpus ) / workers_per_arena rllib_config [ 'num_gpus' ] = { \"grid_search\" : [ num_gpus ]} rllib_config [ 'num_gpus_per_worker' ] = { \"grid_search\" : [ num_gpus_per_worker ]} else : num_gpus = gpus_available / num_trials if 'num_gpus' not in rllib_config : rllib_config [ 'num_gpus' ] = { \"grid_search\" : [ num_gpus ]} print ( f \"rllib_config['num_gpus'] = { rllib_config [ 'num_gpus' ] } \" ) else : rllib_config [ 'num_gpus' ] = { \"grid_search\" : [ rllib_config [ 'num_gpus' ]]} print ( f \"rllib_config['num_gpus'] = { rllib_config [ 'num_gpus' ] } -- no updates\" ) update_rollout_fragment_length ( rllib_config ) update_train_batch_size ( rllib_config ) update_sgd_minibatch_size ( rllib_config , sgd_minibatch_size_percentage ) print ( \"*\" * 50 )","title":"auto_configure_rllib_config()"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.get_cpu_avail","text":"get the cpu avail","title":"get_cpu_avail()"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.get_cpu_avail--parameters","text":"ray_resources : dict resources dict","title":"Parameters"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.get_cpu_avail--returns","text":"float num cpu Source code in corl/libraries/rllib_setup_util.py def get_cpu_avail ( auto_rllib_config_setup , ray_resources : dict ) -> float : \"\"\"get the cpu avail Parameters ---------- ray_resources : dict resources dict Returns ------- float num cpu \"\"\" cpus_available : float = 1 if auto_rllib_config_setup . ignore_hyper_threads : cpus_available = ray_resources [ \"CPU\" ] / 2 else : cpus_available = ray_resources [ \"CPU\" ] return cpus_available","title":"Returns"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.get_gpu_avail","text":"get the gpu avail","title":"get_gpu_avail()"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.get_gpu_avail--parameters","text":"ray_resources : dict resources dict","title":"Parameters"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.get_gpu_avail--returns","text":"float num gpu Source code in corl/libraries/rllib_setup_util.py def get_gpu_avail ( ray_resources : dict ) -> float : \"\"\"get the gpu avail Parameters ---------- ray_resources : dict resources dict Returns ------- float num gpu \"\"\" if \"GPU\" in ray_resources . keys (): gpus_available = ray_resources [ \"GPU\" ] else : gpus_available = COMMON_CONFIG [ \"num_gpus\" ] return gpus_available","title":"Returns"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.update_rollout_fragment_length","text":"Attempts to auto fill the rollout fragment length","title":"update_rollout_fragment_length()"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.update_rollout_fragment_length--parameters","text":"rllib_config : dict the current config Source code in corl/libraries/rllib_setup_util.py def update_rollout_fragment_length ( rllib_config : dict ) -> None : \"\"\"Attempts to auto fill the rollout fragment length Parameters ---------- rllib_config : dict the current config \"\"\" if \"horizon\" in rllib_config . keys (): temp_horizon = None if isinstance ( rllib_config [ 'horizon' ], dict ) and len ( rllib_config [ 'horizon' ][ \"grid_search\" ]) == 1 : temp_horizon = rllib_config [ 'horizon' ][ \"grid_search\" ] = 0 elif isinstance ( rllib_config [ 'horizon' ], int ): temp_horizon = rllib_config [ 'horizon' ] if \"rollout_fragment_length\" not in rllib_config . keys (): rllib_config [ 'rollout_fragment_length' ] = { \"grid_search\" : [ temp_horizon ]} print ( f \"rllib_config['rollout_fragment_length'] = { rllib_config [ 'rollout_fragment_length' ] } \" ) else : rllib_config [ 'rollout_fragment_length' ] = { \"grid_search\" : [ rllib_config [ 'rollout_fragment_length' ]]} print ( f \"rllib_config['rollout_fragment_length'] = { rllib_config [ 'rollout_fragment_length' ] } -- no updates\" )","title":"Parameters"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.update_sgd_minibatch_size","text":"Attempts to auto fill the sgd_minibatch_size","title":"update_sgd_minibatch_size()"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.update_sgd_minibatch_size--parameters","text":"rllib_config : dict the current config Source code in corl/libraries/rllib_setup_util.py def update_sgd_minibatch_size ( rllib_config : dict , sgd_minibatch_size_percentage : float ) -> None : \"\"\"Attempts to auto fill the sgd_minibatch_size Parameters ---------- rllib_config : dict the current config \"\"\" if \"sgd_minibatch_size\" not in rllib_config . keys (): rllib_config [ 'sgd_minibatch_size' ] = { \"grid_search\" : [ int ( rllib_config [ 'train_batch_size' ][ 'grid_search' ][ 0 ] * sgd_minibatch_size_percentage )] } print ( f \"rllib_config['sgd_minibatch_size'] = { rllib_config [ 'sgd_minibatch_size' ] } \" ) else : rllib_config [ 'sgd_minibatch_size' ] = { \"grid_search\" : [ rllib_config [ 'sgd_minibatch_size' ]]} print ( f \"rllib_config['sgd_minibatch_size'] = { rllib_config [ 'sgd_minibatch_size' ] } -- no updates\" )","title":"Parameters"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.update_train_batch_size","text":"Attempts to auto fill the train batch size","title":"update_train_batch_size()"},{"location":"reference/libraries/rllib_setup_util/#corl.libraries.rllib_setup_util.update_train_batch_size--parameters","text":"rllib_config : dict the current config Source code in corl/libraries/rllib_setup_util.py def update_train_batch_size ( rllib_config : dict ) -> None : \"\"\"Attempts to auto fill the train batch size Parameters ---------- rllib_config : dict the current config \"\"\" if \"train_batch_size\" not in rllib_config . keys (): rllib_config [ 'train_batch_size' ] = { \"grid_search\" : [ int ( rllib_config [ 'num_workers' ][ 'grid_search' ][ 0 ] * rllib_config [ 'rollout_fragment_length' ][ 'grid_search' ][ 0 ])] } print ( f \"rllib_config['train_batch_size'] = { rllib_config [ 'train_batch_size' ] } \" ) else : rllib_config [ 'train_batch_size' ] = { \"grid_search\" : [ rllib_config [ 'train_batch_size' ]]} print ( f \"rllib_config['train_batch_size'] = { rllib_config [ 'train_batch_size' ] } -- no updates\" )","title":"Parameters"},{"location":"reference/libraries/state_dict/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. state Dict - Leverage https://github.com/ramazanpolat/StateDict - StateDict ( OrderedDict ) \u00a4 [summary] Parameters \u00a4 OrderedDict : [type] [description] Source code in corl/libraries/state_dict.py class StateDict ( OrderedDict ): \"\"\"[summary] Parameters ---------- OrderedDict : [type] [description] \"\"\" def __init__ ( self , * args , ** kwargs ): self . _recurse = kwargs . pop ( \"recurse\" , True ) super () . __init__ ( * args , ** kwargs ) if self . _recurse : super () . __init__ ( StateDict . recursive_attrdict ( self )) @staticmethod def recursive_attrdict ( obj ): \"\"\"Walks a simple data structure, converting dictionary to StateDict. Supports lists, tuples, and dictionaries. \"\"\" ret = obj if isinstance ( obj , dict ): if issubclass ( type ( obj ), StateDict ): for ( k , v ) in obj . items (): obj [ k ] = StateDict . recursive_attrdict ( v ) ret = obj else : ret = StateDict ( dict (( str ( k ), StateDict . recursive_attrdict ( v )) for ( k , v ) in obj . items ()), recurse = False , ) elif isinstance ( obj , list ): ret = list ( StateDict . recursive_attrdict ( i ) for i in obj ) elif isinstance ( obj , tuple ): ret = tuple ( StateDict . recursive_attrdict ( i ) for i in obj ) return ret def __setattr__ ( self , name , value ): if isinstance ( name , str ) and name [ 0 ] != \"_\" : self . __setitem__ ( name , value ) super () . __setattr__ ( name , value ) def __getattr__ ( self , name ): _item = self . get ( name ) # __getitem__(key) if key in self else None if name not in self and _item is None : # attempt to ignore builtins and private attributes if isinstance ( name , str ) and name [ 0 ] != \"_\" : raise KeyError ( f \" { name } \" ) return _item def __deepcopy__ ( self , memo ): return StateDict ( copy . deepcopy ( dict ( self )), recurse = self . _recurse ) def __delattr__ ( self , name ): self . __delitem__ ( name ) super () . __delattr__ ( name ) def __dir__ ( self ): return list ( super () . __dir__ ()) + [ str ( k ) for k in self . keys ()] def keys ( self ): return OrderedDict ( sorted ( super () . items ())) . keys () def values ( self ): return OrderedDict ( sorted ( super () . items ())) . values () def items ( self ): return OrderedDict ( sorted ( super () . items ())) . items () @staticmethod def merge ( dl ): \"\"\"[summary] Parameters ---------- dl : [type] [description] Returns ------- [type] [description] \"\"\" r = {} for d in dl : r = StateDict ( sorted ( StateDict . merge_two ( r , d ))) return StateDict ( sorted ( r . items ())) @staticmethod def stack_values ( dl ): \"\"\"[summary] Parameters ---------- dl : [type] [description] Returns ------- [type] [description] \"\"\" ak = set () . union ( * dl ) r = { k : [] for k in ak } for d in dl : for k , v in d . items (): r [ k ] . append ( v ) return r @staticmethod def merge_two ( d1 , d2 , replace = True ): \"\"\"Merges two dicts Parameters ---------- d1 : [type] [description] d2 : [type] [description] replace : bool, optional [description], by default True Yields ------ [type] [description] Raises ------ an [description] \"\"\" for k in set ( d1 . keys ()) . union ( d2 . keys ()): if k in d1 and k in d2 : if isinstance ( d1 [ k ], dict ) and isinstance ( d2 [ k ], dict ): yield ( k , StateDict ( sorted ( StateDict . merge_two ( d1 [ k ], d2 [ k ])))) elif replace : # replace the value on the first with the value in the second yield ( k , d2 [ k ]) else : # maybe raise an exception, but default is to proceed without whining pass elif k in d1 : yield ( k , d1 [ k ]) else : yield ( k , d2 [ k ]) def to_dict ( self ) -> dict : \"\"\"Converts to a dictionary Returns ------- dict the dict form \"\"\" temp = copy . deepcopy ( self ) for k , v in temp . items (): if isinstance ( v , StateDict ): temp [ k ] = v . to_dict () return temp items ( self ) \u00a4 D.items() -> a set-like object providing a view on D's items Source code in corl/libraries/state_dict.py def items ( self ): return OrderedDict ( sorted ( super () . items ())) . items () keys ( self ) \u00a4 D.keys() -> a set-like object providing a view on D's keys Source code in corl/libraries/state_dict.py def keys ( self ): return OrderedDict ( sorted ( super () . items ())) . keys () merge ( dl ) staticmethod \u00a4 [summary] Parameters \u00a4 dl : [type] [description] Returns \u00a4 [type] [description] Source code in corl/libraries/state_dict.py @staticmethod def merge ( dl ): \"\"\"[summary] Parameters ---------- dl : [type] [description] Returns ------- [type] [description] \"\"\" r = {} for d in dl : r = StateDict ( sorted ( StateDict . merge_two ( r , d ))) return StateDict ( sorted ( r . items ())) merge_two ( d1 , d2 , replace = True ) staticmethod \u00a4 Merges two dicts Parameters \u00a4 d1 : [type] [description] d2 : [type] [description] replace : bool, optional [description], by default True Yields \u00a4 [type] [description] Raises \u00a4 an [description] Source code in corl/libraries/state_dict.py @staticmethod def merge_two ( d1 , d2 , replace = True ): \"\"\"Merges two dicts Parameters ---------- d1 : [type] [description] d2 : [type] [description] replace : bool, optional [description], by default True Yields ------ [type] [description] Raises ------ an [description] \"\"\" for k in set ( d1 . keys ()) . union ( d2 . keys ()): if k in d1 and k in d2 : if isinstance ( d1 [ k ], dict ) and isinstance ( d2 [ k ], dict ): yield ( k , StateDict ( sorted ( StateDict . merge_two ( d1 [ k ], d2 [ k ])))) elif replace : # replace the value on the first with the value in the second yield ( k , d2 [ k ]) else : # maybe raise an exception, but default is to proceed without whining pass elif k in d1 : yield ( k , d1 [ k ]) else : yield ( k , d2 [ k ]) recursive_attrdict ( obj ) staticmethod \u00a4 Walks a simple data structure, converting dictionary to StateDict. Supports lists, tuples, and dictionaries. Source code in corl/libraries/state_dict.py @staticmethod def recursive_attrdict ( obj ): \"\"\"Walks a simple data structure, converting dictionary to StateDict. Supports lists, tuples, and dictionaries. \"\"\" ret = obj if isinstance ( obj , dict ): if issubclass ( type ( obj ), StateDict ): for ( k , v ) in obj . items (): obj [ k ] = StateDict . recursive_attrdict ( v ) ret = obj else : ret = StateDict ( dict (( str ( k ), StateDict . recursive_attrdict ( v )) for ( k , v ) in obj . items ()), recurse = False , ) elif isinstance ( obj , list ): ret = list ( StateDict . recursive_attrdict ( i ) for i in obj ) elif isinstance ( obj , tuple ): ret = tuple ( StateDict . recursive_attrdict ( i ) for i in obj ) return ret stack_values ( dl ) staticmethod \u00a4 [summary] Parameters \u00a4 dl : [type] [description] Returns \u00a4 [type] [description] Source code in corl/libraries/state_dict.py @staticmethod def stack_values ( dl ): \"\"\"[summary] Parameters ---------- dl : [type] [description] Returns ------- [type] [description] \"\"\" ak = set () . union ( * dl ) r = { k : [] for k in ak } for d in dl : for k , v in d . items (): r [ k ] . append ( v ) return r to_dict ( self ) \u00a4 Converts to a dictionary Returns \u00a4 dict the dict form Source code in corl/libraries/state_dict.py def to_dict ( self ) -> dict : \"\"\"Converts to a dictionary Returns ------- dict the dict form \"\"\" temp = copy . deepcopy ( self ) for k , v in temp . items (): if isinstance ( v , StateDict ): temp [ k ] = v . to_dict () return temp values ( self ) \u00a4 D.values() -> an object providing a view on D's values Source code in corl/libraries/state_dict.py def values ( self ): return OrderedDict ( sorted ( super () . items ())) . values ()","title":"State dict"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict","text":"[summary]","title":"StateDict"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict--parameters","text":"OrderedDict : [type] [description] Source code in corl/libraries/state_dict.py class StateDict ( OrderedDict ): \"\"\"[summary] Parameters ---------- OrderedDict : [type] [description] \"\"\" def __init__ ( self , * args , ** kwargs ): self . _recurse = kwargs . pop ( \"recurse\" , True ) super () . __init__ ( * args , ** kwargs ) if self . _recurse : super () . __init__ ( StateDict . recursive_attrdict ( self )) @staticmethod def recursive_attrdict ( obj ): \"\"\"Walks a simple data structure, converting dictionary to StateDict. Supports lists, tuples, and dictionaries. \"\"\" ret = obj if isinstance ( obj , dict ): if issubclass ( type ( obj ), StateDict ): for ( k , v ) in obj . items (): obj [ k ] = StateDict . recursive_attrdict ( v ) ret = obj else : ret = StateDict ( dict (( str ( k ), StateDict . recursive_attrdict ( v )) for ( k , v ) in obj . items ()), recurse = False , ) elif isinstance ( obj , list ): ret = list ( StateDict . recursive_attrdict ( i ) for i in obj ) elif isinstance ( obj , tuple ): ret = tuple ( StateDict . recursive_attrdict ( i ) for i in obj ) return ret def __setattr__ ( self , name , value ): if isinstance ( name , str ) and name [ 0 ] != \"_\" : self . __setitem__ ( name , value ) super () . __setattr__ ( name , value ) def __getattr__ ( self , name ): _item = self . get ( name ) # __getitem__(key) if key in self else None if name not in self and _item is None : # attempt to ignore builtins and private attributes if isinstance ( name , str ) and name [ 0 ] != \"_\" : raise KeyError ( f \" { name } \" ) return _item def __deepcopy__ ( self , memo ): return StateDict ( copy . deepcopy ( dict ( self )), recurse = self . _recurse ) def __delattr__ ( self , name ): self . __delitem__ ( name ) super () . __delattr__ ( name ) def __dir__ ( self ): return list ( super () . __dir__ ()) + [ str ( k ) for k in self . keys ()] def keys ( self ): return OrderedDict ( sorted ( super () . items ())) . keys () def values ( self ): return OrderedDict ( sorted ( super () . items ())) . values () def items ( self ): return OrderedDict ( sorted ( super () . items ())) . items () @staticmethod def merge ( dl ): \"\"\"[summary] Parameters ---------- dl : [type] [description] Returns ------- [type] [description] \"\"\" r = {} for d in dl : r = StateDict ( sorted ( StateDict . merge_two ( r , d ))) return StateDict ( sorted ( r . items ())) @staticmethod def stack_values ( dl ): \"\"\"[summary] Parameters ---------- dl : [type] [description] Returns ------- [type] [description] \"\"\" ak = set () . union ( * dl ) r = { k : [] for k in ak } for d in dl : for k , v in d . items (): r [ k ] . append ( v ) return r @staticmethod def merge_two ( d1 , d2 , replace = True ): \"\"\"Merges two dicts Parameters ---------- d1 : [type] [description] d2 : [type] [description] replace : bool, optional [description], by default True Yields ------ [type] [description] Raises ------ an [description] \"\"\" for k in set ( d1 . keys ()) . union ( d2 . keys ()): if k in d1 and k in d2 : if isinstance ( d1 [ k ], dict ) and isinstance ( d2 [ k ], dict ): yield ( k , StateDict ( sorted ( StateDict . merge_two ( d1 [ k ], d2 [ k ])))) elif replace : # replace the value on the first with the value in the second yield ( k , d2 [ k ]) else : # maybe raise an exception, but default is to proceed without whining pass elif k in d1 : yield ( k , d1 [ k ]) else : yield ( k , d2 [ k ]) def to_dict ( self ) -> dict : \"\"\"Converts to a dictionary Returns ------- dict the dict form \"\"\" temp = copy . deepcopy ( self ) for k , v in temp . items (): if isinstance ( v , StateDict ): temp [ k ] = v . to_dict () return temp","title":"Parameters"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.items","text":"D.items() -> a set-like object providing a view on D's items Source code in corl/libraries/state_dict.py def items ( self ): return OrderedDict ( sorted ( super () . items ())) . items ()","title":"items()"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.keys","text":"D.keys() -> a set-like object providing a view on D's keys Source code in corl/libraries/state_dict.py def keys ( self ): return OrderedDict ( sorted ( super () . items ())) . keys ()","title":"keys()"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.merge","text":"[summary]","title":"merge()"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.merge--parameters","text":"dl : [type] [description]","title":"Parameters"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.merge--returns","text":"[type] [description] Source code in corl/libraries/state_dict.py @staticmethod def merge ( dl ): \"\"\"[summary] Parameters ---------- dl : [type] [description] Returns ------- [type] [description] \"\"\" r = {} for d in dl : r = StateDict ( sorted ( StateDict . merge_two ( r , d ))) return StateDict ( sorted ( r . items ()))","title":"Returns"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.merge_two","text":"Merges two dicts","title":"merge_two()"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.merge_two--parameters","text":"d1 : [type] [description] d2 : [type] [description] replace : bool, optional [description], by default True","title":"Parameters"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.merge_two--yields","text":"[type] [description]","title":"Yields"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.merge_two--raises","text":"an [description] Source code in corl/libraries/state_dict.py @staticmethod def merge_two ( d1 , d2 , replace = True ): \"\"\"Merges two dicts Parameters ---------- d1 : [type] [description] d2 : [type] [description] replace : bool, optional [description], by default True Yields ------ [type] [description] Raises ------ an [description] \"\"\" for k in set ( d1 . keys ()) . union ( d2 . keys ()): if k in d1 and k in d2 : if isinstance ( d1 [ k ], dict ) and isinstance ( d2 [ k ], dict ): yield ( k , StateDict ( sorted ( StateDict . merge_two ( d1 [ k ], d2 [ k ])))) elif replace : # replace the value on the first with the value in the second yield ( k , d2 [ k ]) else : # maybe raise an exception, but default is to proceed without whining pass elif k in d1 : yield ( k , d1 [ k ]) else : yield ( k , d2 [ k ])","title":"Raises"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.recursive_attrdict","text":"Walks a simple data structure, converting dictionary to StateDict. Supports lists, tuples, and dictionaries. Source code in corl/libraries/state_dict.py @staticmethod def recursive_attrdict ( obj ): \"\"\"Walks a simple data structure, converting dictionary to StateDict. Supports lists, tuples, and dictionaries. \"\"\" ret = obj if isinstance ( obj , dict ): if issubclass ( type ( obj ), StateDict ): for ( k , v ) in obj . items (): obj [ k ] = StateDict . recursive_attrdict ( v ) ret = obj else : ret = StateDict ( dict (( str ( k ), StateDict . recursive_attrdict ( v )) for ( k , v ) in obj . items ()), recurse = False , ) elif isinstance ( obj , list ): ret = list ( StateDict . recursive_attrdict ( i ) for i in obj ) elif isinstance ( obj , tuple ): ret = tuple ( StateDict . recursive_attrdict ( i ) for i in obj ) return ret","title":"recursive_attrdict()"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.stack_values","text":"[summary]","title":"stack_values()"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.stack_values--parameters","text":"dl : [type] [description]","title":"Parameters"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.stack_values--returns","text":"[type] [description] Source code in corl/libraries/state_dict.py @staticmethod def stack_values ( dl ): \"\"\"[summary] Parameters ---------- dl : [type] [description] Returns ------- [type] [description] \"\"\" ak = set () . union ( * dl ) r = { k : [] for k in ak } for d in dl : for k , v in d . items (): r [ k ] . append ( v ) return r","title":"Returns"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.to_dict","text":"Converts to a dictionary","title":"to_dict()"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.to_dict--returns","text":"dict the dict form Source code in corl/libraries/state_dict.py def to_dict ( self ) -> dict : \"\"\"Converts to a dictionary Returns ------- dict the dict form \"\"\" temp = copy . deepcopy ( self ) for k , v in temp . items (): if isinstance ( v , StateDict ): temp [ k ] = v . to_dict () return temp","title":"Returns"},{"location":"reference/libraries/state_dict/#corl.libraries.state_dict.StateDict.values","text":"D.values() -> an object providing a view on D's values Source code in corl/libraries/state_dict.py def values ( self ): return OrderedDict ( sorted ( super () . items ())) . values ()","title":"values()"},{"location":"reference/libraries/units/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Module containing unit dimensions and functions to convert between units Acceleration ( Enum ) \u00a4 Acceleration Source code in corl/libraries/units.py class Acceleration ( enum . Enum ): \"\"\"Acceleration \"\"\" knots_per_sec = ( 1.0 , [ \"knots/s\" ]) meter_per_sec_2 = ( 0.51444563 , [ \"m/s^2\" ]) feet_per_sec_2 = ( 1.68780986 , [ \"ft/s^2\" ]) standard_gravity = ( 0.05245885496 , [ \"g\" , \"G\" , \"gravity\" , \"standard_gravity\" ]) DEFAULT = knots_per_sec Angle ( Enum ) \u00a4 Angle dimension Source code in corl/libraries/units.py class Angle ( enum . Enum ): \"\"\"Angle dimension \"\"\" Degree = ( 180.0 , [ \"deg\" , \"degree\" , \"degrees\" ]) Rad = ( math . pi , [ \"rad\" , \"radian\" , \"radians\" ]) DEFAULT = Rad AngularSpeed ( Enum ) \u00a4 Angular speed dimension Source code in corl/libraries/units.py class AngularSpeed ( enum . Enum ): \"\"\"Angular speed dimension \"\"\" radians_per_sec = ( 1 , [ \"rad/s\" , \"r/s\" ]) degrees_per_sec = ( 57.2958 , [ \"deg/s\" ]) DEFAULT = radians_per_sec CalibratedSpeed ( Enum ) \u00a4 Speed dimension for calibrated airspeed Source code in corl/libraries/units.py class CalibratedSpeed ( enum . Enum ): \"\"\"Speed dimension for calibrated airspeed \"\"\" Meter_per_Sec = ( Speed . Meter_per_Sec . value [ 0 ], [ \"mpscas\" ]) Knots = ( Speed . Knots . value [ 0 ], [ \"kcas\" ]) Feet_per_Min = ( Speed . Feet_per_Min . value [ 0 ], [ \"fpmcas\" ]) Feet_per_Sec = ( Speed . Feet_per_Sec . value [ 0 ], [ \"fpscas\" ]) DEFAULT = Knots Distance ( Enum ) \u00a4 Distance dimension Source code in corl/libraries/units.py class Distance ( enum . Enum ): \"\"\"Distance dimension \"\"\" Meter = ( 1.0 , [ \"m\" , \"meter\" , \"meters\" ]) Feet = ( 3.28084 , [ \"ft\" , \"feet\" ]) Nautical_Mile = ( 1 / 1852 , [ \"nm\" ]) DEFAULT = Meter Force ( Enum ) \u00a4 Force Source code in corl/libraries/units.py class Force ( enum . Enum ): \"\"\"Force \"\"\" Newton = ( 1.0 , [ \"N\" , \"newton\" , \"newtons\" ]) PoundForce = ( 0.22481 , [ \"pound-force\" ]) DEFAULT = Newton IndicatedSpeed ( Enum ) \u00a4 Speed dimension for indicated airspeed Source code in corl/libraries/units.py class IndicatedSpeed ( enum . Enum ): \"\"\"Speed dimension for indicated airspeed \"\"\" Meter_per_Sec = ( Speed . Meter_per_Sec . value [ 0 ], [ \"mpsias\" ]) Knots = ( Speed . Knots . value [ 0 ], [ \"kias\" ]) Feet_per_Min = ( Speed . Feet_per_Min . value [ 0 ], [ \"fpmias\" ]) Feet_per_Sec = ( Speed . Feet_per_Sec . value [ 0 ], [ \"fpsias\" ]) DEFAULT = Knots MachSpeed ( Enum ) \u00a4 Mach speed dimension Source code in corl/libraries/units.py class MachSpeed ( enum . Enum ): \"\"\"Mach speed dimension \"\"\" Mach = ( 1.0 , [ \"mach\" , \"Ma\" ]) DEFAULT = Mach NoneUnitType ( Enum ) \u00a4 None Source code in corl/libraries/units.py class NoneUnitType ( enum . Enum ): \"\"\"None \"\"\" NoneUnit = ( 1.0 , [ \"N/A\" , \"None\" , \"none\" ]) DEFAULT = NoneUnit PartOfWhole ( Enum ) \u00a4 PartOfWhole Source code in corl/libraries/units.py class PartOfWhole ( enum . Enum ): \"\"\"PartOfWhole \"\"\" Fraction = ( 1.0 , [ \"fraction\" ]) Percent = ( 100.0 , [ \"percent\" ]) DEFAULT = Fraction Speed ( Enum ) \u00a4 Speed dimension Source code in corl/libraries/units.py class Speed ( enum . Enum ): \"\"\"Speed dimension \"\"\" Meter_per_Sec = ( 1 , [ \"m/s\" , \"m_s\" ]) Knots = ( 1.94384 , [ \"knot\" , \"knots\" , \"kts\" ]) Feet_per_Min = ( 196.8504 , [ \"ft/min\" , \"feet/min\" ]) Feet_per_Sec = ( 3.28084 , [ \"ft/s\" , \"feet/second\" , \"feet/sec\" ]) DEFAULT = Knots Time ( Enum ) \u00a4 Time dimension Source code in corl/libraries/units.py class Time ( enum . Enum ): \"\"\"Time dimension \"\"\" Second = ( 1.0 , [ \"s\" , \"sec\" , \"second\" , \"seconds\" ]) Hour = ( 0.0002777778 , [ \"hr\" , \"hour\" ]) DEFAULT = Second TrueSpeed ( Enum ) \u00a4 Speed dimension for true airspeed Source code in corl/libraries/units.py class TrueSpeed ( enum . Enum ): \"\"\"Speed dimension for true airspeed \"\"\" Meter_per_Sec = ( Speed . Meter_per_Sec . value [ 0 ], [ \"mpstas\" ]) Knots = ( Speed . Knots . value [ 0 ], [ \"ktas\" ]) Feet_per_Min = ( Speed . Feet_per_Min . value [ 0 ], [ \"fpmtas\" ]) Feet_per_Sec = ( Speed . Feet_per_Sec . value [ 0 ], [ \"fpstas\" ]) DEFAULT = Knots ValueWithUnits ( BaseModel ) pydantic-model \u00a4 Wrap a value together with its units Attributes \u00a4 value : typing.Any The value units : typing.Union[None, enum.Enum] The units Source code in corl/libraries/units.py class ValueWithUnits ( BaseModel ): \"\"\"Wrap a value together with its units Attributes ---------- value : typing.Any The value units : typing.Union[None, enum.Enum] The units \"\"\" value : typing . Union [ StrictInt , StrictFloat , bool , str ] units : typing . Optional [ enum . Enum ] @validator ( 'value' , pre = True ) def value_validator ( cls , v ): \"\"\"Validate value\"\"\" # automatically convert numpy types to native types when needed # tolist will convert scalar or array to python native type # tolist returns a single value (the scalar) when calling it on a single value return getattr ( v , \"tolist\" , lambda : v )() @validator ( 'units' , pre = True ) def convert_string_units ( cls , v ): \"\"\"Build units out of string\"\"\" if isinstance ( v , str ): return GetUnitFromStr ( v ) if v is None : return NoneUnitType . NoneUnit return v @root_validator def str_no_units ( cls , values ): \"\"\"Confirm that string values have no units\"\"\" if isinstance ( values . get ( 'value' ), str ): assert values . get ( 'units' ) is NoneUnitType . NoneUnit , 'String values must have unit None' return values def convert_to ( self , to_unit : typing . Union [ None , str , enum . Enum ]) -> float : \"\"\"Convert this value to another unit Parameters ---------- to_unit : typing.Union[None, str, enum.Enum] unit type to convert to Returns ------- numbers.Real converted value \"\"\" return Convert ( value = self . value , from_unit = self . units , to_unit = to_unit ) # type: ignore class PrincipalValueNormalization ( enum . Enum ): \"\"\"Enumeration of the type of Principal Value normalization desired. Enumeration Values ------------------ Positive Normalize to be within the range [0, T], for periodicity T. Centered Normalize to be within the range [-T/2, T/2], for periodicity T. \"\"\" Positive = enum . auto () Centered = enum . auto () def as_units ( self , units : typing . Union [ None , str , enum . Enum ]) -> typing . Any : \"\"\"View the number in some other units Parameters ---------- units : typing.Union[None, str, enum.Enum] The desired units Returns ------- float The value in the desired units \"\"\" if units is None and self . units is None : return self . value if units is None or self . units is None : raise RuntimeError ( f 'Incompatible units involving None: { units } <> { self . units } ' ) if units == self . units : return self . value assert isinstance ( self . value , ( int , float )) return Convert ( value = self . value , from_unit = self . units , to_unit = units ) def convert ( self , units : typing . Union [ None , str , enum . Enum ]) -> 'ValueWithUnits' : \"\"\"Convert the internal representation to new units Parameters ---------- units : typing.Union[None, str, enum.Enum] The desired units \"\"\" if units is None and self . units is None : return self if units is None or self . units is None : raise RuntimeError ( 'Incompatible units involving None' ) new_units = GetUnitFromStr ( units ) if isinstance ( units , str ) else units self . value = self . as_units ( new_units ) self . units = new_units return self def __add__ ( self , other : 'ValueWithUnits' ) -> 'ValueWithUnits' : \"\"\"Implement addition\"\"\" raw_value = self . value + other . as_units ( self . units ) return ValueWithUnits ( value = raw_value , units = self . units ) def __sub__ ( self , other : 'ValueWithUnits' ) -> 'ValueWithUnits' : \"\"\"Implement subtraction\"\"\" raw_value = self . value - other . as_units ( self . units ) return ValueWithUnits ( value = raw_value , units = self . units ) def normalize_to_principal_value ( self , method ) -> None : \"\"\"Normalize an angular unit to its principal value. Parameters ---------- method : PrincipalValueNormalization The type of normalization desired. \"\"\" if not isinstance ( self . units , Angle ): raise ValueError ( 'Cannot normalize non-angular value' ) if not isinstance ( self . value , ( int , float )): raise TypeError ( 'Can only normalize numeric values' ) self . value = typing . cast ( typing . Union [ int , float ], self . value ) if self . units == Angle . Degree : period : float = 360 elif self . units == Angle . Rad : period = 2 * math . pi else : raise ValueError ( f 'Fix normalize_to_principal_value for { self . units } ' ) # Normalize to [-period, period] self . value = self . value % period # Normalize to [0, period] if self . value < 0 : self . value += period if method == self . PrincipalValueNormalization . Positive : return if method == self . PrincipalValueNormalization . Centered : if self . value > period / 2 : self . value -= period return raise ValueError ( f 'Fix normalize_to_principal_value for { method } ' ) def __str__ ( self ): if self . units is not None : return f ' { self . value } { self . units . value [ 1 ][ 0 ] } ' return str ( self . value ) def __repr__ ( self ): return str ( self ) PrincipalValueNormalization ( Enum ) \u00a4 Enumeration of the type of Principal Value normalization desired. Enumeration Values \u00a4 Positive Normalize to be within the range [0, T], for periodicity T. Centered Normalize to be within the range [-T/2, T/2], for periodicity T. Source code in corl/libraries/units.py class PrincipalValueNormalization ( enum . Enum ): \"\"\"Enumeration of the type of Principal Value normalization desired. Enumeration Values ------------------ Positive Normalize to be within the range [0, T], for periodicity T. Centered Normalize to be within the range [-T/2, T/2], for periodicity T. \"\"\" Positive = enum . auto () Centered = enum . auto () __add__ ( self , other ) special \u00a4 Implement addition Source code in corl/libraries/units.py def __add__ ( self , other : 'ValueWithUnits' ) -> 'ValueWithUnits' : \"\"\"Implement addition\"\"\" raw_value = self . value + other . as_units ( self . units ) return ValueWithUnits ( value = raw_value , units = self . units ) __repr__ ( self ) special \u00a4 Return repr(self). Source code in corl/libraries/units.py def __repr__ ( self ): return str ( self ) __str__ ( self ) special \u00a4 Return str(self). Source code in corl/libraries/units.py def __str__ ( self ): if self . units is not None : return f ' { self . value } { self . units . value [ 1 ][ 0 ] } ' return str ( self . value ) __sub__ ( self , other ) special \u00a4 Implement subtraction Source code in corl/libraries/units.py def __sub__ ( self , other : 'ValueWithUnits' ) -> 'ValueWithUnits' : \"\"\"Implement subtraction\"\"\" raw_value = self . value - other . as_units ( self . units ) return ValueWithUnits ( value = raw_value , units = self . units ) as_units ( self , units ) \u00a4 View the number in some other units Parameters \u00a4 units : typing.Union[None, str, enum.Enum] The desired units Returns \u00a4 float The value in the desired units Source code in corl/libraries/units.py def as_units ( self , units : typing . Union [ None , str , enum . Enum ]) -> typing . Any : \"\"\"View the number in some other units Parameters ---------- units : typing.Union[None, str, enum.Enum] The desired units Returns ------- float The value in the desired units \"\"\" if units is None and self . units is None : return self . value if units is None or self . units is None : raise RuntimeError ( f 'Incompatible units involving None: { units } <> { self . units } ' ) if units == self . units : return self . value assert isinstance ( self . value , ( int , float )) return Convert ( value = self . value , from_unit = self . units , to_unit = units ) convert ( self , units ) \u00a4 Convert the internal representation to new units Parameters \u00a4 units : typing.Union[None, str, enum.Enum] The desired units Source code in corl/libraries/units.py def convert ( self , units : typing . Union [ None , str , enum . Enum ]) -> 'ValueWithUnits' : \"\"\"Convert the internal representation to new units Parameters ---------- units : typing.Union[None, str, enum.Enum] The desired units \"\"\" if units is None and self . units is None : return self if units is None or self . units is None : raise RuntimeError ( 'Incompatible units involving None' ) new_units = GetUnitFromStr ( units ) if isinstance ( units , str ) else units self . value = self . as_units ( new_units ) self . units = new_units return self convert_string_units ( v ) classmethod \u00a4 Build units out of string Source code in corl/libraries/units.py @validator ( 'units' , pre = True ) def convert_string_units ( cls , v ): \"\"\"Build units out of string\"\"\" if isinstance ( v , str ): return GetUnitFromStr ( v ) if v is None : return NoneUnitType . NoneUnit return v convert_to ( self , to_unit ) \u00a4 Convert this value to another unit Parameters \u00a4 to_unit : typing.Union[None, str, enum.Enum] unit type to convert to Returns \u00a4 numbers.Real converted value Source code in corl/libraries/units.py def convert_to ( self , to_unit : typing . Union [ None , str , enum . Enum ]) -> float : \"\"\"Convert this value to another unit Parameters ---------- to_unit : typing.Union[None, str, enum.Enum] unit type to convert to Returns ------- numbers.Real converted value \"\"\" return Convert ( value = self . value , from_unit = self . units , to_unit = to_unit ) # type: ignore normalize_to_principal_value ( self , method ) \u00a4 Normalize an angular unit to its principal value. Parameters \u00a4 method : PrincipalValueNormalization The type of normalization desired. Source code in corl/libraries/units.py def normalize_to_principal_value ( self , method ) -> None : \"\"\"Normalize an angular unit to its principal value. Parameters ---------- method : PrincipalValueNormalization The type of normalization desired. \"\"\" if not isinstance ( self . units , Angle ): raise ValueError ( 'Cannot normalize non-angular value' ) if not isinstance ( self . value , ( int , float )): raise TypeError ( 'Can only normalize numeric values' ) self . value = typing . cast ( typing . Union [ int , float ], self . value ) if self . units == Angle . Degree : period : float = 360 elif self . units == Angle . Rad : period = 2 * math . pi else : raise ValueError ( f 'Fix normalize_to_principal_value for { self . units } ' ) # Normalize to [-period, period] self . value = self . value % period # Normalize to [0, period] if self . value < 0 : self . value += period if method == self . PrincipalValueNormalization . Positive : return if method == self . PrincipalValueNormalization . Centered : if self . value > period / 2 : self . value -= period return raise ValueError ( f 'Fix normalize_to_principal_value for { method } ' ) str_no_units ( values ) classmethod \u00a4 Confirm that string values have no units Source code in corl/libraries/units.py @root_validator def str_no_units ( cls , values ): \"\"\"Confirm that string values have no units\"\"\" if isinstance ( values . get ( 'value' ), str ): assert values . get ( 'units' ) is NoneUnitType . NoneUnit , 'String values must have unit None' return values value_validator ( v ) classmethod \u00a4 Validate value Source code in corl/libraries/units.py @validator ( 'value' , pre = True ) def value_validator ( cls , v ): \"\"\"Validate value\"\"\" # automatically convert numpy types to native types when needed # tolist will convert scalar or array to python native type # tolist returns a single value (the scalar) when calling it on a single value return getattr ( v , \"tolist\" , lambda : v )() Weight ( Enum ) \u00a4 Weight Source code in corl/libraries/units.py class Weight ( enum . Enum ): \"\"\"Weight \"\"\" Kilogram = ( 1.0 , [ \"kg\" , \"kilogram\" ]) Pound = ( 2.20462 , [ \"lb\" , \"lbs\" , \"pound\" , \"pounds\" ]) DEFAULT = Kilogram Convert ( value , from_unit , to_unit ) \u00a4 Convert a value from a unit to another unit Exceptions: Type Description RuntimeError Thrown if the unit's dimensions do not match Returns: Type Description float float -- Converted value Source code in corl/libraries/units.py def Convert ( value : float , from_unit : typing . Union [ str , enum . Enum ], to_unit : typing . Union [ str , enum . Enum ]) -> float : \"\"\"Convert a value from a unit to another unit Arguments: value {float} -- Value to convert from_unit {typing.Union[str, enum.Enum]} -- Unit that provided value is in to_unit {typing.Union[str, enum.Enum]} -- Desired Unit Raises: RuntimeError: Thrown if the unit's dimensions do not match Returns: float -- Converted value \"\"\" if isinstance ( from_unit , str ): from_unit = GetUnitFromStr ( from_unit ) if isinstance ( to_unit , str ): to_unit = GetUnitFromStr ( to_unit ) if isinstance ( from_unit , type ( to_unit )) is False : raise RuntimeError ( f \"Dimensions do not match! { from_unit } -> { to_unit } \" ) return value * to_unit . value [ 0 ] / from_unit . value [ 0 ] # type: ignore ConvertToDefault ( value , from_unit ) \u00a4 Convert a value from a unit to the default unit for its type Returns: Type Description float float -- Converted value Source code in corl/libraries/units.py def ConvertToDefault ( value : float , from_unit : typing . Union [ None , str , enum . Enum ]) -> float : \"\"\"Convert a value from a unit to the default unit for its type Arguments: value {float} -- Value to convert from_unit {typing.Union[str, enum.Enum]} -- Unit that provided value is in Returns: float -- Converted value \"\"\" if from_unit is None : return value if isinstance ( from_unit , str ): from_unit = GetUnitFromStr ( from_unit ) to_unit = type ( from_unit )[ 'DEFAULT' ] return Convert ( value = value , from_unit = from_unit , to_unit = to_unit ) GetStrFromUnit ( unit ) \u00a4 Given a unit determine the string that unit corresponds to. Source code in corl/libraries/units.py @lru_cache ( maxsize = 50 ) def GetStrFromUnit ( unit : enum . Enum ) -> str : \"\"\"Given a unit determine the string that unit corresponds to. \"\"\" return unit . value [ 1 ][ 0 ] GetUnitFromStr ( string ) \u00a4 Given a string determine the unit that string corresponds to. Source code in corl/libraries/units.py @lru_cache ( maxsize = 50 ) def GetUnitFromStr ( string : str ) -> enum . Enum : \"\"\"Given a string determine the unit that string corresponds to. \"\"\" for Dimension in Dimensions : for Unit in Dimension : if string in Unit . value [ 1 ]: return Unit raise RuntimeError ( f \" { string } could not be matched to any known unit, please check act3/util/units.py for potential units\" )","title":"Units"},{"location":"reference/libraries/units/#corl.libraries.units.Acceleration","text":"Acceleration Source code in corl/libraries/units.py class Acceleration ( enum . Enum ): \"\"\"Acceleration \"\"\" knots_per_sec = ( 1.0 , [ \"knots/s\" ]) meter_per_sec_2 = ( 0.51444563 , [ \"m/s^2\" ]) feet_per_sec_2 = ( 1.68780986 , [ \"ft/s^2\" ]) standard_gravity = ( 0.05245885496 , [ \"g\" , \"G\" , \"gravity\" , \"standard_gravity\" ]) DEFAULT = knots_per_sec","title":"Acceleration"},{"location":"reference/libraries/units/#corl.libraries.units.Angle","text":"Angle dimension Source code in corl/libraries/units.py class Angle ( enum . Enum ): \"\"\"Angle dimension \"\"\" Degree = ( 180.0 , [ \"deg\" , \"degree\" , \"degrees\" ]) Rad = ( math . pi , [ \"rad\" , \"radian\" , \"radians\" ]) DEFAULT = Rad","title":"Angle"},{"location":"reference/libraries/units/#corl.libraries.units.AngularSpeed","text":"Angular speed dimension Source code in corl/libraries/units.py class AngularSpeed ( enum . Enum ): \"\"\"Angular speed dimension \"\"\" radians_per_sec = ( 1 , [ \"rad/s\" , \"r/s\" ]) degrees_per_sec = ( 57.2958 , [ \"deg/s\" ]) DEFAULT = radians_per_sec","title":"AngularSpeed"},{"location":"reference/libraries/units/#corl.libraries.units.CalibratedSpeed","text":"Speed dimension for calibrated airspeed Source code in corl/libraries/units.py class CalibratedSpeed ( enum . Enum ): \"\"\"Speed dimension for calibrated airspeed \"\"\" Meter_per_Sec = ( Speed . Meter_per_Sec . value [ 0 ], [ \"mpscas\" ]) Knots = ( Speed . Knots . value [ 0 ], [ \"kcas\" ]) Feet_per_Min = ( Speed . Feet_per_Min . value [ 0 ], [ \"fpmcas\" ]) Feet_per_Sec = ( Speed . Feet_per_Sec . value [ 0 ], [ \"fpscas\" ]) DEFAULT = Knots","title":"CalibratedSpeed"},{"location":"reference/libraries/units/#corl.libraries.units.Distance","text":"Distance dimension Source code in corl/libraries/units.py class Distance ( enum . Enum ): \"\"\"Distance dimension \"\"\" Meter = ( 1.0 , [ \"m\" , \"meter\" , \"meters\" ]) Feet = ( 3.28084 , [ \"ft\" , \"feet\" ]) Nautical_Mile = ( 1 / 1852 , [ \"nm\" ]) DEFAULT = Meter","title":"Distance"},{"location":"reference/libraries/units/#corl.libraries.units.Force","text":"Force Source code in corl/libraries/units.py class Force ( enum . Enum ): \"\"\"Force \"\"\" Newton = ( 1.0 , [ \"N\" , \"newton\" , \"newtons\" ]) PoundForce = ( 0.22481 , [ \"pound-force\" ]) DEFAULT = Newton","title":"Force"},{"location":"reference/libraries/units/#corl.libraries.units.IndicatedSpeed","text":"Speed dimension for indicated airspeed Source code in corl/libraries/units.py class IndicatedSpeed ( enum . Enum ): \"\"\"Speed dimension for indicated airspeed \"\"\" Meter_per_Sec = ( Speed . Meter_per_Sec . value [ 0 ], [ \"mpsias\" ]) Knots = ( Speed . Knots . value [ 0 ], [ \"kias\" ]) Feet_per_Min = ( Speed . Feet_per_Min . value [ 0 ], [ \"fpmias\" ]) Feet_per_Sec = ( Speed . Feet_per_Sec . value [ 0 ], [ \"fpsias\" ]) DEFAULT = Knots","title":"IndicatedSpeed"},{"location":"reference/libraries/units/#corl.libraries.units.MachSpeed","text":"Mach speed dimension Source code in corl/libraries/units.py class MachSpeed ( enum . Enum ): \"\"\"Mach speed dimension \"\"\" Mach = ( 1.0 , [ \"mach\" , \"Ma\" ]) DEFAULT = Mach","title":"MachSpeed"},{"location":"reference/libraries/units/#corl.libraries.units.NoneUnitType","text":"None Source code in corl/libraries/units.py class NoneUnitType ( enum . Enum ): \"\"\"None \"\"\" NoneUnit = ( 1.0 , [ \"N/A\" , \"None\" , \"none\" ]) DEFAULT = NoneUnit","title":"NoneUnitType"},{"location":"reference/libraries/units/#corl.libraries.units.PartOfWhole","text":"PartOfWhole Source code in corl/libraries/units.py class PartOfWhole ( enum . Enum ): \"\"\"PartOfWhole \"\"\" Fraction = ( 1.0 , [ \"fraction\" ]) Percent = ( 100.0 , [ \"percent\" ]) DEFAULT = Fraction","title":"PartOfWhole"},{"location":"reference/libraries/units/#corl.libraries.units.Speed","text":"Speed dimension Source code in corl/libraries/units.py class Speed ( enum . Enum ): \"\"\"Speed dimension \"\"\" Meter_per_Sec = ( 1 , [ \"m/s\" , \"m_s\" ]) Knots = ( 1.94384 , [ \"knot\" , \"knots\" , \"kts\" ]) Feet_per_Min = ( 196.8504 , [ \"ft/min\" , \"feet/min\" ]) Feet_per_Sec = ( 3.28084 , [ \"ft/s\" , \"feet/second\" , \"feet/sec\" ]) DEFAULT = Knots","title":"Speed"},{"location":"reference/libraries/units/#corl.libraries.units.Time","text":"Time dimension Source code in corl/libraries/units.py class Time ( enum . Enum ): \"\"\"Time dimension \"\"\" Second = ( 1.0 , [ \"s\" , \"sec\" , \"second\" , \"seconds\" ]) Hour = ( 0.0002777778 , [ \"hr\" , \"hour\" ]) DEFAULT = Second","title":"Time"},{"location":"reference/libraries/units/#corl.libraries.units.TrueSpeed","text":"Speed dimension for true airspeed Source code in corl/libraries/units.py class TrueSpeed ( enum . Enum ): \"\"\"Speed dimension for true airspeed \"\"\" Meter_per_Sec = ( Speed . Meter_per_Sec . value [ 0 ], [ \"mpstas\" ]) Knots = ( Speed . Knots . value [ 0 ], [ \"ktas\" ]) Feet_per_Min = ( Speed . Feet_per_Min . value [ 0 ], [ \"fpmtas\" ]) Feet_per_Sec = ( Speed . Feet_per_Sec . value [ 0 ], [ \"fpstas\" ]) DEFAULT = Knots","title":"TrueSpeed"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits","text":"Wrap a value together with its units","title":"ValueWithUnits"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits--attributes","text":"value : typing.Any The value units : typing.Union[None, enum.Enum] The units Source code in corl/libraries/units.py class ValueWithUnits ( BaseModel ): \"\"\"Wrap a value together with its units Attributes ---------- value : typing.Any The value units : typing.Union[None, enum.Enum] The units \"\"\" value : typing . Union [ StrictInt , StrictFloat , bool , str ] units : typing . Optional [ enum . Enum ] @validator ( 'value' , pre = True ) def value_validator ( cls , v ): \"\"\"Validate value\"\"\" # automatically convert numpy types to native types when needed # tolist will convert scalar or array to python native type # tolist returns a single value (the scalar) when calling it on a single value return getattr ( v , \"tolist\" , lambda : v )() @validator ( 'units' , pre = True ) def convert_string_units ( cls , v ): \"\"\"Build units out of string\"\"\" if isinstance ( v , str ): return GetUnitFromStr ( v ) if v is None : return NoneUnitType . NoneUnit return v @root_validator def str_no_units ( cls , values ): \"\"\"Confirm that string values have no units\"\"\" if isinstance ( values . get ( 'value' ), str ): assert values . get ( 'units' ) is NoneUnitType . NoneUnit , 'String values must have unit None' return values def convert_to ( self , to_unit : typing . Union [ None , str , enum . Enum ]) -> float : \"\"\"Convert this value to another unit Parameters ---------- to_unit : typing.Union[None, str, enum.Enum] unit type to convert to Returns ------- numbers.Real converted value \"\"\" return Convert ( value = self . value , from_unit = self . units , to_unit = to_unit ) # type: ignore class PrincipalValueNormalization ( enum . Enum ): \"\"\"Enumeration of the type of Principal Value normalization desired. Enumeration Values ------------------ Positive Normalize to be within the range [0, T], for periodicity T. Centered Normalize to be within the range [-T/2, T/2], for periodicity T. \"\"\" Positive = enum . auto () Centered = enum . auto () def as_units ( self , units : typing . Union [ None , str , enum . Enum ]) -> typing . Any : \"\"\"View the number in some other units Parameters ---------- units : typing.Union[None, str, enum.Enum] The desired units Returns ------- float The value in the desired units \"\"\" if units is None and self . units is None : return self . value if units is None or self . units is None : raise RuntimeError ( f 'Incompatible units involving None: { units } <> { self . units } ' ) if units == self . units : return self . value assert isinstance ( self . value , ( int , float )) return Convert ( value = self . value , from_unit = self . units , to_unit = units ) def convert ( self , units : typing . Union [ None , str , enum . Enum ]) -> 'ValueWithUnits' : \"\"\"Convert the internal representation to new units Parameters ---------- units : typing.Union[None, str, enum.Enum] The desired units \"\"\" if units is None and self . units is None : return self if units is None or self . units is None : raise RuntimeError ( 'Incompatible units involving None' ) new_units = GetUnitFromStr ( units ) if isinstance ( units , str ) else units self . value = self . as_units ( new_units ) self . units = new_units return self def __add__ ( self , other : 'ValueWithUnits' ) -> 'ValueWithUnits' : \"\"\"Implement addition\"\"\" raw_value = self . value + other . as_units ( self . units ) return ValueWithUnits ( value = raw_value , units = self . units ) def __sub__ ( self , other : 'ValueWithUnits' ) -> 'ValueWithUnits' : \"\"\"Implement subtraction\"\"\" raw_value = self . value - other . as_units ( self . units ) return ValueWithUnits ( value = raw_value , units = self . units ) def normalize_to_principal_value ( self , method ) -> None : \"\"\"Normalize an angular unit to its principal value. Parameters ---------- method : PrincipalValueNormalization The type of normalization desired. \"\"\" if not isinstance ( self . units , Angle ): raise ValueError ( 'Cannot normalize non-angular value' ) if not isinstance ( self . value , ( int , float )): raise TypeError ( 'Can only normalize numeric values' ) self . value = typing . cast ( typing . Union [ int , float ], self . value ) if self . units == Angle . Degree : period : float = 360 elif self . units == Angle . Rad : period = 2 * math . pi else : raise ValueError ( f 'Fix normalize_to_principal_value for { self . units } ' ) # Normalize to [-period, period] self . value = self . value % period # Normalize to [0, period] if self . value < 0 : self . value += period if method == self . PrincipalValueNormalization . Positive : return if method == self . PrincipalValueNormalization . Centered : if self . value > period / 2 : self . value -= period return raise ValueError ( f 'Fix normalize_to_principal_value for { method } ' ) def __str__ ( self ): if self . units is not None : return f ' { self . value } { self . units . value [ 1 ][ 0 ] } ' return str ( self . value ) def __repr__ ( self ): return str ( self )","title":"Attributes"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.PrincipalValueNormalization","text":"Enumeration of the type of Principal Value normalization desired.","title":"PrincipalValueNormalization"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.PrincipalValueNormalization--enumeration-values","text":"Positive Normalize to be within the range [0, T], for periodicity T. Centered Normalize to be within the range [-T/2, T/2], for periodicity T. Source code in corl/libraries/units.py class PrincipalValueNormalization ( enum . Enum ): \"\"\"Enumeration of the type of Principal Value normalization desired. Enumeration Values ------------------ Positive Normalize to be within the range [0, T], for periodicity T. Centered Normalize to be within the range [-T/2, T/2], for periodicity T. \"\"\" Positive = enum . auto () Centered = enum . auto ()","title":"Enumeration Values"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.__add__","text":"Implement addition Source code in corl/libraries/units.py def __add__ ( self , other : 'ValueWithUnits' ) -> 'ValueWithUnits' : \"\"\"Implement addition\"\"\" raw_value = self . value + other . as_units ( self . units ) return ValueWithUnits ( value = raw_value , units = self . units )","title":"__add__()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.__repr__","text":"Return repr(self). Source code in corl/libraries/units.py def __repr__ ( self ): return str ( self )","title":"__repr__()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.__str__","text":"Return str(self). Source code in corl/libraries/units.py def __str__ ( self ): if self . units is not None : return f ' { self . value } { self . units . value [ 1 ][ 0 ] } ' return str ( self . value )","title":"__str__()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.__sub__","text":"Implement subtraction Source code in corl/libraries/units.py def __sub__ ( self , other : 'ValueWithUnits' ) -> 'ValueWithUnits' : \"\"\"Implement subtraction\"\"\" raw_value = self . value - other . as_units ( self . units ) return ValueWithUnits ( value = raw_value , units = self . units )","title":"__sub__()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.as_units","text":"View the number in some other units","title":"as_units()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.as_units--parameters","text":"units : typing.Union[None, str, enum.Enum] The desired units","title":"Parameters"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.as_units--returns","text":"float The value in the desired units Source code in corl/libraries/units.py def as_units ( self , units : typing . Union [ None , str , enum . Enum ]) -> typing . Any : \"\"\"View the number in some other units Parameters ---------- units : typing.Union[None, str, enum.Enum] The desired units Returns ------- float The value in the desired units \"\"\" if units is None and self . units is None : return self . value if units is None or self . units is None : raise RuntimeError ( f 'Incompatible units involving None: { units } <> { self . units } ' ) if units == self . units : return self . value assert isinstance ( self . value , ( int , float )) return Convert ( value = self . value , from_unit = self . units , to_unit = units )","title":"Returns"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.convert","text":"Convert the internal representation to new units","title":"convert()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.convert--parameters","text":"units : typing.Union[None, str, enum.Enum] The desired units Source code in corl/libraries/units.py def convert ( self , units : typing . Union [ None , str , enum . Enum ]) -> 'ValueWithUnits' : \"\"\"Convert the internal representation to new units Parameters ---------- units : typing.Union[None, str, enum.Enum] The desired units \"\"\" if units is None and self . units is None : return self if units is None or self . units is None : raise RuntimeError ( 'Incompatible units involving None' ) new_units = GetUnitFromStr ( units ) if isinstance ( units , str ) else units self . value = self . as_units ( new_units ) self . units = new_units return self","title":"Parameters"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.convert_string_units","text":"Build units out of string Source code in corl/libraries/units.py @validator ( 'units' , pre = True ) def convert_string_units ( cls , v ): \"\"\"Build units out of string\"\"\" if isinstance ( v , str ): return GetUnitFromStr ( v ) if v is None : return NoneUnitType . NoneUnit return v","title":"convert_string_units()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.convert_to","text":"Convert this value to another unit","title":"convert_to()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.convert_to--parameters","text":"to_unit : typing.Union[None, str, enum.Enum] unit type to convert to","title":"Parameters"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.convert_to--returns","text":"numbers.Real converted value Source code in corl/libraries/units.py def convert_to ( self , to_unit : typing . Union [ None , str , enum . Enum ]) -> float : \"\"\"Convert this value to another unit Parameters ---------- to_unit : typing.Union[None, str, enum.Enum] unit type to convert to Returns ------- numbers.Real converted value \"\"\" return Convert ( value = self . value , from_unit = self . units , to_unit = to_unit ) # type: ignore","title":"Returns"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.normalize_to_principal_value","text":"Normalize an angular unit to its principal value.","title":"normalize_to_principal_value()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.normalize_to_principal_value--parameters","text":"method : PrincipalValueNormalization The type of normalization desired. Source code in corl/libraries/units.py def normalize_to_principal_value ( self , method ) -> None : \"\"\"Normalize an angular unit to its principal value. Parameters ---------- method : PrincipalValueNormalization The type of normalization desired. \"\"\" if not isinstance ( self . units , Angle ): raise ValueError ( 'Cannot normalize non-angular value' ) if not isinstance ( self . value , ( int , float )): raise TypeError ( 'Can only normalize numeric values' ) self . value = typing . cast ( typing . Union [ int , float ], self . value ) if self . units == Angle . Degree : period : float = 360 elif self . units == Angle . Rad : period = 2 * math . pi else : raise ValueError ( f 'Fix normalize_to_principal_value for { self . units } ' ) # Normalize to [-period, period] self . value = self . value % period # Normalize to [0, period] if self . value < 0 : self . value += period if method == self . PrincipalValueNormalization . Positive : return if method == self . PrincipalValueNormalization . Centered : if self . value > period / 2 : self . value -= period return raise ValueError ( f 'Fix normalize_to_principal_value for { method } ' )","title":"Parameters"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.str_no_units","text":"Confirm that string values have no units Source code in corl/libraries/units.py @root_validator def str_no_units ( cls , values ): \"\"\"Confirm that string values have no units\"\"\" if isinstance ( values . get ( 'value' ), str ): assert values . get ( 'units' ) is NoneUnitType . NoneUnit , 'String values must have unit None' return values","title":"str_no_units()"},{"location":"reference/libraries/units/#corl.libraries.units.ValueWithUnits.value_validator","text":"Validate value Source code in corl/libraries/units.py @validator ( 'value' , pre = True ) def value_validator ( cls , v ): \"\"\"Validate value\"\"\" # automatically convert numpy types to native types when needed # tolist will convert scalar or array to python native type # tolist returns a single value (the scalar) when calling it on a single value return getattr ( v , \"tolist\" , lambda : v )()","title":"value_validator()"},{"location":"reference/libraries/units/#corl.libraries.units.Weight","text":"Weight Source code in corl/libraries/units.py class Weight ( enum . Enum ): \"\"\"Weight \"\"\" Kilogram = ( 1.0 , [ \"kg\" , \"kilogram\" ]) Pound = ( 2.20462 , [ \"lb\" , \"lbs\" , \"pound\" , \"pounds\" ]) DEFAULT = Kilogram","title":"Weight"},{"location":"reference/libraries/units/#corl.libraries.units.Convert","text":"Convert a value from a unit to another unit Exceptions: Type Description RuntimeError Thrown if the unit's dimensions do not match Returns: Type Description float float -- Converted value Source code in corl/libraries/units.py def Convert ( value : float , from_unit : typing . Union [ str , enum . Enum ], to_unit : typing . Union [ str , enum . Enum ]) -> float : \"\"\"Convert a value from a unit to another unit Arguments: value {float} -- Value to convert from_unit {typing.Union[str, enum.Enum]} -- Unit that provided value is in to_unit {typing.Union[str, enum.Enum]} -- Desired Unit Raises: RuntimeError: Thrown if the unit's dimensions do not match Returns: float -- Converted value \"\"\" if isinstance ( from_unit , str ): from_unit = GetUnitFromStr ( from_unit ) if isinstance ( to_unit , str ): to_unit = GetUnitFromStr ( to_unit ) if isinstance ( from_unit , type ( to_unit )) is False : raise RuntimeError ( f \"Dimensions do not match! { from_unit } -> { to_unit } \" ) return value * to_unit . value [ 0 ] / from_unit . value [ 0 ] # type: ignore","title":"Convert()"},{"location":"reference/libraries/units/#corl.libraries.units.ConvertToDefault","text":"Convert a value from a unit to the default unit for its type Returns: Type Description float float -- Converted value Source code in corl/libraries/units.py def ConvertToDefault ( value : float , from_unit : typing . Union [ None , str , enum . Enum ]) -> float : \"\"\"Convert a value from a unit to the default unit for its type Arguments: value {float} -- Value to convert from_unit {typing.Union[str, enum.Enum]} -- Unit that provided value is in Returns: float -- Converted value \"\"\" if from_unit is None : return value if isinstance ( from_unit , str ): from_unit = GetUnitFromStr ( from_unit ) to_unit = type ( from_unit )[ 'DEFAULT' ] return Convert ( value = value , from_unit = from_unit , to_unit = to_unit )","title":"ConvertToDefault()"},{"location":"reference/libraries/units/#corl.libraries.units.GetStrFromUnit","text":"Given a unit determine the string that unit corresponds to. Source code in corl/libraries/units.py @lru_cache ( maxsize = 50 ) def GetStrFromUnit ( unit : enum . Enum ) -> str : \"\"\"Given a unit determine the string that unit corresponds to. \"\"\" return unit . value [ 1 ][ 0 ]","title":"GetStrFromUnit()"},{"location":"reference/libraries/units/#corl.libraries.units.GetUnitFromStr","text":"Given a string determine the unit that string corresponds to. Source code in corl/libraries/units.py @lru_cache ( maxsize = 50 ) def GetUnitFromStr ( string : str ) -> enum . Enum : \"\"\"Given a string determine the unit that string corresponds to. \"\"\" for Dimension in Dimensions : for Unit in Dimension : if string in Unit . value [ 1 ]: return Unit raise RuntimeError ( f \" { string } could not be matched to any known unit, please check act3/util/units.py for potential units\" )","title":"GetUnitFromStr()"},{"location":"reference/models/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Models"},{"location":"reference/models/frame_stacking/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. The following module contains the implementation for the stacked observation FrameStackingModel ( TFModelV2 ) \u00a4 The following class is a slight modification of the base Fully Connected Model within RLLIB. The model adds on the ability to do frame stacking. This is done within the model to (1) enable the HPARAM search over the setting for the number of frames - Not possible with environment wrappers (2) enable a more in line view of the framestacking inline with paper representation. Note : Unlike the environment wrapper which just concatenates the data into one large vector and sends through network . This implementation will process N identical obs ( vectors ) through all hidden layers until the last FC layer which inputs are flattened to ensure that we are only producing the expected number of output actions The architecture produced is located below : - Note : Parallel paths . These are only generated if the Value function is defined as not sharing network . In the case that the network is shared only a single path would exist . - Note : All of the default capability of the default RLLIB model is maintained and may not be shown in diagram . Further testing needed for other paths . FC Layers 1 - N |-----| |---------->| Dense |-| | |-----| |-| |-------| |-----| | |-----| |------->| Flatten |---->| FC |---| | |-----| |-------| |-----| | N Obs |--------| | Model -------------> | | | ( opt ) N Reward | | |-------> [ FC Out ] ------> Actions -------------> | Inputs | ( opt ) N Action | | |-------> [ Value Out ] ------> Values -------------> |--------| | | |-----| | |---------->| Dense |-| | |-----| |-| |-------| |-----| | |-----| |------->| Flatten |---->| FC |---| |-----| |-------| |-----| |----------------------------------------- | | Note : Ensure final output is in | | 1 X ACTION not 1 X Frame X ACTION | |------------------------------------------| Inputs = 1 X Frames X ( Obs + Rewards + Actions ) Outputs = 1 X Actions The following is a example summary of the model from tensor flow on the base Single Environments Model : \"functional_1\" __________________________________________________________________________________________________ Layer ( type ) Output Shape Param # Connected to ================================================================================================== observations ( InputLayer ) [( None , 5 , 77 )] 0 __________________________________________________________________________________________________ fc_0 ( Dense ) ( None , 5 , 256 ) 19968 observations [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_value_0 ( Dense ) ( None , 5 , 256 ) 19968 observations [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_flatten_1 ( Flatten ) ( None , 1280 ) 0 fc_0 [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_value_flatten_1 ( Flatten ) ( None , 1280 ) 0 fc_value_0 [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_1 ( Dense ) ( None , 256 ) 327936 fc_flatten_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_value_1 ( Dense ) ( None , 256 ) 327936 fc_value_flatten_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_out ( Dense ) ( None , 51 ) 13107 fc_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ value_out ( Dense ) ( None , 1 ) 257 fc_value_1 [ 0 ][ 0 ] ================================================================================================== Total params : 709 , 172 Trainable params : 709 , 172 Non - trainable params : 0 __________________________________________________________________________________________________ Source code in corl/models/frame_stacking.py class FrameStackingModel ( TFModelV2 ): # pylint: disable=abstract-method \"\"\" The following class is a slight modification of the base Fully Connected Model within RLLIB. The model adds on the ability to do frame stacking. This is done within the model to (1) enable the HPARAM search over the setting for the number of frames - Not possible with environment wrappers (2) enable a more in line view of the framestacking inline with paper representation. Note: Unlike the environment wrapper which just concatenates the data into one large vector and sends through network. This implementation will process N identical obs (vectors) through all hidden layers until the last FC layer which inputs are flattened to ensure that we are only producing the expected number of output actions The architecture produced is located below: - Note: Parallel paths. These are only generated if the Value function is defined as not sharing network. In the case that the network is shared only a single path would exist. - Note: All of the default capability of the default RLLIB model is maintained and may not be shown in diagram. Further testing needed for other paths. FC Layers 1-N |-----| |---------->|Dense|-| | |-----| |-| |-------| |-----| | |-----| |------->|Flatten|---->| FC |---| | |-----| |-------| |-----| | N Obs |--------| | Model -------------> | | | (opt) N Reward | | |------->[FC Out ] ------> Actions -------------> | Inputs | (opt) N Action | | |------->[Value Out] ------> Values -------------> |--------| | | |-----| | |---------->|Dense|-| | |-----| |-| |-------| |-----| | |-----| |------->|Flatten|---->| FC |---| |-----| |-------| |-----| |----------------------------------------- | | Note: Ensure final output is in | | 1 X ACTION not 1 X Frame X ACTION | |------------------------------------------| Inputs = 1 X Frames X (Obs + Rewards + Actions) Outputs = 1 X Actions The following is a example summary of the model from tensor flow on the base Single Environments Model: \"functional_1\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== observations (InputLayer) [(None, 5, 77)] 0 __________________________________________________________________________________________________ fc_0 (Dense) (None, 5, 256) 19968 observations[0][0] __________________________________________________________________________________________________ fc_value_0 (Dense) (None, 5, 256) 19968 observations[0][0] __________________________________________________________________________________________________ fc_flatten_1 (Flatten) (None, 1280) 0 fc_0[0][0] __________________________________________________________________________________________________ fc_value_flatten_1 (Flatten) (None, 1280) 0 fc_value_0[0][0] __________________________________________________________________________________________________ fc_1 (Dense) (None, 256) 327936 fc_flatten_1[0][0] __________________________________________________________________________________________________ fc_value_1 (Dense) (None, 256) 327936 fc_value_flatten_1[0][0] __________________________________________________________________________________________________ fc_out (Dense) (None, 51) 13107 fc_1[0][0] __________________________________________________________________________________________________ value_out (Dense) (None, 1) 257 fc_value_1[0][0] ================================================================================================== Total params: 709,172 Trainable params: 709,172 Non-trainable params: 0 __________________________________________________________________________________________________ Arguments: TFModelV2 {[type]} -- [description] \"\"\" PREV_N_OBS = \"prev_n_obs\" PREV_N_REWARDS = \"prev_n_rewards\" PREV_N_ACTIONS = \"prev_n_actions\" def __init__ ( self , obs_space , action_space , num_outputs , model_config , name , post_fcnet_hiddens = None , num_frames : int = 4 , include_actions : bool = True , include_rewards : bool = True , ): \"\"\"Class constructor Arguments: obs_space (gym.spaces.Space): Observation space of the target gym env. This may have an `original_space` attribute that specifies how to unflatten the tensor into a ragged tensor. action_space (gym.spaces.Space): Action space of the target gym env. num_outputs (int): Number of output units of the model. model_config (ModelConfigDict): Config for the model, documented in ModelCatalog. name (str): Name (scope) for the model. This method should create any variables used by the model. Keyword Arguments: num_frames {int} -- The number of frames to stack (default: {4}) include_actions {int} -- Whether or not to include actions as part of frame stacking (default: True) include_actions {int} -- Whether or not to include actions as part of frame stacking (default: True) Returns: [type] -- [description] \"\"\" if post_fcnet_hiddens is None : post_fcnet_hiddens = [] # Initializes a ModelV2 object. TFModelV2 . __init__ ( self , obs_space , action_space , num_outputs , model_config , name ) # This model specific items self . num_frames = num_frames # Base model items self . num_outputs = num_outputs # Read out the model configuration parameters passed by RLLIB. Note this is maintained to ensure # compatibility with existing setup free_log_std , hiddens , activation , no_final_linear , vf_share_layers = self . get_config_opts () # Generate free-floating bias variables for the second half of # the outputs. if free_log_std : assert num_outputs % 2 == 0 , ( \"num_outputs must be divisible by two\" , num_outputs ) num_outputs = num_outputs // 2 self . log_std_var = tf . Variable ([ 0.0 ] * num_outputs , dtype = tf . float32 , name = \"log_std\" ) # Create the input layers for the observations, actions, and rewards flattened_action_space = flatten_space ( action_space ) observations , actions , rewards = self . create_input_layers ( obs_space , flattened_action_space ) # Select the input layer configuration based on input arguments self . include_rewards = include_rewards self . include_actions = include_actions self . input_list , self . inputs = FrameStackingModel . select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ) # Create layers 0 to second-last. last_layer = self . create_dense_hidden_layers ( hiddens , self . inputs , activation , \"fc\" ) # The action distribution outputs. logits_out , last_layer = self . create_last_fc_layer_output ( no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ) # Concat the log std vars to the end of the state-dependent means. if free_log_std and logits_out is not None : def tiled_log_std ( x ): return tf . tile ( tf . expand_dims ( self . log_std_var , 0 ), [ tf . shape ( x )[ 0 ], 1 ]) log_std_out = tf . keras . layers . Lambda ( tiled_log_std )( self . inputs ) logits_out = tf . keras . layers . Concatenate ( axis = 1 )([ logits_out , log_std_out ]) last_vf_layer = self . build_vf_network ( vf_share_layers , self . inputs , hiddens , post_fcnet_hiddens , activation ) value_out = tf . keras . layers . Dense ( 1 , name = \"value_out\" , activation = None , kernel_initializer = normc_initializer ( 0.01 ) )( last_vf_layer if last_vf_layer is not None else last_layer ) self . base_model = tf . keras . Model ( self . input_list , [( logits_out if logits_out is not None else last_layer ), value_out ]) # print(self.base_model.summary()) self . register_view_requirements ( num_frames , obs_space , flattened_action_space ) self . _value_out = None @staticmethod def select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ): \"\"\"Sets up the input layer based on the configuration of the arguments to the model Arguments: include_rewards {bool} -- Flag to indicate that rewards should be part of frame stacking include_actions {bool} -- Flag to indicate that the actions should be part of frame stacking observations {Tensor} -- [description] actions {Tensor} -- [description] rewards {Tensor} -- [description] \"\"\" # Last hidden layer output (before logits outputs). if include_actions and not include_rewards : input_list = [ observations , actions ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) elif not include_actions and include_rewards : input_list = [ observations , rewards ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) elif include_actions and include_rewards : input_list = [ observations , actions , rewards ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) else : input_list = [ observations ] inputs = observations return input_list , inputs def create_input_layers ( self , obs_space , action_space ): \"\"\"Creats the input layers for starting the graph Arguments: obs_space {gym.Space} -- The input space - flattended action_space {gym.Space} -- The input space - flattended Returns: tuple[tensor] -- The input layers for observations, rewards, actions \"\"\" # (?, Number of Frames, 1) rewards = tf . keras . layers . Input ( shape = ( self . num_frames , 1 ), name = \"rewards\" ) # (?, Number of Frames, len obs flatten) observations = tf . keras . layers . Input ( shape = ( self . num_frames , obs_space . shape [ 0 ]), name = \"observations\" ) # (?, Number of Frames, len actions flatten) actions = tf . keras . layers . Input ( shape = ( self . num_frames , len ( action_space )), name = \"actions\" ) return observations , actions , rewards def create_last_fc_layer_output ( self , no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ): \"\"\"[summary] Arguments: no_final_linear {[type]} -- [description] num_outputs {[type]} -- [description] activation {[type]} -- [description] last_layer {[type]} -- [description] hiddens {[type]} -- [description] obs_space {[type]} -- [description] Returns: [type] -- [description] \"\"\" # The action distribution outputs. logits_out = None # The last layer is adjusted to be of size num_outputs, but it's a # layer with activation. if no_final_linear and num_outputs : logits_out = tf . keras . layers . Dense ( num_outputs , name = \"fc_out\" , activation = activation , kernel_initializer = normc_initializer ( 1.0 ) )( last_layer ) # Finish the layers with the provided sizes (`hiddens`), plus - # iff num_outputs > 0 - a last linear layer of size num_outputs. else : if len ( hiddens ) > 0 : last_layer = FrameStackingModel . flatten_plus_dense ( hiddens , post_fcnet_hiddens , last_layer , activation , \"fc\" , len ( hiddens ) - 1 ) if num_outputs : logits_out = tf . keras . layers . Dense ( num_outputs , name = \"fc_out\" , activation = None , kernel_initializer = normc_initializer ( 0.01 ))( last_layer ) # Adjust num_outputs to be the number of nodes in the last layer. else : self . num_outputs = ([ int ( np . product ( obs_space . shape ))] + hiddens [ - 1 :])[ - 1 ] return logits_out , last_layer def build_vf_network ( self , vf_share_layers , inputs , hiddens , flatten_plus_dense , activation ): \"\"\"Creates the value function network if configured in model config Arguments: vf_share_layers {[type]} -- [description] inputs {[type]} -- [description] hiddens {[type]} -- [description] activation {[type]} -- [description] Returns: [type] -- [description] \"\"\" last_vf_layer = None if not vf_share_layers : # Build a parallel set of hidden layers for the value net. value_function_prefix = \"fc_value\" last_vf_layer = self . create_dense_hidden_layers ( hiddens , inputs , activation , value_function_prefix ) last_vf_layer = self . flatten_plus_dense ( hiddens , flatten_plus_dense , last_vf_layer , activation , value_function_prefix , len ( hiddens ) - 1 ) return last_vf_layer def register_view_requirements ( self , num_frames : int , obs_space , flattened_action_space ): \"\"\"Sets up the view requirements for the forward pass call Arguments: num_frames {int} -- The number of frames to stack obs_space {[type]} -- The observation space definition flattened_action_space {[type]} -- flattened action space \"\"\" self . view_requirements [ FrameStackingModel . PREV_N_OBS ] = ViewRequirement ( data_col = \"obs\" , shift = \"- {} :0\" . format ( num_frames - 1 ), space = obs_space ) if self . include_rewards : self . view_requirements [ FrameStackingModel . PREV_N_REWARDS ] = ViewRequirement ( data_col = \"rewards\" , shift = \"- {} :-1\" . format ( self . num_frames )) if self . include_actions : self . view_requirements [ FrameStackingModel . PREV_N_ACTIONS ] = ViewRequirement ( data_col = \"actions\" , shift = \"- {} :-1\" . format ( self . num_frames ), space = gym . spaces . box . Box ( low =- np . inf , high = np . inf , shape = ( len ( flattened_action_space ), ), dtype = np . int64 ) ) def forward ( self , input_dict : Dict [ str , TensorType ], state : List [ TensorType ], seq_lens : TensorType ) -> ( TensorType , List [ TensorType ]): # type: ignore \"\"\" Call the model with the given input tensors and state. Any complex observations (dicts, tuples, etc.) will be unpacked by __call__ before being passed to forward(). To access the flattened observation tensor, refer to input_dict[\u201cobs\u201d]. This method can be called any number of times. In eager execution, each call to forward() will eagerly evaluate the model. In symbolic execution, each call to forward creates a computation graph that operates over the variables of this model (i.e., shares weights). Custom models should override this instead of __call__. Arguments: input_dict (dict) \u2013 dictionary of input tensors, including \u201cobs\u201d, \u201cobs_flat\u201d, \u201cprev_action\u201d, \u201cprev_reward\u201d, \u201cis_training\u201d, \u201ceps_id\u201d, \u201cagent_id\u201d, \u201cinfos\u201d, and \u201ct\u201d. state (list) \u2013 list of state tensors with sizes matching those returned by get_initial_state + the batch dimension seq_lens (Tensor) \u2013 1d tensor holding input sequence lengths Returns: The model output tensor of size [BATCH, num_outputs], and the new RNN state. \"\"\" if self . include_actions and not self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_ACTIONS ]]) elif not self . include_actions and self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_REWARDS ]]) elif self . include_actions and self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_ACTIONS ], input_dict [ FrameStackingModel . PREV_N_REWARDS ]]) else : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ]]) return model_out , state def value_function ( self ) -> TensorType : \"\"\"Returns the value function output for the most recent forward pass. Note that a forward call has to be performed first, before this methods can return anything and thus that calling this method does not cause an extra forward pass through the network. Returns: value estimate tensor of shape [BATCH]. \"\"\" return tf . reshape ( self . _value_out , [ - 1 ]) def get_config_opts ( self ): \"\"\"Gets the configuration options utilizes by the frame stacking model Returns: Tuple -- configuration options for the models (Bool, list[int], function, Bool, Bool) \"\"\" hiddens = self . model_config . get ( \"fcnet_hiddens\" , []) + self . model_config . get ( \"post_fcnet_hiddens\" , []) activation = self . model_config . get ( \"fcnet_activation\" ) if not self . model_config . get ( \"fcnet_hiddens\" , []): activation = self . model_config . get ( \"post_fcnet_activation\" ) activation = get_activation_fn ( activation ) no_final_linear = self . model_config . get ( \"no_final_linear\" ) vf_share_layers = self . model_config . get ( \"vf_share_layers\" ) free_log_std = self . model_config . get ( \"free_log_std\" ) return free_log_std , hiddens , activation , no_final_linear , vf_share_layers @staticmethod def create_dense_hidden_layers ( hiddens , layer , activation , prefix : str ): \"\"\"Creates the hidden dense layers Arguments: hiddens {List[int]} -- The list of hidden layers for the FC componets layer {Tensor} -- [description] --- TODO Remove as not needed to pass in... activation {Function} -- [description] prefix {str} -- The string to use for the naming of the layer Returns: [type] -- [description] \"\"\" for index , size in enumerate ( hiddens [: - 1 ]): dense_name = f \" { prefix } _ { index } \" layer = tf . keras . layers . Dense ( size , name = dense_name , activation = activation , kernel_initializer = normc_initializer ( 1.0 ))( layer ) return layer @staticmethod def flatten_plus_dense ( hiddens , post_fcnet_hiddens , layer , activation , prefix : str , index : int ): \"\"\"Creates the final/last dense layer with flatten to ensure the correct output size Arguments: hiddens {List[int]} -- List containing the size of each hidden layer layer {Tensor} -- [description] activation {function]} -- [description] prefix {str} -- The string to use when creating the layers index {int} -- The index of the layer to add the flatten on. Returns: Tensor -- The layer just created with flatten + FD \"\"\" flatten_name = f \" { prefix } _flatten_ { index } \" dense_name = f \" { prefix } _ { index } \" layer = tf . keras . layers . Flatten ( name = flatten_name )( layer ) layer = tf . keras . layers . Dense ( hiddens [ index ], name = dense_name , activation = activation , kernel_initializer = normc_initializer ( 1.0 ))( layer ) for index_cat , size in enumerate ( post_fcnet_hiddens ): layer = tf . keras . layers . Dense ( size , name = f \" { dense_name } _cat_ { index_cat } \" , activation = activation , kernel_initializer = normc_initializer ( 1.0 ) )( layer ) return layer __init__ ( self , obs_space , action_space , num_outputs , model_config , name , post_fcnet_hiddens = None , num_frames = 4 , include_actions = True , include_rewards = True ) special \u00a4 Class constructor Parameters: Name Type Description Default obs_space gym.spaces.Space Observation space of the target gym env. This may have an original_space attribute that specifies how to unflatten the tensor into a ragged tensor. required action_space gym.spaces.Space Action space of the target gym env. required num_outputs int Number of output units of the model. required model_config ModelConfigDict Config for the model, documented in ModelCatalog. required name str Name (scope) for the model. required This method should create any variables used by the model. Keyword arguments: Name Type Description num_frames {int} -- The number of frames to stack (default {4}) include_actions {int} -- Whether or not to include actions as part of frame stacking (default True) include_actions {int} -- Whether or not to include actions as part of frame stacking (default True) Returns: Type Description [type] -- [description] Source code in corl/models/frame_stacking.py def __init__ ( self , obs_space , action_space , num_outputs , model_config , name , post_fcnet_hiddens = None , num_frames : int = 4 , include_actions : bool = True , include_rewards : bool = True , ): \"\"\"Class constructor Arguments: obs_space (gym.spaces.Space): Observation space of the target gym env. This may have an `original_space` attribute that specifies how to unflatten the tensor into a ragged tensor. action_space (gym.spaces.Space): Action space of the target gym env. num_outputs (int): Number of output units of the model. model_config (ModelConfigDict): Config for the model, documented in ModelCatalog. name (str): Name (scope) for the model. This method should create any variables used by the model. Keyword Arguments: num_frames {int} -- The number of frames to stack (default: {4}) include_actions {int} -- Whether or not to include actions as part of frame stacking (default: True) include_actions {int} -- Whether or not to include actions as part of frame stacking (default: True) Returns: [type] -- [description] \"\"\" if post_fcnet_hiddens is None : post_fcnet_hiddens = [] # Initializes a ModelV2 object. TFModelV2 . __init__ ( self , obs_space , action_space , num_outputs , model_config , name ) # This model specific items self . num_frames = num_frames # Base model items self . num_outputs = num_outputs # Read out the model configuration parameters passed by RLLIB. Note this is maintained to ensure # compatibility with existing setup free_log_std , hiddens , activation , no_final_linear , vf_share_layers = self . get_config_opts () # Generate free-floating bias variables for the second half of # the outputs. if free_log_std : assert num_outputs % 2 == 0 , ( \"num_outputs must be divisible by two\" , num_outputs ) num_outputs = num_outputs // 2 self . log_std_var = tf . Variable ([ 0.0 ] * num_outputs , dtype = tf . float32 , name = \"log_std\" ) # Create the input layers for the observations, actions, and rewards flattened_action_space = flatten_space ( action_space ) observations , actions , rewards = self . create_input_layers ( obs_space , flattened_action_space ) # Select the input layer configuration based on input arguments self . include_rewards = include_rewards self . include_actions = include_actions self . input_list , self . inputs = FrameStackingModel . select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ) # Create layers 0 to second-last. last_layer = self . create_dense_hidden_layers ( hiddens , self . inputs , activation , \"fc\" ) # The action distribution outputs. logits_out , last_layer = self . create_last_fc_layer_output ( no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ) # Concat the log std vars to the end of the state-dependent means. if free_log_std and logits_out is not None : def tiled_log_std ( x ): return tf . tile ( tf . expand_dims ( self . log_std_var , 0 ), [ tf . shape ( x )[ 0 ], 1 ]) log_std_out = tf . keras . layers . Lambda ( tiled_log_std )( self . inputs ) logits_out = tf . keras . layers . Concatenate ( axis = 1 )([ logits_out , log_std_out ]) last_vf_layer = self . build_vf_network ( vf_share_layers , self . inputs , hiddens , post_fcnet_hiddens , activation ) value_out = tf . keras . layers . Dense ( 1 , name = \"value_out\" , activation = None , kernel_initializer = normc_initializer ( 0.01 ) )( last_vf_layer if last_vf_layer is not None else last_layer ) self . base_model = tf . keras . Model ( self . input_list , [( logits_out if logits_out is not None else last_layer ), value_out ]) # print(self.base_model.summary()) self . register_view_requirements ( num_frames , obs_space , flattened_action_space ) self . _value_out = None build_vf_network ( self , vf_share_layers , inputs , hiddens , flatten_plus_dense , activation ) \u00a4 Creates the value function network if configured in model config Returns: Type Description [type] -- [description] Source code in corl/models/frame_stacking.py def build_vf_network ( self , vf_share_layers , inputs , hiddens , flatten_plus_dense , activation ): \"\"\"Creates the value function network if configured in model config Arguments: vf_share_layers {[type]} -- [description] inputs {[type]} -- [description] hiddens {[type]} -- [description] activation {[type]} -- [description] Returns: [type] -- [description] \"\"\" last_vf_layer = None if not vf_share_layers : # Build a parallel set of hidden layers for the value net. value_function_prefix = \"fc_value\" last_vf_layer = self . create_dense_hidden_layers ( hiddens , inputs , activation , value_function_prefix ) last_vf_layer = self . flatten_plus_dense ( hiddens , flatten_plus_dense , last_vf_layer , activation , value_function_prefix , len ( hiddens ) - 1 ) return last_vf_layer create_dense_hidden_layers ( hiddens , layer , activation , prefix ) staticmethod \u00a4 Creates the hidden dense layers Returns: Type Description [type] -- [description] Source code in corl/models/frame_stacking.py @staticmethod def create_dense_hidden_layers ( hiddens , layer , activation , prefix : str ): \"\"\"Creates the hidden dense layers Arguments: hiddens {List[int]} -- The list of hidden layers for the FC componets layer {Tensor} -- [description] --- TODO Remove as not needed to pass in... activation {Function} -- [description] prefix {str} -- The string to use for the naming of the layer Returns: [type] -- [description] \"\"\" for index , size in enumerate ( hiddens [: - 1 ]): dense_name = f \" { prefix } _ { index } \" layer = tf . keras . layers . Dense ( size , name = dense_name , activation = activation , kernel_initializer = normc_initializer ( 1.0 ))( layer ) return layer create_input_layers ( self , obs_space , action_space ) \u00a4 Creats the input layers for starting the graph Returns: Type Description tuple[tensor] -- The input layers for observations, rewards, actions Source code in corl/models/frame_stacking.py def create_input_layers ( self , obs_space , action_space ): \"\"\"Creats the input layers for starting the graph Arguments: obs_space {gym.Space} -- The input space - flattended action_space {gym.Space} -- The input space - flattended Returns: tuple[tensor] -- The input layers for observations, rewards, actions \"\"\" # (?, Number of Frames, 1) rewards = tf . keras . layers . Input ( shape = ( self . num_frames , 1 ), name = \"rewards\" ) # (?, Number of Frames, len obs flatten) observations = tf . keras . layers . Input ( shape = ( self . num_frames , obs_space . shape [ 0 ]), name = \"observations\" ) # (?, Number of Frames, len actions flatten) actions = tf . keras . layers . Input ( shape = ( self . num_frames , len ( action_space )), name = \"actions\" ) return observations , actions , rewards create_last_fc_layer_output ( self , no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ) \u00a4 [summary] Returns: Type Description [type] -- [description] Source code in corl/models/frame_stacking.py def create_last_fc_layer_output ( self , no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ): \"\"\"[summary] Arguments: no_final_linear {[type]} -- [description] num_outputs {[type]} -- [description] activation {[type]} -- [description] last_layer {[type]} -- [description] hiddens {[type]} -- [description] obs_space {[type]} -- [description] Returns: [type] -- [description] \"\"\" # The action distribution outputs. logits_out = None # The last layer is adjusted to be of size num_outputs, but it's a # layer with activation. if no_final_linear and num_outputs : logits_out = tf . keras . layers . Dense ( num_outputs , name = \"fc_out\" , activation = activation , kernel_initializer = normc_initializer ( 1.0 ) )( last_layer ) # Finish the layers with the provided sizes (`hiddens`), plus - # iff num_outputs > 0 - a last linear layer of size num_outputs. else : if len ( hiddens ) > 0 : last_layer = FrameStackingModel . flatten_plus_dense ( hiddens , post_fcnet_hiddens , last_layer , activation , \"fc\" , len ( hiddens ) - 1 ) if num_outputs : logits_out = tf . keras . layers . Dense ( num_outputs , name = \"fc_out\" , activation = None , kernel_initializer = normc_initializer ( 0.01 ))( last_layer ) # Adjust num_outputs to be the number of nodes in the last layer. else : self . num_outputs = ([ int ( np . product ( obs_space . shape ))] + hiddens [ - 1 :])[ - 1 ] return logits_out , last_layer flatten_plus_dense ( hiddens , post_fcnet_hiddens , layer , activation , prefix , index ) staticmethod \u00a4 Creates the final/last dense layer with flatten to ensure the correct output size Returns: Type Description Tensor -- The layer just created with flatten + FD Source code in corl/models/frame_stacking.py @staticmethod def flatten_plus_dense ( hiddens , post_fcnet_hiddens , layer , activation , prefix : str , index : int ): \"\"\"Creates the final/last dense layer with flatten to ensure the correct output size Arguments: hiddens {List[int]} -- List containing the size of each hidden layer layer {Tensor} -- [description] activation {function]} -- [description] prefix {str} -- The string to use when creating the layers index {int} -- The index of the layer to add the flatten on. Returns: Tensor -- The layer just created with flatten + FD \"\"\" flatten_name = f \" { prefix } _flatten_ { index } \" dense_name = f \" { prefix } _ { index } \" layer = tf . keras . layers . Flatten ( name = flatten_name )( layer ) layer = tf . keras . layers . Dense ( hiddens [ index ], name = dense_name , activation = activation , kernel_initializer = normc_initializer ( 1.0 ))( layer ) for index_cat , size in enumerate ( post_fcnet_hiddens ): layer = tf . keras . layers . Dense ( size , name = f \" { dense_name } _cat_ { index_cat } \" , activation = activation , kernel_initializer = normc_initializer ( 1.0 ) )( layer ) return layer forward ( self , input_dict , state , seq_lens ) \u00a4 Call the model with the given input tensors and state. Any complex observations (dicts, tuples, etc.) will be unpacked by call before being passed to forward(). To access the flattened observation tensor, refer to input_dict[\u201cobs\u201d]. This method can be called any number of times. In eager execution, each call to forward() will eagerly evaluate the model. In symbolic execution, each call to forward creates a computation graph that operates over the variables of this model (i.e., shares weights). Custom models should override this instead of call . Returns: Type Description (Any, List[Any]) The model output tensor of size [BATCH, num_outputs], and the new RNN state. Source code in corl/models/frame_stacking.py def forward ( self , input_dict : Dict [ str , TensorType ], state : List [ TensorType ], seq_lens : TensorType ) -> ( TensorType , List [ TensorType ]): # type: ignore \"\"\" Call the model with the given input tensors and state. Any complex observations (dicts, tuples, etc.) will be unpacked by __call__ before being passed to forward(). To access the flattened observation tensor, refer to input_dict[\u201cobs\u201d]. This method can be called any number of times. In eager execution, each call to forward() will eagerly evaluate the model. In symbolic execution, each call to forward creates a computation graph that operates over the variables of this model (i.e., shares weights). Custom models should override this instead of __call__. Arguments: input_dict (dict) \u2013 dictionary of input tensors, including \u201cobs\u201d, \u201cobs_flat\u201d, \u201cprev_action\u201d, \u201cprev_reward\u201d, \u201cis_training\u201d, \u201ceps_id\u201d, \u201cagent_id\u201d, \u201cinfos\u201d, and \u201ct\u201d. state (list) \u2013 list of state tensors with sizes matching those returned by get_initial_state + the batch dimension seq_lens (Tensor) \u2013 1d tensor holding input sequence lengths Returns: The model output tensor of size [BATCH, num_outputs], and the new RNN state. \"\"\" if self . include_actions and not self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_ACTIONS ]]) elif not self . include_actions and self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_REWARDS ]]) elif self . include_actions and self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_ACTIONS ], input_dict [ FrameStackingModel . PREV_N_REWARDS ]]) else : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ]]) return model_out , state get_config_opts ( self ) \u00a4 Gets the configuration options utilizes by the frame stacking model Returns: Type Description Tuple -- configuration options for the models (Bool, list[int], function, Bool, Bool) Source code in corl/models/frame_stacking.py def get_config_opts ( self ): \"\"\"Gets the configuration options utilizes by the frame stacking model Returns: Tuple -- configuration options for the models (Bool, list[int], function, Bool, Bool) \"\"\" hiddens = self . model_config . get ( \"fcnet_hiddens\" , []) + self . model_config . get ( \"post_fcnet_hiddens\" , []) activation = self . model_config . get ( \"fcnet_activation\" ) if not self . model_config . get ( \"fcnet_hiddens\" , []): activation = self . model_config . get ( \"post_fcnet_activation\" ) activation = get_activation_fn ( activation ) no_final_linear = self . model_config . get ( \"no_final_linear\" ) vf_share_layers = self . model_config . get ( \"vf_share_layers\" ) free_log_std = self . model_config . get ( \"free_log_std\" ) return free_log_std , hiddens , activation , no_final_linear , vf_share_layers register_view_requirements ( self , num_frames , obs_space , flattened_action_space ) \u00a4 Sets up the view requirements for the forward pass call Source code in corl/models/frame_stacking.py def register_view_requirements ( self , num_frames : int , obs_space , flattened_action_space ): \"\"\"Sets up the view requirements for the forward pass call Arguments: num_frames {int} -- The number of frames to stack obs_space {[type]} -- The observation space definition flattened_action_space {[type]} -- flattened action space \"\"\" self . view_requirements [ FrameStackingModel . PREV_N_OBS ] = ViewRequirement ( data_col = \"obs\" , shift = \"- {} :0\" . format ( num_frames - 1 ), space = obs_space ) if self . include_rewards : self . view_requirements [ FrameStackingModel . PREV_N_REWARDS ] = ViewRequirement ( data_col = \"rewards\" , shift = \"- {} :-1\" . format ( self . num_frames )) if self . include_actions : self . view_requirements [ FrameStackingModel . PREV_N_ACTIONS ] = ViewRequirement ( data_col = \"actions\" , shift = \"- {} :-1\" . format ( self . num_frames ), space = gym . spaces . box . Box ( low =- np . inf , high = np . inf , shape = ( len ( flattened_action_space ), ), dtype = np . int64 ) ) select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ) staticmethod \u00a4 Sets up the input layer based on the configuration of the arguments to the model Source code in corl/models/frame_stacking.py @staticmethod def select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ): \"\"\"Sets up the input layer based on the configuration of the arguments to the model Arguments: include_rewards {bool} -- Flag to indicate that rewards should be part of frame stacking include_actions {bool} -- Flag to indicate that the actions should be part of frame stacking observations {Tensor} -- [description] actions {Tensor} -- [description] rewards {Tensor} -- [description] \"\"\" # Last hidden layer output (before logits outputs). if include_actions and not include_rewards : input_list = [ observations , actions ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) elif not include_actions and include_rewards : input_list = [ observations , rewards ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) elif include_actions and include_rewards : input_list = [ observations , actions , rewards ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) else : input_list = [ observations ] inputs = observations return input_list , inputs value_function ( self ) \u00a4 Returns the value function output for the most recent forward pass. Note that a forward call has to be performed first, before this methods can return anything and thus that calling this method does not cause an extra forward pass through the network. Returns: Type Description Any value estimate tensor of shape [BATCH]. Source code in corl/models/frame_stacking.py def value_function ( self ) -> TensorType : \"\"\"Returns the value function output for the most recent forward pass. Note that a forward call has to be performed first, before this methods can return anything and thus that calling this method does not cause an extra forward pass through the network. Returns: value estimate tensor of shape [BATCH]. \"\"\" return tf . reshape ( self . _value_out , [ - 1 ])","title":"Frame stacking"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel","text":"The following class is a slight modification of the base Fully Connected Model within RLLIB. The model adds on the ability to do frame stacking. This is done within the model to (1) enable the HPARAM search over the setting for the number of frames - Not possible with environment wrappers (2) enable a more in line view of the framestacking inline with paper representation. Note : Unlike the environment wrapper which just concatenates the data into one large vector and sends through network . This implementation will process N identical obs ( vectors ) through all hidden layers until the last FC layer which inputs are flattened to ensure that we are only producing the expected number of output actions The architecture produced is located below : - Note : Parallel paths . These are only generated if the Value function is defined as not sharing network . In the case that the network is shared only a single path would exist . - Note : All of the default capability of the default RLLIB model is maintained and may not be shown in diagram . Further testing needed for other paths . FC Layers 1 - N |-----| |---------->| Dense |-| | |-----| |-| |-------| |-----| | |-----| |------->| Flatten |---->| FC |---| | |-----| |-------| |-----| | N Obs |--------| | Model -------------> | | | ( opt ) N Reward | | |-------> [ FC Out ] ------> Actions -------------> | Inputs | ( opt ) N Action | | |-------> [ Value Out ] ------> Values -------------> |--------| | | |-----| | |---------->| Dense |-| | |-----| |-| |-------| |-----| | |-----| |------->| Flatten |---->| FC |---| |-----| |-------| |-----| |----------------------------------------- | | Note : Ensure final output is in | | 1 X ACTION not 1 X Frame X ACTION | |------------------------------------------| Inputs = 1 X Frames X ( Obs + Rewards + Actions ) Outputs = 1 X Actions The following is a example summary of the model from tensor flow on the base Single Environments Model : \"functional_1\" __________________________________________________________________________________________________ Layer ( type ) Output Shape Param # Connected to ================================================================================================== observations ( InputLayer ) [( None , 5 , 77 )] 0 __________________________________________________________________________________________________ fc_0 ( Dense ) ( None , 5 , 256 ) 19968 observations [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_value_0 ( Dense ) ( None , 5 , 256 ) 19968 observations [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_flatten_1 ( Flatten ) ( None , 1280 ) 0 fc_0 [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_value_flatten_1 ( Flatten ) ( None , 1280 ) 0 fc_value_0 [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_1 ( Dense ) ( None , 256 ) 327936 fc_flatten_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_value_1 ( Dense ) ( None , 256 ) 327936 fc_value_flatten_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ fc_out ( Dense ) ( None , 51 ) 13107 fc_1 [ 0 ][ 0 ] __________________________________________________________________________________________________ value_out ( Dense ) ( None , 1 ) 257 fc_value_1 [ 0 ][ 0 ] ================================================================================================== Total params : 709 , 172 Trainable params : 709 , 172 Non - trainable params : 0 __________________________________________________________________________________________________ Source code in corl/models/frame_stacking.py class FrameStackingModel ( TFModelV2 ): # pylint: disable=abstract-method \"\"\" The following class is a slight modification of the base Fully Connected Model within RLLIB. The model adds on the ability to do frame stacking. This is done within the model to (1) enable the HPARAM search over the setting for the number of frames - Not possible with environment wrappers (2) enable a more in line view of the framestacking inline with paper representation. Note: Unlike the environment wrapper which just concatenates the data into one large vector and sends through network. This implementation will process N identical obs (vectors) through all hidden layers until the last FC layer which inputs are flattened to ensure that we are only producing the expected number of output actions The architecture produced is located below: - Note: Parallel paths. These are only generated if the Value function is defined as not sharing network. In the case that the network is shared only a single path would exist. - Note: All of the default capability of the default RLLIB model is maintained and may not be shown in diagram. Further testing needed for other paths. FC Layers 1-N |-----| |---------->|Dense|-| | |-----| |-| |-------| |-----| | |-----| |------->|Flatten|---->| FC |---| | |-----| |-------| |-----| | N Obs |--------| | Model -------------> | | | (opt) N Reward | | |------->[FC Out ] ------> Actions -------------> | Inputs | (opt) N Action | | |------->[Value Out] ------> Values -------------> |--------| | | |-----| | |---------->|Dense|-| | |-----| |-| |-------| |-----| | |-----| |------->|Flatten|---->| FC |---| |-----| |-------| |-----| |----------------------------------------- | | Note: Ensure final output is in | | 1 X ACTION not 1 X Frame X ACTION | |------------------------------------------| Inputs = 1 X Frames X (Obs + Rewards + Actions) Outputs = 1 X Actions The following is a example summary of the model from tensor flow on the base Single Environments Model: \"functional_1\" __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== observations (InputLayer) [(None, 5, 77)] 0 __________________________________________________________________________________________________ fc_0 (Dense) (None, 5, 256) 19968 observations[0][0] __________________________________________________________________________________________________ fc_value_0 (Dense) (None, 5, 256) 19968 observations[0][0] __________________________________________________________________________________________________ fc_flatten_1 (Flatten) (None, 1280) 0 fc_0[0][0] __________________________________________________________________________________________________ fc_value_flatten_1 (Flatten) (None, 1280) 0 fc_value_0[0][0] __________________________________________________________________________________________________ fc_1 (Dense) (None, 256) 327936 fc_flatten_1[0][0] __________________________________________________________________________________________________ fc_value_1 (Dense) (None, 256) 327936 fc_value_flatten_1[0][0] __________________________________________________________________________________________________ fc_out (Dense) (None, 51) 13107 fc_1[0][0] __________________________________________________________________________________________________ value_out (Dense) (None, 1) 257 fc_value_1[0][0] ================================================================================================== Total params: 709,172 Trainable params: 709,172 Non-trainable params: 0 __________________________________________________________________________________________________ Arguments: TFModelV2 {[type]} -- [description] \"\"\" PREV_N_OBS = \"prev_n_obs\" PREV_N_REWARDS = \"prev_n_rewards\" PREV_N_ACTIONS = \"prev_n_actions\" def __init__ ( self , obs_space , action_space , num_outputs , model_config , name , post_fcnet_hiddens = None , num_frames : int = 4 , include_actions : bool = True , include_rewards : bool = True , ): \"\"\"Class constructor Arguments: obs_space (gym.spaces.Space): Observation space of the target gym env. This may have an `original_space` attribute that specifies how to unflatten the tensor into a ragged tensor. action_space (gym.spaces.Space): Action space of the target gym env. num_outputs (int): Number of output units of the model. model_config (ModelConfigDict): Config for the model, documented in ModelCatalog. name (str): Name (scope) for the model. This method should create any variables used by the model. Keyword Arguments: num_frames {int} -- The number of frames to stack (default: {4}) include_actions {int} -- Whether or not to include actions as part of frame stacking (default: True) include_actions {int} -- Whether or not to include actions as part of frame stacking (default: True) Returns: [type] -- [description] \"\"\" if post_fcnet_hiddens is None : post_fcnet_hiddens = [] # Initializes a ModelV2 object. TFModelV2 . __init__ ( self , obs_space , action_space , num_outputs , model_config , name ) # This model specific items self . num_frames = num_frames # Base model items self . num_outputs = num_outputs # Read out the model configuration parameters passed by RLLIB. Note this is maintained to ensure # compatibility with existing setup free_log_std , hiddens , activation , no_final_linear , vf_share_layers = self . get_config_opts () # Generate free-floating bias variables for the second half of # the outputs. if free_log_std : assert num_outputs % 2 == 0 , ( \"num_outputs must be divisible by two\" , num_outputs ) num_outputs = num_outputs // 2 self . log_std_var = tf . Variable ([ 0.0 ] * num_outputs , dtype = tf . float32 , name = \"log_std\" ) # Create the input layers for the observations, actions, and rewards flattened_action_space = flatten_space ( action_space ) observations , actions , rewards = self . create_input_layers ( obs_space , flattened_action_space ) # Select the input layer configuration based on input arguments self . include_rewards = include_rewards self . include_actions = include_actions self . input_list , self . inputs = FrameStackingModel . select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ) # Create layers 0 to second-last. last_layer = self . create_dense_hidden_layers ( hiddens , self . inputs , activation , \"fc\" ) # The action distribution outputs. logits_out , last_layer = self . create_last_fc_layer_output ( no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ) # Concat the log std vars to the end of the state-dependent means. if free_log_std and logits_out is not None : def tiled_log_std ( x ): return tf . tile ( tf . expand_dims ( self . log_std_var , 0 ), [ tf . shape ( x )[ 0 ], 1 ]) log_std_out = tf . keras . layers . Lambda ( tiled_log_std )( self . inputs ) logits_out = tf . keras . layers . Concatenate ( axis = 1 )([ logits_out , log_std_out ]) last_vf_layer = self . build_vf_network ( vf_share_layers , self . inputs , hiddens , post_fcnet_hiddens , activation ) value_out = tf . keras . layers . Dense ( 1 , name = \"value_out\" , activation = None , kernel_initializer = normc_initializer ( 0.01 ) )( last_vf_layer if last_vf_layer is not None else last_layer ) self . base_model = tf . keras . Model ( self . input_list , [( logits_out if logits_out is not None else last_layer ), value_out ]) # print(self.base_model.summary()) self . register_view_requirements ( num_frames , obs_space , flattened_action_space ) self . _value_out = None @staticmethod def select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ): \"\"\"Sets up the input layer based on the configuration of the arguments to the model Arguments: include_rewards {bool} -- Flag to indicate that rewards should be part of frame stacking include_actions {bool} -- Flag to indicate that the actions should be part of frame stacking observations {Tensor} -- [description] actions {Tensor} -- [description] rewards {Tensor} -- [description] \"\"\" # Last hidden layer output (before logits outputs). if include_actions and not include_rewards : input_list = [ observations , actions ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) elif not include_actions and include_rewards : input_list = [ observations , rewards ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) elif include_actions and include_rewards : input_list = [ observations , actions , rewards ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) else : input_list = [ observations ] inputs = observations return input_list , inputs def create_input_layers ( self , obs_space , action_space ): \"\"\"Creats the input layers for starting the graph Arguments: obs_space {gym.Space} -- The input space - flattended action_space {gym.Space} -- The input space - flattended Returns: tuple[tensor] -- The input layers for observations, rewards, actions \"\"\" # (?, Number of Frames, 1) rewards = tf . keras . layers . Input ( shape = ( self . num_frames , 1 ), name = \"rewards\" ) # (?, Number of Frames, len obs flatten) observations = tf . keras . layers . Input ( shape = ( self . num_frames , obs_space . shape [ 0 ]), name = \"observations\" ) # (?, Number of Frames, len actions flatten) actions = tf . keras . layers . Input ( shape = ( self . num_frames , len ( action_space )), name = \"actions\" ) return observations , actions , rewards def create_last_fc_layer_output ( self , no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ): \"\"\"[summary] Arguments: no_final_linear {[type]} -- [description] num_outputs {[type]} -- [description] activation {[type]} -- [description] last_layer {[type]} -- [description] hiddens {[type]} -- [description] obs_space {[type]} -- [description] Returns: [type] -- [description] \"\"\" # The action distribution outputs. logits_out = None # The last layer is adjusted to be of size num_outputs, but it's a # layer with activation. if no_final_linear and num_outputs : logits_out = tf . keras . layers . Dense ( num_outputs , name = \"fc_out\" , activation = activation , kernel_initializer = normc_initializer ( 1.0 ) )( last_layer ) # Finish the layers with the provided sizes (`hiddens`), plus - # iff num_outputs > 0 - a last linear layer of size num_outputs. else : if len ( hiddens ) > 0 : last_layer = FrameStackingModel . flatten_plus_dense ( hiddens , post_fcnet_hiddens , last_layer , activation , \"fc\" , len ( hiddens ) - 1 ) if num_outputs : logits_out = tf . keras . layers . Dense ( num_outputs , name = \"fc_out\" , activation = None , kernel_initializer = normc_initializer ( 0.01 ))( last_layer ) # Adjust num_outputs to be the number of nodes in the last layer. else : self . num_outputs = ([ int ( np . product ( obs_space . shape ))] + hiddens [ - 1 :])[ - 1 ] return logits_out , last_layer def build_vf_network ( self , vf_share_layers , inputs , hiddens , flatten_plus_dense , activation ): \"\"\"Creates the value function network if configured in model config Arguments: vf_share_layers {[type]} -- [description] inputs {[type]} -- [description] hiddens {[type]} -- [description] activation {[type]} -- [description] Returns: [type] -- [description] \"\"\" last_vf_layer = None if not vf_share_layers : # Build a parallel set of hidden layers for the value net. value_function_prefix = \"fc_value\" last_vf_layer = self . create_dense_hidden_layers ( hiddens , inputs , activation , value_function_prefix ) last_vf_layer = self . flatten_plus_dense ( hiddens , flatten_plus_dense , last_vf_layer , activation , value_function_prefix , len ( hiddens ) - 1 ) return last_vf_layer def register_view_requirements ( self , num_frames : int , obs_space , flattened_action_space ): \"\"\"Sets up the view requirements for the forward pass call Arguments: num_frames {int} -- The number of frames to stack obs_space {[type]} -- The observation space definition flattened_action_space {[type]} -- flattened action space \"\"\" self . view_requirements [ FrameStackingModel . PREV_N_OBS ] = ViewRequirement ( data_col = \"obs\" , shift = \"- {} :0\" . format ( num_frames - 1 ), space = obs_space ) if self . include_rewards : self . view_requirements [ FrameStackingModel . PREV_N_REWARDS ] = ViewRequirement ( data_col = \"rewards\" , shift = \"- {} :-1\" . format ( self . num_frames )) if self . include_actions : self . view_requirements [ FrameStackingModel . PREV_N_ACTIONS ] = ViewRequirement ( data_col = \"actions\" , shift = \"- {} :-1\" . format ( self . num_frames ), space = gym . spaces . box . Box ( low =- np . inf , high = np . inf , shape = ( len ( flattened_action_space ), ), dtype = np . int64 ) ) def forward ( self , input_dict : Dict [ str , TensorType ], state : List [ TensorType ], seq_lens : TensorType ) -> ( TensorType , List [ TensorType ]): # type: ignore \"\"\" Call the model with the given input tensors and state. Any complex observations (dicts, tuples, etc.) will be unpacked by __call__ before being passed to forward(). To access the flattened observation tensor, refer to input_dict[\u201cobs\u201d]. This method can be called any number of times. In eager execution, each call to forward() will eagerly evaluate the model. In symbolic execution, each call to forward creates a computation graph that operates over the variables of this model (i.e., shares weights). Custom models should override this instead of __call__. Arguments: input_dict (dict) \u2013 dictionary of input tensors, including \u201cobs\u201d, \u201cobs_flat\u201d, \u201cprev_action\u201d, \u201cprev_reward\u201d, \u201cis_training\u201d, \u201ceps_id\u201d, \u201cagent_id\u201d, \u201cinfos\u201d, and \u201ct\u201d. state (list) \u2013 list of state tensors with sizes matching those returned by get_initial_state + the batch dimension seq_lens (Tensor) \u2013 1d tensor holding input sequence lengths Returns: The model output tensor of size [BATCH, num_outputs], and the new RNN state. \"\"\" if self . include_actions and not self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_ACTIONS ]]) elif not self . include_actions and self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_REWARDS ]]) elif self . include_actions and self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_ACTIONS ], input_dict [ FrameStackingModel . PREV_N_REWARDS ]]) else : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ]]) return model_out , state def value_function ( self ) -> TensorType : \"\"\"Returns the value function output for the most recent forward pass. Note that a forward call has to be performed first, before this methods can return anything and thus that calling this method does not cause an extra forward pass through the network. Returns: value estimate tensor of shape [BATCH]. \"\"\" return tf . reshape ( self . _value_out , [ - 1 ]) def get_config_opts ( self ): \"\"\"Gets the configuration options utilizes by the frame stacking model Returns: Tuple -- configuration options for the models (Bool, list[int], function, Bool, Bool) \"\"\" hiddens = self . model_config . get ( \"fcnet_hiddens\" , []) + self . model_config . get ( \"post_fcnet_hiddens\" , []) activation = self . model_config . get ( \"fcnet_activation\" ) if not self . model_config . get ( \"fcnet_hiddens\" , []): activation = self . model_config . get ( \"post_fcnet_activation\" ) activation = get_activation_fn ( activation ) no_final_linear = self . model_config . get ( \"no_final_linear\" ) vf_share_layers = self . model_config . get ( \"vf_share_layers\" ) free_log_std = self . model_config . get ( \"free_log_std\" ) return free_log_std , hiddens , activation , no_final_linear , vf_share_layers @staticmethod def create_dense_hidden_layers ( hiddens , layer , activation , prefix : str ): \"\"\"Creates the hidden dense layers Arguments: hiddens {List[int]} -- The list of hidden layers for the FC componets layer {Tensor} -- [description] --- TODO Remove as not needed to pass in... activation {Function} -- [description] prefix {str} -- The string to use for the naming of the layer Returns: [type] -- [description] \"\"\" for index , size in enumerate ( hiddens [: - 1 ]): dense_name = f \" { prefix } _ { index } \" layer = tf . keras . layers . Dense ( size , name = dense_name , activation = activation , kernel_initializer = normc_initializer ( 1.0 ))( layer ) return layer @staticmethod def flatten_plus_dense ( hiddens , post_fcnet_hiddens , layer , activation , prefix : str , index : int ): \"\"\"Creates the final/last dense layer with flatten to ensure the correct output size Arguments: hiddens {List[int]} -- List containing the size of each hidden layer layer {Tensor} -- [description] activation {function]} -- [description] prefix {str} -- The string to use when creating the layers index {int} -- The index of the layer to add the flatten on. Returns: Tensor -- The layer just created with flatten + FD \"\"\" flatten_name = f \" { prefix } _flatten_ { index } \" dense_name = f \" { prefix } _ { index } \" layer = tf . keras . layers . Flatten ( name = flatten_name )( layer ) layer = tf . keras . layers . Dense ( hiddens [ index ], name = dense_name , activation = activation , kernel_initializer = normc_initializer ( 1.0 ))( layer ) for index_cat , size in enumerate ( post_fcnet_hiddens ): layer = tf . keras . layers . Dense ( size , name = f \" { dense_name } _cat_ { index_cat } \" , activation = activation , kernel_initializer = normc_initializer ( 1.0 ) )( layer ) return layer","title":"FrameStackingModel"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.__init__","text":"Class constructor Parameters: Name Type Description Default obs_space gym.spaces.Space Observation space of the target gym env. This may have an original_space attribute that specifies how to unflatten the tensor into a ragged tensor. required action_space gym.spaces.Space Action space of the target gym env. required num_outputs int Number of output units of the model. required model_config ModelConfigDict Config for the model, documented in ModelCatalog. required name str Name (scope) for the model. required This method should create any variables used by the model. Keyword arguments: Name Type Description num_frames {int} -- The number of frames to stack (default {4}) include_actions {int} -- Whether or not to include actions as part of frame stacking (default True) include_actions {int} -- Whether or not to include actions as part of frame stacking (default True) Returns: Type Description [type] -- [description] Source code in corl/models/frame_stacking.py def __init__ ( self , obs_space , action_space , num_outputs , model_config , name , post_fcnet_hiddens = None , num_frames : int = 4 , include_actions : bool = True , include_rewards : bool = True , ): \"\"\"Class constructor Arguments: obs_space (gym.spaces.Space): Observation space of the target gym env. This may have an `original_space` attribute that specifies how to unflatten the tensor into a ragged tensor. action_space (gym.spaces.Space): Action space of the target gym env. num_outputs (int): Number of output units of the model. model_config (ModelConfigDict): Config for the model, documented in ModelCatalog. name (str): Name (scope) for the model. This method should create any variables used by the model. Keyword Arguments: num_frames {int} -- The number of frames to stack (default: {4}) include_actions {int} -- Whether or not to include actions as part of frame stacking (default: True) include_actions {int} -- Whether or not to include actions as part of frame stacking (default: True) Returns: [type] -- [description] \"\"\" if post_fcnet_hiddens is None : post_fcnet_hiddens = [] # Initializes a ModelV2 object. TFModelV2 . __init__ ( self , obs_space , action_space , num_outputs , model_config , name ) # This model specific items self . num_frames = num_frames # Base model items self . num_outputs = num_outputs # Read out the model configuration parameters passed by RLLIB. Note this is maintained to ensure # compatibility with existing setup free_log_std , hiddens , activation , no_final_linear , vf_share_layers = self . get_config_opts () # Generate free-floating bias variables for the second half of # the outputs. if free_log_std : assert num_outputs % 2 == 0 , ( \"num_outputs must be divisible by two\" , num_outputs ) num_outputs = num_outputs // 2 self . log_std_var = tf . Variable ([ 0.0 ] * num_outputs , dtype = tf . float32 , name = \"log_std\" ) # Create the input layers for the observations, actions, and rewards flattened_action_space = flatten_space ( action_space ) observations , actions , rewards = self . create_input_layers ( obs_space , flattened_action_space ) # Select the input layer configuration based on input arguments self . include_rewards = include_rewards self . include_actions = include_actions self . input_list , self . inputs = FrameStackingModel . select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ) # Create layers 0 to second-last. last_layer = self . create_dense_hidden_layers ( hiddens , self . inputs , activation , \"fc\" ) # The action distribution outputs. logits_out , last_layer = self . create_last_fc_layer_output ( no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ) # Concat the log std vars to the end of the state-dependent means. if free_log_std and logits_out is not None : def tiled_log_std ( x ): return tf . tile ( tf . expand_dims ( self . log_std_var , 0 ), [ tf . shape ( x )[ 0 ], 1 ]) log_std_out = tf . keras . layers . Lambda ( tiled_log_std )( self . inputs ) logits_out = tf . keras . layers . Concatenate ( axis = 1 )([ logits_out , log_std_out ]) last_vf_layer = self . build_vf_network ( vf_share_layers , self . inputs , hiddens , post_fcnet_hiddens , activation ) value_out = tf . keras . layers . Dense ( 1 , name = \"value_out\" , activation = None , kernel_initializer = normc_initializer ( 0.01 ) )( last_vf_layer if last_vf_layer is not None else last_layer ) self . base_model = tf . keras . Model ( self . input_list , [( logits_out if logits_out is not None else last_layer ), value_out ]) # print(self.base_model.summary()) self . register_view_requirements ( num_frames , obs_space , flattened_action_space ) self . _value_out = None","title":"__init__()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.build_vf_network","text":"Creates the value function network if configured in model config Returns: Type Description [type] -- [description] Source code in corl/models/frame_stacking.py def build_vf_network ( self , vf_share_layers , inputs , hiddens , flatten_plus_dense , activation ): \"\"\"Creates the value function network if configured in model config Arguments: vf_share_layers {[type]} -- [description] inputs {[type]} -- [description] hiddens {[type]} -- [description] activation {[type]} -- [description] Returns: [type] -- [description] \"\"\" last_vf_layer = None if not vf_share_layers : # Build a parallel set of hidden layers for the value net. value_function_prefix = \"fc_value\" last_vf_layer = self . create_dense_hidden_layers ( hiddens , inputs , activation , value_function_prefix ) last_vf_layer = self . flatten_plus_dense ( hiddens , flatten_plus_dense , last_vf_layer , activation , value_function_prefix , len ( hiddens ) - 1 ) return last_vf_layer","title":"build_vf_network()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.create_dense_hidden_layers","text":"Creates the hidden dense layers Returns: Type Description [type] -- [description] Source code in corl/models/frame_stacking.py @staticmethod def create_dense_hidden_layers ( hiddens , layer , activation , prefix : str ): \"\"\"Creates the hidden dense layers Arguments: hiddens {List[int]} -- The list of hidden layers for the FC componets layer {Tensor} -- [description] --- TODO Remove as not needed to pass in... activation {Function} -- [description] prefix {str} -- The string to use for the naming of the layer Returns: [type] -- [description] \"\"\" for index , size in enumerate ( hiddens [: - 1 ]): dense_name = f \" { prefix } _ { index } \" layer = tf . keras . layers . Dense ( size , name = dense_name , activation = activation , kernel_initializer = normc_initializer ( 1.0 ))( layer ) return layer","title":"create_dense_hidden_layers()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.create_input_layers","text":"Creats the input layers for starting the graph Returns: Type Description tuple[tensor] -- The input layers for observations, rewards, actions Source code in corl/models/frame_stacking.py def create_input_layers ( self , obs_space , action_space ): \"\"\"Creats the input layers for starting the graph Arguments: obs_space {gym.Space} -- The input space - flattended action_space {gym.Space} -- The input space - flattended Returns: tuple[tensor] -- The input layers for observations, rewards, actions \"\"\" # (?, Number of Frames, 1) rewards = tf . keras . layers . Input ( shape = ( self . num_frames , 1 ), name = \"rewards\" ) # (?, Number of Frames, len obs flatten) observations = tf . keras . layers . Input ( shape = ( self . num_frames , obs_space . shape [ 0 ]), name = \"observations\" ) # (?, Number of Frames, len actions flatten) actions = tf . keras . layers . Input ( shape = ( self . num_frames , len ( action_space )), name = \"actions\" ) return observations , actions , rewards","title":"create_input_layers()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.create_last_fc_layer_output","text":"[summary] Returns: Type Description [type] -- [description] Source code in corl/models/frame_stacking.py def create_last_fc_layer_output ( self , no_final_linear , num_outputs , activation , last_layer , hiddens , post_fcnet_hiddens , obs_space ): \"\"\"[summary] Arguments: no_final_linear {[type]} -- [description] num_outputs {[type]} -- [description] activation {[type]} -- [description] last_layer {[type]} -- [description] hiddens {[type]} -- [description] obs_space {[type]} -- [description] Returns: [type] -- [description] \"\"\" # The action distribution outputs. logits_out = None # The last layer is adjusted to be of size num_outputs, but it's a # layer with activation. if no_final_linear and num_outputs : logits_out = tf . keras . layers . Dense ( num_outputs , name = \"fc_out\" , activation = activation , kernel_initializer = normc_initializer ( 1.0 ) )( last_layer ) # Finish the layers with the provided sizes (`hiddens`), plus - # iff num_outputs > 0 - a last linear layer of size num_outputs. else : if len ( hiddens ) > 0 : last_layer = FrameStackingModel . flatten_plus_dense ( hiddens , post_fcnet_hiddens , last_layer , activation , \"fc\" , len ( hiddens ) - 1 ) if num_outputs : logits_out = tf . keras . layers . Dense ( num_outputs , name = \"fc_out\" , activation = None , kernel_initializer = normc_initializer ( 0.01 ))( last_layer ) # Adjust num_outputs to be the number of nodes in the last layer. else : self . num_outputs = ([ int ( np . product ( obs_space . shape ))] + hiddens [ - 1 :])[ - 1 ] return logits_out , last_layer","title":"create_last_fc_layer_output()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.flatten_plus_dense","text":"Creates the final/last dense layer with flatten to ensure the correct output size Returns: Type Description Tensor -- The layer just created with flatten + FD Source code in corl/models/frame_stacking.py @staticmethod def flatten_plus_dense ( hiddens , post_fcnet_hiddens , layer , activation , prefix : str , index : int ): \"\"\"Creates the final/last dense layer with flatten to ensure the correct output size Arguments: hiddens {List[int]} -- List containing the size of each hidden layer layer {Tensor} -- [description] activation {function]} -- [description] prefix {str} -- The string to use when creating the layers index {int} -- The index of the layer to add the flatten on. Returns: Tensor -- The layer just created with flatten + FD \"\"\" flatten_name = f \" { prefix } _flatten_ { index } \" dense_name = f \" { prefix } _ { index } \" layer = tf . keras . layers . Flatten ( name = flatten_name )( layer ) layer = tf . keras . layers . Dense ( hiddens [ index ], name = dense_name , activation = activation , kernel_initializer = normc_initializer ( 1.0 ))( layer ) for index_cat , size in enumerate ( post_fcnet_hiddens ): layer = tf . keras . layers . Dense ( size , name = f \" { dense_name } _cat_ { index_cat } \" , activation = activation , kernel_initializer = normc_initializer ( 1.0 ) )( layer ) return layer","title":"flatten_plus_dense()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.forward","text":"Call the model with the given input tensors and state. Any complex observations (dicts, tuples, etc.) will be unpacked by call before being passed to forward(). To access the flattened observation tensor, refer to input_dict[\u201cobs\u201d]. This method can be called any number of times. In eager execution, each call to forward() will eagerly evaluate the model. In symbolic execution, each call to forward creates a computation graph that operates over the variables of this model (i.e., shares weights). Custom models should override this instead of call . Returns: Type Description (Any, List[Any]) The model output tensor of size [BATCH, num_outputs], and the new RNN state. Source code in corl/models/frame_stacking.py def forward ( self , input_dict : Dict [ str , TensorType ], state : List [ TensorType ], seq_lens : TensorType ) -> ( TensorType , List [ TensorType ]): # type: ignore \"\"\" Call the model with the given input tensors and state. Any complex observations (dicts, tuples, etc.) will be unpacked by __call__ before being passed to forward(). To access the flattened observation tensor, refer to input_dict[\u201cobs\u201d]. This method can be called any number of times. In eager execution, each call to forward() will eagerly evaluate the model. In symbolic execution, each call to forward creates a computation graph that operates over the variables of this model (i.e., shares weights). Custom models should override this instead of __call__. Arguments: input_dict (dict) \u2013 dictionary of input tensors, including \u201cobs\u201d, \u201cobs_flat\u201d, \u201cprev_action\u201d, \u201cprev_reward\u201d, \u201cis_training\u201d, \u201ceps_id\u201d, \u201cagent_id\u201d, \u201cinfos\u201d, and \u201ct\u201d. state (list) \u2013 list of state tensors with sizes matching those returned by get_initial_state + the batch dimension seq_lens (Tensor) \u2013 1d tensor holding input sequence lengths Returns: The model output tensor of size [BATCH, num_outputs], and the new RNN state. \"\"\" if self . include_actions and not self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_ACTIONS ]]) elif not self . include_actions and self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_REWARDS ]]) elif self . include_actions and self . include_rewards : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ], input_dict [ FrameStackingModel . PREV_N_ACTIONS ], input_dict [ FrameStackingModel . PREV_N_REWARDS ]]) else : model_out , self . _value_out = self . base_model ([ input_dict [ FrameStackingModel . PREV_N_OBS ]]) return model_out , state","title":"forward()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.get_config_opts","text":"Gets the configuration options utilizes by the frame stacking model Returns: Type Description Tuple -- configuration options for the models (Bool, list[int], function, Bool, Bool) Source code in corl/models/frame_stacking.py def get_config_opts ( self ): \"\"\"Gets the configuration options utilizes by the frame stacking model Returns: Tuple -- configuration options for the models (Bool, list[int], function, Bool, Bool) \"\"\" hiddens = self . model_config . get ( \"fcnet_hiddens\" , []) + self . model_config . get ( \"post_fcnet_hiddens\" , []) activation = self . model_config . get ( \"fcnet_activation\" ) if not self . model_config . get ( \"fcnet_hiddens\" , []): activation = self . model_config . get ( \"post_fcnet_activation\" ) activation = get_activation_fn ( activation ) no_final_linear = self . model_config . get ( \"no_final_linear\" ) vf_share_layers = self . model_config . get ( \"vf_share_layers\" ) free_log_std = self . model_config . get ( \"free_log_std\" ) return free_log_std , hiddens , activation , no_final_linear , vf_share_layers","title":"get_config_opts()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.register_view_requirements","text":"Sets up the view requirements for the forward pass call Source code in corl/models/frame_stacking.py def register_view_requirements ( self , num_frames : int , obs_space , flattened_action_space ): \"\"\"Sets up the view requirements for the forward pass call Arguments: num_frames {int} -- The number of frames to stack obs_space {[type]} -- The observation space definition flattened_action_space {[type]} -- flattened action space \"\"\" self . view_requirements [ FrameStackingModel . PREV_N_OBS ] = ViewRequirement ( data_col = \"obs\" , shift = \"- {} :0\" . format ( num_frames - 1 ), space = obs_space ) if self . include_rewards : self . view_requirements [ FrameStackingModel . PREV_N_REWARDS ] = ViewRequirement ( data_col = \"rewards\" , shift = \"- {} :-1\" . format ( self . num_frames )) if self . include_actions : self . view_requirements [ FrameStackingModel . PREV_N_ACTIONS ] = ViewRequirement ( data_col = \"actions\" , shift = \"- {} :-1\" . format ( self . num_frames ), space = gym . spaces . box . Box ( low =- np . inf , high = np . inf , shape = ( len ( flattened_action_space ), ), dtype = np . int64 ) )","title":"register_view_requirements()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.select_input_layer_configuration","text":"Sets up the input layer based on the configuration of the arguments to the model Source code in corl/models/frame_stacking.py @staticmethod def select_input_layer_configuration ( include_rewards , include_actions , observations , actions , rewards ): \"\"\"Sets up the input layer based on the configuration of the arguments to the model Arguments: include_rewards {bool} -- Flag to indicate that rewards should be part of frame stacking include_actions {bool} -- Flag to indicate that the actions should be part of frame stacking observations {Tensor} -- [description] actions {Tensor} -- [description] rewards {Tensor} -- [description] \"\"\" # Last hidden layer output (before logits outputs). if include_actions and not include_rewards : input_list = [ observations , actions ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) elif not include_actions and include_rewards : input_list = [ observations , rewards ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) elif include_actions and include_rewards : input_list = [ observations , actions , rewards ] inputs = tf . keras . layers . Concatenate ( axis =- 1 )( input_list ) else : input_list = [ observations ] inputs = observations return input_list , inputs","title":"select_input_layer_configuration()"},{"location":"reference/models/frame_stacking/#corl.models.frame_stacking.FrameStackingModel.value_function","text":"Returns the value function output for the most recent forward pass. Note that a forward call has to be performed first, before this methods can return anything and thus that calling this method does not cause an extra forward pass through the network. Returns: Type Description Any value estimate tensor of shape [BATCH]. Source code in corl/models/frame_stacking.py def value_function ( self ) -> TensorType : \"\"\"Returns the value function output for the most recent forward pass. Note that a forward call has to be performed first, before this methods can return anything and thus that calling this method does not cause an extra forward pass through the network. Returns: value estimate tensor of shape [BATCH]. \"\"\" return tf . reshape ( self . _value_out , [ - 1 ])","title":"value_function()"},{"location":"reference/models/torch_frame_stack/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. TorchFrameStack ( TorchModelV2 , Module ) \u00a4 Generic fully connected network. Source code in corl/models/torch_frame_stack.py class TorchFrameStack ( TorchModelV2 , nn . Module ): # type: ignore \"\"\"Generic fully connected network.\"\"\" PREV_N_OBS = \"prev_n_obs\" def __init__ ( self , obs_space : gym . spaces . Space , action_space : gym . spaces . Space , num_outputs : int , model_config : ModelConfigDict , name : str , num_frames : int = 1 ): TorchModelV2 . __init__ ( self , obs_space , action_space , num_outputs , model_config , name ) nn . Module . __init__ ( self ) hiddens = list ( model_config . get ( \"fcnet_hiddens\" , [])) post_fcnet_hiddens = list ( model_config . get ( \"post_fcnet_hiddens\" , [])) activation = model_config . get ( \"fcnet_activation\" ) if not model_config . get ( \"fcnet_hiddens\" , []): activation = model_config . get ( \"post_fcnet_activation\" ) no_final_linear = model_config . get ( \"no_final_linear\" ) self . vf_share_layers = model_config . get ( \"vf_share_layers\" ) self . free_log_std = model_config . get ( \"free_log_std\" ) num_frames = model_config [ \"custom_model_config\" ] . get ( \"num_frames\" , 1 ) self . view_requirements [ TorchFrameStack . PREV_N_OBS ] = ViewRequirement ( data_col = \"obs\" , shift = \"- {} :0\" . format ( num_frames - 1 ), space = obs_space ) # Generate free-floating bias variables for the second half of # the outputs. if self . free_log_std : assert num_outputs % 2 == 0 , ( \"num_outputs must be divisible by two\" , num_outputs ) num_outputs = num_outputs // 2 layers = [] prev_layer_size = int ( obs_space . shape [ - 1 ]) self . _logits = None # Create layers 0 to second-last. for size in hiddens [: - 1 ]: layers . append ( SlimFC ( in_size = prev_layer_size , out_size = size , initializer = normc_initializer ( 1.0 ), activation_fn = activation )) prev_layer_size = size layers . append ( nn . Flatten ()) prev_layer_size = size * num_frames for size in post_fcnet_hiddens : layers . append ( SlimFC ( in_size = prev_layer_size , out_size = size , initializer = normc_initializer ( 1.0 ), activation_fn = activation )) prev_layer_size = size # The last layer is adjusted to be of size num_outputs, but it's a # layer with activation. if no_final_linear and num_outputs : layers . append ( SlimFC ( in_size = prev_layer_size , out_size = num_outputs , initializer = normc_initializer ( 1.0 ), activation_fn = activation ) ) prev_layer_size = num_outputs # Finish the layers with the provided sizes (`hiddens`), plus - # iff num_outputs > 0 - a last linear layer of size num_outputs. else : if len ( hiddens ) > 0 : layers . append ( SlimFC ( in_size = prev_layer_size , out_size = hiddens [ - 1 ], initializer = normc_initializer ( 1.0 ), activation_fn = activation ) ) prev_layer_size = hiddens [ - 1 ] if num_outputs : self . _logits = SlimFC ( in_size = prev_layer_size , out_size = num_outputs , initializer = normc_initializer ( 0.01 ), activation_fn = None ) else : self . num_outputs = ([ int ( np . product ( obs_space . shape ))] + hiddens [ - 1 :])[ - 1 ] # Layer to add the log std vars to the state-dependent means. if self . free_log_std and self . _logits : self . _append_free_log_std = AppendBiasLayer ( num_outputs ) self . _hidden_layers = nn . Sequential ( * layers ) self . _value_branch_separate = None if not self . vf_share_layers : # Build a parallel set of hidden layers for the value net. prev_vf_layer_size = int ( obs_space . shape [ - 1 ]) vf_layers = [] for size in hiddens [: - 1 ]: vf_layers . append ( SlimFC ( in_size = prev_vf_layer_size , out_size = size , activation_fn = activation , initializer = normc_initializer ( 1.0 )) ) prev_vf_layer_size = size vf_layers . append ( nn . Flatten ()) prev_vf_layer_size = size * num_frames vf_layers . append ( SlimFC ( in_size = prev_vf_layer_size , out_size = hiddens [ - 1 ], activation_fn = activation , initializer = normc_initializer ( 1.0 )) ) prev_vf_layer_size = hiddens [ - 1 ] # for size in hiddens: # vf_layers.append( # SlimFC( # in_size=prev_vf_layer_size, # out_size=size, # activation_fn=activation, # initializer=normc_initializer(1.0))) # prev_vf_layer_size = size self . _value_branch_separate = nn . Sequential ( * vf_layers ) self . _value_branch = SlimFC ( in_size = prev_layer_size , out_size = 1 , initializer = normc_initializer ( 0.01 ), activation_fn = None ) # print(\"*************************************************\") # print(model_config) # print(\"************************************************\") # print(num_frames) # print(\"************************************************\") # print(self._hidden_layers) # print(self._logits) # print(\"***********************************************\") # print(self._value_branch_separate) # print(self._value_branch) # # exit(1) # Holds the current \"base\" output (before logits layer). self . _features = None # Holds the last input, in case value branch is separate. self . _last_flat_in = None @override ( TorchModelV2 ) def forward ( self , input_dict : Dict [ str , TensorType ], state : List [ TensorType ], seq_lens : TensorType ) -> ( TensorType , List [ TensorType ]): # type: ignore self . _last_flat_in = input_dict [ TorchFrameStack . PREV_N_OBS ] . float () self . _features = self . _hidden_layers ( self . _last_flat_in ) logits = self . _logits ( self . _features ) if self . _logits else \\ self . _features if self . free_log_std : logits = self . _append_free_log_std ( logits ) return logits , state @override ( TorchModelV2 ) def value_function ( self ) -> TensorType : assert self . _features is not None , \"must call forward() first\" if self . _value_branch_separate : return self . _value_branch ( self . _value_branch_separate ( self . _last_flat_in )) . squeeze ( 1 ) else : return self . _value_branch ( self . _features ) . squeeze ( 1 ) forward ( self , input_dict , state , seq_lens ) \u00a4 Call the model with the given input tensors and state. Any complex observations (dicts, tuples, etc.) will be unpacked by call before being passed to forward(). To access the flattened observation tensor, refer to input_dict[\"obs_flat\"]. This method can be called any number of times. In eager execution, each call to forward() will eagerly evaluate the model. In symbolic execution, each call to forward creates a computation graph that operates over the variables of this model (i.e., shares weights). Custom models should override this instead of call . Parameters: Name Type Description Default input_dict Dict[str, Any] dictionary of input tensors, including \"obs\", \"obs_flat\", \"prev_action\", \"prev_reward\", \"is_training\", \"eps_id\", \"agent_id\", \"infos\", and \"t\". required state List[Any] list of state tensors with sizes matching those returned by get_initial_state + the batch dimension required seq_lens Any 1d tensor holding input sequence lengths required Returns: Type Description (Any, List[Any]) A tuple consisting of the model output tensor of size [BATCH, num_outputs] and the list of new RNN state(s) if any. Examples: >>> import numpy as np >>> from ray.rllib.models.modelv2 import ModelV2 >>> class MyModel ( ModelV2 ): ... # ... >>> def forward ( self , input_dict , state , seq_lens ): >>> model_out , self . _value_out = self . base_model ( ... input_dict [ \"obs\" ]) >>> return model_out , state Source code in corl/models/torch_frame_stack.py @override ( TorchModelV2 ) def forward ( self , input_dict : Dict [ str , TensorType ], state : List [ TensorType ], seq_lens : TensorType ) -> ( TensorType , List [ TensorType ]): # type: ignore self . _last_flat_in = input_dict [ TorchFrameStack . PREV_N_OBS ] . float () self . _features = self . _hidden_layers ( self . _last_flat_in ) logits = self . _logits ( self . _features ) if self . _logits else \\ self . _features if self . free_log_std : logits = self . _append_free_log_std ( logits ) return logits , state value_function ( self ) \u00a4 Returns the value function output for the most recent forward pass. Note that a forward call has to be performed first, before this methods can return anything and thus that calling this method does not cause an extra forward pass through the network. Returns: Type Description Any Value estimate tensor of shape [BATCH]. Source code in corl/models/torch_frame_stack.py @override ( TorchModelV2 ) def value_function ( self ) -> TensorType : assert self . _features is not None , \"must call forward() first\" if self . _value_branch_separate : return self . _value_branch ( self . _value_branch_separate ( self . _last_flat_in )) . squeeze ( 1 ) else : return self . _value_branch ( self . _features ) . squeeze ( 1 )","title":"Torch frame stack"},{"location":"reference/models/torch_frame_stack/#corl.models.torch_frame_stack.TorchFrameStack","text":"Generic fully connected network. Source code in corl/models/torch_frame_stack.py class TorchFrameStack ( TorchModelV2 , nn . Module ): # type: ignore \"\"\"Generic fully connected network.\"\"\" PREV_N_OBS = \"prev_n_obs\" def __init__ ( self , obs_space : gym . spaces . Space , action_space : gym . spaces . Space , num_outputs : int , model_config : ModelConfigDict , name : str , num_frames : int = 1 ): TorchModelV2 . __init__ ( self , obs_space , action_space , num_outputs , model_config , name ) nn . Module . __init__ ( self ) hiddens = list ( model_config . get ( \"fcnet_hiddens\" , [])) post_fcnet_hiddens = list ( model_config . get ( \"post_fcnet_hiddens\" , [])) activation = model_config . get ( \"fcnet_activation\" ) if not model_config . get ( \"fcnet_hiddens\" , []): activation = model_config . get ( \"post_fcnet_activation\" ) no_final_linear = model_config . get ( \"no_final_linear\" ) self . vf_share_layers = model_config . get ( \"vf_share_layers\" ) self . free_log_std = model_config . get ( \"free_log_std\" ) num_frames = model_config [ \"custom_model_config\" ] . get ( \"num_frames\" , 1 ) self . view_requirements [ TorchFrameStack . PREV_N_OBS ] = ViewRequirement ( data_col = \"obs\" , shift = \"- {} :0\" . format ( num_frames - 1 ), space = obs_space ) # Generate free-floating bias variables for the second half of # the outputs. if self . free_log_std : assert num_outputs % 2 == 0 , ( \"num_outputs must be divisible by two\" , num_outputs ) num_outputs = num_outputs // 2 layers = [] prev_layer_size = int ( obs_space . shape [ - 1 ]) self . _logits = None # Create layers 0 to second-last. for size in hiddens [: - 1 ]: layers . append ( SlimFC ( in_size = prev_layer_size , out_size = size , initializer = normc_initializer ( 1.0 ), activation_fn = activation )) prev_layer_size = size layers . append ( nn . Flatten ()) prev_layer_size = size * num_frames for size in post_fcnet_hiddens : layers . append ( SlimFC ( in_size = prev_layer_size , out_size = size , initializer = normc_initializer ( 1.0 ), activation_fn = activation )) prev_layer_size = size # The last layer is adjusted to be of size num_outputs, but it's a # layer with activation. if no_final_linear and num_outputs : layers . append ( SlimFC ( in_size = prev_layer_size , out_size = num_outputs , initializer = normc_initializer ( 1.0 ), activation_fn = activation ) ) prev_layer_size = num_outputs # Finish the layers with the provided sizes (`hiddens`), plus - # iff num_outputs > 0 - a last linear layer of size num_outputs. else : if len ( hiddens ) > 0 : layers . append ( SlimFC ( in_size = prev_layer_size , out_size = hiddens [ - 1 ], initializer = normc_initializer ( 1.0 ), activation_fn = activation ) ) prev_layer_size = hiddens [ - 1 ] if num_outputs : self . _logits = SlimFC ( in_size = prev_layer_size , out_size = num_outputs , initializer = normc_initializer ( 0.01 ), activation_fn = None ) else : self . num_outputs = ([ int ( np . product ( obs_space . shape ))] + hiddens [ - 1 :])[ - 1 ] # Layer to add the log std vars to the state-dependent means. if self . free_log_std and self . _logits : self . _append_free_log_std = AppendBiasLayer ( num_outputs ) self . _hidden_layers = nn . Sequential ( * layers ) self . _value_branch_separate = None if not self . vf_share_layers : # Build a parallel set of hidden layers for the value net. prev_vf_layer_size = int ( obs_space . shape [ - 1 ]) vf_layers = [] for size in hiddens [: - 1 ]: vf_layers . append ( SlimFC ( in_size = prev_vf_layer_size , out_size = size , activation_fn = activation , initializer = normc_initializer ( 1.0 )) ) prev_vf_layer_size = size vf_layers . append ( nn . Flatten ()) prev_vf_layer_size = size * num_frames vf_layers . append ( SlimFC ( in_size = prev_vf_layer_size , out_size = hiddens [ - 1 ], activation_fn = activation , initializer = normc_initializer ( 1.0 )) ) prev_vf_layer_size = hiddens [ - 1 ] # for size in hiddens: # vf_layers.append( # SlimFC( # in_size=prev_vf_layer_size, # out_size=size, # activation_fn=activation, # initializer=normc_initializer(1.0))) # prev_vf_layer_size = size self . _value_branch_separate = nn . Sequential ( * vf_layers ) self . _value_branch = SlimFC ( in_size = prev_layer_size , out_size = 1 , initializer = normc_initializer ( 0.01 ), activation_fn = None ) # print(\"*************************************************\") # print(model_config) # print(\"************************************************\") # print(num_frames) # print(\"************************************************\") # print(self._hidden_layers) # print(self._logits) # print(\"***********************************************\") # print(self._value_branch_separate) # print(self._value_branch) # # exit(1) # Holds the current \"base\" output (before logits layer). self . _features = None # Holds the last input, in case value branch is separate. self . _last_flat_in = None @override ( TorchModelV2 ) def forward ( self , input_dict : Dict [ str , TensorType ], state : List [ TensorType ], seq_lens : TensorType ) -> ( TensorType , List [ TensorType ]): # type: ignore self . _last_flat_in = input_dict [ TorchFrameStack . PREV_N_OBS ] . float () self . _features = self . _hidden_layers ( self . _last_flat_in ) logits = self . _logits ( self . _features ) if self . _logits else \\ self . _features if self . free_log_std : logits = self . _append_free_log_std ( logits ) return logits , state @override ( TorchModelV2 ) def value_function ( self ) -> TensorType : assert self . _features is not None , \"must call forward() first\" if self . _value_branch_separate : return self . _value_branch ( self . _value_branch_separate ( self . _last_flat_in )) . squeeze ( 1 ) else : return self . _value_branch ( self . _features ) . squeeze ( 1 )","title":"TorchFrameStack"},{"location":"reference/models/torch_frame_stack/#corl.models.torch_frame_stack.TorchFrameStack.forward","text":"Call the model with the given input tensors and state. Any complex observations (dicts, tuples, etc.) will be unpacked by call before being passed to forward(). To access the flattened observation tensor, refer to input_dict[\"obs_flat\"]. This method can be called any number of times. In eager execution, each call to forward() will eagerly evaluate the model. In symbolic execution, each call to forward creates a computation graph that operates over the variables of this model (i.e., shares weights). Custom models should override this instead of call . Parameters: Name Type Description Default input_dict Dict[str, Any] dictionary of input tensors, including \"obs\", \"obs_flat\", \"prev_action\", \"prev_reward\", \"is_training\", \"eps_id\", \"agent_id\", \"infos\", and \"t\". required state List[Any] list of state tensors with sizes matching those returned by get_initial_state + the batch dimension required seq_lens Any 1d tensor holding input sequence lengths required Returns: Type Description (Any, List[Any]) A tuple consisting of the model output tensor of size [BATCH, num_outputs] and the list of new RNN state(s) if any. Examples: >>> import numpy as np >>> from ray.rllib.models.modelv2 import ModelV2 >>> class MyModel ( ModelV2 ): ... # ... >>> def forward ( self , input_dict , state , seq_lens ): >>> model_out , self . _value_out = self . base_model ( ... input_dict [ \"obs\" ]) >>> return model_out , state Source code in corl/models/torch_frame_stack.py @override ( TorchModelV2 ) def forward ( self , input_dict : Dict [ str , TensorType ], state : List [ TensorType ], seq_lens : TensorType ) -> ( TensorType , List [ TensorType ]): # type: ignore self . _last_flat_in = input_dict [ TorchFrameStack . PREV_N_OBS ] . float () self . _features = self . _hidden_layers ( self . _last_flat_in ) logits = self . _logits ( self . _features ) if self . _logits else \\ self . _features if self . free_log_std : logits = self . _append_free_log_std ( logits ) return logits , state","title":"forward()"},{"location":"reference/models/torch_frame_stack/#corl.models.torch_frame_stack.TorchFrameStack.value_function","text":"Returns the value function output for the most recent forward pass. Note that a forward call has to be performed first, before this methods can return anything and thus that calling this method does not cause an extra forward pass through the network. Returns: Type Description Any Value estimate tensor of shape [BATCH]. Source code in corl/models/torch_frame_stack.py @override ( TorchModelV2 ) def value_function ( self ) -> TensorType : assert self . _features is not None , \"must call forward() first\" if self . _value_branch_separate : return self . _value_branch ( self . _value_branch_separate ( self . _last_flat_in )) . squeeze ( 1 ) else : return self . _value_branch ( self . _features ) . squeeze ( 1 )","title":"value_function()"},{"location":"reference/parsers/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Parsers"},{"location":"reference/parsers/yaml_loader/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Loader ( SafeLoader ) \u00a4 YAML Loader with !include constructor. Source code in corl/parsers/yaml_loader.py class Loader ( yaml . SafeLoader ): # pylint: disable=too-few-public-methods,W0223 \"\"\"YAML Loader with `!include` constructor.\"\"\" def __init__ ( self , stream : IO ) -> None : \"\"\"Initialise Loader.\"\"\" try : self . _root = os . path . split ( stream . name )[ 0 ] except AttributeError : self . _root = os . path . curdir super () . __init__ ( stream ) self . _include_mapping : dict = {} def construct_python_tuple ( self , node ): \"\"\"Adds in the capability to process tuples in yaml files \"\"\" return tuple ( self . construct_sequence ( node )) def construct_sequence ( self , node , deep = False ): \"\"\"Construct a sequence from a YAML sequence node This method extends yaml.constructor.BaseConstructor.construct_sequence by adding support for children with the tag `!include-extend`. Any object with this tag should be constructable to produce a sequence of objects. Even though `!include-extend` is a tag on the child object, the sequence produced by this child is not added as a single element to the sequence being produced by this method. Rather, the output sequence is extended with this list. Any children with other tags are appended into the list in the same manner as yaml.constructor.BaseConstructor.construct_sequence. Examples -------- Loader.add_constructor(\"!include-extend\", construct_include) with open(\"primary.yml\", \"r\") as fp: config = yaml.load(fp, Loader) <file primary.yml> root: tree1: - apple - banana - cherry tree2: - type: int value: 3 - type: float value: 3.14 - type: str value: pi tree3: - date - elderberry - !include-extend secondary.yml - mango <file secondary.yml> - fig - grape - honeydew - jackfruit - kiwi - lemon The output of the code above is: config = { 'root': { 'tree1': ['apple', 'banana', 'cherry'], 'tree2': [ {'type': 'int', 'value': 3}, {'type': 'float', 'value': 3.14}, {'type': 'str', 'value': 'pi'} ], 'tree3': ['date', 'elderberry', 'fig', 'grape', 'honeydew', 'jackfruit', 'kiwi', 'lemon', 'mango'] } } \"\"\" if not isinstance ( node , SequenceNode ): return super () . construct_sequence ( node , deep = deep ) output = [] for child in node . value : this_output = self . construct_object ( child , deep = deep ) if child . tag == '!include-extend' : if not isinstance ( this_output , collections . abc . Sequence ): raise ConstructorError ( None , None , \"expected a sequence returned by 'include-extend', but found %s \" % type ( this_output ) . __name__ , child . start_mark ) output . extend ( this_output ) else : output . append ( this_output ) return output def flatten_mapping ( self , node ): merge = [] index = 0 while index < len ( node . value ): key_node , value_node = node . value [ index ] if key_node . tag == 'tag:yaml.org,2002:merge' : del node . value [ index ] if isinstance ( value_node , yaml . MappingNode ): self . flatten_mapping ( value_node ) merge . extend ( value_node . value ) elif isinstance ( value_node , yaml . SequenceNode ): submerge = [] for subnode in value_node . value : if not isinstance ( subnode , yaml . MappingNode ): raise yaml . ConstructorError ( \"while constructing a mapping\" , node . start_mark , \"expected a mapping for merging, but found %s \" % subnode . id , subnode . start_mark ) self . flatten_mapping ( subnode ) submerge . append ( subnode . value ) submerge . reverse () for value in submerge : merge . extend ( value ) else : # TODO FIGURE OUT HOW TO DUMP AND ACCESS THE BASE NODE!!!! # if value_node.tag == '!include-direct': # filename = os.path.realpath(os.path.join(self._root, self.construct_scalar(value_node))) # d = yaml.dump(self._include_mapping[filename]) # # for k, v in .items(): # # sk = yaml.ScalarNode(tag='tag:yaml.org,2002:str', value=str(k)) # # if isinstance(v, int): # # sv = yaml.ScalarNode(tag='tag:yaml.org,2002:int', value=str(v)) # # else: # # sv = yaml.ScalarNode(tag='tag:yaml.org,2002:seq', value=str(v)) # # merge.extend([(sk, sv)]) # else: raise yaml . ConstructorError ( \"while constructing a mapping\" , node . start_mark , \"expected a mapping or list of mappings for merging, but found %s \" % value_node . id , value_node . start_mark ) elif key_node . tag == 'tag:yaml.org,2002:value' : key_node . tag = 'tag:yaml.org,2002:str' index += 1 else : index += 1 if merge : node . value = merge + node . value __init__ ( self , stream ) special \u00a4 Initialise Loader. Source code in corl/parsers/yaml_loader.py def __init__ ( self , stream : IO ) -> None : \"\"\"Initialise Loader.\"\"\" try : self . _root = os . path . split ( stream . name )[ 0 ] except AttributeError : self . _root = os . path . curdir super () . __init__ ( stream ) self . _include_mapping : dict = {} construct_python_tuple ( self , node ) \u00a4 Adds in the capability to process tuples in yaml files Source code in corl/parsers/yaml_loader.py def construct_python_tuple ( self , node ): \"\"\"Adds in the capability to process tuples in yaml files \"\"\" return tuple ( self . construct_sequence ( node )) construct_sequence ( self , node , deep = False ) \u00a4 Construct a sequence from a YAML sequence node This method extends yaml.constructor.BaseConstructor.construct_sequence by adding support for children with the tag !include-extend . Any object with this tag should be constructable to produce a sequence of objects. Even though !include-extend is a tag on the child object, the sequence produced by this child is not added as a single element to the sequence being produced by this method. Rather, the output sequence is extended with this list. Any children with other tags are appended into the list in the same manner as yaml.constructor.BaseConstructor.construct_sequence. Examples \u00a4 Loader.add_constructor(\"!include-extend\", construct_include) with open(\"primary.yml\", \"r\") as fp: config = yaml.load(fp, Loader) root: tree1: - apple - banana - cherry tree2: - type: int value: 3 - type: float value: 3.14 - type: str value: pi tree3: - date - elderberry - !include-extend secondary.yml - mango - fig - grape - honeydew - jackfruit - kiwi - lemon The output of the code above is: config = { 'root': { 'tree1': ['apple', 'banana', 'cherry'], 'tree2': [ {'type': 'int', 'value': 3}, {'type': 'float', 'value': 3.14}, {'type': 'str', 'value': 'pi'} ], 'tree3': ['date', 'elderberry', 'fig', 'grape', 'honeydew', 'jackfruit', 'kiwi', 'lemon', 'mango'] } } Source code in corl/parsers/yaml_loader.py def construct_sequence ( self , node , deep = False ): \"\"\"Construct a sequence from a YAML sequence node This method extends yaml.constructor.BaseConstructor.construct_sequence by adding support for children with the tag `!include-extend`. Any object with this tag should be constructable to produce a sequence of objects. Even though `!include-extend` is a tag on the child object, the sequence produced by this child is not added as a single element to the sequence being produced by this method. Rather, the output sequence is extended with this list. Any children with other tags are appended into the list in the same manner as yaml.constructor.BaseConstructor.construct_sequence. Examples -------- Loader.add_constructor(\"!include-extend\", construct_include) with open(\"primary.yml\", \"r\") as fp: config = yaml.load(fp, Loader) <file primary.yml> root: tree1: - apple - banana - cherry tree2: - type: int value: 3 - type: float value: 3.14 - type: str value: pi tree3: - date - elderberry - !include-extend secondary.yml - mango <file secondary.yml> - fig - grape - honeydew - jackfruit - kiwi - lemon The output of the code above is: config = { 'root': { 'tree1': ['apple', 'banana', 'cherry'], 'tree2': [ {'type': 'int', 'value': 3}, {'type': 'float', 'value': 3.14}, {'type': 'str', 'value': 'pi'} ], 'tree3': ['date', 'elderberry', 'fig', 'grape', 'honeydew', 'jackfruit', 'kiwi', 'lemon', 'mango'] } } \"\"\" if not isinstance ( node , SequenceNode ): return super () . construct_sequence ( node , deep = deep ) output = [] for child in node . value : this_output = self . construct_object ( child , deep = deep ) if child . tag == '!include-extend' : if not isinstance ( this_output , collections . abc . Sequence ): raise ConstructorError ( None , None , \"expected a sequence returned by 'include-extend', but found %s \" % type ( this_output ) . __name__ , child . start_mark ) output . extend ( this_output ) else : output . append ( this_output ) return output apply_patches ( config ) \u00a4 updates the base setup with patches Returns: Type Description The combined dict Source code in corl/parsers/yaml_loader.py def apply_patches ( config ): \"\"\"updates the base setup with patches Arguments: config [dict, list] -- The base and patch if list, else dict Returns: The combined dict \"\"\" def merge ( source , destination ): \"\"\" run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , {}) merge ( value , node ) else : destination [ key ] = value return destination if isinstance ( config , list ): config_new = copy . deepcopy ( config [ 0 ]) for item in config [ 1 :]: if item is not None : config_new = merge ( item , config_new ) return config_new return config construct_include ( loader , node ) \u00a4 Include file referenced at node. Source code in corl/parsers/yaml_loader.py def construct_include ( loader : Loader , node : yaml . Node ) -> Any : \"\"\"Include file referenced at node.\"\"\" filename = os . path . realpath ( os . path . join ( loader . _root , loader . construct_scalar ( node ))) # type: ignore # pylint: disable=protected-access # noqa: E501 extension = os . path . splitext ( filename )[ 1 ] . lstrip ( \".\" ) with open ( filename , \"r\" ) as fp : if extension in ( \"yaml\" , \"yml\" ): # pylint: disable=no-else-return return yaml . load ( fp , Loader ) elif extension in ( \"json\" , ): return json . load ( fp ) else : return \"\" . join ( fp . readlines ()) construct_include_arr ( loader , node ) \u00a4 Identical to above, but accepts an array and appends results as an array. Source code in corl/parsers/yaml_loader.py def construct_include_arr ( loader : Loader , node : yaml . Node ) -> Any : \"\"\"Identical to above, but accepts an array and appends results as an array.\"\"\" sequence = loader . construct_sequence ( node ) data : typing . List = [] for item in sequence : filename = os . path . abspath ( os . path . join ( loader . _root , item )) # pylint: disable=protected-access extension = os . path . splitext ( filename )[ 1 ] . lstrip ( \".\" ) with open ( filename , \"r\" ) as f : if extension in ( \"yaml\" , \"yml\" ): # pylint: disable=no-else-return data = data + ( yaml . load ( f , Loader )) elif extension in ( \"json\" , ): data = data + ( json . load ( f )) else : data = data + ( \"\" . join ( f . readlines ())) # type: ignore return data construct_include_direct ( loader , node ) \u00a4 Include file referenced at node. Source code in corl/parsers/yaml_loader.py def construct_include_direct ( loader : Loader , node : yaml . Node ) -> Any : \"\"\"Include file referenced at node.\"\"\" filename = os . path . realpath ( os . path . join ( loader . _root , loader . construct_scalar ( node ))) # type: ignore # pylint: disable=protected-access # noqa: E501 extension = os . path . splitext ( filename )[ 1 ] . lstrip ( \".\" ) with open ( filename , \"r\" ) as fp : if extension in ( \"yaml\" , \"yml\" ): # pylint: disable=no-else-return temp = yaml . load ( fp , Loader ) loader . _include_mapping [ filename ] = temp # pylint: disable=protected-access return temp elif extension in ( \"json\" , ): return json . load ( fp ) else : return \"\" . join ( fp . readlines ()) construct_tune_function ( loader , node ) \u00a4 Include expression referenced at node. Source code in corl/parsers/yaml_loader.py def construct_tune_function ( loader : Loader , node : yaml . Node ) -> Any : # pylint: disable=unused-argument \"\"\"Include expression referenced at node.\"\"\" if isinstance ( node . value , str ) and \"tune\" in node . value : return eval ( node . value ) # pylint: disable=eval-used return node . value load_file ( config_filename ) \u00a4 Utility function to load in a specified yaml file Source code in corl/parsers/yaml_loader.py def load_file ( config_filename : str ): \"\"\" Utility function to load in a specified yaml file \"\"\" with open ( config_filename , \"r\" ) as fp : config = yaml . load ( fp , Loader ) return config separate_config ( config ) \u00a4 Utility function to separate the env specific configs from the tune configs Source code in corl/parsers/yaml_loader.py def separate_config ( config : typing . Dict ): \"\"\" Utility function to separate the env specific configs from the tune configs \"\"\" # we can call ray.init without any arguments so default is no arguments ray_config = apply_patches ( config . get ( \"ray_config\" , {})) # we must have a tune config or else we cannot call tune.run if \"tune_config\" not in config : raise ValueError ( f \"Could not find a tune_config in { config } \" ) tune_config = apply_patches ( config [ \"tune_config\" ]) # must also get an env_config or else we don't know which environment to run``` if \"env_config\" in config : env_config = apply_patches ( config [ \"env_config\" ]) else : raise ValueError ( f \"Could not find a env_config in { config } or rllib_config\" ) # must get a rllib config in some way or else we aren't going to run anything useful rllib_configs = {} if \"rllib_configs\" in config : for key , value in config [ \"rllib_configs\" ] . items (): rllib_configs [ key ] = apply_patches ( value ) else : raise ValueError ( f \"Could not find a rllib_config in { config } or 'config' in tune_config\" ) for key in rllib_configs : rllib_configs [ key ][ \"env_config\" ] = copy . deepcopy ( env_config ) rllib_configs [ key ][ \"env_config\" ] . setdefault ( \"environment\" , {})[ \"horizon\" ] = rllib_configs [ key ] . get ( \"horizon\" , 1000 ) # a trainable config is not necessary trainable_config = apply_patches ( config . get ( \"trainable_config\" , None )) return ray_config , rllib_configs , tune_config , trainable_config","title":"Yaml loader"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.Loader","text":"YAML Loader with !include constructor. Source code in corl/parsers/yaml_loader.py class Loader ( yaml . SafeLoader ): # pylint: disable=too-few-public-methods,W0223 \"\"\"YAML Loader with `!include` constructor.\"\"\" def __init__ ( self , stream : IO ) -> None : \"\"\"Initialise Loader.\"\"\" try : self . _root = os . path . split ( stream . name )[ 0 ] except AttributeError : self . _root = os . path . curdir super () . __init__ ( stream ) self . _include_mapping : dict = {} def construct_python_tuple ( self , node ): \"\"\"Adds in the capability to process tuples in yaml files \"\"\" return tuple ( self . construct_sequence ( node )) def construct_sequence ( self , node , deep = False ): \"\"\"Construct a sequence from a YAML sequence node This method extends yaml.constructor.BaseConstructor.construct_sequence by adding support for children with the tag `!include-extend`. Any object with this tag should be constructable to produce a sequence of objects. Even though `!include-extend` is a tag on the child object, the sequence produced by this child is not added as a single element to the sequence being produced by this method. Rather, the output sequence is extended with this list. Any children with other tags are appended into the list in the same manner as yaml.constructor.BaseConstructor.construct_sequence. Examples -------- Loader.add_constructor(\"!include-extend\", construct_include) with open(\"primary.yml\", \"r\") as fp: config = yaml.load(fp, Loader) <file primary.yml> root: tree1: - apple - banana - cherry tree2: - type: int value: 3 - type: float value: 3.14 - type: str value: pi tree3: - date - elderberry - !include-extend secondary.yml - mango <file secondary.yml> - fig - grape - honeydew - jackfruit - kiwi - lemon The output of the code above is: config = { 'root': { 'tree1': ['apple', 'banana', 'cherry'], 'tree2': [ {'type': 'int', 'value': 3}, {'type': 'float', 'value': 3.14}, {'type': 'str', 'value': 'pi'} ], 'tree3': ['date', 'elderberry', 'fig', 'grape', 'honeydew', 'jackfruit', 'kiwi', 'lemon', 'mango'] } } \"\"\" if not isinstance ( node , SequenceNode ): return super () . construct_sequence ( node , deep = deep ) output = [] for child in node . value : this_output = self . construct_object ( child , deep = deep ) if child . tag == '!include-extend' : if not isinstance ( this_output , collections . abc . Sequence ): raise ConstructorError ( None , None , \"expected a sequence returned by 'include-extend', but found %s \" % type ( this_output ) . __name__ , child . start_mark ) output . extend ( this_output ) else : output . append ( this_output ) return output def flatten_mapping ( self , node ): merge = [] index = 0 while index < len ( node . value ): key_node , value_node = node . value [ index ] if key_node . tag == 'tag:yaml.org,2002:merge' : del node . value [ index ] if isinstance ( value_node , yaml . MappingNode ): self . flatten_mapping ( value_node ) merge . extend ( value_node . value ) elif isinstance ( value_node , yaml . SequenceNode ): submerge = [] for subnode in value_node . value : if not isinstance ( subnode , yaml . MappingNode ): raise yaml . ConstructorError ( \"while constructing a mapping\" , node . start_mark , \"expected a mapping for merging, but found %s \" % subnode . id , subnode . start_mark ) self . flatten_mapping ( subnode ) submerge . append ( subnode . value ) submerge . reverse () for value in submerge : merge . extend ( value ) else : # TODO FIGURE OUT HOW TO DUMP AND ACCESS THE BASE NODE!!!! # if value_node.tag == '!include-direct': # filename = os.path.realpath(os.path.join(self._root, self.construct_scalar(value_node))) # d = yaml.dump(self._include_mapping[filename]) # # for k, v in .items(): # # sk = yaml.ScalarNode(tag='tag:yaml.org,2002:str', value=str(k)) # # if isinstance(v, int): # # sv = yaml.ScalarNode(tag='tag:yaml.org,2002:int', value=str(v)) # # else: # # sv = yaml.ScalarNode(tag='tag:yaml.org,2002:seq', value=str(v)) # # merge.extend([(sk, sv)]) # else: raise yaml . ConstructorError ( \"while constructing a mapping\" , node . start_mark , \"expected a mapping or list of mappings for merging, but found %s \" % value_node . id , value_node . start_mark ) elif key_node . tag == 'tag:yaml.org,2002:value' : key_node . tag = 'tag:yaml.org,2002:str' index += 1 else : index += 1 if merge : node . value = merge + node . value","title":"Loader"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.Loader.__init__","text":"Initialise Loader. Source code in corl/parsers/yaml_loader.py def __init__ ( self , stream : IO ) -> None : \"\"\"Initialise Loader.\"\"\" try : self . _root = os . path . split ( stream . name )[ 0 ] except AttributeError : self . _root = os . path . curdir super () . __init__ ( stream ) self . _include_mapping : dict = {}","title":"__init__()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.Loader.construct_python_tuple","text":"Adds in the capability to process tuples in yaml files Source code in corl/parsers/yaml_loader.py def construct_python_tuple ( self , node ): \"\"\"Adds in the capability to process tuples in yaml files \"\"\" return tuple ( self . construct_sequence ( node ))","title":"construct_python_tuple()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.Loader.construct_sequence","text":"Construct a sequence from a YAML sequence node This method extends yaml.constructor.BaseConstructor.construct_sequence by adding support for children with the tag !include-extend . Any object with this tag should be constructable to produce a sequence of objects. Even though !include-extend is a tag on the child object, the sequence produced by this child is not added as a single element to the sequence being produced by this method. Rather, the output sequence is extended with this list. Any children with other tags are appended into the list in the same manner as yaml.constructor.BaseConstructor.construct_sequence.","title":"construct_sequence()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.Loader.construct_sequence--examples","text":"Loader.add_constructor(\"!include-extend\", construct_include) with open(\"primary.yml\", \"r\") as fp: config = yaml.load(fp, Loader) root: tree1: - apple - banana - cherry tree2: - type: int value: 3 - type: float value: 3.14 - type: str value: pi tree3: - date - elderberry - !include-extend secondary.yml - mango - fig - grape - honeydew - jackfruit - kiwi - lemon The output of the code above is: config = { 'root': { 'tree1': ['apple', 'banana', 'cherry'], 'tree2': [ {'type': 'int', 'value': 3}, {'type': 'float', 'value': 3.14}, {'type': 'str', 'value': 'pi'} ], 'tree3': ['date', 'elderberry', 'fig', 'grape', 'honeydew', 'jackfruit', 'kiwi', 'lemon', 'mango'] } } Source code in corl/parsers/yaml_loader.py def construct_sequence ( self , node , deep = False ): \"\"\"Construct a sequence from a YAML sequence node This method extends yaml.constructor.BaseConstructor.construct_sequence by adding support for children with the tag `!include-extend`. Any object with this tag should be constructable to produce a sequence of objects. Even though `!include-extend` is a tag on the child object, the sequence produced by this child is not added as a single element to the sequence being produced by this method. Rather, the output sequence is extended with this list. Any children with other tags are appended into the list in the same manner as yaml.constructor.BaseConstructor.construct_sequence. Examples -------- Loader.add_constructor(\"!include-extend\", construct_include) with open(\"primary.yml\", \"r\") as fp: config = yaml.load(fp, Loader) <file primary.yml> root: tree1: - apple - banana - cherry tree2: - type: int value: 3 - type: float value: 3.14 - type: str value: pi tree3: - date - elderberry - !include-extend secondary.yml - mango <file secondary.yml> - fig - grape - honeydew - jackfruit - kiwi - lemon The output of the code above is: config = { 'root': { 'tree1': ['apple', 'banana', 'cherry'], 'tree2': [ {'type': 'int', 'value': 3}, {'type': 'float', 'value': 3.14}, {'type': 'str', 'value': 'pi'} ], 'tree3': ['date', 'elderberry', 'fig', 'grape', 'honeydew', 'jackfruit', 'kiwi', 'lemon', 'mango'] } } \"\"\" if not isinstance ( node , SequenceNode ): return super () . construct_sequence ( node , deep = deep ) output = [] for child in node . value : this_output = self . construct_object ( child , deep = deep ) if child . tag == '!include-extend' : if not isinstance ( this_output , collections . abc . Sequence ): raise ConstructorError ( None , None , \"expected a sequence returned by 'include-extend', but found %s \" % type ( this_output ) . __name__ , child . start_mark ) output . extend ( this_output ) else : output . append ( this_output ) return output","title":"Examples"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.apply_patches","text":"updates the base setup with patches Returns: Type Description The combined dict Source code in corl/parsers/yaml_loader.py def apply_patches ( config ): \"\"\"updates the base setup with patches Arguments: config [dict, list] -- The base and patch if list, else dict Returns: The combined dict \"\"\" def merge ( source , destination ): \"\"\" run me with nosetests --with-doctest file.py >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } } >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } } >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } } True \"\"\" for key , value in source . items (): if isinstance ( value , dict ): # get node or create one node = destination . setdefault ( key , {}) merge ( value , node ) else : destination [ key ] = value return destination if isinstance ( config , list ): config_new = copy . deepcopy ( config [ 0 ]) for item in config [ 1 :]: if item is not None : config_new = merge ( item , config_new ) return config_new return config","title":"apply_patches()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.construct_include","text":"Include file referenced at node. Source code in corl/parsers/yaml_loader.py def construct_include ( loader : Loader , node : yaml . Node ) -> Any : \"\"\"Include file referenced at node.\"\"\" filename = os . path . realpath ( os . path . join ( loader . _root , loader . construct_scalar ( node ))) # type: ignore # pylint: disable=protected-access # noqa: E501 extension = os . path . splitext ( filename )[ 1 ] . lstrip ( \".\" ) with open ( filename , \"r\" ) as fp : if extension in ( \"yaml\" , \"yml\" ): # pylint: disable=no-else-return return yaml . load ( fp , Loader ) elif extension in ( \"json\" , ): return json . load ( fp ) else : return \"\" . join ( fp . readlines ())","title":"construct_include()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.construct_include_arr","text":"Identical to above, but accepts an array and appends results as an array. Source code in corl/parsers/yaml_loader.py def construct_include_arr ( loader : Loader , node : yaml . Node ) -> Any : \"\"\"Identical to above, but accepts an array and appends results as an array.\"\"\" sequence = loader . construct_sequence ( node ) data : typing . List = [] for item in sequence : filename = os . path . abspath ( os . path . join ( loader . _root , item )) # pylint: disable=protected-access extension = os . path . splitext ( filename )[ 1 ] . lstrip ( \".\" ) with open ( filename , \"r\" ) as f : if extension in ( \"yaml\" , \"yml\" ): # pylint: disable=no-else-return data = data + ( yaml . load ( f , Loader )) elif extension in ( \"json\" , ): data = data + ( json . load ( f )) else : data = data + ( \"\" . join ( f . readlines ())) # type: ignore return data","title":"construct_include_arr()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.construct_include_direct","text":"Include file referenced at node. Source code in corl/parsers/yaml_loader.py def construct_include_direct ( loader : Loader , node : yaml . Node ) -> Any : \"\"\"Include file referenced at node.\"\"\" filename = os . path . realpath ( os . path . join ( loader . _root , loader . construct_scalar ( node ))) # type: ignore # pylint: disable=protected-access # noqa: E501 extension = os . path . splitext ( filename )[ 1 ] . lstrip ( \".\" ) with open ( filename , \"r\" ) as fp : if extension in ( \"yaml\" , \"yml\" ): # pylint: disable=no-else-return temp = yaml . load ( fp , Loader ) loader . _include_mapping [ filename ] = temp # pylint: disable=protected-access return temp elif extension in ( \"json\" , ): return json . load ( fp ) else : return \"\" . join ( fp . readlines ())","title":"construct_include_direct()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.construct_tune_function","text":"Include expression referenced at node. Source code in corl/parsers/yaml_loader.py def construct_tune_function ( loader : Loader , node : yaml . Node ) -> Any : # pylint: disable=unused-argument \"\"\"Include expression referenced at node.\"\"\" if isinstance ( node . value , str ) and \"tune\" in node . value : return eval ( node . value ) # pylint: disable=eval-used return node . value","title":"construct_tune_function()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.load_file","text":"Utility function to load in a specified yaml file Source code in corl/parsers/yaml_loader.py def load_file ( config_filename : str ): \"\"\" Utility function to load in a specified yaml file \"\"\" with open ( config_filename , \"r\" ) as fp : config = yaml . load ( fp , Loader ) return config","title":"load_file()"},{"location":"reference/parsers/yaml_loader/#corl.parsers.yaml_loader.separate_config","text":"Utility function to separate the env specific configs from the tune configs Source code in corl/parsers/yaml_loader.py def separate_config ( config : typing . Dict ): \"\"\" Utility function to separate the env specific configs from the tune configs \"\"\" # we can call ray.init without any arguments so default is no arguments ray_config = apply_patches ( config . get ( \"ray_config\" , {})) # we must have a tune config or else we cannot call tune.run if \"tune_config\" not in config : raise ValueError ( f \"Could not find a tune_config in { config } \" ) tune_config = apply_patches ( config [ \"tune_config\" ]) # must also get an env_config or else we don't know which environment to run``` if \"env_config\" in config : env_config = apply_patches ( config [ \"env_config\" ]) else : raise ValueError ( f \"Could not find a env_config in { config } or rllib_config\" ) # must get a rllib config in some way or else we aren't going to run anything useful rllib_configs = {} if \"rllib_configs\" in config : for key , value in config [ \"rllib_configs\" ] . items (): rllib_configs [ key ] = apply_patches ( value ) else : raise ValueError ( f \"Could not find a rllib_config in { config } or 'config' in tune_config\" ) for key in rllib_configs : rllib_configs [ key ][ \"env_config\" ] = copy . deepcopy ( env_config ) rllib_configs [ key ][ \"env_config\" ] . setdefault ( \"environment\" , {})[ \"horizon\" ] = rllib_configs [ key ] . get ( \"horizon\" , 1000 ) # a trainable config is not necessary trainable_config = apply_patches ( config . get ( \"trainable_config\" , None )) return ray_config , rllib_configs , tune_config , trainable_config","title":"separate_config()"},{"location":"reference/policies/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Policies"},{"location":"reference/policies/base_policy/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BasePolicyValidator ( BaseModel ) pydantic-model \u00a4 Base Policy Validator to subclass for Experiments subclassing BaseExperiment Source code in corl/policies/base_policy.py class BasePolicyValidator ( BaseModel ): \"\"\" Base Policy Validator to subclass for Experiments subclassing BaseExperiment \"\"\" ...","title":"Base policy"},{"location":"reference/policies/base_policy/#corl.policies.base_policy.BasePolicyValidator","text":"Base Policy Validator to subclass for Experiments subclassing BaseExperiment Source code in corl/policies/base_policy.py class BasePolicyValidator ( BaseModel ): \"\"\" Base Policy Validator to subclass for Experiments subclassing BaseExperiment \"\"\" ...","title":"BasePolicyValidator"},{"location":"reference/policies/custom_policy/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Custom Policy CustomPolicy ( Policy ) \u00a4 Custom base policy. Source code in corl/policies/custom_policy.py class CustomPolicy ( Policy ): # pylint: disable=abstract-method \"\"\"Custom base policy. \"\"\" def __init__ ( self , observation_space , action_space , config ): self . validated_config : CustomPolicyValidator = self . get_validator ( act_space = action_space , obs_space = observation_space , ** config ) Policy . __init__ ( self , observation_space , action_space , config ) self . time_extractor = self . validated_config . time_extractor . construct_extractors () self . _reset () @property def get_validator ( self ) -> typing . Type [ BasePolicyValidator ]: \"\"\" Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class \"\"\" return CustomPolicyValidator def _reset ( self ): \"\"\"This must be overriden in order to reset the state between runs \"\"\" ... def learn_on_batch ( self , samples ): return {} def get_weights ( self ): return {} def set_weights ( self , weights ): return def compute_actions_from_input_dict ( self , input_dict : typing . Union [ SampleBatch , typing . Dict [ str , TensorStructType ]], explore : bool = None , timestep : typing . Optional [ int ] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: \"\"\"Computes actions from collected samples (across multiple-agents). Takes an input dict (usually a SampleBatch) as its main data input. This allows for using this method in case a more complex input pattern (view requirements) is needed, for example when the Model requires the last n observations, the last m actions/rewards, or a combination of any of these. Args: input_dict: A SampleBatch or input dict containing the Tensors to compute actions. `input_dict` already abides to the Policy's as well as the Model's view requirements and can thus be passed to the Model as-is. explore: Whether to pick an exploitation or exploration action (default: None -> use self.config[\"explore\"]). timestep: The current (sampling) time step. episodes: This provides access to all of the internal episodes' state, which may be useful for model-based or multi-agent algorithms. Keyword Args: kwargs: Forward compatibility placeholder. Returns: actions: Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs: List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info: Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. \"\"\" # Default implementation just passes obs, prev-a/r, and states on to # `self.compute_actions()`. agent_index = input_dict [ SampleBatch . AGENT_INDEX ][ 0 ] episode_id = input_dict [ SampleBatch . EPS_ID ][ 0 ] episode = [ eps for eps in episodes if eps . episode_id == episode_id ][ 0 ] # type: ignore agent_id : str = [ aid for aid in episode . _agent_to_index if episode . _agent_to_index [ aid ] == agent_index # pylint: disable=protected-access ][ 0 ] obs_batch = input_dict [ SampleBatch . OBS ] info = episode . last_info_for ( agent_id ) if info is None : info = {} if 'platform_obs' in info : sim_time = self . time_extractor . value ( info [ 'platform_obs' ][ agent_id ], full_extraction = True ) else : self . _reset () sim_time = - 1 state_batches = [ s for k , s in input_dict . items () if k [: 9 ] == \"state_in_\" ] return self . compute_actions ( obs_batch , state_batches , prev_action_batch = input_dict . get ( SampleBatch . PREV_ACTIONS ), prev_reward_batch = input_dict . get ( SampleBatch . PREV_REWARDS ), info_batch = input_dict . get ( SampleBatch . INFOS ), # type: ignore explore = explore , timestep = timestep , episodes = episodes , sim_time = sim_time , agent_id = agent_id , info = info , episode = episode , ** kwargs , ) def compute_actions ( self , obs_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ], state_batches : typing . Optional [ typing . List [ TensorType ]] = None , prev_action_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , prev_reward_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , info_batch : typing . Optional [ typing . Dict [ str , list ]] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , explore : typing . Optional [ bool ] = None , timestep : typing . Optional [ int ] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: actions , state_outs , info = self . custom_compute_actions ( obs_batch , state_batches = state_batches , prev_action_batch = prev_action_batch , prev_reward_batch = prev_reward_batch , info_batch = info_batch , episodes = episodes , explore = explore , timestep = timestep , ** kwargs ) if self . validated_config . normalize_controls : for i , action in enumerate ( actions ): actions [ i ] = EnvSpaceUtil . scale_sample_from_space ( self . validated_config . act_space , action ) return actions , state_outs , info @abstractmethod def custom_compute_actions ( self , obs_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ], state_batches : typing . Optional [ typing . List [ TensorType ]] = None , prev_action_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , prev_reward_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , info_batch : typing . Optional [ typing . Dict [ str , list ]] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , explore : typing . Optional [ bool ] = None , timestep : typing . Optional [ int ] = None , sim_time : typing . Optional [ float ] = None , agent_id : typing . Optional [ str ] = None , info : typing . Optional [ dict ] = None , episode : typing . Optional [ Episode ] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: \"\"\"Computes actions for the current policy. Args: obs_batch: Batch of observations. state_batches: List of RNN state input batches, if any. prev_action_batch: Batch of previous action values. prev_reward_batch: Batch of previous rewards. info_batch: Batch of info objects. episodes: List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. explore: Whether to pick an exploitation or exploration action. Set to None (default) for using the value of `self.config[\"explore\"]`. timestep: The current (sampling) time step. Keyword Args: kwargs: Forward compatibility placeholder Returns: actions (TensorType): Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. \"\"\" raise NotImplementedError get_validator : Type [ corl . policies . base_policy . BasePolicyValidator ] property readonly \u00a4 Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , ** kwargs ) \u00a4 Computes actions for the current policy. Parameters: Name Type Description Default obs_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of observations. required state_batches Optional[List[Any]] List of RNN state input batches, if any. None prev_action_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of previous action values. None prev_reward_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of previous rewards. None info_batch Optional[Dict[str, list]] Batch of info objects. None episodes Optional[List[ray.rllib.evaluation.episode.Episode]] List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. None explore Optional[bool] Whether to pick an exploitation or exploration action. Set to None (default) for using the value of self.config[\"explore\"] . None timestep Optional[int] The current (sampling) time step. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder Returns: Type Description actions (TensorType) Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/custom_policy.py def compute_actions ( self , obs_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ], state_batches : typing . Optional [ typing . List [ TensorType ]] = None , prev_action_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , prev_reward_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , info_batch : typing . Optional [ typing . Dict [ str , list ]] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , explore : typing . Optional [ bool ] = None , timestep : typing . Optional [ int ] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: actions , state_outs , info = self . custom_compute_actions ( obs_batch , state_batches = state_batches , prev_action_batch = prev_action_batch , prev_reward_batch = prev_reward_batch , info_batch = info_batch , episodes = episodes , explore = explore , timestep = timestep , ** kwargs ) if self . validated_config . normalize_controls : for i , action in enumerate ( actions ): actions [ i ] = EnvSpaceUtil . scale_sample_from_space ( self . validated_config . act_space , action ) return actions , state_outs , info compute_actions_from_input_dict ( self , input_dict , explore = None , timestep = None , episodes = None , ** kwargs ) \u00a4 Computes actions from collected samples (across multiple-agents). Takes an input dict (usually a SampleBatch) as its main data input. This allows for using this method in case a more complex input pattern (view requirements) is needed, for example when the Model requires the last n observations, the last m actions/rewards, or a combination of any of these. Parameters: Name Type Description Default input_dict Union[ray.rllib.policy.sample_batch.SampleBatch, Dict[str, Union[Any, dict, tuple]]] A SampleBatch or input dict containing the Tensors to compute actions. input_dict already abides to the Policy's as well as the Model's view requirements and can thus be passed to the Model as-is. required explore bool Whether to pick an exploitation or exploration action (default: None -> use self.config[\"explore\"]). None timestep Optional[int] The current (sampling) time step. None episodes Optional[List[ray.rllib.evaluation.episode.Episode]] This provides access to all of the internal episodes' state, which may be useful for model-based or multi-agent algorithms. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder. Returns: Type Description actions Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs: List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info: Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/custom_policy.py def compute_actions_from_input_dict ( self , input_dict : typing . Union [ SampleBatch , typing . Dict [ str , TensorStructType ]], explore : bool = None , timestep : typing . Optional [ int ] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: \"\"\"Computes actions from collected samples (across multiple-agents). Takes an input dict (usually a SampleBatch) as its main data input. This allows for using this method in case a more complex input pattern (view requirements) is needed, for example when the Model requires the last n observations, the last m actions/rewards, or a combination of any of these. Args: input_dict: A SampleBatch or input dict containing the Tensors to compute actions. `input_dict` already abides to the Policy's as well as the Model's view requirements and can thus be passed to the Model as-is. explore: Whether to pick an exploitation or exploration action (default: None -> use self.config[\"explore\"]). timestep: The current (sampling) time step. episodes: This provides access to all of the internal episodes' state, which may be useful for model-based or multi-agent algorithms. Keyword Args: kwargs: Forward compatibility placeholder. Returns: actions: Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs: List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info: Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. \"\"\" # Default implementation just passes obs, prev-a/r, and states on to # `self.compute_actions()`. agent_index = input_dict [ SampleBatch . AGENT_INDEX ][ 0 ] episode_id = input_dict [ SampleBatch . EPS_ID ][ 0 ] episode = [ eps for eps in episodes if eps . episode_id == episode_id ][ 0 ] # type: ignore agent_id : str = [ aid for aid in episode . _agent_to_index if episode . _agent_to_index [ aid ] == agent_index # pylint: disable=protected-access ][ 0 ] obs_batch = input_dict [ SampleBatch . OBS ] info = episode . last_info_for ( agent_id ) if info is None : info = {} if 'platform_obs' in info : sim_time = self . time_extractor . value ( info [ 'platform_obs' ][ agent_id ], full_extraction = True ) else : self . _reset () sim_time = - 1 state_batches = [ s for k , s in input_dict . items () if k [: 9 ] == \"state_in_\" ] return self . compute_actions ( obs_batch , state_batches , prev_action_batch = input_dict . get ( SampleBatch . PREV_ACTIONS ), prev_reward_batch = input_dict . get ( SampleBatch . PREV_REWARDS ), info_batch = input_dict . get ( SampleBatch . INFOS ), # type: ignore explore = explore , timestep = timestep , episodes = episodes , sim_time = sim_time , agent_id = agent_id , info = info , episode = episode , ** kwargs , ) custom_compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , sim_time = None , agent_id = None , info = None , episode = None , ** kwargs ) \u00a4 Computes actions for the current policy. Parameters: Name Type Description Default obs_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of observations. required state_batches Optional[List[Any]] List of RNN state input batches, if any. None prev_action_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of previous action values. None prev_reward_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of previous rewards. None info_batch Optional[Dict[str, list]] Batch of info objects. None episodes Optional[List[ray.rllib.evaluation.episode.Episode]] List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. None explore Optional[bool] Whether to pick an exploitation or exploration action. Set to None (default) for using the value of self.config[\"explore\"] . None timestep Optional[int] The current (sampling) time step. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder Returns: Type Description actions (TensorType) Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/custom_policy.py @abstractmethod def custom_compute_actions ( self , obs_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ], state_batches : typing . Optional [ typing . List [ TensorType ]] = None , prev_action_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , prev_reward_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , info_batch : typing . Optional [ typing . Dict [ str , list ]] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , explore : typing . Optional [ bool ] = None , timestep : typing . Optional [ int ] = None , sim_time : typing . Optional [ float ] = None , agent_id : typing . Optional [ str ] = None , info : typing . Optional [ dict ] = None , episode : typing . Optional [ Episode ] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: \"\"\"Computes actions for the current policy. Args: obs_batch: Batch of observations. state_batches: List of RNN state input batches, if any. prev_action_batch: Batch of previous action values. prev_reward_batch: Batch of previous rewards. info_batch: Batch of info objects. episodes: List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. explore: Whether to pick an exploitation or exploration action. Set to None (default) for using the value of `self.config[\"explore\"]`. timestep: The current (sampling) time step. Keyword Args: kwargs: Forward compatibility placeholder Returns: actions (TensorType): Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. \"\"\" raise NotImplementedError get_weights ( self ) \u00a4 Returns model weights. Note: The return value of this method will reside under the \"weights\" key in the return value of Policy.get_state(). Model weights are only one part of a Policy's state. Other state information contains: optimizer variables, exploration state, and global state vars such as the sampling timestep. Returns: Type Description Serializable copy or view of model weights. Source code in corl/policies/custom_policy.py def get_weights ( self ): return {} learn_on_batch ( self , samples ) \u00a4 Perform one learning update, given samples . Either this method or the combination of compute_gradients and apply_gradients must be implemented by subclasses. Parameters: Name Type Description Default samples The SampleBatch object to learn from. required Returns: Type Description Dictionary of extra metadata from compute_gradients() . Examples: >>> policy , sample_batch = ... >>> policy . learn_on_batch ( sample_batch ) Source code in corl/policies/custom_policy.py def learn_on_batch ( self , samples ): return {} set_weights ( self , weights ) \u00a4 Sets this Policy's model's weights. Note: Model weights are only one part of a Policy's state. Other state information contains: optimizer variables, exploration state, and global state vars such as the sampling timestep. Parameters: Name Type Description Default weights Serializable copy or view of model weights. required Source code in corl/policies/custom_policy.py def set_weights ( self , weights ): return CustomPolicyValidator ( BasePolicyValidator ) pydantic-model \u00a4 Base validator for the CustomPolicy Source code in corl/policies/custom_policy.py class CustomPolicyValidator ( BasePolicyValidator ): \"\"\"Base validator for the CustomPolicy\"\"\" act_space : gym . Space obs_space : gym . Space time_extractor : ObservationExtractorValidator # rllib assumes that actions have been normalized and calls 'unsquash_action' prior to sending it to the environment: # https://github.com/ray-project/ray/blob/c78bd809ce4b2ec0e48c77aa461684ee1e6f259b/rllib/evaluation/sampler.py#L1241 normalize_controls : bool = True controllers : typing . List [ typing . Tuple ] class Config : \"\"\"pydantic configuration options\"\"\" arbitrary_types_allowed = True @validator ( 'act_space' ) def validate_act_space ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that it's an instance of an gym.Space\"\"\" assert isinstance ( v , gym . Space ) # TODO Issue warning if the action space is normalized return v @validator ( 'time_extractor' , always = True ) def validate_extractor ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"Ensures the time_extractor can actually extract data from the space and that it's not normalized\"\"\" try : time_space = v . construct_extractors () . space ( values [ 'obs_space' ] . original_space ) if isinstance ( time_space , gym . spaces . Box ): assert np . isinf ( time_space . high [ v . indices [ 0 ]]), \"time_space must not be normalized\" except Exception as e : raise RuntimeError ( f \"Failed to extract time using { v } from { values [ 'obs_space' ] . original_space } \" ) from e return v @validator ( 'controllers' , pre = True , always = True ) def validate_controllers ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that the controllers match the action_space\"\"\" tuple_list = [] for iterable in v : tuple_list . append ( tuple ( iterable )) assert len ( tuple_list ) == len ( set ( tuple_list )), 'controller definitions must be unique' sample_control = flatten_dict . flatten ( values [ 'act_space' ] . sample ()) for tuple_key in tuple_list : assert tuple_key in sample_control , f 'controller { tuple_key } not found in action_space: { list ( sample_control . keys ()) } ' return tuple_list Config \u00a4 pydantic configuration options Source code in corl/policies/custom_policy.py class Config : \"\"\"pydantic configuration options\"\"\" arbitrary_types_allowed = True validate_act_space ( v ) classmethod \u00a4 validate that it's an instance of an gym.Space Source code in corl/policies/custom_policy.py @validator ( 'act_space' ) def validate_act_space ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that it's an instance of an gym.Space\"\"\" assert isinstance ( v , gym . Space ) # TODO Issue warning if the action space is normalized return v validate_controllers ( v , values ) classmethod \u00a4 validate that the controllers match the action_space Source code in corl/policies/custom_policy.py @validator ( 'controllers' , pre = True , always = True ) def validate_controllers ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that the controllers match the action_space\"\"\" tuple_list = [] for iterable in v : tuple_list . append ( tuple ( iterable )) assert len ( tuple_list ) == len ( set ( tuple_list )), 'controller definitions must be unique' sample_control = flatten_dict . flatten ( values [ 'act_space' ] . sample ()) for tuple_key in tuple_list : assert tuple_key in sample_control , f 'controller { tuple_key } not found in action_space: { list ( sample_control . keys ()) } ' return tuple_list validate_extractor ( v , values ) classmethod \u00a4 Ensures the time_extractor can actually extract data from the space and that it's not normalized Source code in corl/policies/custom_policy.py @validator ( 'time_extractor' , always = True ) def validate_extractor ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"Ensures the time_extractor can actually extract data from the space and that it's not normalized\"\"\" try : time_space = v . construct_extractors () . space ( values [ 'obs_space' ] . original_space ) if isinstance ( time_space , gym . spaces . Box ): assert np . isinf ( time_space . high [ v . indices [ 0 ]]), \"time_space must not be normalized\" except Exception as e : raise RuntimeError ( f \"Failed to extract time using { v } from { values [ 'obs_space' ] . original_space } \" ) from e return v","title":"Custom policy"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicy","text":"Custom base policy. Source code in corl/policies/custom_policy.py class CustomPolicy ( Policy ): # pylint: disable=abstract-method \"\"\"Custom base policy. \"\"\" def __init__ ( self , observation_space , action_space , config ): self . validated_config : CustomPolicyValidator = self . get_validator ( act_space = action_space , obs_space = observation_space , ** config ) Policy . __init__ ( self , observation_space , action_space , config ) self . time_extractor = self . validated_config . time_extractor . construct_extractors () self . _reset () @property def get_validator ( self ) -> typing . Type [ BasePolicyValidator ]: \"\"\" Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class \"\"\" return CustomPolicyValidator def _reset ( self ): \"\"\"This must be overriden in order to reset the state between runs \"\"\" ... def learn_on_batch ( self , samples ): return {} def get_weights ( self ): return {} def set_weights ( self , weights ): return def compute_actions_from_input_dict ( self , input_dict : typing . Union [ SampleBatch , typing . Dict [ str , TensorStructType ]], explore : bool = None , timestep : typing . Optional [ int ] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: \"\"\"Computes actions from collected samples (across multiple-agents). Takes an input dict (usually a SampleBatch) as its main data input. This allows for using this method in case a more complex input pattern (view requirements) is needed, for example when the Model requires the last n observations, the last m actions/rewards, or a combination of any of these. Args: input_dict: A SampleBatch or input dict containing the Tensors to compute actions. `input_dict` already abides to the Policy's as well as the Model's view requirements and can thus be passed to the Model as-is. explore: Whether to pick an exploitation or exploration action (default: None -> use self.config[\"explore\"]). timestep: The current (sampling) time step. episodes: This provides access to all of the internal episodes' state, which may be useful for model-based or multi-agent algorithms. Keyword Args: kwargs: Forward compatibility placeholder. Returns: actions: Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs: List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info: Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. \"\"\" # Default implementation just passes obs, prev-a/r, and states on to # `self.compute_actions()`. agent_index = input_dict [ SampleBatch . AGENT_INDEX ][ 0 ] episode_id = input_dict [ SampleBatch . EPS_ID ][ 0 ] episode = [ eps for eps in episodes if eps . episode_id == episode_id ][ 0 ] # type: ignore agent_id : str = [ aid for aid in episode . _agent_to_index if episode . _agent_to_index [ aid ] == agent_index # pylint: disable=protected-access ][ 0 ] obs_batch = input_dict [ SampleBatch . OBS ] info = episode . last_info_for ( agent_id ) if info is None : info = {} if 'platform_obs' in info : sim_time = self . time_extractor . value ( info [ 'platform_obs' ][ agent_id ], full_extraction = True ) else : self . _reset () sim_time = - 1 state_batches = [ s for k , s in input_dict . items () if k [: 9 ] == \"state_in_\" ] return self . compute_actions ( obs_batch , state_batches , prev_action_batch = input_dict . get ( SampleBatch . PREV_ACTIONS ), prev_reward_batch = input_dict . get ( SampleBatch . PREV_REWARDS ), info_batch = input_dict . get ( SampleBatch . INFOS ), # type: ignore explore = explore , timestep = timestep , episodes = episodes , sim_time = sim_time , agent_id = agent_id , info = info , episode = episode , ** kwargs , ) def compute_actions ( self , obs_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ], state_batches : typing . Optional [ typing . List [ TensorType ]] = None , prev_action_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , prev_reward_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , info_batch : typing . Optional [ typing . Dict [ str , list ]] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , explore : typing . Optional [ bool ] = None , timestep : typing . Optional [ int ] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: actions , state_outs , info = self . custom_compute_actions ( obs_batch , state_batches = state_batches , prev_action_batch = prev_action_batch , prev_reward_batch = prev_reward_batch , info_batch = info_batch , episodes = episodes , explore = explore , timestep = timestep , ** kwargs ) if self . validated_config . normalize_controls : for i , action in enumerate ( actions ): actions [ i ] = EnvSpaceUtil . scale_sample_from_space ( self . validated_config . act_space , action ) return actions , state_outs , info @abstractmethod def custom_compute_actions ( self , obs_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ], state_batches : typing . Optional [ typing . List [ TensorType ]] = None , prev_action_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , prev_reward_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , info_batch : typing . Optional [ typing . Dict [ str , list ]] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , explore : typing . Optional [ bool ] = None , timestep : typing . Optional [ int ] = None , sim_time : typing . Optional [ float ] = None , agent_id : typing . Optional [ str ] = None , info : typing . Optional [ dict ] = None , episode : typing . Optional [ Episode ] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: \"\"\"Computes actions for the current policy. Args: obs_batch: Batch of observations. state_batches: List of RNN state input batches, if any. prev_action_batch: Batch of previous action values. prev_reward_batch: Batch of previous rewards. info_batch: Batch of info objects. episodes: List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. explore: Whether to pick an exploitation or exploration action. Set to None (default) for using the value of `self.config[\"explore\"]`. timestep: The current (sampling) time step. Keyword Args: kwargs: Forward compatibility placeholder Returns: actions (TensorType): Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. \"\"\" raise NotImplementedError","title":"CustomPolicy"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicy.get_validator","text":"Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class","title":"get_validator"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicy.compute_actions","text":"Computes actions for the current policy. Parameters: Name Type Description Default obs_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of observations. required state_batches Optional[List[Any]] List of RNN state input batches, if any. None prev_action_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of previous action values. None prev_reward_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of previous rewards. None info_batch Optional[Dict[str, list]] Batch of info objects. None episodes Optional[List[ray.rllib.evaluation.episode.Episode]] List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. None explore Optional[bool] Whether to pick an exploitation or exploration action. Set to None (default) for using the value of self.config[\"explore\"] . None timestep Optional[int] The current (sampling) time step. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder Returns: Type Description actions (TensorType) Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/custom_policy.py def compute_actions ( self , obs_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ], state_batches : typing . Optional [ typing . List [ TensorType ]] = None , prev_action_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , prev_reward_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , info_batch : typing . Optional [ typing . Dict [ str , list ]] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , explore : typing . Optional [ bool ] = None , timestep : typing . Optional [ int ] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: actions , state_outs , info = self . custom_compute_actions ( obs_batch , state_batches = state_batches , prev_action_batch = prev_action_batch , prev_reward_batch = prev_reward_batch , info_batch = info_batch , episodes = episodes , explore = explore , timestep = timestep , ** kwargs ) if self . validated_config . normalize_controls : for i , action in enumerate ( actions ): actions [ i ] = EnvSpaceUtil . scale_sample_from_space ( self . validated_config . act_space , action ) return actions , state_outs , info","title":"compute_actions()"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicy.compute_actions_from_input_dict","text":"Computes actions from collected samples (across multiple-agents). Takes an input dict (usually a SampleBatch) as its main data input. This allows for using this method in case a more complex input pattern (view requirements) is needed, for example when the Model requires the last n observations, the last m actions/rewards, or a combination of any of these. Parameters: Name Type Description Default input_dict Union[ray.rllib.policy.sample_batch.SampleBatch, Dict[str, Union[Any, dict, tuple]]] A SampleBatch or input dict containing the Tensors to compute actions. input_dict already abides to the Policy's as well as the Model's view requirements and can thus be passed to the Model as-is. required explore bool Whether to pick an exploitation or exploration action (default: None -> use self.config[\"explore\"]). None timestep Optional[int] The current (sampling) time step. None episodes Optional[List[ray.rllib.evaluation.episode.Episode]] This provides access to all of the internal episodes' state, which may be useful for model-based or multi-agent algorithms. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder. Returns: Type Description actions Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs: List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info: Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/custom_policy.py def compute_actions_from_input_dict ( self , input_dict : typing . Union [ SampleBatch , typing . Dict [ str , TensorStructType ]], explore : bool = None , timestep : typing . Optional [ int ] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: \"\"\"Computes actions from collected samples (across multiple-agents). Takes an input dict (usually a SampleBatch) as its main data input. This allows for using this method in case a more complex input pattern (view requirements) is needed, for example when the Model requires the last n observations, the last m actions/rewards, or a combination of any of these. Args: input_dict: A SampleBatch or input dict containing the Tensors to compute actions. `input_dict` already abides to the Policy's as well as the Model's view requirements and can thus be passed to the Model as-is. explore: Whether to pick an exploitation or exploration action (default: None -> use self.config[\"explore\"]). timestep: The current (sampling) time step. episodes: This provides access to all of the internal episodes' state, which may be useful for model-based or multi-agent algorithms. Keyword Args: kwargs: Forward compatibility placeholder. Returns: actions: Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs: List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info: Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. \"\"\" # Default implementation just passes obs, prev-a/r, and states on to # `self.compute_actions()`. agent_index = input_dict [ SampleBatch . AGENT_INDEX ][ 0 ] episode_id = input_dict [ SampleBatch . EPS_ID ][ 0 ] episode = [ eps for eps in episodes if eps . episode_id == episode_id ][ 0 ] # type: ignore agent_id : str = [ aid for aid in episode . _agent_to_index if episode . _agent_to_index [ aid ] == agent_index # pylint: disable=protected-access ][ 0 ] obs_batch = input_dict [ SampleBatch . OBS ] info = episode . last_info_for ( agent_id ) if info is None : info = {} if 'platform_obs' in info : sim_time = self . time_extractor . value ( info [ 'platform_obs' ][ agent_id ], full_extraction = True ) else : self . _reset () sim_time = - 1 state_batches = [ s for k , s in input_dict . items () if k [: 9 ] == \"state_in_\" ] return self . compute_actions ( obs_batch , state_batches , prev_action_batch = input_dict . get ( SampleBatch . PREV_ACTIONS ), prev_reward_batch = input_dict . get ( SampleBatch . PREV_REWARDS ), info_batch = input_dict . get ( SampleBatch . INFOS ), # type: ignore explore = explore , timestep = timestep , episodes = episodes , sim_time = sim_time , agent_id = agent_id , info = info , episode = episode , ** kwargs , )","title":"compute_actions_from_input_dict()"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicy.custom_compute_actions","text":"Computes actions for the current policy. Parameters: Name Type Description Default obs_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of observations. required state_batches Optional[List[Any]] List of RNN state input batches, if any. None prev_action_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of previous action values. None prev_reward_batch Union[List[Union[Any, dict, tuple]], Any, dict, tuple] Batch of previous rewards. None info_batch Optional[Dict[str, list]] Batch of info objects. None episodes Optional[List[ray.rllib.evaluation.episode.Episode]] List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. None explore Optional[bool] Whether to pick an exploitation or exploration action. Set to None (default) for using the value of self.config[\"explore\"] . None timestep Optional[int] The current (sampling) time step. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder Returns: Type Description actions (TensorType) Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/custom_policy.py @abstractmethod def custom_compute_actions ( self , obs_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ], state_batches : typing . Optional [ typing . List [ TensorType ]] = None , prev_action_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , prev_reward_batch : typing . Union [ typing . List [ TensorStructType ], TensorStructType ] = None , info_batch : typing . Optional [ typing . Dict [ str , list ]] = None , episodes : typing . Optional [ typing . List [ Episode ]] = None , explore : typing . Optional [ bool ] = None , timestep : typing . Optional [ int ] = None , sim_time : typing . Optional [ float ] = None , agent_id : typing . Optional [ str ] = None , info : typing . Optional [ dict ] = None , episode : typing . Optional [ Episode ] = None , ** kwargs ) -> typing . Tuple [ TensorType , typing . List [ TensorType ], typing . Dict [ str , TensorType ]]: \"\"\"Computes actions for the current policy. Args: obs_batch: Batch of observations. state_batches: List of RNN state input batches, if any. prev_action_batch: Batch of previous action values. prev_reward_batch: Batch of previous rewards. info_batch: Batch of info objects. episodes: List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. explore: Whether to pick an exploitation or exploration action. Set to None (default) for using the value of `self.config[\"explore\"]`. timestep: The current (sampling) time step. Keyword Args: kwargs: Forward compatibility placeholder Returns: actions (TensorType): Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. \"\"\" raise NotImplementedError","title":"custom_compute_actions()"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicy.get_weights","text":"Returns model weights. Note: The return value of this method will reside under the \"weights\" key in the return value of Policy.get_state(). Model weights are only one part of a Policy's state. Other state information contains: optimizer variables, exploration state, and global state vars such as the sampling timestep. Returns: Type Description Serializable copy or view of model weights. Source code in corl/policies/custom_policy.py def get_weights ( self ): return {}","title":"get_weights()"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicy.learn_on_batch","text":"Perform one learning update, given samples . Either this method or the combination of compute_gradients and apply_gradients must be implemented by subclasses. Parameters: Name Type Description Default samples The SampleBatch object to learn from. required Returns: Type Description Dictionary of extra metadata from compute_gradients() . Examples: >>> policy , sample_batch = ... >>> policy . learn_on_batch ( sample_batch ) Source code in corl/policies/custom_policy.py def learn_on_batch ( self , samples ): return {}","title":"learn_on_batch()"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicy.set_weights","text":"Sets this Policy's model's weights. Note: Model weights are only one part of a Policy's state. Other state information contains: optimizer variables, exploration state, and global state vars such as the sampling timestep. Parameters: Name Type Description Default weights Serializable copy or view of model weights. required Source code in corl/policies/custom_policy.py def set_weights ( self , weights ): return","title":"set_weights()"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicyValidator","text":"Base validator for the CustomPolicy Source code in corl/policies/custom_policy.py class CustomPolicyValidator ( BasePolicyValidator ): \"\"\"Base validator for the CustomPolicy\"\"\" act_space : gym . Space obs_space : gym . Space time_extractor : ObservationExtractorValidator # rllib assumes that actions have been normalized and calls 'unsquash_action' prior to sending it to the environment: # https://github.com/ray-project/ray/blob/c78bd809ce4b2ec0e48c77aa461684ee1e6f259b/rllib/evaluation/sampler.py#L1241 normalize_controls : bool = True controllers : typing . List [ typing . Tuple ] class Config : \"\"\"pydantic configuration options\"\"\" arbitrary_types_allowed = True @validator ( 'act_space' ) def validate_act_space ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that it's an instance of an gym.Space\"\"\" assert isinstance ( v , gym . Space ) # TODO Issue warning if the action space is normalized return v @validator ( 'time_extractor' , always = True ) def validate_extractor ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"Ensures the time_extractor can actually extract data from the space and that it's not normalized\"\"\" try : time_space = v . construct_extractors () . space ( values [ 'obs_space' ] . original_space ) if isinstance ( time_space , gym . spaces . Box ): assert np . isinf ( time_space . high [ v . indices [ 0 ]]), \"time_space must not be normalized\" except Exception as e : raise RuntimeError ( f \"Failed to extract time using { v } from { values [ 'obs_space' ] . original_space } \" ) from e return v @validator ( 'controllers' , pre = True , always = True ) def validate_controllers ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that the controllers match the action_space\"\"\" tuple_list = [] for iterable in v : tuple_list . append ( tuple ( iterable )) assert len ( tuple_list ) == len ( set ( tuple_list )), 'controller definitions must be unique' sample_control = flatten_dict . flatten ( values [ 'act_space' ] . sample ()) for tuple_key in tuple_list : assert tuple_key in sample_control , f 'controller { tuple_key } not found in action_space: { list ( sample_control . keys ()) } ' return tuple_list","title":"CustomPolicyValidator"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicyValidator.Config","text":"pydantic configuration options Source code in corl/policies/custom_policy.py class Config : \"\"\"pydantic configuration options\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicyValidator.validate_act_space","text":"validate that it's an instance of an gym.Space Source code in corl/policies/custom_policy.py @validator ( 'act_space' ) def validate_act_space ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that it's an instance of an gym.Space\"\"\" assert isinstance ( v , gym . Space ) # TODO Issue warning if the action space is normalized return v","title":"validate_act_space()"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicyValidator.validate_controllers","text":"validate that the controllers match the action_space Source code in corl/policies/custom_policy.py @validator ( 'controllers' , pre = True , always = True ) def validate_controllers ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that the controllers match the action_space\"\"\" tuple_list = [] for iterable in v : tuple_list . append ( tuple ( iterable )) assert len ( tuple_list ) == len ( set ( tuple_list )), 'controller definitions must be unique' sample_control = flatten_dict . flatten ( values [ 'act_space' ] . sample ()) for tuple_key in tuple_list : assert tuple_key in sample_control , f 'controller { tuple_key } not found in action_space: { list ( sample_control . keys ()) } ' return tuple_list","title":"validate_controllers()"},{"location":"reference/policies/custom_policy/#corl.policies.custom_policy.CustomPolicyValidator.validate_extractor","text":"Ensures the time_extractor can actually extract data from the space and that it's not normalized Source code in corl/policies/custom_policy.py @validator ( 'time_extractor' , always = True ) def validate_extractor ( cls , v , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"Ensures the time_extractor can actually extract data from the space and that it's not normalized\"\"\" try : time_space = v . construct_extractors () . space ( values [ 'obs_space' ] . original_space ) if isinstance ( time_space , gym . spaces . Box ): assert np . isinf ( time_space . high [ v . indices [ 0 ]]), \"time_space must not be normalized\" except Exception as e : raise RuntimeError ( f \"Failed to extract time using { v } from { values [ 'obs_space' ] . original_space } \" ) from e return v","title":"validate_extractor()"},{"location":"reference/policies/random_action/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Module with base implimentations for Observations RandomActionPolicy ( Policy ) \u00a4 Random action policy. Source code in corl/policies/random_action.py class RandomActionPolicy ( Policy ): # pylint: disable=abstract-method \"\"\"Random action policy. \"\"\" def __init__ ( self , observation_space , action_space , config ): Policy . __init__ ( self , observation_space , action_space , config ) self . view_requirements = { key : value for key , value in self . view_requirements . items () if key != SampleBatch . PREV_ACTIONS } def compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , ** kwargs ): return [ self . action_space . sample () for _ in obs_batch ], [], {} def learn_on_batch ( self , samples ): return {} def get_weights ( self ): return {} def set_weights ( self , weights ): return compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , ** kwargs ) \u00a4 Computes actions for the current policy. Parameters: Name Type Description Default obs_batch Batch of observations. required state_batches List of RNN state input batches, if any. None prev_action_batch Batch of previous action values. None prev_reward_batch Batch of previous rewards. None info_batch Batch of info objects. None episodes List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. None explore Whether to pick an exploitation or exploration action. Set to None (default) for using the value of self.config[\"explore\"] . None timestep The current (sampling) time step. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder Returns: Type Description actions (TensorType) Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/random_action.py def compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , ** kwargs ): return [ self . action_space . sample () for _ in obs_batch ], [], {} get_weights ( self ) \u00a4 Returns model weights. Note: The return value of this method will reside under the \"weights\" key in the return value of Policy.get_state(). Model weights are only one part of a Policy's state. Other state information contains: optimizer variables, exploration state, and global state vars such as the sampling timestep. Returns: Type Description Serializable copy or view of model weights. Source code in corl/policies/random_action.py def get_weights ( self ): return {} learn_on_batch ( self , samples ) \u00a4 Perform one learning update, given samples . Either this method or the combination of compute_gradients and apply_gradients must be implemented by subclasses. Parameters: Name Type Description Default samples The SampleBatch object to learn from. required Returns: Type Description Dictionary of extra metadata from compute_gradients() . Examples: >>> policy , sample_batch = ... >>> policy . learn_on_batch ( sample_batch ) Source code in corl/policies/random_action.py def learn_on_batch ( self , samples ): return {} set_weights ( self , weights ) \u00a4 Sets this Policy's model's weights. Note: Model weights are only one part of a Policy's state. Other state information contains: optimizer variables, exploration state, and global state vars such as the sampling timestep. Parameters: Name Type Description Default weights Serializable copy or view of model weights. required Source code in corl/policies/random_action.py def set_weights ( self , weights ): return","title":"Random action"},{"location":"reference/policies/random_action/#corl.policies.random_action.RandomActionPolicy","text":"Random action policy. Source code in corl/policies/random_action.py class RandomActionPolicy ( Policy ): # pylint: disable=abstract-method \"\"\"Random action policy. \"\"\" def __init__ ( self , observation_space , action_space , config ): Policy . __init__ ( self , observation_space , action_space , config ) self . view_requirements = { key : value for key , value in self . view_requirements . items () if key != SampleBatch . PREV_ACTIONS } def compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , ** kwargs ): return [ self . action_space . sample () for _ in obs_batch ], [], {} def learn_on_batch ( self , samples ): return {} def get_weights ( self ): return {} def set_weights ( self , weights ): return","title":"RandomActionPolicy"},{"location":"reference/policies/random_action/#corl.policies.random_action.RandomActionPolicy.compute_actions","text":"Computes actions for the current policy. Parameters: Name Type Description Default obs_batch Batch of observations. required state_batches List of RNN state input batches, if any. None prev_action_batch Batch of previous action values. None prev_reward_batch Batch of previous rewards. None info_batch Batch of info objects. None episodes List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. None explore Whether to pick an exploitation or exploration action. Set to None (default) for using the value of self.config[\"explore\"] . None timestep The current (sampling) time step. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder Returns: Type Description actions (TensorType) Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/random_action.py def compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , ** kwargs ): return [ self . action_space . sample () for _ in obs_batch ], [], {}","title":"compute_actions()"},{"location":"reference/policies/random_action/#corl.policies.random_action.RandomActionPolicy.get_weights","text":"Returns model weights. Note: The return value of this method will reside under the \"weights\" key in the return value of Policy.get_state(). Model weights are only one part of a Policy's state. Other state information contains: optimizer variables, exploration state, and global state vars such as the sampling timestep. Returns: Type Description Serializable copy or view of model weights. Source code in corl/policies/random_action.py def get_weights ( self ): return {}","title":"get_weights()"},{"location":"reference/policies/random_action/#corl.policies.random_action.RandomActionPolicy.learn_on_batch","text":"Perform one learning update, given samples . Either this method or the combination of compute_gradients and apply_gradients must be implemented by subclasses. Parameters: Name Type Description Default samples The SampleBatch object to learn from. required Returns: Type Description Dictionary of extra metadata from compute_gradients() . Examples: >>> policy , sample_batch = ... >>> policy . learn_on_batch ( sample_batch ) Source code in corl/policies/random_action.py def learn_on_batch ( self , samples ): return {}","title":"learn_on_batch()"},{"location":"reference/policies/random_action/#corl.policies.random_action.RandomActionPolicy.set_weights","text":"Sets this Policy's model's weights. Note: Model weights are only one part of a Policy's state. Other state information contains: optimizer variables, exploration state, and global state vars such as the sampling timestep. Parameters: Name Type Description Default weights Serializable copy or view of model weights. required Source code in corl/policies/random_action.py def set_weights ( self , weights ): return","title":"set_weights()"},{"location":"reference/policies/scripted_action/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Scripted Action Policy ScriptedActionPolicy ( CustomPolicy ) \u00a4 Scripted action policy. Source code in corl/policies/scripted_action.py class ScriptedActionPolicy ( CustomPolicy ): # pylint: disable=abstract-method \"\"\"Scripted action policy. \"\"\" def __init__ ( self , observation_space , action_space , config ): super () . __init__ ( observation_space , action_space , config ) self . _input_index : int self . _last_action : dict @property def get_validator ( self ) -> typing . Type [ BasePolicyValidator ]: \"\"\" Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class \"\"\" return ScriptedActionPolicyValidator def _reset ( self ): super () . _reset () self . _input_index = 0 self . _last_action = EnvSpaceUtil . get_zero_sample_from_space ( self . validated_config . act_space ) def custom_compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , sim_time = None , agent_id = None , info = None , episode = None , ** kwargs ): for control_index in range ( self . _input_index , len ( self . validated_config . control_times )): control_time = self . validated_config . control_times [ control_index ] if sim_time >= control_time : # apply control_list to controls control_values = self . validated_config . control_values [ control_index ] self . _input_index = control_index + 1 self . _last_action = control_values return [ control_values ], [], {} break if self . validated_config . missing_action_policy == 'repeat_last_action' : return [ self . _last_action ], [], {} self . _last_action = self . validated_config . default_action return [ self . validated_config . default_action ], [], {} get_validator : Type [ corl . policies . base_policy . BasePolicyValidator ] property readonly \u00a4 Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class custom_compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , sim_time = None , agent_id = None , info = None , episode = None , ** kwargs ) \u00a4 Computes actions for the current policy. Parameters: Name Type Description Default obs_batch Batch of observations. required state_batches List of RNN state input batches, if any. None prev_action_batch Batch of previous action values. None prev_reward_batch Batch of previous rewards. None info_batch Batch of info objects. None episodes List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. None explore Whether to pick an exploitation or exploration action. Set to None (default) for using the value of self.config[\"explore\"] . None timestep The current (sampling) time step. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder Returns: Type Description actions (TensorType) Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/scripted_action.py def custom_compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , sim_time = None , agent_id = None , info = None , episode = None , ** kwargs ): for control_index in range ( self . _input_index , len ( self . validated_config . control_times )): control_time = self . validated_config . control_times [ control_index ] if sim_time >= control_time : # apply control_list to controls control_values = self . validated_config . control_values [ control_index ] self . _input_index = control_index + 1 self . _last_action = control_values return [ control_values ], [], {} break if self . validated_config . missing_action_policy == 'repeat_last_action' : return [ self . _last_action ], [], {} self . _last_action = self . validated_config . default_action return [ self . validated_config . default_action ], [], {} ScriptedActionPolicyValidator ( CustomPolicyValidator ) pydantic-model \u00a4 Validator for the ScriptedActionPolicy Source code in corl/policies/scripted_action.py class ScriptedActionPolicyValidator ( CustomPolicyValidator ): \"\"\"Validator for the ScriptedActionPolicy\"\"\" control_times : typing . List [ float ] control_values : typing . List [ typing . Dict ] missing_action_policy : typing . Literal [ 'default_action' , 'repeat_last_action' ] default_action : typing . Optional [ typing . Dict ] = None class Config : \"\"\"pydantic configuration options\"\"\" arbitrary_types_allowed = True @validator ( 'control_times' ) def sort_control_times ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\"Ensures that control_times are in order\"\"\" assert v == sorted ( v ), \"control_times must be in order\" return v @staticmethod def convert_control_value ( controls , controller_key_paths , sample_control ): \"\"\"converts the controls into a dict\"\"\" flat_sample_control = flatten_dict . flatten ( sample_control ) assert len ( controls ) == len ( controller_key_paths ), 'mismatch between number of controllers and length of control values' assert len ( controller_key_paths ) == len ( flat_sample_control ), 'mismatch between number of controllers and the action_space' flat_control_dict = {} for i , ctrl_value in enumerate ( controls ): try : controller_key = controller_key_paths [ i ] sample_value = flat_sample_control [ controller_key ] if isinstance ( sample_value , np . ndarray ): flat_control_dict [ controller_key ] = np . add ( sample_value * 0 , ctrl_value , dtype = sample_value . dtype ) else : flat_control_dict [ controller_key ] = type ( sample_value )( ctrl_value ) except Exception as e : raise RuntimeError ( f '@idx: { i } , controller: { controller_key } , control_value: { ctrl_value } ' ) from e control_dict = flatten_dict . unflatten ( flat_control_dict ) return control_dict @staticmethod def convert_control_values ( action_space , controller_key_paths , control_times , controls_list ): \"\"\"converts the input_control values into dictionary and validates it agaist the action space\"\"\" assert len ( controls_list ) == len ( control_times ), 'mismatch between number of control_times and control_values' sample_control = action_space . sample () converted_control_list : typing . List [ typing . Dict ] = [] for controls in controls_list : control_dict = ScriptedActionPolicyValidator . convert_control_value ( controls , controller_key_paths , sample_control ) EnvSpaceUtil . deep_sanity_check_space_sample ( action_space , control_dict ) converted_control_list . append ( control_dict ) return converted_control_list @validator ( 'control_values' , pre = True , always = True ) def validate_control_values ( cls , controls_list , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that control_values match the controllers\"\"\" if 'controllers' not in values or 'control_times' not in values : raise ValueError ( f 'Could not run \"validate_control_values\" because previous items failed: { values } ' ) return ScriptedActionPolicyValidator . convert_control_values ( values [ 'act_space' ], values [ 'controllers' ], values [ 'control_times' ], controls_list ) @validator ( 'missing_action_policy' ) def validate_missing_action_policy ( cls , missing_action_policy , values ): \"\"\"validates the missing action policy\"\"\" if missing_action_policy == 'repeat_last_action' : assert values [ 'control_times' ][ 0 ] == 0 , \"missing control_time for t=0\" return missing_action_policy @validator ( 'default_action' , pre = True ) def validate_default_action ( cls , default_action , values ): \"\"\"validates that the default_action is consistent with the missing_action_policy\"\"\" if values [ \"missing_action_policy\" ] == 'default_action' : assert default_action is not None , 'default_action is requried when using the default_action missing_action_policy' action_space = values [ 'act_space' ] action = ScriptedActionPolicyValidator . convert_control_value ( default_action , values [ 'controllers' ], action_space . sample ()) return action assert default_action is None , 'default_action is invalid except when using the default_action missing_action_policy' return default_action Config \u00a4 pydantic configuration options Source code in corl/policies/scripted_action.py class Config : \"\"\"pydantic configuration options\"\"\" arbitrary_types_allowed = True convert_control_value ( controls , controller_key_paths , sample_control ) staticmethod \u00a4 converts the controls into a dict Source code in corl/policies/scripted_action.py @staticmethod def convert_control_value ( controls , controller_key_paths , sample_control ): \"\"\"converts the controls into a dict\"\"\" flat_sample_control = flatten_dict . flatten ( sample_control ) assert len ( controls ) == len ( controller_key_paths ), 'mismatch between number of controllers and length of control values' assert len ( controller_key_paths ) == len ( flat_sample_control ), 'mismatch between number of controllers and the action_space' flat_control_dict = {} for i , ctrl_value in enumerate ( controls ): try : controller_key = controller_key_paths [ i ] sample_value = flat_sample_control [ controller_key ] if isinstance ( sample_value , np . ndarray ): flat_control_dict [ controller_key ] = np . add ( sample_value * 0 , ctrl_value , dtype = sample_value . dtype ) else : flat_control_dict [ controller_key ] = type ( sample_value )( ctrl_value ) except Exception as e : raise RuntimeError ( f '@idx: { i } , controller: { controller_key } , control_value: { ctrl_value } ' ) from e control_dict = flatten_dict . unflatten ( flat_control_dict ) return control_dict convert_control_values ( action_space , controller_key_paths , control_times , controls_list ) staticmethod \u00a4 converts the input_control values into dictionary and validates it agaist the action space Source code in corl/policies/scripted_action.py @staticmethod def convert_control_values ( action_space , controller_key_paths , control_times , controls_list ): \"\"\"converts the input_control values into dictionary and validates it agaist the action space\"\"\" assert len ( controls_list ) == len ( control_times ), 'mismatch between number of control_times and control_values' sample_control = action_space . sample () converted_control_list : typing . List [ typing . Dict ] = [] for controls in controls_list : control_dict = ScriptedActionPolicyValidator . convert_control_value ( controls , controller_key_paths , sample_control ) EnvSpaceUtil . deep_sanity_check_space_sample ( action_space , control_dict ) converted_control_list . append ( control_dict ) return converted_control_list sort_control_times ( v ) classmethod \u00a4 Ensures that control_times are in order Source code in corl/policies/scripted_action.py @validator ( 'control_times' ) def sort_control_times ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\"Ensures that control_times are in order\"\"\" assert v == sorted ( v ), \"control_times must be in order\" return v validate_control_values ( controls_list , values ) classmethod \u00a4 validate that control_values match the controllers Source code in corl/policies/scripted_action.py @validator ( 'control_values' , pre = True , always = True ) def validate_control_values ( cls , controls_list , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that control_values match the controllers\"\"\" if 'controllers' not in values or 'control_times' not in values : raise ValueError ( f 'Could not run \"validate_control_values\" because previous items failed: { values } ' ) return ScriptedActionPolicyValidator . convert_control_values ( values [ 'act_space' ], values [ 'controllers' ], values [ 'control_times' ], controls_list ) validate_default_action ( default_action , values ) classmethod \u00a4 validates that the default_action is consistent with the missing_action_policy Source code in corl/policies/scripted_action.py @validator ( 'default_action' , pre = True ) def validate_default_action ( cls , default_action , values ): \"\"\"validates that the default_action is consistent with the missing_action_policy\"\"\" if values [ \"missing_action_policy\" ] == 'default_action' : assert default_action is not None , 'default_action is requried when using the default_action missing_action_policy' action_space = values [ 'act_space' ] action = ScriptedActionPolicyValidator . convert_control_value ( default_action , values [ 'controllers' ], action_space . sample ()) return action assert default_action is None , 'default_action is invalid except when using the default_action missing_action_policy' return default_action validate_missing_action_policy ( missing_action_policy , values ) classmethod \u00a4 validates the missing action policy Source code in corl/policies/scripted_action.py @validator ( 'missing_action_policy' ) def validate_missing_action_policy ( cls , missing_action_policy , values ): \"\"\"validates the missing action policy\"\"\" if missing_action_policy == 'repeat_last_action' : assert values [ 'control_times' ][ 0 ] == 0 , \"missing control_time for t=0\" return missing_action_policy","title":"Scripted action"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicy","text":"Scripted action policy. Source code in corl/policies/scripted_action.py class ScriptedActionPolicy ( CustomPolicy ): # pylint: disable=abstract-method \"\"\"Scripted action policy. \"\"\" def __init__ ( self , observation_space , action_space , config ): super () . __init__ ( observation_space , action_space , config ) self . _input_index : int self . _last_action : dict @property def get_validator ( self ) -> typing . Type [ BasePolicyValidator ]: \"\"\" Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class \"\"\" return ScriptedActionPolicyValidator def _reset ( self ): super () . _reset () self . _input_index = 0 self . _last_action = EnvSpaceUtil . get_zero_sample_from_space ( self . validated_config . act_space ) def custom_compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , sim_time = None , agent_id = None , info = None , episode = None , ** kwargs ): for control_index in range ( self . _input_index , len ( self . validated_config . control_times )): control_time = self . validated_config . control_times [ control_index ] if sim_time >= control_time : # apply control_list to controls control_values = self . validated_config . control_values [ control_index ] self . _input_index = control_index + 1 self . _last_action = control_values return [ control_values ], [], {} break if self . validated_config . missing_action_policy == 'repeat_last_action' : return [ self . _last_action ], [], {} self . _last_action = self . validated_config . default_action return [ self . validated_config . default_action ], [], {}","title":"ScriptedActionPolicy"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicy.get_validator","text":"Get the validator for this experiment class, the kwargs sent to the experiment class will be validated using this object and add a self.config attr to the experiment class","title":"get_validator"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicy.custom_compute_actions","text":"Computes actions for the current policy. Parameters: Name Type Description Default obs_batch Batch of observations. required state_batches List of RNN state input batches, if any. None prev_action_batch Batch of previous action values. None prev_reward_batch Batch of previous rewards. None info_batch Batch of info objects. None episodes List of Episode objects, one for each obs in obs_batch. This provides access to all of the internal episode state, which may be useful for model-based or multi-agent algorithms. None explore Whether to pick an exploitation or exploration action. Set to None (default) for using the value of self.config[\"explore\"] . None timestep The current (sampling) time step. None Keyword arguments: Name Type Description kwargs Forward compatibility placeholder Returns: Type Description actions (TensorType) Batch of output actions, with shape like [BATCH_SIZE, ACTION_SHAPE]. state_outs (List[TensorType]): List of RNN state output batches, if any, each with shape [BATCH_SIZE, STATE_SIZE]. info (List[dict]): Dictionary of extra feature batches, if any, with shape like {\"f1\": [BATCH_SIZE, ...], \"f2\": [BATCH_SIZE, ...]}. Source code in corl/policies/scripted_action.py def custom_compute_actions ( self , obs_batch , state_batches = None , prev_action_batch = None , prev_reward_batch = None , info_batch = None , episodes = None , explore = None , timestep = None , sim_time = None , agent_id = None , info = None , episode = None , ** kwargs ): for control_index in range ( self . _input_index , len ( self . validated_config . control_times )): control_time = self . validated_config . control_times [ control_index ] if sim_time >= control_time : # apply control_list to controls control_values = self . validated_config . control_values [ control_index ] self . _input_index = control_index + 1 self . _last_action = control_values return [ control_values ], [], {} break if self . validated_config . missing_action_policy == 'repeat_last_action' : return [ self . _last_action ], [], {} self . _last_action = self . validated_config . default_action return [ self . validated_config . default_action ], [], {}","title":"custom_compute_actions()"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicyValidator","text":"Validator for the ScriptedActionPolicy Source code in corl/policies/scripted_action.py class ScriptedActionPolicyValidator ( CustomPolicyValidator ): \"\"\"Validator for the ScriptedActionPolicy\"\"\" control_times : typing . List [ float ] control_values : typing . List [ typing . Dict ] missing_action_policy : typing . Literal [ 'default_action' , 'repeat_last_action' ] default_action : typing . Optional [ typing . Dict ] = None class Config : \"\"\"pydantic configuration options\"\"\" arbitrary_types_allowed = True @validator ( 'control_times' ) def sort_control_times ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\"Ensures that control_times are in order\"\"\" assert v == sorted ( v ), \"control_times must be in order\" return v @staticmethod def convert_control_value ( controls , controller_key_paths , sample_control ): \"\"\"converts the controls into a dict\"\"\" flat_sample_control = flatten_dict . flatten ( sample_control ) assert len ( controls ) == len ( controller_key_paths ), 'mismatch between number of controllers and length of control values' assert len ( controller_key_paths ) == len ( flat_sample_control ), 'mismatch between number of controllers and the action_space' flat_control_dict = {} for i , ctrl_value in enumerate ( controls ): try : controller_key = controller_key_paths [ i ] sample_value = flat_sample_control [ controller_key ] if isinstance ( sample_value , np . ndarray ): flat_control_dict [ controller_key ] = np . add ( sample_value * 0 , ctrl_value , dtype = sample_value . dtype ) else : flat_control_dict [ controller_key ] = type ( sample_value )( ctrl_value ) except Exception as e : raise RuntimeError ( f '@idx: { i } , controller: { controller_key } , control_value: { ctrl_value } ' ) from e control_dict = flatten_dict . unflatten ( flat_control_dict ) return control_dict @staticmethod def convert_control_values ( action_space , controller_key_paths , control_times , controls_list ): \"\"\"converts the input_control values into dictionary and validates it agaist the action space\"\"\" assert len ( controls_list ) == len ( control_times ), 'mismatch between number of control_times and control_values' sample_control = action_space . sample () converted_control_list : typing . List [ typing . Dict ] = [] for controls in controls_list : control_dict = ScriptedActionPolicyValidator . convert_control_value ( controls , controller_key_paths , sample_control ) EnvSpaceUtil . deep_sanity_check_space_sample ( action_space , control_dict ) converted_control_list . append ( control_dict ) return converted_control_list @validator ( 'control_values' , pre = True , always = True ) def validate_control_values ( cls , controls_list , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that control_values match the controllers\"\"\" if 'controllers' not in values or 'control_times' not in values : raise ValueError ( f 'Could not run \"validate_control_values\" because previous items failed: { values } ' ) return ScriptedActionPolicyValidator . convert_control_values ( values [ 'act_space' ], values [ 'controllers' ], values [ 'control_times' ], controls_list ) @validator ( 'missing_action_policy' ) def validate_missing_action_policy ( cls , missing_action_policy , values ): \"\"\"validates the missing action policy\"\"\" if missing_action_policy == 'repeat_last_action' : assert values [ 'control_times' ][ 0 ] == 0 , \"missing control_time for t=0\" return missing_action_policy @validator ( 'default_action' , pre = True ) def validate_default_action ( cls , default_action , values ): \"\"\"validates that the default_action is consistent with the missing_action_policy\"\"\" if values [ \"missing_action_policy\" ] == 'default_action' : assert default_action is not None , 'default_action is requried when using the default_action missing_action_policy' action_space = values [ 'act_space' ] action = ScriptedActionPolicyValidator . convert_control_value ( default_action , values [ 'controllers' ], action_space . sample ()) return action assert default_action is None , 'default_action is invalid except when using the default_action missing_action_policy' return default_action","title":"ScriptedActionPolicyValidator"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicyValidator.Config","text":"pydantic configuration options Source code in corl/policies/scripted_action.py class Config : \"\"\"pydantic configuration options\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicyValidator.convert_control_value","text":"converts the controls into a dict Source code in corl/policies/scripted_action.py @staticmethod def convert_control_value ( controls , controller_key_paths , sample_control ): \"\"\"converts the controls into a dict\"\"\" flat_sample_control = flatten_dict . flatten ( sample_control ) assert len ( controls ) == len ( controller_key_paths ), 'mismatch between number of controllers and length of control values' assert len ( controller_key_paths ) == len ( flat_sample_control ), 'mismatch between number of controllers and the action_space' flat_control_dict = {} for i , ctrl_value in enumerate ( controls ): try : controller_key = controller_key_paths [ i ] sample_value = flat_sample_control [ controller_key ] if isinstance ( sample_value , np . ndarray ): flat_control_dict [ controller_key ] = np . add ( sample_value * 0 , ctrl_value , dtype = sample_value . dtype ) else : flat_control_dict [ controller_key ] = type ( sample_value )( ctrl_value ) except Exception as e : raise RuntimeError ( f '@idx: { i } , controller: { controller_key } , control_value: { ctrl_value } ' ) from e control_dict = flatten_dict . unflatten ( flat_control_dict ) return control_dict","title":"convert_control_value()"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicyValidator.convert_control_values","text":"converts the input_control values into dictionary and validates it agaist the action space Source code in corl/policies/scripted_action.py @staticmethod def convert_control_values ( action_space , controller_key_paths , control_times , controls_list ): \"\"\"converts the input_control values into dictionary and validates it agaist the action space\"\"\" assert len ( controls_list ) == len ( control_times ), 'mismatch between number of control_times and control_values' sample_control = action_space . sample () converted_control_list : typing . List [ typing . Dict ] = [] for controls in controls_list : control_dict = ScriptedActionPolicyValidator . convert_control_value ( controls , controller_key_paths , sample_control ) EnvSpaceUtil . deep_sanity_check_space_sample ( action_space , control_dict ) converted_control_list . append ( control_dict ) return converted_control_list","title":"convert_control_values()"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicyValidator.sort_control_times","text":"Ensures that control_times are in order Source code in corl/policies/scripted_action.py @validator ( 'control_times' ) def sort_control_times ( cls , v ): # pylint: disable=no-self-argument, no-self-use \"\"\"Ensures that control_times are in order\"\"\" assert v == sorted ( v ), \"control_times must be in order\" return v","title":"sort_control_times()"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicyValidator.validate_control_values","text":"validate that control_values match the controllers Source code in corl/policies/scripted_action.py @validator ( 'control_values' , pre = True , always = True ) def validate_control_values ( cls , controls_list , values ): # pylint: disable=no-self-argument, no-self-use \"\"\"validate that control_values match the controllers\"\"\" if 'controllers' not in values or 'control_times' not in values : raise ValueError ( f 'Could not run \"validate_control_values\" because previous items failed: { values } ' ) return ScriptedActionPolicyValidator . convert_control_values ( values [ 'act_space' ], values [ 'controllers' ], values [ 'control_times' ], controls_list )","title":"validate_control_values()"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicyValidator.validate_default_action","text":"validates that the default_action is consistent with the missing_action_policy Source code in corl/policies/scripted_action.py @validator ( 'default_action' , pre = True ) def validate_default_action ( cls , default_action , values ): \"\"\"validates that the default_action is consistent with the missing_action_policy\"\"\" if values [ \"missing_action_policy\" ] == 'default_action' : assert default_action is not None , 'default_action is requried when using the default_action missing_action_policy' action_space = values [ 'act_space' ] action = ScriptedActionPolicyValidator . convert_control_value ( default_action , values [ 'controllers' ], action_space . sample ()) return action assert default_action is None , 'default_action is invalid except when using the default_action missing_action_policy' return default_action","title":"validate_default_action()"},{"location":"reference/policies/scripted_action/#corl.policies.scripted_action.ScriptedActionPolicyValidator.validate_missing_action_policy","text":"validates the missing action policy Source code in corl/policies/scripted_action.py @validator ( 'missing_action_policy' ) def validate_missing_action_policy ( cls , missing_action_policy , values ): \"\"\"validates the missing action policy\"\"\" if missing_action_policy == 'repeat_last_action' : assert values [ 'control_times' ][ 0 ] == 0 , \"missing control_time for t=0\" return missing_action_policy","title":"validate_missing_action_policy()"},{"location":"reference/rewards/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Rewards"},{"location":"reference/rewards/base_measurement_operation/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Module with base implimentations for Observations BaseMeasurementOperation ( RewardFuncBase ) \u00a4 Base class for any reward that is to operate on a measurement of some kind Source code in corl/rewards/base_measurement_operation.py class BaseMeasurementOperation ( RewardFuncBase ): # pylint: disable=abstract-method \"\"\"Base class for any reward that is to operate on a measurement of some kind \"\"\" @property def get_validator ( self ) -> typing . Type [ BaseMeasurementOperationValidator ]: return BaseMeasurementOperationValidator def __init__ ( self , ** kwargs ) -> None : self . config : BaseMeasurementOperationValidator super () . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( self . name ) self . extractor : ExtractorSet = self . config . observation . construct_extractors () get_validator : Type [ BaseMeasurementOperationValidator ] property readonly \u00a4 Returns pydantic validator associated with this class BaseMeasurementOperationValidator ( RewardFuncBaseValidator ) pydantic-model \u00a4 observation: Dict of observation extractor arguments described in ObservationExtractorValidator Source code in corl/rewards/base_measurement_operation.py class BaseMeasurementOperationValidator ( RewardFuncBaseValidator ): \"\"\" observation: Dict of observation extractor arguments described in ObservationExtractorValidator \"\"\" observation : ObservationExtractorValidator ObservationExtractorValidator ( BaseModel ) pydantic-model \u00a4 The Fields the extractor must access to the nested obs ex: [ObserveSensor_Sensor_AltitudeDiff, direct_observation_diff] indices: List of indices to extract from the glue Source code in corl/rewards/base_measurement_operation.py class ObservationExtractorValidator ( BaseModel ): \"\"\" fields: The Fields the extractor must access to the nested obs ex: [ObserveSensor_Sensor_AltitudeDiff, direct_observation_diff] indices: List of indices to extract from the glue \"\"\" fields : typing . List [ str ] indices : typing . Union [ int , typing . List [ int ]] = [] def construct_extractors ( self ): \"\"\" Builds extractor methods for extracting value, observation_space, and unit from an observation glue Parameters ---------- platforms : List[str] The platforms the glue is observing, needed to compute the glue's prefix Returns ------- ExtractorSet Named Tuple of value, space, and unit extractors \"\"\" def obs_extractor ( obs , * _ , full_extraction = False ): indices = [] if full_extraction : indices = self . indices return ObservationExtractor ( observation = obs , fields = self . fields , indices = indices ) def obs_space_extractor ( obs , * _ ): return ObservationSpaceExtractor ( observation_space = obs , fields = self . fields ) def unit_extractor ( obs , * _ ): return ObservationSpaceExtractor ( observation_space = obs , fields = self . fields ) return ExtractorSet ( obs_extractor , obs_space_extractor , unit_extractor ) @staticmethod def get_curr_and_next_observation ( extractor , observation , next_observation , allow_array : bool = False ): \"\"\"Helper function to extract the current and next observation \"\"\" curr_metric = extractor ( observation ) next_metric = extractor ( next_observation ) # MTB - It seems that rewardDict must be set to a scalar. # Therefore we must extract the scalar value from observation # Unsure how this will work if an observation isn't a ndarray of size 1, # so just making it fail if that ever happens (i.e. kick the can) if isinstance ( curr_metric , np . ndarray ): if len ( curr_metric ) != len ( next_metric ): raise RuntimeError ( \"Length of arrays do not match, this is a nonop\" ) if not allow_array : if len ( curr_metric ) != 1 : raise RuntimeError ( \"The observation attempting to do potential based shaping is not a scalar, unsure how to procede\" ) curr_metric = curr_metric [ 0 ] next_metric = next_metric [ 0 ] else : raise RuntimeError ( \"The extracted observation is not a type that is known how to handle\" ) return ( curr_metric , next_metric ) construct_extractors ( self ) \u00a4 Builds extractor methods for extracting value, observation_space, and unit from an observation glue Parameters \u00a4 platforms : List[str] The platforms the glue is observing, needed to compute the glue's prefix Returns \u00a4 ExtractorSet Named Tuple of value, space, and unit extractors Source code in corl/rewards/base_measurement_operation.py def construct_extractors ( self ): \"\"\" Builds extractor methods for extracting value, observation_space, and unit from an observation glue Parameters ---------- platforms : List[str] The platforms the glue is observing, needed to compute the glue's prefix Returns ------- ExtractorSet Named Tuple of value, space, and unit extractors \"\"\" def obs_extractor ( obs , * _ , full_extraction = False ): indices = [] if full_extraction : indices = self . indices return ObservationExtractor ( observation = obs , fields = self . fields , indices = indices ) def obs_space_extractor ( obs , * _ ): return ObservationSpaceExtractor ( observation_space = obs , fields = self . fields ) def unit_extractor ( obs , * _ ): return ObservationSpaceExtractor ( observation_space = obs , fields = self . fields ) return ExtractorSet ( obs_extractor , obs_space_extractor , unit_extractor ) get_curr_and_next_observation ( extractor , observation , next_observation , allow_array = False ) staticmethod \u00a4 Helper function to extract the current and next observation Source code in corl/rewards/base_measurement_operation.py @staticmethod def get_curr_and_next_observation ( extractor , observation , next_observation , allow_array : bool = False ): \"\"\"Helper function to extract the current and next observation \"\"\" curr_metric = extractor ( observation ) next_metric = extractor ( next_observation ) # MTB - It seems that rewardDict must be set to a scalar. # Therefore we must extract the scalar value from observation # Unsure how this will work if an observation isn't a ndarray of size 1, # so just making it fail if that ever happens (i.e. kick the can) if isinstance ( curr_metric , np . ndarray ): if len ( curr_metric ) != len ( next_metric ): raise RuntimeError ( \"Length of arrays do not match, this is a nonop\" ) if not allow_array : if len ( curr_metric ) != 1 : raise RuntimeError ( \"The observation attempting to do potential based shaping is not a scalar, unsure how to procede\" ) curr_metric = curr_metric [ 0 ] next_metric = next_metric [ 0 ] else : raise RuntimeError ( \"The extracted observation is not a type that is known how to handle\" ) return ( curr_metric , next_metric )","title":"Base measurement operation"},{"location":"reference/rewards/base_measurement_operation/#corl.rewards.base_measurement_operation.BaseMeasurementOperation","text":"Base class for any reward that is to operate on a measurement of some kind Source code in corl/rewards/base_measurement_operation.py class BaseMeasurementOperation ( RewardFuncBase ): # pylint: disable=abstract-method \"\"\"Base class for any reward that is to operate on a measurement of some kind \"\"\" @property def get_validator ( self ) -> typing . Type [ BaseMeasurementOperationValidator ]: return BaseMeasurementOperationValidator def __init__ ( self , ** kwargs ) -> None : self . config : BaseMeasurementOperationValidator super () . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( self . name ) self . extractor : ExtractorSet = self . config . observation . construct_extractors ()","title":"BaseMeasurementOperation"},{"location":"reference/rewards/base_measurement_operation/#corl.rewards.base_measurement_operation.BaseMeasurementOperation.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/base_measurement_operation/#corl.rewards.base_measurement_operation.BaseMeasurementOperationValidator","text":"observation: Dict of observation extractor arguments described in ObservationExtractorValidator Source code in corl/rewards/base_measurement_operation.py class BaseMeasurementOperationValidator ( RewardFuncBaseValidator ): \"\"\" observation: Dict of observation extractor arguments described in ObservationExtractorValidator \"\"\" observation : ObservationExtractorValidator","title":"BaseMeasurementOperationValidator"},{"location":"reference/rewards/base_measurement_operation/#corl.rewards.base_measurement_operation.ObservationExtractorValidator","text":"The Fields the extractor must access to the nested obs ex: [ObserveSensor_Sensor_AltitudeDiff, direct_observation_diff] indices: List of indices to extract from the glue Source code in corl/rewards/base_measurement_operation.py class ObservationExtractorValidator ( BaseModel ): \"\"\" fields: The Fields the extractor must access to the nested obs ex: [ObserveSensor_Sensor_AltitudeDiff, direct_observation_diff] indices: List of indices to extract from the glue \"\"\" fields : typing . List [ str ] indices : typing . Union [ int , typing . List [ int ]] = [] def construct_extractors ( self ): \"\"\" Builds extractor methods for extracting value, observation_space, and unit from an observation glue Parameters ---------- platforms : List[str] The platforms the glue is observing, needed to compute the glue's prefix Returns ------- ExtractorSet Named Tuple of value, space, and unit extractors \"\"\" def obs_extractor ( obs , * _ , full_extraction = False ): indices = [] if full_extraction : indices = self . indices return ObservationExtractor ( observation = obs , fields = self . fields , indices = indices ) def obs_space_extractor ( obs , * _ ): return ObservationSpaceExtractor ( observation_space = obs , fields = self . fields ) def unit_extractor ( obs , * _ ): return ObservationSpaceExtractor ( observation_space = obs , fields = self . fields ) return ExtractorSet ( obs_extractor , obs_space_extractor , unit_extractor ) @staticmethod def get_curr_and_next_observation ( extractor , observation , next_observation , allow_array : bool = False ): \"\"\"Helper function to extract the current and next observation \"\"\" curr_metric = extractor ( observation ) next_metric = extractor ( next_observation ) # MTB - It seems that rewardDict must be set to a scalar. # Therefore we must extract the scalar value from observation # Unsure how this will work if an observation isn't a ndarray of size 1, # so just making it fail if that ever happens (i.e. kick the can) if isinstance ( curr_metric , np . ndarray ): if len ( curr_metric ) != len ( next_metric ): raise RuntimeError ( \"Length of arrays do not match, this is a nonop\" ) if not allow_array : if len ( curr_metric ) != 1 : raise RuntimeError ( \"The observation attempting to do potential based shaping is not a scalar, unsure how to procede\" ) curr_metric = curr_metric [ 0 ] next_metric = next_metric [ 0 ] else : raise RuntimeError ( \"The extracted observation is not a type that is known how to handle\" ) return ( curr_metric , next_metric )","title":"ObservationExtractorValidator"},{"location":"reference/rewards/base_measurement_operation/#corl.rewards.base_measurement_operation.ObservationExtractorValidator.construct_extractors","text":"Builds extractor methods for extracting value, observation_space, and unit from an observation glue","title":"construct_extractors()"},{"location":"reference/rewards/base_measurement_operation/#corl.rewards.base_measurement_operation.ObservationExtractorValidator.construct_extractors--parameters","text":"platforms : List[str] The platforms the glue is observing, needed to compute the glue's prefix","title":"Parameters"},{"location":"reference/rewards/base_measurement_operation/#corl.rewards.base_measurement_operation.ObservationExtractorValidator.construct_extractors--returns","text":"ExtractorSet Named Tuple of value, space, and unit extractors Source code in corl/rewards/base_measurement_operation.py def construct_extractors ( self ): \"\"\" Builds extractor methods for extracting value, observation_space, and unit from an observation glue Parameters ---------- platforms : List[str] The platforms the glue is observing, needed to compute the glue's prefix Returns ------- ExtractorSet Named Tuple of value, space, and unit extractors \"\"\" def obs_extractor ( obs , * _ , full_extraction = False ): indices = [] if full_extraction : indices = self . indices return ObservationExtractor ( observation = obs , fields = self . fields , indices = indices ) def obs_space_extractor ( obs , * _ ): return ObservationSpaceExtractor ( observation_space = obs , fields = self . fields ) def unit_extractor ( obs , * _ ): return ObservationSpaceExtractor ( observation_space = obs , fields = self . fields ) return ExtractorSet ( obs_extractor , obs_space_extractor , unit_extractor )","title":"Returns"},{"location":"reference/rewards/base_measurement_operation/#corl.rewards.base_measurement_operation.ObservationExtractorValidator.get_curr_and_next_observation","text":"Helper function to extract the current and next observation Source code in corl/rewards/base_measurement_operation.py @staticmethod def get_curr_and_next_observation ( extractor , observation , next_observation , allow_array : bool = False ): \"\"\"Helper function to extract the current and next observation \"\"\" curr_metric = extractor ( observation ) next_metric = extractor ( next_observation ) # MTB - It seems that rewardDict must be set to a scalar. # Therefore we must extract the scalar value from observation # Unsure how this will work if an observation isn't a ndarray of size 1, # so just making it fail if that ever happens (i.e. kick the can) if isinstance ( curr_metric , np . ndarray ): if len ( curr_metric ) != len ( next_metric ): raise RuntimeError ( \"Length of arrays do not match, this is a nonop\" ) if not allow_array : if len ( curr_metric ) != 1 : raise RuntimeError ( \"The observation attempting to do potential based shaping is not a scalar, unsure how to procede\" ) curr_metric = curr_metric [ 0 ] next_metric = next_metric [ 0 ] else : raise RuntimeError ( \"The extracted observation is not a type that is known how to handle\" ) return ( curr_metric , next_metric )","title":"get_curr_and_next_observation()"},{"location":"reference/rewards/episode_done/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Reward that uses episode state to accumulate reward EpisodeDoneNameReward ( EpisodeDoneReward ) \u00a4 Reward that responds once to individual dones. Source code in corl/rewards/episode_done.py class EpisodeDoneNameReward ( EpisodeDoneReward ): \"\"\"Reward that responds once to individual dones.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeDoneNameRewardValidator super () . __init__ ( ** kwargs ) self . _done_name_func = {} for name , args in self . config . rewarded_dones . items (): if isinstance ( args , NegativeExponentialScaling ): self . _done_name_func [ name ] = partial ( self . exp_scaling , scale = args . scale , eps = args . eps ) else : self . _done_name_func [ name ] = partial ( self . constant_scaling , scale = args ) @property def get_validator ( self ) -> typing . Type [ EpisodeDoneNameRewardValidator ]: return EpisodeDoneNameRewardValidator def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: if self . config . missing_method == _MissingMethod . zero : return self . _done_name_func . get ( done_name , self . zero_scaling ) return self . _done_name_func [ done_name ] get_validator : Type [ corl . rewards . episode_done . EpisodeDoneNameRewardValidator ] property readonly \u00a4 Returns pydantic validator associated with this class get_scaling_method ( self , done_name , done_code ) \u00a4 Get the scaling method for a particular done name and code. Source code in corl/rewards/episode_done.py def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: if self . config . missing_method == _MissingMethod . zero : return self . _done_name_func . get ( done_name , self . zero_scaling ) return self . _done_name_func [ done_name ] EpisodeDoneNameRewardValidator ( EpisodeDoneRewardValidator ) pydantic-model \u00a4 Validation for EpisodeDoneNameReward. Source code in corl/rewards/episode_done.py class EpisodeDoneNameRewardValidator ( EpisodeDoneRewardValidator ): \"\"\"Validation for EpisodeDoneNameReward.\"\"\" rewarded_dones : typing . Dict [ str , typing . Union [ NegativeExponentialScaling , float ]] = {} missing_method : _MissingMethod = _MissingMethod . zero @validator ( 'rewarded_dones' , always = True ) def not_empty ( cls , v ): \"\"\"Ensure that some done condition is specified.\"\"\" if len ( v ) == 0 : raise ValueError ( 'Rewarded dones cannot be empty' ) return v not_empty ( v ) classmethod \u00a4 Ensure that some done condition is specified. Source code in corl/rewards/episode_done.py @validator ( 'rewarded_dones' , always = True ) def not_empty ( cls , v ): \"\"\"Ensure that some done condition is specified.\"\"\" if len ( v ) == 0 : raise ValueError ( 'Rewarded dones cannot be empty' ) return v EpisodeDoneReward ( RewardFuncBase ) \u00a4 Base class for rewards that give rewards based on done information. Source code in corl/rewards/episode_done.py class EpisodeDoneReward ( RewardFuncBase ): \"\"\"Base class for rewards that give rewards based on done information.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeDoneRewardValidator super () . __init__ ( ** kwargs ) self . _counter = 0 self . _already_recorded : typing . Set [ str ] = set () self . _status_codes : typing . Dict [ DoneStatusCodes , typing . List [ str ]] = { x : [] for x in DoneStatusCodes } @property def get_validator ( self ) -> typing . Type [ EpisodeDoneRewardValidator ]: return EpisodeDoneRewardValidator @staticmethod def exp_scaling ( x , * , scale : float , eps : float ) -> float : \"\"\"Scale as a negative exponential.\"\"\" return scale * np . exp ( - np . abs ( x / eps )) @staticmethod def constant_scaling ( _ , * , scale : float ) -> float : \"\"\"Scale by a constant.\"\"\" return scale @staticmethod def zero_scaling ( _ ) -> float : \"\"\"Scale as zero.\"\"\" return 0 @abc . abstractmethod def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: \"\"\"Get the scaling method for a particular done name and code.\"\"\" raise NotImplementedError () def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space , observation_units ) -> RewardDict : reward = RewardDict () reward [ self . config . agent_name ] = 0 if \"_\" in self . config . agent_name : platform_name = self . config . agent_name . split ( \"_\" , 1 )[ 0 ] else : platform_name = self . config . agent_name done_state = next_state . episode_state . get ( platform_name , {}) for done_name , done_code in done_state . items (): if done_name in self . _already_recorded : continue self . _already_recorded . add ( done_name ) self . _status_codes [ done_code ] . append ( done_name ) if not self . config . skip_win_lose_sanity_check : if len ( self . _status_codes [ DoneStatusCodes . WIN ]) > 0 and len ( self . _status_codes [ DoneStatusCodes . LOSE ]) > 0 : raise RuntimeError ( \"EpisodeDoneReward found both WIN and LOSS set during this episode, \" \"if this is intended set skip_sanity_check=True\" ) # this will loop starting from win and go down consolidate_break = False for done_status in DoneStatusCodes : for done_name in self . _status_codes [ done_status ]: reward [ self . config . agent_name ] += self . get_scaling_method ( done_name , done_status )( self . _counter ) if self . config . consolidate : consolidate_break = True break if consolidate_break : break self . _counter += 1 / next_state . sim_update_rate return reward get_validator : Type [ corl . rewards . episode_done . EpisodeDoneRewardValidator ] property readonly \u00a4 Returns pydantic validator associated with this class constant_scaling ( _ , * , scale ) staticmethod \u00a4 Scale by a constant. Source code in corl/rewards/episode_done.py @staticmethod def constant_scaling ( _ , * , scale : float ) -> float : \"\"\"Scale by a constant.\"\"\" return scale exp_scaling ( x , * , scale , eps ) staticmethod \u00a4 Scale as a negative exponential. Source code in corl/rewards/episode_done.py @staticmethod def exp_scaling ( x , * , scale : float , eps : float ) -> float : \"\"\"Scale as a negative exponential.\"\"\" return scale * np . exp ( - np . abs ( x / eps )) get_scaling_method ( self , done_name , done_code ) \u00a4 Get the scaling method for a particular done name and code. Source code in corl/rewards/episode_done.py @abc . abstractmethod def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: \"\"\"Get the scaling method for a particular done name and code.\"\"\" raise NotImplementedError () zero_scaling ( _ ) staticmethod \u00a4 Scale as zero. Source code in corl/rewards/episode_done.py @staticmethod def zero_scaling ( _ ) -> float : \"\"\"Scale as zero.\"\"\" return 0 EpisodeDoneRewardValidator ( RewardFuncBaseValidator ) pydantic-model \u00a4 if this done condition should attempt to reduce down to WLD in the case of multiple done conditions being set, with Win taking higher precedence notes if the done condition should skip the sanity check checking for both WIN and LOSS in an episodes done results, in case it was intentional Source code in corl/rewards/episode_done.py class EpisodeDoneRewardValidator ( RewardFuncBaseValidator ): \"\"\" consolidate: if this done condition should attempt to reduce down to WLD in the case of multiple done conditions being set, with Win taking higher precedence skip_win_lose_sanity_check: notes if the done condition should skip the sanity check checking for both WIN and LOSS in an episodes done results, in case it was intentional \"\"\" consolidate : bool = False skip_win_lose_sanity_check : bool = True EpisodeDoneStateReward ( EpisodeDoneReward ) \u00a4 Reward that responds to done condition state, once per done condition triggered. Source code in corl/rewards/episode_done.py class EpisodeDoneStateReward ( EpisodeDoneReward ): \"\"\"Reward that responds to done condition state, once per done condition triggered.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeDoneStateRewardValidator super () . __init__ ( ** kwargs ) self . _status_code_func = {} for code in DoneStatusCodes : code_name = code . name . lower () if not hasattr ( self . config , code_name ): raise RuntimeError ( f 'Unknown done status code: { code_name } ' ) code_data = getattr ( self . config , code_name ) if isinstance ( code_data , NegativeExponentialScaling ): self . _status_code_func [ code_name ] = partial ( self . exp_scaling , scale = code_data . scale , eps = code_data . eps ) else : self . _status_code_func [ code_name ] = partial ( self . constant_scaling , scale = code_data ) @property def get_validator ( self ) -> typing . Type [ EpisodeDoneStateRewardValidator ]: return EpisodeDoneStateRewardValidator def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: return self . _status_code_func [ done_code . name . lower ()] get_validator : Type [ corl . rewards . episode_done . EpisodeDoneStateRewardValidator ] property readonly \u00a4 Returns pydantic validator associated with this class get_scaling_method ( self , done_name , done_code ) \u00a4 Get the scaling method for a particular done name and code. Source code in corl/rewards/episode_done.py def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: return self . _status_code_func [ done_code . name . lower ()] EpisodeDoneStateRewardValidator ( EpisodeDoneRewardValidator ) pydantic-model \u00a4 Validation for EpisodeDoneStateReward. Source code in corl/rewards/episode_done.py class EpisodeDoneStateRewardValidator ( EpisodeDoneRewardValidator ): \"\"\"Validation for EpisodeDoneStateReward.\"\"\" win : typing . Union [ NegativeExponentialScaling , float ] = 0 partial_win : typing . Union [ NegativeExponentialScaling , float ] = 0 draw : typing . Union [ NegativeExponentialScaling , float ] = 0 partial_loss : typing . Union [ NegativeExponentialScaling , float ] = 0 lose : typing . Union [ NegativeExponentialScaling , float ] = 0 NegativeExponentialScaling ( BaseModel ) pydantic-model \u00a4 Validation entry for negative exponential scaling. Source code in corl/rewards/episode_done.py class NegativeExponentialScaling ( BaseModel ): \"\"\"Validation entry for negative exponential scaling.\"\"\" scale : NonNegativeFloat eps : PositiveFloat","title":"Episode done"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneNameReward","text":"Reward that responds once to individual dones. Source code in corl/rewards/episode_done.py class EpisodeDoneNameReward ( EpisodeDoneReward ): \"\"\"Reward that responds once to individual dones.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeDoneNameRewardValidator super () . __init__ ( ** kwargs ) self . _done_name_func = {} for name , args in self . config . rewarded_dones . items (): if isinstance ( args , NegativeExponentialScaling ): self . _done_name_func [ name ] = partial ( self . exp_scaling , scale = args . scale , eps = args . eps ) else : self . _done_name_func [ name ] = partial ( self . constant_scaling , scale = args ) @property def get_validator ( self ) -> typing . Type [ EpisodeDoneNameRewardValidator ]: return EpisodeDoneNameRewardValidator def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: if self . config . missing_method == _MissingMethod . zero : return self . _done_name_func . get ( done_name , self . zero_scaling ) return self . _done_name_func [ done_name ]","title":"EpisodeDoneNameReward"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneNameReward.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneNameReward.get_scaling_method","text":"Get the scaling method for a particular done name and code. Source code in corl/rewards/episode_done.py def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: if self . config . missing_method == _MissingMethod . zero : return self . _done_name_func . get ( done_name , self . zero_scaling ) return self . _done_name_func [ done_name ]","title":"get_scaling_method()"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneNameRewardValidator","text":"Validation for EpisodeDoneNameReward. Source code in corl/rewards/episode_done.py class EpisodeDoneNameRewardValidator ( EpisodeDoneRewardValidator ): \"\"\"Validation for EpisodeDoneNameReward.\"\"\" rewarded_dones : typing . Dict [ str , typing . Union [ NegativeExponentialScaling , float ]] = {} missing_method : _MissingMethod = _MissingMethod . zero @validator ( 'rewarded_dones' , always = True ) def not_empty ( cls , v ): \"\"\"Ensure that some done condition is specified.\"\"\" if len ( v ) == 0 : raise ValueError ( 'Rewarded dones cannot be empty' ) return v","title":"EpisodeDoneNameRewardValidator"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneNameRewardValidator.not_empty","text":"Ensure that some done condition is specified. Source code in corl/rewards/episode_done.py @validator ( 'rewarded_dones' , always = True ) def not_empty ( cls , v ): \"\"\"Ensure that some done condition is specified.\"\"\" if len ( v ) == 0 : raise ValueError ( 'Rewarded dones cannot be empty' ) return v","title":"not_empty()"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneReward","text":"Base class for rewards that give rewards based on done information. Source code in corl/rewards/episode_done.py class EpisodeDoneReward ( RewardFuncBase ): \"\"\"Base class for rewards that give rewards based on done information.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeDoneRewardValidator super () . __init__ ( ** kwargs ) self . _counter = 0 self . _already_recorded : typing . Set [ str ] = set () self . _status_codes : typing . Dict [ DoneStatusCodes , typing . List [ str ]] = { x : [] for x in DoneStatusCodes } @property def get_validator ( self ) -> typing . Type [ EpisodeDoneRewardValidator ]: return EpisodeDoneRewardValidator @staticmethod def exp_scaling ( x , * , scale : float , eps : float ) -> float : \"\"\"Scale as a negative exponential.\"\"\" return scale * np . exp ( - np . abs ( x / eps )) @staticmethod def constant_scaling ( _ , * , scale : float ) -> float : \"\"\"Scale by a constant.\"\"\" return scale @staticmethod def zero_scaling ( _ ) -> float : \"\"\"Scale as zero.\"\"\" return 0 @abc . abstractmethod def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: \"\"\"Get the scaling method for a particular done name and code.\"\"\" raise NotImplementedError () def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space , observation_units ) -> RewardDict : reward = RewardDict () reward [ self . config . agent_name ] = 0 if \"_\" in self . config . agent_name : platform_name = self . config . agent_name . split ( \"_\" , 1 )[ 0 ] else : platform_name = self . config . agent_name done_state = next_state . episode_state . get ( platform_name , {}) for done_name , done_code in done_state . items (): if done_name in self . _already_recorded : continue self . _already_recorded . add ( done_name ) self . _status_codes [ done_code ] . append ( done_name ) if not self . config . skip_win_lose_sanity_check : if len ( self . _status_codes [ DoneStatusCodes . WIN ]) > 0 and len ( self . _status_codes [ DoneStatusCodes . LOSE ]) > 0 : raise RuntimeError ( \"EpisodeDoneReward found both WIN and LOSS set during this episode, \" \"if this is intended set skip_sanity_check=True\" ) # this will loop starting from win and go down consolidate_break = False for done_status in DoneStatusCodes : for done_name in self . _status_codes [ done_status ]: reward [ self . config . agent_name ] += self . get_scaling_method ( done_name , done_status )( self . _counter ) if self . config . consolidate : consolidate_break = True break if consolidate_break : break self . _counter += 1 / next_state . sim_update_rate return reward","title":"EpisodeDoneReward"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneReward.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneReward.constant_scaling","text":"Scale by a constant. Source code in corl/rewards/episode_done.py @staticmethod def constant_scaling ( _ , * , scale : float ) -> float : \"\"\"Scale by a constant.\"\"\" return scale","title":"constant_scaling()"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneReward.exp_scaling","text":"Scale as a negative exponential. Source code in corl/rewards/episode_done.py @staticmethod def exp_scaling ( x , * , scale : float , eps : float ) -> float : \"\"\"Scale as a negative exponential.\"\"\" return scale * np . exp ( - np . abs ( x / eps ))","title":"exp_scaling()"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneReward.get_scaling_method","text":"Get the scaling method for a particular done name and code. Source code in corl/rewards/episode_done.py @abc . abstractmethod def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: \"\"\"Get the scaling method for a particular done name and code.\"\"\" raise NotImplementedError ()","title":"get_scaling_method()"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneReward.zero_scaling","text":"Scale as zero. Source code in corl/rewards/episode_done.py @staticmethod def zero_scaling ( _ ) -> float : \"\"\"Scale as zero.\"\"\" return 0","title":"zero_scaling()"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneRewardValidator","text":"if this done condition should attempt to reduce down to WLD in the case of multiple done conditions being set, with Win taking higher precedence notes if the done condition should skip the sanity check checking for both WIN and LOSS in an episodes done results, in case it was intentional Source code in corl/rewards/episode_done.py class EpisodeDoneRewardValidator ( RewardFuncBaseValidator ): \"\"\" consolidate: if this done condition should attempt to reduce down to WLD in the case of multiple done conditions being set, with Win taking higher precedence skip_win_lose_sanity_check: notes if the done condition should skip the sanity check checking for both WIN and LOSS in an episodes done results, in case it was intentional \"\"\" consolidate : bool = False skip_win_lose_sanity_check : bool = True","title":"EpisodeDoneRewardValidator"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneStateReward","text":"Reward that responds to done condition state, once per done condition triggered. Source code in corl/rewards/episode_done.py class EpisodeDoneStateReward ( EpisodeDoneReward ): \"\"\"Reward that responds to done condition state, once per done condition triggered.\"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : EpisodeDoneStateRewardValidator super () . __init__ ( ** kwargs ) self . _status_code_func = {} for code in DoneStatusCodes : code_name = code . name . lower () if not hasattr ( self . config , code_name ): raise RuntimeError ( f 'Unknown done status code: { code_name } ' ) code_data = getattr ( self . config , code_name ) if isinstance ( code_data , NegativeExponentialScaling ): self . _status_code_func [ code_name ] = partial ( self . exp_scaling , scale = code_data . scale , eps = code_data . eps ) else : self . _status_code_func [ code_name ] = partial ( self . constant_scaling , scale = code_data ) @property def get_validator ( self ) -> typing . Type [ EpisodeDoneStateRewardValidator ]: return EpisodeDoneStateRewardValidator def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: return self . _status_code_func [ done_code . name . lower ()]","title":"EpisodeDoneStateReward"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneStateReward.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneStateReward.get_scaling_method","text":"Get the scaling method for a particular done name and code. Source code in corl/rewards/episode_done.py def get_scaling_method ( self , done_name , done_code ) -> typing . Callable [[ int ], float ]: return self . _status_code_func [ done_code . name . lower ()]","title":"get_scaling_method()"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.EpisodeDoneStateRewardValidator","text":"Validation for EpisodeDoneStateReward. Source code in corl/rewards/episode_done.py class EpisodeDoneStateRewardValidator ( EpisodeDoneRewardValidator ): \"\"\"Validation for EpisodeDoneStateReward.\"\"\" win : typing . Union [ NegativeExponentialScaling , float ] = 0 partial_win : typing . Union [ NegativeExponentialScaling , float ] = 0 draw : typing . Union [ NegativeExponentialScaling , float ] = 0 partial_loss : typing . Union [ NegativeExponentialScaling , float ] = 0 lose : typing . Union [ NegativeExponentialScaling , float ] = 0","title":"EpisodeDoneStateRewardValidator"},{"location":"reference/rewards/episode_done/#corl.rewards.episode_done.NegativeExponentialScaling","text":"Validation entry for negative exponential scaling. Source code in corl/rewards/episode_done.py class NegativeExponentialScaling ( BaseModel ): \"\"\"Validation entry for negative exponential scaling.\"\"\" scale : NonNegativeFloat eps : PositiveFloat","title":"NegativeExponentialScaling"},{"location":"reference/rewards/exponential_decay_from_target_value/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. ExponentialDecayFromTargetValue ( BaseMeasurementOperation ) \u00a4 Exponential Decay from Target Value wraps some sort of observation and takes in a target value, the reward based on the difference between the target value and the observation. the reward exponentially decays in value the further you are away from the target value. causing a spike at the target value Source code in corl/rewards/exponential_decay_from_target_value.py class ExponentialDecayFromTargetValue ( BaseMeasurementOperation ): \"\"\" Exponential Decay from Target Value wraps some sort of observation and takes in a target value, the reward based on the difference between the target value and the observation. the reward exponentially decays in value the further you are away from the target value. causing a spike at the target value \"\"\" @property def get_validator ( self ) -> typing . Type [ ExponentialDecayFromTargetValueValidator ]: return ExponentialDecayFromTargetValueValidator def __init__ ( self , ** kwargs ) -> None : self . config : ExponentialDecayFromTargetValueValidator super () . __init__ ( ** kwargs ) self . _last_value = None self . _logger = logging . getLogger ( self . name ) def __call__ ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ) -> RewardDict : reward = RewardDict () reward [ self . config . agent_name ] = 0 obs = self . extractor . value ( next_observation [ self . config . agent_name ], get_platform_by_name ( next_state , self . config . agent_name ))[ self . config . index ] def get_wrap_diff ( A , B ): \"\"\"Returns the min diff angle RAD A Deg A RAD A Deg B abs_diff_mod_360 M Diff 1 Diff A-B M 2 Diff 1.047197551 60 2.094395102 120 60 60 -60 -60 2.094395102 120 1.047197551 60 60 60 60 60 -2.094395102 -120 -1.047197551 -60 60 60 -60 -60 6.108652382 350 0.1745329252 10 340 20 340 -20 -2.967059728 -170 2.967059728 170 340 20 -340 20 Arguments: A {float} -- Angle 1 - rad or deg B {float} -- Angle 2 - rad or deg Returns: float -- the min diff angle \"\"\" # Convert to degrees if needed. temp_a = math . degrees ( A ) if self . config . is_rad else A temp_b = math . degrees ( B ) if self . config . is_rad else B if self . config . method : # Compute the diff as abs min angle (always positive) abs_diff_mod_360 = abs ( temp_a - temp_b ) % 360 result = 360 - abs_diff_mod_360 if abs_diff_mod_360 > 180 else abs_diff_mod_360 else : # Compute the diff as min angle maintain sign for direction diff = temp_a - temp_b result = ( diff + 180 ) % 360 - 180 # Return the diff in deg if radians if needed. return math . radians ( result ) if self . config . is_rad else result if self . config . is_wrap : # Note: this always returns the min angle diff and is positive diff = get_wrap_diff ( obs , self . config . target_value ) else : diff = obs - self . config . target_value abs_diff = abs ( diff ) if self . _last_value is None : self . _last_value = abs_diff func_applied = 0 if not self . config . closer or (( self . _last_value >= abs_diff ) or abs_diff < self . config . closer_tolerance ): func_applied = np . exp ( - np . abs ( diff / self . config . eps )) reward [ self . config . agent_name ] = self . config . reward_scale * func_applied self . _last_value = abs_diff return reward get_validator : Type [ corl . rewards . exponential_decay_from_target_value . ExponentialDecayFromTargetValueValidator ] property readonly \u00a4 Returns pydantic validator associated with this class ExponentialDecayFromTargetValueValidator ( BaseMeasurementOperationValidator ) pydantic-model \u00a4 reward scale: scale of this reward, this would be the maximum reward value for a given timestep the length of the reward curve for the exponential decay, would recommend playing with this value in a plotting software to determine the value you need target_value: the value with which to take the difference from the wrapped observation value index: the index with which to pull data out of the observation extractor, useful if len(observation) > 1 is_wrap: if the obs difference needs to wrap around 0/360 is_rad: if the obs difference is in terms of radians to only reward with this reward if the difference to the target value is less than the last timestep the difference tolerance at which point the agent is close enough to the target value that \"closer\" is not a concern Source code in corl/rewards/exponential_decay_from_target_value.py class ExponentialDecayFromTargetValueValidator ( BaseMeasurementOperationValidator ): \"\"\" reward scale: scale of this reward, this would be the maximum reward value for a given timestep eps: the length of the reward curve for the exponential decay, would recommend playing with this value in a plotting software to determine the value you need target_value: the value with which to take the difference from the wrapped observation value index: the index with which to pull data out of the observation extractor, useful if len(observation) > 1 is_wrap: if the obs difference needs to wrap around 0/360 is_rad: if the obs difference is in terms of radians closer: to only reward with this reward if the difference to the target value is less than the last timestep closer_tolerance: the difference tolerance at which point the agent is close enough to the target value that \"closer\" is not a concern \"\"\" reward_scale : float eps : float target_value : typing . Optional [ float ] = 0 index : typing . Optional [ int ] = 0 is_wrap : typing . Optional [ bool ] = False is_rad : typing . Optional [ bool ] = False method : typing . Optional [ bool ] = False closer : typing . Optional [ bool ] = False closer_tolerance : typing . Optional [ float ] = 0.0","title":"Exponential decay from target value"},{"location":"reference/rewards/exponential_decay_from_target_value/#corl.rewards.exponential_decay_from_target_value.ExponentialDecayFromTargetValue","text":"Exponential Decay from Target Value wraps some sort of observation and takes in a target value, the reward based on the difference between the target value and the observation. the reward exponentially decays in value the further you are away from the target value. causing a spike at the target value Source code in corl/rewards/exponential_decay_from_target_value.py class ExponentialDecayFromTargetValue ( BaseMeasurementOperation ): \"\"\" Exponential Decay from Target Value wraps some sort of observation and takes in a target value, the reward based on the difference between the target value and the observation. the reward exponentially decays in value the further you are away from the target value. causing a spike at the target value \"\"\" @property def get_validator ( self ) -> typing . Type [ ExponentialDecayFromTargetValueValidator ]: return ExponentialDecayFromTargetValueValidator def __init__ ( self , ** kwargs ) -> None : self . config : ExponentialDecayFromTargetValueValidator super () . __init__ ( ** kwargs ) self . _last_value = None self . _logger = logging . getLogger ( self . name ) def __call__ ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ) -> RewardDict : reward = RewardDict () reward [ self . config . agent_name ] = 0 obs = self . extractor . value ( next_observation [ self . config . agent_name ], get_platform_by_name ( next_state , self . config . agent_name ))[ self . config . index ] def get_wrap_diff ( A , B ): \"\"\"Returns the min diff angle RAD A Deg A RAD A Deg B abs_diff_mod_360 M Diff 1 Diff A-B M 2 Diff 1.047197551 60 2.094395102 120 60 60 -60 -60 2.094395102 120 1.047197551 60 60 60 60 60 -2.094395102 -120 -1.047197551 -60 60 60 -60 -60 6.108652382 350 0.1745329252 10 340 20 340 -20 -2.967059728 -170 2.967059728 170 340 20 -340 20 Arguments: A {float} -- Angle 1 - rad or deg B {float} -- Angle 2 - rad or deg Returns: float -- the min diff angle \"\"\" # Convert to degrees if needed. temp_a = math . degrees ( A ) if self . config . is_rad else A temp_b = math . degrees ( B ) if self . config . is_rad else B if self . config . method : # Compute the diff as abs min angle (always positive) abs_diff_mod_360 = abs ( temp_a - temp_b ) % 360 result = 360 - abs_diff_mod_360 if abs_diff_mod_360 > 180 else abs_diff_mod_360 else : # Compute the diff as min angle maintain sign for direction diff = temp_a - temp_b result = ( diff + 180 ) % 360 - 180 # Return the diff in deg if radians if needed. return math . radians ( result ) if self . config . is_rad else result if self . config . is_wrap : # Note: this always returns the min angle diff and is positive diff = get_wrap_diff ( obs , self . config . target_value ) else : diff = obs - self . config . target_value abs_diff = abs ( diff ) if self . _last_value is None : self . _last_value = abs_diff func_applied = 0 if not self . config . closer or (( self . _last_value >= abs_diff ) or abs_diff < self . config . closer_tolerance ): func_applied = np . exp ( - np . abs ( diff / self . config . eps )) reward [ self . config . agent_name ] = self . config . reward_scale * func_applied self . _last_value = abs_diff return reward","title":"ExponentialDecayFromTargetValue"},{"location":"reference/rewards/exponential_decay_from_target_value/#corl.rewards.exponential_decay_from_target_value.ExponentialDecayFromTargetValue.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/exponential_decay_from_target_value/#corl.rewards.exponential_decay_from_target_value.ExponentialDecayFromTargetValueValidator","text":"reward scale: scale of this reward, this would be the maximum reward value for a given timestep the length of the reward curve for the exponential decay, would recommend playing with this value in a plotting software to determine the value you need target_value: the value with which to take the difference from the wrapped observation value index: the index with which to pull data out of the observation extractor, useful if len(observation) > 1 is_wrap: if the obs difference needs to wrap around 0/360 is_rad: if the obs difference is in terms of radians to only reward with this reward if the difference to the target value is less than the last timestep the difference tolerance at which point the agent is close enough to the target value that \"closer\" is not a concern Source code in corl/rewards/exponential_decay_from_target_value.py class ExponentialDecayFromTargetValueValidator ( BaseMeasurementOperationValidator ): \"\"\" reward scale: scale of this reward, this would be the maximum reward value for a given timestep eps: the length of the reward curve for the exponential decay, would recommend playing with this value in a plotting software to determine the value you need target_value: the value with which to take the difference from the wrapped observation value index: the index with which to pull data out of the observation extractor, useful if len(observation) > 1 is_wrap: if the obs difference needs to wrap around 0/360 is_rad: if the obs difference is in terms of radians closer: to only reward with this reward if the difference to the target value is less than the last timestep closer_tolerance: the difference tolerance at which point the agent is close enough to the target value that \"closer\" is not a concern \"\"\" reward_scale : float eps : float target_value : typing . Optional [ float ] = 0 index : typing . Optional [ int ] = 0 is_wrap : typing . Optional [ bool ] = False is_rad : typing . Optional [ bool ] = False method : typing . Optional [ bool ] = False closer : typing . Optional [ bool ] = False closer_tolerance : typing . Optional [ float ] = 0.0","title":"ExponentialDecayFromTargetValueValidator"},{"location":"reference/rewards/multi_measurement_operation/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Module with implementation for multiple Observations MultiMeasurementOperation ( RewardFuncBase ) \u00a4 Base class for any reward that is to operate on multiple measurements of some kind Source code in corl/rewards/multi_measurement_operation.py class MultiMeasurementOperation ( RewardFuncBase ): # pylint: disable=abstract-method \"\"\"Base class for any reward that is to operate on multiple measurements of some kind \"\"\" @property def get_validator ( self ) -> typing . Type [ MultiMeasurementOperationValidator ]: return MultiMeasurementOperationValidator def __init__ ( self , ** kwargs ) -> None : self . config : MultiMeasurementOperationValidator super () . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( self . name ) # construct extractors self . extractors : typing . Dict [ str , ExtractorSet ] = {} for key , observation in self . config . observations . items (): self . extractors [ key ] = observation . construct_extractors () get_validator : Type [ corl . rewards . multi_measurement_operation . MultiMeasurementOperationValidator ] property readonly \u00a4 Returns pydantic validator associated with this class MultiMeasurementOperationValidator ( RewardFuncBaseValidator ) pydantic-model \u00a4 observations: Dict of dicts of observation extractor arguments described in ObservationExtractorValidator Source code in corl/rewards/multi_measurement_operation.py class MultiMeasurementOperationValidator ( RewardFuncBaseValidator ): \"\"\" observations: Dict of dicts of observation extractor arguments described in ObservationExtractorValidator \"\"\" observations : typing . Dict [ str , ObservationExtractorValidator ]","title":"Multi measurement operation"},{"location":"reference/rewards/multi_measurement_operation/#corl.rewards.multi_measurement_operation.MultiMeasurementOperation","text":"Base class for any reward that is to operate on multiple measurements of some kind Source code in corl/rewards/multi_measurement_operation.py class MultiMeasurementOperation ( RewardFuncBase ): # pylint: disable=abstract-method \"\"\"Base class for any reward that is to operate on multiple measurements of some kind \"\"\" @property def get_validator ( self ) -> typing . Type [ MultiMeasurementOperationValidator ]: return MultiMeasurementOperationValidator def __init__ ( self , ** kwargs ) -> None : self . config : MultiMeasurementOperationValidator super () . __init__ ( ** kwargs ) self . _logger = logging . getLogger ( self . name ) # construct extractors self . extractors : typing . Dict [ str , ExtractorSet ] = {} for key , observation in self . config . observations . items (): self . extractors [ key ] = observation . construct_extractors ()","title":"MultiMeasurementOperation"},{"location":"reference/rewards/multi_measurement_operation/#corl.rewards.multi_measurement_operation.MultiMeasurementOperation.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/multi_measurement_operation/#corl.rewards.multi_measurement_operation.MultiMeasurementOperationValidator","text":"observations: Dict of dicts of observation extractor arguments described in ObservationExtractorValidator Source code in corl/rewards/multi_measurement_operation.py class MultiMeasurementOperationValidator ( RewardFuncBaseValidator ): \"\"\" observations: Dict of dicts of observation extractor arguments described in ObservationExtractorValidator \"\"\" observations : typing . Dict [ str , ObservationExtractorValidator ]","title":"MultiMeasurementOperationValidator"},{"location":"reference/rewards/openai_gym_reward/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Reward for OpenAIGymSimulator OpenAIGymReward ( RewardFuncBase ) \u00a4 Reward for OpenAiGymSimulator that rewards the reward coming from the simulator provided state and reports it Source code in corl/rewards/openai_gym_reward.py class OpenAIGymReward ( RewardFuncBase ): \"\"\" Reward for OpenAiGymSimulator that rewards the reward coming from the simulator provided state and reports it \"\"\" def __call__ ( self , observation , action , next_observation , state , next_state , observation_space , observation_units , ): reward_dict = RewardDict () reward_dict [ self . config . agent_name ] = next_state . rewards [ self . config . agent_name ] return reward_dict","title":"Openai gym reward"},{"location":"reference/rewards/openai_gym_reward/#corl.rewards.openai_gym_reward.OpenAIGymReward","text":"Reward for OpenAiGymSimulator that rewards the reward coming from the simulator provided state and reports it Source code in corl/rewards/openai_gym_reward.py class OpenAIGymReward ( RewardFuncBase ): \"\"\" Reward for OpenAiGymSimulator that rewards the reward coming from the simulator provided state and reports it \"\"\" def __call__ ( self , observation , action , next_observation , state , next_state , observation_space , observation_units , ): reward_dict = RewardDict () reward_dict [ self . config . agent_name ] = next_state . rewards [ self . config . agent_name ] return reward_dict","title":"OpenAIGymReward"},{"location":"reference/rewards/reward_func_base/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Reward Functor Base Module RewardFuncBase ( EnvFuncBase ) \u00a4 The base implementation for reward functors Source code in corl/rewards/reward_func_base.py class RewardFuncBase ( EnvFuncBase ): \"\"\"The base implementation for reward functors \"\"\" def __init__ ( self , ** kwargs ): self . config : RewardFuncBaseValidator = self . get_validator ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ RewardFuncBaseValidator ]: \"\"\"Returns pydantic validator associated with this class \"\"\" return RewardFuncBaseValidator @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : ... def post_process_trajectory ( self , agent_id , state , batch , episode , policy ): # pylint: disable=unused-argument,no-self-use \"\"\"Allows the user to modify the trajectory of the episode in the batch collected during an rllib callback. WARNING: This function is dangerous you can completly destroy training using this Use it only as a last resort \"\"\" ... @property def name ( self ) -> str : \"\"\" gets the name fo the functor Returns ------- str The name of the functor \"\"\" return type ( self ) . __name__ if self . config . name is None else self . config . name get_validator : Type [ corl . rewards . reward_func_base . RewardFuncBaseValidator ] property readonly \u00a4 Returns pydantic validator associated with this class name : str property readonly \u00a4 gets the name fo the functor Returns \u00a4 str The name of the functor post_process_trajectory ( self , agent_id , state , batch , episode , policy ) \u00a4 Allows the user to modify the trajectory of the episode in the batch collected during an rllib callback. WARNING: This function is dangerous you can completly destroy training using this Use it only as a last resort Source code in corl/rewards/reward_func_base.py def post_process_trajectory ( self , agent_id , state , batch , episode , policy ): # pylint: disable=unused-argument,no-self-use \"\"\"Allows the user to modify the trajectory of the episode in the batch collected during an rllib callback. WARNING: This function is dangerous you can completly destroy training using this Use it only as a last resort \"\"\" ... RewardFuncBaseValidator ( BaseModel ) pydantic-model \u00a4 name: Name of reward functor agent_name: Name of agent the reward functor belongs to Source code in corl/rewards/reward_func_base.py class RewardFuncBaseValidator ( BaseModel ): \"\"\" name: Name of reward functor agent_name: Name of agent the reward functor belongs to \"\"\" name : typing . Optional [ str ] agent_name : str","title":"Reward func base"},{"location":"reference/rewards/reward_func_base/#corl.rewards.reward_func_base.RewardFuncBase","text":"The base implementation for reward functors Source code in corl/rewards/reward_func_base.py class RewardFuncBase ( EnvFuncBase ): \"\"\"The base implementation for reward functors \"\"\" def __init__ ( self , ** kwargs ): self . config : RewardFuncBaseValidator = self . get_validator ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ RewardFuncBaseValidator ]: \"\"\"Returns pydantic validator associated with this class \"\"\" return RewardFuncBaseValidator @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : ... def post_process_trajectory ( self , agent_id , state , batch , episode , policy ): # pylint: disable=unused-argument,no-self-use \"\"\"Allows the user to modify the trajectory of the episode in the batch collected during an rllib callback. WARNING: This function is dangerous you can completly destroy training using this Use it only as a last resort \"\"\" ... @property def name ( self ) -> str : \"\"\" gets the name fo the functor Returns ------- str The name of the functor \"\"\" return type ( self ) . __name__ if self . config . name is None else self . config . name","title":"RewardFuncBase"},{"location":"reference/rewards/reward_func_base/#corl.rewards.reward_func_base.RewardFuncBase.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/reward_func_base/#corl.rewards.reward_func_base.RewardFuncBase.name","text":"gets the name fo the functor","title":"name"},{"location":"reference/rewards/reward_func_base/#corl.rewards.reward_func_base.RewardFuncBase.name--returns","text":"str The name of the functor","title":"Returns"},{"location":"reference/rewards/reward_func_base/#corl.rewards.reward_func_base.RewardFuncBase.post_process_trajectory","text":"Allows the user to modify the trajectory of the episode in the batch collected during an rllib callback. WARNING: This function is dangerous you can completly destroy training using this Use it only as a last resort Source code in corl/rewards/reward_func_base.py def post_process_trajectory ( self , agent_id , state , batch , episode , policy ): # pylint: disable=unused-argument,no-self-use \"\"\"Allows the user to modify the trajectory of the episode in the batch collected during an rllib callback. WARNING: This function is dangerous you can completly destroy training using this Use it only as a last resort \"\"\" ...","title":"post_process_trajectory()"},{"location":"reference/rewards/reward_func_base/#corl.rewards.reward_func_base.RewardFuncBaseValidator","text":"name: Name of reward functor agent_name: Name of agent the reward functor belongs to Source code in corl/rewards/reward_func_base.py class RewardFuncBaseValidator ( BaseModel ): \"\"\" name: Name of reward functor agent_name: Name of agent the reward functor belongs to \"\"\" name : typing . Optional [ str ] agent_name : str","title":"RewardFuncBaseValidator"},{"location":"reference/rewards/reward_func_dict_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseDictWrapperReward ( RewardFuncBase ) \u00a4 A base object that rewards can inherit in order to \"wrap\" multiple reward instances Source code in corl/rewards/reward_func_dict_wrapper.py class BaseDictWrapperReward ( RewardFuncBase ): \"\"\"A base object that rewards can inherit in order to \"wrap\" multiple reward instances \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseDictWrapperRewardValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseDictWrapperRewardValidator ]: return BaseDictWrapperRewardValidator def rewards ( self ) -> typing . Dict [ str , RewardFuncBase ]: \"\"\"Get the wrapped reward instances \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : ... get_validator : Type [ corl . rewards . reward_func_dict_wrapper . BaseDictWrapperRewardValidator ] property readonly \u00a4 Returns pydantic validator associated with this class rewards ( self ) \u00a4 Get the wrapped reward instances Source code in corl/rewards/reward_func_dict_wrapper.py def rewards ( self ) -> typing . Dict [ str , RewardFuncBase ]: \"\"\"Get the wrapped reward instances \"\"\" return self . config . wrapped BaseDictWrapperRewardValidator ( RewardFuncBaseValidator ) pydantic-model \u00a4 wrapped - the wrapped reward instances Source code in corl/rewards/reward_func_dict_wrapper.py class BaseDictWrapperRewardValidator ( RewardFuncBaseValidator ): \"\"\" wrapped - the wrapped reward instances \"\"\" wrapped : typing . Dict [ str , RewardFuncBase ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Reward func dict wrapper"},{"location":"reference/rewards/reward_func_dict_wrapper/#corl.rewards.reward_func_dict_wrapper.BaseDictWrapperReward","text":"A base object that rewards can inherit in order to \"wrap\" multiple reward instances Source code in corl/rewards/reward_func_dict_wrapper.py class BaseDictWrapperReward ( RewardFuncBase ): \"\"\"A base object that rewards can inherit in order to \"wrap\" multiple reward instances \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseDictWrapperRewardValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseDictWrapperRewardValidator ]: return BaseDictWrapperRewardValidator def rewards ( self ) -> typing . Dict [ str , RewardFuncBase ]: \"\"\"Get the wrapped reward instances \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : ...","title":"BaseDictWrapperReward"},{"location":"reference/rewards/reward_func_dict_wrapper/#corl.rewards.reward_func_dict_wrapper.BaseDictWrapperReward.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/reward_func_dict_wrapper/#corl.rewards.reward_func_dict_wrapper.BaseDictWrapperReward.rewards","text":"Get the wrapped reward instances Source code in corl/rewards/reward_func_dict_wrapper.py def rewards ( self ) -> typing . Dict [ str , RewardFuncBase ]: \"\"\"Get the wrapped reward instances \"\"\" return self . config . wrapped","title":"rewards()"},{"location":"reference/rewards/reward_func_dict_wrapper/#corl.rewards.reward_func_dict_wrapper.BaseDictWrapperRewardValidator","text":"wrapped - the wrapped reward instances Source code in corl/rewards/reward_func_dict_wrapper.py class BaseDictWrapperRewardValidator ( RewardFuncBaseValidator ): \"\"\" wrapped - the wrapped reward instances \"\"\" wrapped : typing . Dict [ str , RewardFuncBase ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseDictWrapperRewardValidator"},{"location":"reference/rewards/reward_func_multi_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseMultiWrapperReward ( RewardFuncBase ) \u00a4 A base object that rewards can inherit in order to \"wrap\" multiple reward instances Source code in corl/rewards/reward_func_multi_wrapper.py class BaseMultiWrapperReward ( RewardFuncBase ): \"\"\"A base object that rewards can inherit in order to \"wrap\" multiple reward instances \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseMultiWrapperRewardValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseMultiWrapperRewardValidator ]: return BaseMultiWrapperRewardValidator def rewards ( self ) -> typing . List [ RewardFuncBase ]: \"\"\"Get the wrapped reward instances \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : ... get_validator : Type [ corl . rewards . reward_func_multi_wrapper . BaseMultiWrapperRewardValidator ] property readonly \u00a4 Returns pydantic validator associated with this class rewards ( self ) \u00a4 Get the wrapped reward instances Source code in corl/rewards/reward_func_multi_wrapper.py def rewards ( self ) -> typing . List [ RewardFuncBase ]: \"\"\"Get the wrapped reward instances \"\"\" return self . config . wrapped BaseMultiWrapperRewardValidator ( RewardFuncBaseValidator ) pydantic-model \u00a4 wrapped - the wrapped reward instances Source code in corl/rewards/reward_func_multi_wrapper.py class BaseMultiWrapperRewardValidator ( RewardFuncBaseValidator ): \"\"\" wrapped - the wrapped reward instances \"\"\" wrapped : typing . List [ RewardFuncBase ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Reward func multi wrapper"},{"location":"reference/rewards/reward_func_multi_wrapper/#corl.rewards.reward_func_multi_wrapper.BaseMultiWrapperReward","text":"A base object that rewards can inherit in order to \"wrap\" multiple reward instances Source code in corl/rewards/reward_func_multi_wrapper.py class BaseMultiWrapperReward ( RewardFuncBase ): \"\"\"A base object that rewards can inherit in order to \"wrap\" multiple reward instances \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseMultiWrapperRewardValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseMultiWrapperRewardValidator ]: return BaseMultiWrapperRewardValidator def rewards ( self ) -> typing . List [ RewardFuncBase ]: \"\"\"Get the wrapped reward instances \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : ...","title":"BaseMultiWrapperReward"},{"location":"reference/rewards/reward_func_multi_wrapper/#corl.rewards.reward_func_multi_wrapper.BaseMultiWrapperReward.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/reward_func_multi_wrapper/#corl.rewards.reward_func_multi_wrapper.BaseMultiWrapperReward.rewards","text":"Get the wrapped reward instances Source code in corl/rewards/reward_func_multi_wrapper.py def rewards ( self ) -> typing . List [ RewardFuncBase ]: \"\"\"Get the wrapped reward instances \"\"\" return self . config . wrapped","title":"rewards()"},{"location":"reference/rewards/reward_func_multi_wrapper/#corl.rewards.reward_func_multi_wrapper.BaseMultiWrapperRewardValidator","text":"wrapped - the wrapped reward instances Source code in corl/rewards/reward_func_multi_wrapper.py class BaseMultiWrapperRewardValidator ( RewardFuncBaseValidator ): \"\"\" wrapped - the wrapped reward instances \"\"\" wrapped : typing . List [ RewardFuncBase ] class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseMultiWrapperRewardValidator"},{"location":"reference/rewards/reward_func_wrapper/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseWrapperReward ( RewardFuncBase ) \u00a4 A base object that rewards can inherit in order to \"wrap\" a single reward instance Source code in corl/rewards/reward_func_wrapper.py class BaseWrapperReward ( RewardFuncBase ): \"\"\"A base object that rewards can inherit in order to \"wrap\" a single reward instance \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseWrapperRewardValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseWrapperRewardValidator ]: return BaseWrapperRewardValidator def reward ( self ) -> RewardFuncBase : \"\"\"Get the wrapped reward instance \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : ... get_validator : Type [ corl . rewards . reward_func_wrapper . BaseWrapperRewardValidator ] property readonly \u00a4 Returns pydantic validator associated with this class reward ( self ) \u00a4 Get the wrapped reward instance Source code in corl/rewards/reward_func_wrapper.py def reward ( self ) -> RewardFuncBase : \"\"\"Get the wrapped reward instance \"\"\" return self . config . wrapped BaseWrapperRewardValidator ( RewardFuncBaseValidator ) pydantic-model \u00a4 wrapped - the wrapped reward instance Source code in corl/rewards/reward_func_wrapper.py class BaseWrapperRewardValidator ( RewardFuncBaseValidator ): \"\"\" wrapped - the wrapped reward instance \"\"\" wrapped : RewardFuncBase class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"Reward func wrapper"},{"location":"reference/rewards/reward_func_wrapper/#corl.rewards.reward_func_wrapper.BaseWrapperReward","text":"A base object that rewards can inherit in order to \"wrap\" a single reward instance Source code in corl/rewards/reward_func_wrapper.py class BaseWrapperReward ( RewardFuncBase ): \"\"\"A base object that rewards can inherit in order to \"wrap\" a single reward instance \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : BaseWrapperRewardValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ) -> typing . Type [ BaseWrapperRewardValidator ]: return BaseWrapperRewardValidator def reward ( self ) -> RewardFuncBase : \"\"\"Get the wrapped reward instance \"\"\" return self . config . wrapped @abc . abstractmethod def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : ...","title":"BaseWrapperReward"},{"location":"reference/rewards/reward_func_wrapper/#corl.rewards.reward_func_wrapper.BaseWrapperReward.get_validator","text":"Returns pydantic validator associated with this class","title":"get_validator"},{"location":"reference/rewards/reward_func_wrapper/#corl.rewards.reward_func_wrapper.BaseWrapperReward.reward","text":"Get the wrapped reward instance Source code in corl/rewards/reward_func_wrapper.py def reward ( self ) -> RewardFuncBase : \"\"\"Get the wrapped reward instance \"\"\" return self . config . wrapped","title":"reward()"},{"location":"reference/rewards/reward_func_wrapper/#corl.rewards.reward_func_wrapper.BaseWrapperRewardValidator","text":"wrapped - the wrapped reward instance Source code in corl/rewards/reward_func_wrapper.py class BaseWrapperRewardValidator ( RewardFuncBaseValidator ): \"\"\" wrapped - the wrapped reward instance \"\"\" wrapped : RewardFuncBase class Config : # pylint: disable=C0115, R0903 arbitrary_types_allowed = True","title":"BaseWrapperRewardValidator"},{"location":"reference/rewards/docking_1d/__init__/","text":"","title":"Docking 1D"},{"location":"reference/rewards/docking_1d/docking_distance_change_reward/","text":"This module implements the Reward Functions and Reward Validators specific to the 1D Docking task. DockingDistanceChangeReward ( RewardFuncBase ) \u00a4 This RewardFuncBase extension is responsible for calculating the reward associated with a change in agent position. Source code in corl/rewards/docking_1d/docking_distance_change_reward.py class DockingDistanceChangeReward ( RewardFuncBase ): \"\"\" This RewardFuncBase extension is responsible for calculating the reward associated with a change in agent position. \"\"\" def __init__ ( self , ** kwargs ): self . config : DockingDistanceChangeRewardValidator super () . __init__ ( ** kwargs ) self . _dist_buffer = RingBuffer ( capacity = 2 , dtype = float ) @property def get_validator ( self ): \"\"\" Method to return class's Validator. \"\"\" return DockingDistanceChangeRewardValidator def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : \"\"\" This method calculates the current position of the agent and compares it to the previous position. The difference is used to return a proportional reward. Parameters ---------- observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space? Returns ------- reward : RewardDict The agent's reward for their change in distance. \"\"\" reward = RewardDict () val = 0 deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) # type: ignore deputy_position = position_sensor . get_measurement () chief_position = np . array ([ 0 ]) # hardcoded to origin distance = abs ( chief_position - deputy_position ) self . _dist_buffer . append ( distance [ 0 ]) if len ( self . _dist_buffer ) == 2 : val = self . config . scale * ( self . _dist_buffer [ 0 ] - self . _dist_buffer [ 1 ]) reward [ self . config . agent_name ] = val return reward get_validator property readonly \u00a4 Method to return class's Validator. __call__ ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ) special \u00a4 This method calculates the current position of the agent and compares it to the previous position. The difference is used to return a proportional reward. Parameters \u00a4 observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space? Returns \u00a4 reward : RewardDict The agent's reward for their change in distance. Source code in corl/rewards/docking_1d/docking_distance_change_reward.py def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : \"\"\" This method calculates the current position of the agent and compares it to the previous position. The difference is used to return a proportional reward. Parameters ---------- observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space? Returns ------- reward : RewardDict The agent's reward for their change in distance. \"\"\" reward = RewardDict () val = 0 deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) # type: ignore deputy_position = position_sensor . get_measurement () chief_position = np . array ([ 0 ]) # hardcoded to origin distance = abs ( chief_position - deputy_position ) self . _dist_buffer . append ( distance [ 0 ]) if len ( self . _dist_buffer ) == 2 : val = self . config . scale * ( self . _dist_buffer [ 0 ] - self . _dist_buffer [ 1 ]) reward [ self . config . agent_name ] = val return reward DockingDistanceChangeRewardValidator ( RewardFuncBaseValidator ) pydantic-model \u00a4 scale: Scalar value to adjust magnitude of the reward Source code in corl/rewards/docking_1d/docking_distance_change_reward.py class DockingDistanceChangeRewardValidator ( RewardFuncBaseValidator ): \"\"\" scale: Scalar value to adjust magnitude of the reward \"\"\" scale : float = 1.0 position_sensor_name : str","title":"Docking distance change reward"},{"location":"reference/rewards/docking_1d/docking_distance_change_reward/#corl.rewards.docking_1d.docking_distance_change_reward.DockingDistanceChangeReward","text":"This RewardFuncBase extension is responsible for calculating the reward associated with a change in agent position. Source code in corl/rewards/docking_1d/docking_distance_change_reward.py class DockingDistanceChangeReward ( RewardFuncBase ): \"\"\" This RewardFuncBase extension is responsible for calculating the reward associated with a change in agent position. \"\"\" def __init__ ( self , ** kwargs ): self . config : DockingDistanceChangeRewardValidator super () . __init__ ( ** kwargs ) self . _dist_buffer = RingBuffer ( capacity = 2 , dtype = float ) @property def get_validator ( self ): \"\"\" Method to return class's Validator. \"\"\" return DockingDistanceChangeRewardValidator def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : \"\"\" This method calculates the current position of the agent and compares it to the previous position. The difference is used to return a proportional reward. Parameters ---------- observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space? Returns ------- reward : RewardDict The agent's reward for their change in distance. \"\"\" reward = RewardDict () val = 0 deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) # type: ignore deputy_position = position_sensor . get_measurement () chief_position = np . array ([ 0 ]) # hardcoded to origin distance = abs ( chief_position - deputy_position ) self . _dist_buffer . append ( distance [ 0 ]) if len ( self . _dist_buffer ) == 2 : val = self . config . scale * ( self . _dist_buffer [ 0 ] - self . _dist_buffer [ 1 ]) reward [ self . config . agent_name ] = val return reward","title":"DockingDistanceChangeReward"},{"location":"reference/rewards/docking_1d/docking_distance_change_reward/#corl.rewards.docking_1d.docking_distance_change_reward.DockingDistanceChangeReward.get_validator","text":"Method to return class's Validator.","title":"get_validator"},{"location":"reference/rewards/docking_1d/docking_distance_change_reward/#corl.rewards.docking_1d.docking_distance_change_reward.DockingDistanceChangeReward.__call__","text":"This method calculates the current position of the agent and compares it to the previous position. The difference is used to return a proportional reward.","title":"__call__()"},{"location":"reference/rewards/docking_1d/docking_distance_change_reward/#corl.rewards.docking_1d.docking_distance_change_reward.DockingDistanceChangeReward.__call__--parameters","text":"observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space?","title":"Parameters"},{"location":"reference/rewards/docking_1d/docking_distance_change_reward/#corl.rewards.docking_1d.docking_distance_change_reward.DockingDistanceChangeReward.__call__--returns","text":"reward : RewardDict The agent's reward for their change in distance. Source code in corl/rewards/docking_1d/docking_distance_change_reward.py def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : \"\"\" This method calculates the current position of the agent and compares it to the previous position. The difference is used to return a proportional reward. Parameters ---------- observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space? Returns ------- reward : RewardDict The agent's reward for their change in distance. \"\"\" reward = RewardDict () val = 0 deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) # type: ignore deputy_position = position_sensor . get_measurement () chief_position = np . array ([ 0 ]) # hardcoded to origin distance = abs ( chief_position - deputy_position ) self . _dist_buffer . append ( distance [ 0 ]) if len ( self . _dist_buffer ) == 2 : val = self . config . scale * ( self . _dist_buffer [ 0 ] - self . _dist_buffer [ 1 ]) reward [ self . config . agent_name ] = val return reward","title":"Returns"},{"location":"reference/rewards/docking_1d/docking_distance_change_reward/#corl.rewards.docking_1d.docking_distance_change_reward.DockingDistanceChangeRewardValidator","text":"scale: Scalar value to adjust magnitude of the reward Source code in corl/rewards/docking_1d/docking_distance_change_reward.py class DockingDistanceChangeRewardValidator ( RewardFuncBaseValidator ): \"\"\" scale: Scalar value to adjust magnitude of the reward \"\"\" scale : float = 1.0 position_sensor_name : str","title":"DockingDistanceChangeRewardValidator"},{"location":"reference/rewards/docking_1d/docking_reward/","text":"This module implements the Reward Functions and Reward Validators specific to the 1D Docking task. DockingReward ( RewardFuncBase ) \u00a4 This Reward Function is responsible for calculating the reward (or penalty) associated with a given docking attempt. Source code in corl/rewards/docking_1d/docking_reward.py class DockingReward ( RewardFuncBase ): \"\"\" This Reward Function is responsible for calculating the reward (or penalty) associated with a given docking attempt. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : DockingRewardValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ): \"\"\" Method to return class's Validator. \"\"\" return DockingRewardValidator def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : \"\"\" This method determines if the agent has succeeded or failed and returns an appropriate reward. Parameters ---------- observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space Returns ------- reward : RewardDict The agent's reward for their docking attempt. \"\"\" reward = RewardDict () value = 0.0 deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) # type: ignore velocity_sensor = get_sensor_by_name ( deputy , self . config . velocity_sensor_name ) # type: ignore position = position_sensor . get_measurement () velocity = velocity_sensor . get_measurement () sim_time = deputy . sim_time # type: ignore chief_position = np . array ([ 0 ]) docking_region_radius = self . config . docking_region_radius distance = abs ( position - chief_position ) in_docking = distance <= docking_region_radius max_velocity_exceeded = self . config . velocity_threshold < velocity # type: ignore if sim_time > self . config . timeout : # episode reached max time value = self . config . timeout_reward elif distance >= self . config . max_goal_distance : # agent exceeded max distance from goal value = self . config . distance_reward elif in_docking and max_velocity_exceeded : # agent exceeded velocity constraint within docking region value = self . config . crash_reward elif in_docking and not max_velocity_exceeded : # agent safely made it to the docking region value = self . config . success_reward if self . config . timeout : # Add time reward component, if timeout specified value += 1 - ( sim_time / self . config . timeout ) reward [ self . config . agent_name ] = value return reward get_validator property readonly \u00a4 Method to return class's Validator. __call__ ( self , observation , action , next_observation , state , next_state , observation_space , observation_units ) special \u00a4 This method determines if the agent has succeeded or failed and returns an appropriate reward. Parameters \u00a4 observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space Returns \u00a4 reward : RewardDict The agent's reward for their docking attempt. Source code in corl/rewards/docking_1d/docking_reward.py def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : \"\"\" This method determines if the agent has succeeded or failed and returns an appropriate reward. Parameters ---------- observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space Returns ------- reward : RewardDict The agent's reward for their docking attempt. \"\"\" reward = RewardDict () value = 0.0 deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) # type: ignore velocity_sensor = get_sensor_by_name ( deputy , self . config . velocity_sensor_name ) # type: ignore position = position_sensor . get_measurement () velocity = velocity_sensor . get_measurement () sim_time = deputy . sim_time # type: ignore chief_position = np . array ([ 0 ]) docking_region_radius = self . config . docking_region_radius distance = abs ( position - chief_position ) in_docking = distance <= docking_region_radius max_velocity_exceeded = self . config . velocity_threshold < velocity # type: ignore if sim_time > self . config . timeout : # episode reached max time value = self . config . timeout_reward elif distance >= self . config . max_goal_distance : # agent exceeded max distance from goal value = self . config . distance_reward elif in_docking and max_velocity_exceeded : # agent exceeded velocity constraint within docking region value = self . config . crash_reward elif in_docking and not max_velocity_exceeded : # agent safely made it to the docking region value = self . config . success_reward if self . config . timeout : # Add time reward component, if timeout specified value += 1 - ( sim_time / self . config . timeout ) reward [ self . config . agent_name ] = value return reward DockingRewardValidator ( RewardFuncBaseValidator ) pydantic-model \u00a4 This Validator ensures the DockingReward's config defines values relevant to successful and unsuccessful docking attempts. Source code in corl/rewards/docking_1d/docking_reward.py class DockingRewardValidator ( RewardFuncBaseValidator ): \"\"\" This Validator ensures the DockingReward's config defines values relevant to successful and unsuccessful docking attempts. \"\"\" success_reward : float timeout_reward : float distance_reward : float crash_reward : float timeout : float docking_region_radius : float max_goal_distance : float velocity_threshold : float position_sensor_name : str velocity_sensor_name : str","title":"Docking reward"},{"location":"reference/rewards/docking_1d/docking_reward/#corl.rewards.docking_1d.docking_reward.DockingReward","text":"This Reward Function is responsible for calculating the reward (or penalty) associated with a given docking attempt. Source code in corl/rewards/docking_1d/docking_reward.py class DockingReward ( RewardFuncBase ): \"\"\" This Reward Function is responsible for calculating the reward (or penalty) associated with a given docking attempt. \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : DockingRewardValidator super () . __init__ ( ** kwargs ) @property def get_validator ( self ): \"\"\" Method to return class's Validator. \"\"\" return DockingRewardValidator def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : \"\"\" This method determines if the agent has succeeded or failed and returns an appropriate reward. Parameters ---------- observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space Returns ------- reward : RewardDict The agent's reward for their docking attempt. \"\"\" reward = RewardDict () value = 0.0 deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) # type: ignore velocity_sensor = get_sensor_by_name ( deputy , self . config . velocity_sensor_name ) # type: ignore position = position_sensor . get_measurement () velocity = velocity_sensor . get_measurement () sim_time = deputy . sim_time # type: ignore chief_position = np . array ([ 0 ]) docking_region_radius = self . config . docking_region_radius distance = abs ( position - chief_position ) in_docking = distance <= docking_region_radius max_velocity_exceeded = self . config . velocity_threshold < velocity # type: ignore if sim_time > self . config . timeout : # episode reached max time value = self . config . timeout_reward elif distance >= self . config . max_goal_distance : # agent exceeded max distance from goal value = self . config . distance_reward elif in_docking and max_velocity_exceeded : # agent exceeded velocity constraint within docking region value = self . config . crash_reward elif in_docking and not max_velocity_exceeded : # agent safely made it to the docking region value = self . config . success_reward if self . config . timeout : # Add time reward component, if timeout specified value += 1 - ( sim_time / self . config . timeout ) reward [ self . config . agent_name ] = value return reward","title":"DockingReward"},{"location":"reference/rewards/docking_1d/docking_reward/#corl.rewards.docking_1d.docking_reward.DockingReward.get_validator","text":"Method to return class's Validator.","title":"get_validator"},{"location":"reference/rewards/docking_1d/docking_reward/#corl.rewards.docking_1d.docking_reward.DockingReward.__call__","text":"This method determines if the agent has succeeded or failed and returns an appropriate reward.","title":"__call__()"},{"location":"reference/rewards/docking_1d/docking_reward/#corl.rewards.docking_1d.docking_reward.DockingReward.__call__--parameters","text":"observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space","title":"Parameters"},{"location":"reference/rewards/docking_1d/docking_reward/#corl.rewards.docking_1d.docking_reward.DockingReward.__call__--returns","text":"reward : RewardDict The agent's reward for their docking attempt. Source code in corl/rewards/docking_1d/docking_reward.py def __call__ ( self , observation : OrderedDict , action , next_observation : OrderedDict , state : StateDict , next_state : StateDict , observation_space : StateDict , observation_units : StateDict , ) -> RewardDict : \"\"\" This method determines if the agent has succeeded or failed and returns an appropriate reward. Parameters ---------- observation : OrderedDict The observations available to the agent from the previous state. action The last action performed by the agent. next_observation : OrderedDict The observations available to the agent from the current state. state : StateDict The previous state of the simulation. next_state : StateDict The current state of the simulation. observation_space : StateDict The agent's observation space. observation_units : StateDict The units corresponding to values in the observation_space Returns ------- reward : RewardDict The agent's reward for their docking attempt. \"\"\" reward = RewardDict () value = 0.0 deputy = get_platform_by_name ( next_state , self . config . agent_name ) position_sensor = get_sensor_by_name ( deputy , self . config . position_sensor_name ) # type: ignore velocity_sensor = get_sensor_by_name ( deputy , self . config . velocity_sensor_name ) # type: ignore position = position_sensor . get_measurement () velocity = velocity_sensor . get_measurement () sim_time = deputy . sim_time # type: ignore chief_position = np . array ([ 0 ]) docking_region_radius = self . config . docking_region_radius distance = abs ( position - chief_position ) in_docking = distance <= docking_region_radius max_velocity_exceeded = self . config . velocity_threshold < velocity # type: ignore if sim_time > self . config . timeout : # episode reached max time value = self . config . timeout_reward elif distance >= self . config . max_goal_distance : # agent exceeded max distance from goal value = self . config . distance_reward elif in_docking and max_velocity_exceeded : # agent exceeded velocity constraint within docking region value = self . config . crash_reward elif in_docking and not max_velocity_exceeded : # agent safely made it to the docking region value = self . config . success_reward if self . config . timeout : # Add time reward component, if timeout specified value += 1 - ( sim_time / self . config . timeout ) reward [ self . config . agent_name ] = value return reward","title":"Returns"},{"location":"reference/rewards/docking_1d/docking_reward/#corl.rewards.docking_1d.docking_reward.DockingRewardValidator","text":"This Validator ensures the DockingReward's config defines values relevant to successful and unsuccessful docking attempts. Source code in corl/rewards/docking_1d/docking_reward.py class DockingRewardValidator ( RewardFuncBaseValidator ): \"\"\" This Validator ensures the DockingReward's config defines values relevant to successful and unsuccessful docking attempts. \"\"\" success_reward : float timeout_reward : float distance_reward : float crash_reward : float timeout : float docking_region_radius : float max_goal_distance : float velocity_threshold : float position_sensor_name : str velocity_sensor_name : str","title":"DockingRewardValidator"},{"location":"reference/simulators/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Simulators"},{"location":"reference/simulators/base_available_platforms/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. BaseAvailablePlatformTypes ( Enum ) \u00a4 Enumeration that outlines the platform types that have been implemented Source code in corl/simulators/base_available_platforms.py class BaseAvailablePlatformTypes ( enum . Enum ): \"\"\"Enumeration that outlines the platform types that have been implemented \"\"\" @abc . abstractclassmethod def ParseFromNameModel ( cls , config : Dict [ str , Any ]) -> BaseAvailablePlatformTypes : # pylint: disable=unused-argument \"\"\" ParseFromNameModel is responsible for returning the platform type being used by a platform configuration that will be provided by some platform configuration Arguments: config {Dict[str, Any]} -- The platform configuration for this platform Returns: BaseAvailablePlatformTypes -- The platform type being used by this platform \"\"\" ...","title":"Base available platforms"},{"location":"reference/simulators/base_available_platforms/#corl.simulators.base_available_platforms.BaseAvailablePlatformTypes","text":"Enumeration that outlines the platform types that have been implemented Source code in corl/simulators/base_available_platforms.py class BaseAvailablePlatformTypes ( enum . Enum ): \"\"\"Enumeration that outlines the platform types that have been implemented \"\"\" @abc . abstractclassmethod def ParseFromNameModel ( cls , config : Dict [ str , Any ]) -> BaseAvailablePlatformTypes : # pylint: disable=unused-argument \"\"\" ParseFromNameModel is responsible for returning the platform type being used by a platform configuration that will be provided by some platform configuration Arguments: config {Dict[str, Any]} -- The platform configuration for this platform Returns: BaseAvailablePlatformTypes -- The platform type being used by this platform \"\"\" ...","title":"BaseAvailablePlatformTypes"},{"location":"reference/simulators/base_parts/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Integration module provides abstraction for integration to enable RL environment connect to simulation environments and/or real environments. The concept is that if we build either simulation or real environments from this common interface we can transition between them. For example if there is an Sim1StickController and a Sim2StickController the interface between them is derived from BaseStickController and thus to the policy the interface will be the same. The base classes have properties which determine what the deriving classes must adhere to in order to comply with the intention of the base class. For example the BasePlatform has a position property which has a position_properties (a MultiBoxProp object) that determines the ranges and types of the position property for the BasePlatform. To differentiate from the agent properties, base integration nomenclature uses control and measurements for the base parts, in contrast to the nomenclature of the agent which is usually action and observation. In this sense actions are made up of controls, and observations are made up of measurements. BaseController ( BasePlatformPart , ABC ) \u00a4 BaseController base abstraction for a controller. A controller is used to move a platform with action commands. The actions are usually changing the desired rates or applied forces to the platform. Source code in corl/simulators/base_parts.py class BaseController ( BasePlatformPart , abc . ABC ): \"\"\" BaseController base abstraction for a controller. A controller is used to move a platform with action commands. The actions are usually changing the desired rates or applied forces to the platform. \"\"\" @property def control_properties ( self ) -> Prop : \"\"\" The properties of the control given to the apply_control function Returns ------- Prop The properties of the control given tot he apply_control function \"\"\" return self . _properties def validate_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to validate a control for this controller. Parameters ---------- control The control to be validated \"\"\" if not self . control_properties . create_space () . contains ( control ): raise ValueError ( f \" { type ( self ) . __name__ } control { control } not in space { self . control_properties . create_space () } values\" ) @abc . abstractmethod def apply_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to apply the control for this controller. Parameters ---------- control The control to be executed by the controller \"\"\" ... @abc . abstractmethod def get_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: \"\"\" Get the previously applied control that was given to the apply_control function Returns ------- previously applied control that was given to the apply_control function \"\"\" def get_validated_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: \"\"\" Get the previously applied control with nan check Returns ------- previously applied control that was given to the apply_control function \"\"\" return nan_check_result ( self . get_applied_control ()) control_properties : Prop property readonly \u00a4 The properties of the control given to the apply_control function Returns \u00a4 Prop The properties of the control given tot he apply_control function apply_control ( self , control ) \u00a4 The generic method to apply the control for this controller. Parameters \u00a4 control The control to be executed by the controller Source code in corl/simulators/base_parts.py @abc . abstractmethod def apply_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to apply the control for this controller. Parameters ---------- control The control to be executed by the controller \"\"\" ... get_applied_control ( self ) \u00a4 Get the previously applied control that was given to the apply_control function Returns previously applied control that was given to the apply_control function Source code in corl/simulators/base_parts.py @abc . abstractmethod def get_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: \"\"\" Get the previously applied control that was given to the apply_control function Returns ------- previously applied control that was given to the apply_control function \"\"\" get_validated_applied_control ( self ) \u00a4 Get the previously applied control with nan check Returns previously applied control that was given to the apply_control function Source code in corl/simulators/base_parts.py def get_validated_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: \"\"\" Get the previously applied control with nan check Returns ------- previously applied control that was given to the apply_control function \"\"\" return nan_check_result ( self . get_applied_control ()) validate_control ( self , control ) \u00a4 The generic method to validate a control for this controller. Parameters \u00a4 control The control to be validated Source code in corl/simulators/base_parts.py def validate_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to validate a control for this controller. Parameters ---------- control The control to be validated \"\"\" if not self . control_properties . create_space () . contains ( control ): raise ValueError ( f \" { type ( self ) . __name__ } control { control } not in space { self . control_properties . create_space () } values\" ) BasePlatformPart ( ABC ) \u00a4 BasePlatformPart abstract class for the classes that will be part of the BasePlatform. This includes controls,and sensors Source code in corl/simulators/base_parts.py class BasePlatformPart ( abc . ABC ): \"\"\" BasePlatformPart abstract class for the classes that will be part of the BasePlatform. This includes controls,and sensors \"\"\" def __init__ ( self , parent_platform , config , property_class ) -> None : config [ \"part_class\" ] = self . __class__ self . config = self . get_validator ( ** config ) self . _properties = property_class ( ** self . config . properties ) self . _parent_platform = parent_platform self . _valid = self . config . initial_validity @property def valid ( self ) -> bool : \"\"\" Signifies if measurements from this part should be trusted as having data that is valid or important to training Returns ------- bool The current state of the part being valid or not \"\"\" return self . _valid def set_valid ( self ) -> None : \"\"\" Signifies that this part should transition to being valid \"\"\" self . _valid = True def set_invalid ( self ) -> None : \"\"\" Signifies that this part should transition to being invalid \"\"\" self . _valid = False @property def name ( self ) -> typing . Optional [ str ]: \"\"\" The name for this platform part Returns ------- str The name for this platform part \"\"\" return self . config . name @property def parent_platform ( self ) -> 'BasePlatform' : # type: ignore # noqa: F821 \"\"\" The parent platform this platform part is attached to Returns ------- BasePlatform The parent platform this platform part is attached to \"\"\" return self . _parent_platform @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return set () @property def get_validator ( self ) -> typing . Type [ BasePlatformPartValidator ]: \"\"\" return the validator that will be used on the configuration of this part \"\"\" return BasePlatformPartValidator @classmethod def embed_properties ( cls , property_class : typing . Type [ Prop ]) -> typing . Type [ BasePlatformPart ]: \"\"\"Embed the properties in the class definition.\"\"\" class DynamicPlatformPart ( cls ): # type: ignore[valid-type,misc] # pylint: disable=missing-class-docstring def __init__ ( self , parent_platform , config ) -> None : super () . __init__ ( parent_platform = parent_platform , config = config , property_class = property_class ) DynamicPlatformPart . __doc__ = cls . __doc__ DynamicPlatformPart . __name__ += f ': { cls . __name__ } : { property_class . __name__ } ' DynamicPlatformPart . __qualname__ += f ': { cls . __name__ } : { property_class . __name__ } ' DynamicPlatformPart . embedded_properties = property_class return DynamicPlatformPart exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness get_validator : Type [ BasePlatformPartValidator ] property readonly \u00a4 return the validator that will be used on the configuration of this part name : Optional [ str ] property readonly \u00a4 The name for this platform part Returns \u00a4 str The name for this platform part parent_platform : 'BasePlatform' property readonly \u00a4 The parent platform this platform part is attached to Returns \u00a4 BasePlatform The parent platform this platform part is attached to valid : bool property readonly \u00a4 Signifies if measurements from this part should be trusted as having data that is valid or important to training Returns \u00a4 bool The current state of the part being valid or not embed_properties ( property_class ) classmethod \u00a4 Embed the properties in the class definition. Source code in corl/simulators/base_parts.py @classmethod def embed_properties ( cls , property_class : typing . Type [ Prop ]) -> typing . Type [ BasePlatformPart ]: \"\"\"Embed the properties in the class definition.\"\"\" class DynamicPlatformPart ( cls ): # type: ignore[valid-type,misc] # pylint: disable=missing-class-docstring def __init__ ( self , parent_platform , config ) -> None : super () . __init__ ( parent_platform = parent_platform , config = config , property_class = property_class ) DynamicPlatformPart . __doc__ = cls . __doc__ DynamicPlatformPart . __name__ += f ': { cls . __name__ } : { property_class . __name__ } ' DynamicPlatformPart . __qualname__ += f ': { cls . __name__ } : { property_class . __name__ } ' DynamicPlatformPart . embedded_properties = property_class return DynamicPlatformPart set_invalid ( self ) \u00a4 Signifies that this part should transition to being invalid Source code in corl/simulators/base_parts.py def set_invalid ( self ) -> None : \"\"\" Signifies that this part should transition to being invalid \"\"\" self . _valid = False set_valid ( self ) \u00a4 Signifies that this part should transition to being valid Source code in corl/simulators/base_parts.py def set_valid ( self ) -> None : \"\"\" Signifies that this part should transition to being valid \"\"\" self . _valid = True BasePlatformPartValidator ( BaseModel ) pydantic-model \u00a4 name: the optional name for this part, the class name will be used otherwise Source code in corl/simulators/base_parts.py class BasePlatformPartValidator ( BaseModel ): \"\"\" name: the optional name for this part, the class name will be used otherwise \"\"\" part_class : PyObject name : typing . Optional [ str ] = None initial_validity : bool = True properties : typing . Optional [ typing . Dict ] = dict () @validator ( 'name' , always = True ) def check_name ( cls , v , values ): \"\"\"Check if agent subclass AgentBase\"\"\" if v is None : assert 'part_class' in values v = PluginLibrary . FindGroup ( values [ 'part_class' ]) return v check_name ( v , values ) classmethod \u00a4 Check if agent subclass AgentBase Source code in corl/simulators/base_parts.py @validator ( 'name' , always = True ) def check_name ( cls , v , values ): \"\"\"Check if agent subclass AgentBase\"\"\" if v is None : assert 'part_class' in values v = PluginLibrary . FindGroup ( values [ 'part_class' ]) return v BaseSensor ( BasePlatformPart , ABC ) \u00a4 BaseSensor base abstraction for a sensor. A sensor is a attached to a platform and provides information about the environment. Source code in corl/simulators/base_parts.py class BaseSensor ( BasePlatformPart , abc . ABC ): \"\"\" BaseSensor base abstraction for a sensor. A sensor is a attached to a platform and provides information about the environment. \"\"\" def __init__ ( self , parent_platform , config , property_class ) -> None : super () . __init__ ( parent_platform = parent_platform , config = config , property_class = property_class ) self . _last_measurement : typing . Optional [ typing . Union [ np . ndarray , typing . Tuple , typing . Dict ]] = None @property def measurement_properties ( self ) -> Prop : \"\"\" The properties of the object returned by the get_measurement function Returns ------- Prop The properties of the measurement returned by the get_measurement function \"\"\" return self . _properties @abc . abstractmethod def _calculate_measurement ( self , state : typing . Tuple ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict ]: \"\"\" The generic method to get calculate measurements from this sensor. This is used to calculate the measurement to be returned by the get_measurement function. This allows caching the measurement to avoid re-calculation Parameters ---------- state: typing.Tuple The current state of the environment used to obtain the measurement Returns ------- typing.Union[np.ndarray, typing.Tuple, typing.Dict] The measurements from this sensor \"\"\" ... def calculate_and_cache_measurement ( self , state : typing . Tuple ): \"\"\" Calculates the measurement and caches the result in the _last_measurement variable Parameters ---------- state: typing.Tuple The current state of the environment used to obtain the measurement \"\"\" measurement = self . _calculate_measurement ( state ) try : nan_check_result ( measurement , True ) self . _last_measurement = measurement except ValueError : warnings . warn ( \"The mover has produced a state that is invalid - NaNs --- Code is going to set broken/damaged - Reuse last state\" ) # raise ValueError(f\"Error calculating Measurement in {self.__class__}\\n\" f\"Measurement: {self._last_measurement}\\n\") from err if self . _last_measurement is None : raise ValueError ( 'Measurement is None' ) def get_measurement ( self ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict , typing . List ]: \"\"\" The generic method to get measurements from this sensor. Returns ------- typing.Union[np.ndarray, typing.Tuple, typing.Dict] The measurements from this sensor \"\"\" if self . _last_measurement is None : raise ValueError ( f 'Measurement is None - may also want to check operable states - ( { type ( self ) } )' ) return self . _last_measurement measurement_properties : Prop property readonly \u00a4 The properties of the object returned by the get_measurement function Returns \u00a4 Prop The properties of the measurement returned by the get_measurement function calculate_and_cache_measurement ( self , state ) \u00a4 Calculates the measurement and caches the result in the _last_measurement variable Parameters \u00a4 typing.Tuple The current state of the environment used to obtain the measurement Source code in corl/simulators/base_parts.py def calculate_and_cache_measurement ( self , state : typing . Tuple ): \"\"\" Calculates the measurement and caches the result in the _last_measurement variable Parameters ---------- state: typing.Tuple The current state of the environment used to obtain the measurement \"\"\" measurement = self . _calculate_measurement ( state ) try : nan_check_result ( measurement , True ) self . _last_measurement = measurement except ValueError : warnings . warn ( \"The mover has produced a state that is invalid - NaNs --- Code is going to set broken/damaged - Reuse last state\" ) # raise ValueError(f\"Error calculating Measurement in {self.__class__}\\n\" f\"Measurement: {self._last_measurement}\\n\") from err if self . _last_measurement is None : raise ValueError ( 'Measurement is None' ) get_measurement ( self ) \u00a4 The generic method to get measurements from this sensor. Returns \u00a4 typing.Union[np.ndarray, typing.Tuple, typing.Dict] The measurements from this sensor Source code in corl/simulators/base_parts.py def get_measurement ( self ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict , typing . List ]: \"\"\" The generic method to get measurements from this sensor. Returns ------- typing.Union[np.ndarray, typing.Tuple, typing.Dict] The measurements from this sensor \"\"\" if self . _last_measurement is None : raise ValueError ( f 'Measurement is None - may also want to check operable states - ( { type ( self ) } )' ) return self . _last_measurement BaseTimeSensor ( BaseSensor ) \u00a4 Base type for a time sensor Source code in corl/simulators/base_parts.py class BaseTimeSensor ( BaseSensor ): \"\"\"Base type for a time sensor Arguments: BaseSensor -- The base class type for the sensor \"\"\" def __init__ ( self , parent_platform , config : typing . Dict = None ): super () . __init__ ( parent_platform , config , base_props . TimeProp ) CommandWithArgs ( BaseModel ) pydantic-model \u00a4 Model for a command with its argumets. Source code in corl/simulators/base_parts.py class CommandWithArgs ( BaseModel ): # type: ignore[no-redef] \"\"\"Model for a command with its argumets.\"\"\" command : str args : typing . List [ typing . Any ] = [] kwargs : typing . Dict [ str , typing . Any ] = {} MutuallyExclusiveParts \u00a4 Class to check to see/controll mutually exclusive parts Source code in corl/simulators/base_parts.py class MutuallyExclusiveParts (): \"\"\" Class to check to see/controll mutually exclusive parts \"\"\" def __init__ ( self , exclusive_parts , allow_other_keys = False ): \"\"\" exclusive_parts: The parts to check exclusivity allow_other_keys: allows exclusivity other than the parts defined here \"\"\" self . _exclusive_parts = exclusive_parts self . allow_other_keys = allow_other_keys def are_platform_parts_mutually_exclusive ( self , * args : BasePlatformPart ): \"\"\"Checks to see if platform parts are mutually exclusive Returns: [type] -- [description] \"\"\" parts = [ platform_part . exclusiveness for platform_part in args ] return self . are_parts_mutually_exclusive ( * parts ) def are_parts_mutually_exclusive ( self , * args ): \"\"\"[summary] Returns: [type] -- [description] \"\"\" total_set = set () for part_exclusive_set in args : for part_exclusiveness in part_exclusive_set : if part_exclusiveness in total_set : return False if part_exclusiveness in self . _exclusive_parts : total_set . add ( part_exclusiveness ) elif not self . allow_other_keys : raise RuntimeError ( f \"Error: you attempted to add a part with the exclusivness { part_exclusiveness } , but \" f \"this plaforms exclusive parts for this component type were { self . _exclusive_parts } , \" \"and the platform specified to not allow other exclusivity\" ) return True def get_duplicate_parts ( self , * args ): \"\"\"[summary] Returns: [type] -- [description] \"\"\" total_list = [] for mutually_exclusive_part in args : total_list += list ( mutually_exclusive_part . exclusiveness ) ret_list = [] for exclusive_key in self . _exclusive_parts : if total_list . count ( exclusive_key ) > 1 : ret_list . append ( exclusive_key ) return ret_list __init__ ( self , exclusive_parts , allow_other_keys = False ) special \u00a4 exclusive_parts: The parts to check exclusivity allow_other_keys: allows exclusivity other than the parts defined here Source code in corl/simulators/base_parts.py def __init__ ( self , exclusive_parts , allow_other_keys = False ): \"\"\" exclusive_parts: The parts to check exclusivity allow_other_keys: allows exclusivity other than the parts defined here \"\"\" self . _exclusive_parts = exclusive_parts self . allow_other_keys = allow_other_keys are_parts_mutually_exclusive ( self , * args ) \u00a4 [summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/base_parts.py def are_parts_mutually_exclusive ( self , * args ): \"\"\"[summary] Returns: [type] -- [description] \"\"\" total_set = set () for part_exclusive_set in args : for part_exclusiveness in part_exclusive_set : if part_exclusiveness in total_set : return False if part_exclusiveness in self . _exclusive_parts : total_set . add ( part_exclusiveness ) elif not self . allow_other_keys : raise RuntimeError ( f \"Error: you attempted to add a part with the exclusivness { part_exclusiveness } , but \" f \"this plaforms exclusive parts for this component type were { self . _exclusive_parts } , \" \"and the platform specified to not allow other exclusivity\" ) return True are_platform_parts_mutually_exclusive ( self , * args ) \u00a4 Checks to see if platform parts are mutually exclusive Returns: Type Description [type] -- [description] Source code in corl/simulators/base_parts.py def are_platform_parts_mutually_exclusive ( self , * args : BasePlatformPart ): \"\"\"Checks to see if platform parts are mutually exclusive Returns: [type] -- [description] \"\"\" parts = [ platform_part . exclusiveness for platform_part in args ] return self . are_parts_mutually_exclusive ( * parts ) get_duplicate_parts ( self , * args ) \u00a4 [summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/base_parts.py def get_duplicate_parts ( self , * args ): \"\"\"[summary] Returns: [type] -- [description] \"\"\" total_list = [] for mutually_exclusive_part in args : total_list += list ( mutually_exclusive_part . exclusiveness ) ret_list = [] for exclusive_key in self . _exclusive_parts : if total_list . count ( exclusive_key ) > 1 : ret_list . append ( exclusive_key ) return ret_list NoOpController ( BaseController ) \u00a4 NoOpController controller define by empty prop can be use when simulator is creating actions for an agent. Source code in corl/simulators/base_parts.py class NoOpController ( BaseController ): \"\"\" NoOpController controller define by empty prop can be use when simulator is creating actions for an agent. \"\"\" def __init__ ( self , parent_platform , config , property_class ) -> None : self . config : NoOpControllerValidator super () . __init__ ( parent_platform = parent_platform , config = config , property_class = property_class ) self . new_control = np . array ([], dtype = np . float32 ) for command_data in self . config . platform_init_commands : func = attrgetter ( command_data . command ) func ( self . parent_platform )( * command_data . args , ** command_data . kwargs ) @property def get_validator ( self ) -> typing . Type [ NoOpControllerValidator ]: \"\"\"Validator for NoOpController\"\"\" return NoOpControllerValidator def apply_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to apply the control for this controller. Parameters ---------- control The control to be executed by the controller \"\"\" self . new_control = control def get_applied_control ( self ): return self . new_control get_validator : Type [ NoOpControllerValidator ] property readonly \u00a4 Validator for NoOpController apply_control ( self , control ) \u00a4 The generic method to apply the control for this controller. Parameters \u00a4 control The control to be executed by the controller Source code in corl/simulators/base_parts.py def apply_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to apply the control for this controller. Parameters ---------- control The control to be executed by the controller \"\"\" self . new_control = control get_applied_control ( self ) \u00a4 Get the previously applied control that was given to the apply_control function Returns previously applied control that was given to the apply_control function Source code in corl/simulators/base_parts.py def get_applied_control ( self ): return self . new_control NoOpControllerValidator ( BasePlatformPartValidator ) pydantic-model \u00a4 Validator for NoOpController platform_init_commands: Allow the user to specify platform initialization commands. The primary purpose of this is to allow verification testing between different simulators under different initial conditions without the complexity of actions being sent to those simulators. Dot notation is supported, so foo.bar.baz will call the method self.parent_platform.foo.bar.baz() . Source code in corl/simulators/base_parts.py class NoOpControllerValidator ( BasePlatformPartValidator ): \"\"\"Validator for NoOpController platform_init_commands: Allow the user to specify platform initialization commands. The primary purpose of this is to allow verification testing between different simulators under different initial conditions without the complexity of actions being sent to those simulators. Dot notation is supported, so `foo.bar.baz` will call the method `self.parent_platform.foo.bar.baz()`. \"\"\" platform_init_commands : typing . List [ CommandWithArgs ] = [] @validator ( 'platform_init_commands' , pre = True , each_item = True ) def expand_command ( cls , v ): \"\"\"Convert simple string form to full form with arguments.\"\"\" if isinstance ( v , str ): return { 'command' : v } return v expand_command ( v ) classmethod \u00a4 Convert simple string form to full form with arguments. Source code in corl/simulators/base_parts.py @validator ( 'platform_init_commands' , pre = True , each_item = True ) def expand_command ( cls , v ): \"\"\"Convert simple string form to full form with arguments.\"\"\" if isinstance ( v , str ): return { 'command' : v } return v","title":"Base parts"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController","text":"BaseController base abstraction for a controller. A controller is used to move a platform with action commands. The actions are usually changing the desired rates or applied forces to the platform. Source code in corl/simulators/base_parts.py class BaseController ( BasePlatformPart , abc . ABC ): \"\"\" BaseController base abstraction for a controller. A controller is used to move a platform with action commands. The actions are usually changing the desired rates or applied forces to the platform. \"\"\" @property def control_properties ( self ) -> Prop : \"\"\" The properties of the control given to the apply_control function Returns ------- Prop The properties of the control given tot he apply_control function \"\"\" return self . _properties def validate_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to validate a control for this controller. Parameters ---------- control The control to be validated \"\"\" if not self . control_properties . create_space () . contains ( control ): raise ValueError ( f \" { type ( self ) . __name__ } control { control } not in space { self . control_properties . create_space () } values\" ) @abc . abstractmethod def apply_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to apply the control for this controller. Parameters ---------- control The control to be executed by the controller \"\"\" ... @abc . abstractmethod def get_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: \"\"\" Get the previously applied control that was given to the apply_control function Returns ------- previously applied control that was given to the apply_control function \"\"\" def get_validated_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: \"\"\" Get the previously applied control with nan check Returns ------- previously applied control that was given to the apply_control function \"\"\" return nan_check_result ( self . get_applied_control ())","title":"BaseController"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController.control_properties","text":"The properties of the control given to the apply_control function","title":"control_properties"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController.control_properties--returns","text":"Prop The properties of the control given tot he apply_control function","title":"Returns"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController.apply_control","text":"The generic method to apply the control for this controller.","title":"apply_control()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController.apply_control--parameters","text":"control The control to be executed by the controller Source code in corl/simulators/base_parts.py @abc . abstractmethod def apply_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to apply the control for this controller. Parameters ---------- control The control to be executed by the controller \"\"\" ...","title":"Parameters"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController.get_applied_control","text":"Get the previously applied control that was given to the apply_control function Returns previously applied control that was given to the apply_control function Source code in corl/simulators/base_parts.py @abc . abstractmethod def get_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: \"\"\" Get the previously applied control that was given to the apply_control function Returns ------- previously applied control that was given to the apply_control function \"\"\"","title":"get_applied_control()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController.get_validated_applied_control","text":"Get the previously applied control with nan check Returns previously applied control that was given to the apply_control function Source code in corl/simulators/base_parts.py def get_validated_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: \"\"\" Get the previously applied control with nan check Returns ------- previously applied control that was given to the apply_control function \"\"\" return nan_check_result ( self . get_applied_control ())","title":"get_validated_applied_control()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController.validate_control","text":"The generic method to validate a control for this controller.","title":"validate_control()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseController.validate_control--parameters","text":"control The control to be validated Source code in corl/simulators/base_parts.py def validate_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to validate a control for this controller. Parameters ---------- control The control to be validated \"\"\" if not self . control_properties . create_space () . contains ( control ): raise ValueError ( f \" { type ( self ) . __name__ } control { control } not in space { self . control_properties . create_space () } values\" )","title":"Parameters"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart","text":"BasePlatformPart abstract class for the classes that will be part of the BasePlatform. This includes controls,and sensors Source code in corl/simulators/base_parts.py class BasePlatformPart ( abc . ABC ): \"\"\" BasePlatformPart abstract class for the classes that will be part of the BasePlatform. This includes controls,and sensors \"\"\" def __init__ ( self , parent_platform , config , property_class ) -> None : config [ \"part_class\" ] = self . __class__ self . config = self . get_validator ( ** config ) self . _properties = property_class ( ** self . config . properties ) self . _parent_platform = parent_platform self . _valid = self . config . initial_validity @property def valid ( self ) -> bool : \"\"\" Signifies if measurements from this part should be trusted as having data that is valid or important to training Returns ------- bool The current state of the part being valid or not \"\"\" return self . _valid def set_valid ( self ) -> None : \"\"\" Signifies that this part should transition to being valid \"\"\" self . _valid = True def set_invalid ( self ) -> None : \"\"\" Signifies that this part should transition to being invalid \"\"\" self . _valid = False @property def name ( self ) -> typing . Optional [ str ]: \"\"\" The name for this platform part Returns ------- str The name for this platform part \"\"\" return self . config . name @property def parent_platform ( self ) -> 'BasePlatform' : # type: ignore # noqa: F821 \"\"\" The parent platform this platform part is attached to Returns ------- BasePlatform The parent platform this platform part is attached to \"\"\" return self . _parent_platform @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return set () @property def get_validator ( self ) -> typing . Type [ BasePlatformPartValidator ]: \"\"\" return the validator that will be used on the configuration of this part \"\"\" return BasePlatformPartValidator @classmethod def embed_properties ( cls , property_class : typing . Type [ Prop ]) -> typing . Type [ BasePlatformPart ]: \"\"\"Embed the properties in the class definition.\"\"\" class DynamicPlatformPart ( cls ): # type: ignore[valid-type,misc] # pylint: disable=missing-class-docstring def __init__ ( self , parent_platform , config ) -> None : super () . __init__ ( parent_platform = parent_platform , config = config , property_class = property_class ) DynamicPlatformPart . __doc__ = cls . __doc__ DynamicPlatformPart . __name__ += f ': { cls . __name__ } : { property_class . __name__ } ' DynamicPlatformPart . __qualname__ += f ': { cls . __name__ } : { property_class . __name__ } ' DynamicPlatformPart . embedded_properties = property_class return DynamicPlatformPart","title":"BasePlatformPart"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.get_validator","text":"return the validator that will be used on the configuration of this part","title":"get_validator"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.name","text":"The name for this platform part","title":"name"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.name--returns","text":"str The name for this platform part","title":"Returns"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.parent_platform","text":"The parent platform this platform part is attached to","title":"parent_platform"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.parent_platform--returns","text":"BasePlatform The parent platform this platform part is attached to","title":"Returns"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.valid","text":"Signifies if measurements from this part should be trusted as having data that is valid or important to training","title":"valid"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.valid--returns","text":"bool The current state of the part being valid or not","title":"Returns"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.embed_properties","text":"Embed the properties in the class definition. Source code in corl/simulators/base_parts.py @classmethod def embed_properties ( cls , property_class : typing . Type [ Prop ]) -> typing . Type [ BasePlatformPart ]: \"\"\"Embed the properties in the class definition.\"\"\" class DynamicPlatformPart ( cls ): # type: ignore[valid-type,misc] # pylint: disable=missing-class-docstring def __init__ ( self , parent_platform , config ) -> None : super () . __init__ ( parent_platform = parent_platform , config = config , property_class = property_class ) DynamicPlatformPart . __doc__ = cls . __doc__ DynamicPlatformPart . __name__ += f ': { cls . __name__ } : { property_class . __name__ } ' DynamicPlatformPart . __qualname__ += f ': { cls . __name__ } : { property_class . __name__ } ' DynamicPlatformPart . embedded_properties = property_class return DynamicPlatformPart","title":"embed_properties()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.set_invalid","text":"Signifies that this part should transition to being invalid Source code in corl/simulators/base_parts.py def set_invalid ( self ) -> None : \"\"\" Signifies that this part should transition to being invalid \"\"\" self . _valid = False","title":"set_invalid()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPart.set_valid","text":"Signifies that this part should transition to being valid Source code in corl/simulators/base_parts.py def set_valid ( self ) -> None : \"\"\" Signifies that this part should transition to being valid \"\"\" self . _valid = True","title":"set_valid()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPartValidator","text":"name: the optional name for this part, the class name will be used otherwise Source code in corl/simulators/base_parts.py class BasePlatformPartValidator ( BaseModel ): \"\"\" name: the optional name for this part, the class name will be used otherwise \"\"\" part_class : PyObject name : typing . Optional [ str ] = None initial_validity : bool = True properties : typing . Optional [ typing . Dict ] = dict () @validator ( 'name' , always = True ) def check_name ( cls , v , values ): \"\"\"Check if agent subclass AgentBase\"\"\" if v is None : assert 'part_class' in values v = PluginLibrary . FindGroup ( values [ 'part_class' ]) return v","title":"BasePlatformPartValidator"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BasePlatformPartValidator.check_name","text":"Check if agent subclass AgentBase Source code in corl/simulators/base_parts.py @validator ( 'name' , always = True ) def check_name ( cls , v , values ): \"\"\"Check if agent subclass AgentBase\"\"\" if v is None : assert 'part_class' in values v = PluginLibrary . FindGroup ( values [ 'part_class' ]) return v","title":"check_name()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseSensor","text":"BaseSensor base abstraction for a sensor. A sensor is a attached to a platform and provides information about the environment. Source code in corl/simulators/base_parts.py class BaseSensor ( BasePlatformPart , abc . ABC ): \"\"\" BaseSensor base abstraction for a sensor. A sensor is a attached to a platform and provides information about the environment. \"\"\" def __init__ ( self , parent_platform , config , property_class ) -> None : super () . __init__ ( parent_platform = parent_platform , config = config , property_class = property_class ) self . _last_measurement : typing . Optional [ typing . Union [ np . ndarray , typing . Tuple , typing . Dict ]] = None @property def measurement_properties ( self ) -> Prop : \"\"\" The properties of the object returned by the get_measurement function Returns ------- Prop The properties of the measurement returned by the get_measurement function \"\"\" return self . _properties @abc . abstractmethod def _calculate_measurement ( self , state : typing . Tuple ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict ]: \"\"\" The generic method to get calculate measurements from this sensor. This is used to calculate the measurement to be returned by the get_measurement function. This allows caching the measurement to avoid re-calculation Parameters ---------- state: typing.Tuple The current state of the environment used to obtain the measurement Returns ------- typing.Union[np.ndarray, typing.Tuple, typing.Dict] The measurements from this sensor \"\"\" ... def calculate_and_cache_measurement ( self , state : typing . Tuple ): \"\"\" Calculates the measurement and caches the result in the _last_measurement variable Parameters ---------- state: typing.Tuple The current state of the environment used to obtain the measurement \"\"\" measurement = self . _calculate_measurement ( state ) try : nan_check_result ( measurement , True ) self . _last_measurement = measurement except ValueError : warnings . warn ( \"The mover has produced a state that is invalid - NaNs --- Code is going to set broken/damaged - Reuse last state\" ) # raise ValueError(f\"Error calculating Measurement in {self.__class__}\\n\" f\"Measurement: {self._last_measurement}\\n\") from err if self . _last_measurement is None : raise ValueError ( 'Measurement is None' ) def get_measurement ( self ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict , typing . List ]: \"\"\" The generic method to get measurements from this sensor. Returns ------- typing.Union[np.ndarray, typing.Tuple, typing.Dict] The measurements from this sensor \"\"\" if self . _last_measurement is None : raise ValueError ( f 'Measurement is None - may also want to check operable states - ( { type ( self ) } )' ) return self . _last_measurement","title":"BaseSensor"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseSensor.measurement_properties","text":"The properties of the object returned by the get_measurement function","title":"measurement_properties"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseSensor.measurement_properties--returns","text":"Prop The properties of the measurement returned by the get_measurement function","title":"Returns"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseSensor.calculate_and_cache_measurement","text":"Calculates the measurement and caches the result in the _last_measurement variable","title":"calculate_and_cache_measurement()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseSensor.calculate_and_cache_measurement--parameters","text":"typing.Tuple The current state of the environment used to obtain the measurement Source code in corl/simulators/base_parts.py def calculate_and_cache_measurement ( self , state : typing . Tuple ): \"\"\" Calculates the measurement and caches the result in the _last_measurement variable Parameters ---------- state: typing.Tuple The current state of the environment used to obtain the measurement \"\"\" measurement = self . _calculate_measurement ( state ) try : nan_check_result ( measurement , True ) self . _last_measurement = measurement except ValueError : warnings . warn ( \"The mover has produced a state that is invalid - NaNs --- Code is going to set broken/damaged - Reuse last state\" ) # raise ValueError(f\"Error calculating Measurement in {self.__class__}\\n\" f\"Measurement: {self._last_measurement}\\n\") from err if self . _last_measurement is None : raise ValueError ( 'Measurement is None' )","title":"Parameters"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseSensor.get_measurement","text":"The generic method to get measurements from this sensor.","title":"get_measurement()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseSensor.get_measurement--returns","text":"typing.Union[np.ndarray, typing.Tuple, typing.Dict] The measurements from this sensor Source code in corl/simulators/base_parts.py def get_measurement ( self ) -> typing . Union [ np . ndarray , typing . Tuple , typing . Dict , typing . List ]: \"\"\" The generic method to get measurements from this sensor. Returns ------- typing.Union[np.ndarray, typing.Tuple, typing.Dict] The measurements from this sensor \"\"\" if self . _last_measurement is None : raise ValueError ( f 'Measurement is None - may also want to check operable states - ( { type ( self ) } )' ) return self . _last_measurement","title":"Returns"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.BaseTimeSensor","text":"Base type for a time sensor Source code in corl/simulators/base_parts.py class BaseTimeSensor ( BaseSensor ): \"\"\"Base type for a time sensor Arguments: BaseSensor -- The base class type for the sensor \"\"\" def __init__ ( self , parent_platform , config : typing . Dict = None ): super () . __init__ ( parent_platform , config , base_props . TimeProp )","title":"BaseTimeSensor"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.CommandWithArgs","text":"Model for a command with its argumets. Source code in corl/simulators/base_parts.py class CommandWithArgs ( BaseModel ): # type: ignore[no-redef] \"\"\"Model for a command with its argumets.\"\"\" command : str args : typing . List [ typing . Any ] = [] kwargs : typing . Dict [ str , typing . Any ] = {}","title":"CommandWithArgs"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.MutuallyExclusiveParts","text":"Class to check to see/controll mutually exclusive parts Source code in corl/simulators/base_parts.py class MutuallyExclusiveParts (): \"\"\" Class to check to see/controll mutually exclusive parts \"\"\" def __init__ ( self , exclusive_parts , allow_other_keys = False ): \"\"\" exclusive_parts: The parts to check exclusivity allow_other_keys: allows exclusivity other than the parts defined here \"\"\" self . _exclusive_parts = exclusive_parts self . allow_other_keys = allow_other_keys def are_platform_parts_mutually_exclusive ( self , * args : BasePlatformPart ): \"\"\"Checks to see if platform parts are mutually exclusive Returns: [type] -- [description] \"\"\" parts = [ platform_part . exclusiveness for platform_part in args ] return self . are_parts_mutually_exclusive ( * parts ) def are_parts_mutually_exclusive ( self , * args ): \"\"\"[summary] Returns: [type] -- [description] \"\"\" total_set = set () for part_exclusive_set in args : for part_exclusiveness in part_exclusive_set : if part_exclusiveness in total_set : return False if part_exclusiveness in self . _exclusive_parts : total_set . add ( part_exclusiveness ) elif not self . allow_other_keys : raise RuntimeError ( f \"Error: you attempted to add a part with the exclusivness { part_exclusiveness } , but \" f \"this plaforms exclusive parts for this component type were { self . _exclusive_parts } , \" \"and the platform specified to not allow other exclusivity\" ) return True def get_duplicate_parts ( self , * args ): \"\"\"[summary] Returns: [type] -- [description] \"\"\" total_list = [] for mutually_exclusive_part in args : total_list += list ( mutually_exclusive_part . exclusiveness ) ret_list = [] for exclusive_key in self . _exclusive_parts : if total_list . count ( exclusive_key ) > 1 : ret_list . append ( exclusive_key ) return ret_list","title":"MutuallyExclusiveParts"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.MutuallyExclusiveParts.__init__","text":"exclusive_parts: The parts to check exclusivity allow_other_keys: allows exclusivity other than the parts defined here Source code in corl/simulators/base_parts.py def __init__ ( self , exclusive_parts , allow_other_keys = False ): \"\"\" exclusive_parts: The parts to check exclusivity allow_other_keys: allows exclusivity other than the parts defined here \"\"\" self . _exclusive_parts = exclusive_parts self . allow_other_keys = allow_other_keys","title":"__init__()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.MutuallyExclusiveParts.are_parts_mutually_exclusive","text":"[summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/base_parts.py def are_parts_mutually_exclusive ( self , * args ): \"\"\"[summary] Returns: [type] -- [description] \"\"\" total_set = set () for part_exclusive_set in args : for part_exclusiveness in part_exclusive_set : if part_exclusiveness in total_set : return False if part_exclusiveness in self . _exclusive_parts : total_set . add ( part_exclusiveness ) elif not self . allow_other_keys : raise RuntimeError ( f \"Error: you attempted to add a part with the exclusivness { part_exclusiveness } , but \" f \"this plaforms exclusive parts for this component type were { self . _exclusive_parts } , \" \"and the platform specified to not allow other exclusivity\" ) return True","title":"are_parts_mutually_exclusive()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.MutuallyExclusiveParts.are_platform_parts_mutually_exclusive","text":"Checks to see if platform parts are mutually exclusive Returns: Type Description [type] -- [description] Source code in corl/simulators/base_parts.py def are_platform_parts_mutually_exclusive ( self , * args : BasePlatformPart ): \"\"\"Checks to see if platform parts are mutually exclusive Returns: [type] -- [description] \"\"\" parts = [ platform_part . exclusiveness for platform_part in args ] return self . are_parts_mutually_exclusive ( * parts )","title":"are_platform_parts_mutually_exclusive()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.MutuallyExclusiveParts.get_duplicate_parts","text":"[summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/base_parts.py def get_duplicate_parts ( self , * args ): \"\"\"[summary] Returns: [type] -- [description] \"\"\" total_list = [] for mutually_exclusive_part in args : total_list += list ( mutually_exclusive_part . exclusiveness ) ret_list = [] for exclusive_key in self . _exclusive_parts : if total_list . count ( exclusive_key ) > 1 : ret_list . append ( exclusive_key ) return ret_list","title":"get_duplicate_parts()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.NoOpController","text":"NoOpController controller define by empty prop can be use when simulator is creating actions for an agent. Source code in corl/simulators/base_parts.py class NoOpController ( BaseController ): \"\"\" NoOpController controller define by empty prop can be use when simulator is creating actions for an agent. \"\"\" def __init__ ( self , parent_platform , config , property_class ) -> None : self . config : NoOpControllerValidator super () . __init__ ( parent_platform = parent_platform , config = config , property_class = property_class ) self . new_control = np . array ([], dtype = np . float32 ) for command_data in self . config . platform_init_commands : func = attrgetter ( command_data . command ) func ( self . parent_platform )( * command_data . args , ** command_data . kwargs ) @property def get_validator ( self ) -> typing . Type [ NoOpControllerValidator ]: \"\"\"Validator for NoOpController\"\"\" return NoOpControllerValidator def apply_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to apply the control for this controller. Parameters ---------- control The control to be executed by the controller \"\"\" self . new_control = control def get_applied_control ( self ): return self . new_control","title":"NoOpController"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.NoOpController.get_validator","text":"Validator for NoOpController","title":"get_validator"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.NoOpController.apply_control","text":"The generic method to apply the control for this controller.","title":"apply_control()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.NoOpController.apply_control--parameters","text":"control The control to be executed by the controller Source code in corl/simulators/base_parts.py def apply_control ( self , control : np . ndarray ) -> None : \"\"\" The generic method to apply the control for this controller. Parameters ---------- control The control to be executed by the controller \"\"\" self . new_control = control","title":"Parameters"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.NoOpController.get_applied_control","text":"Get the previously applied control that was given to the apply_control function Returns previously applied control that was given to the apply_control function Source code in corl/simulators/base_parts.py def get_applied_control ( self ): return self . new_control","title":"get_applied_control()"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.NoOpControllerValidator","text":"Validator for NoOpController platform_init_commands: Allow the user to specify platform initialization commands. The primary purpose of this is to allow verification testing between different simulators under different initial conditions without the complexity of actions being sent to those simulators. Dot notation is supported, so foo.bar.baz will call the method self.parent_platform.foo.bar.baz() . Source code in corl/simulators/base_parts.py class NoOpControllerValidator ( BasePlatformPartValidator ): \"\"\"Validator for NoOpController platform_init_commands: Allow the user to specify platform initialization commands. The primary purpose of this is to allow verification testing between different simulators under different initial conditions without the complexity of actions being sent to those simulators. Dot notation is supported, so `foo.bar.baz` will call the method `self.parent_platform.foo.bar.baz()`. \"\"\" platform_init_commands : typing . List [ CommandWithArgs ] = [] @validator ( 'platform_init_commands' , pre = True , each_item = True ) def expand_command ( cls , v ): \"\"\"Convert simple string form to full form with arguments.\"\"\" if isinstance ( v , str ): return { 'command' : v } return v","title":"NoOpControllerValidator"},{"location":"reference/simulators/base_parts/#corl.simulators.base_parts.NoOpControllerValidator.expand_command","text":"Convert simple string form to full form with arguments. Source code in corl/simulators/base_parts.py @validator ( 'platform_init_commands' , pre = True , each_item = True ) def expand_command ( cls , v ): \"\"\"Convert simple string form to full form with arguments.\"\"\" if isinstance ( v , str ): return { 'command' : v } return v","title":"expand_command()"},{"location":"reference/simulators/base_platform/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Base Platform Abstract Class Object BasePlatform ( ABC ) \u00a4 BasePlatform base abstraction for a platform object. Platforms can be aircraft, land vehicles, ground radar, satellites etc. Platforms have platform properties that describe the platform, for example position, velocity, etc. Platforms have platform parts which consist of controls, or sensors. Source code in corl/simulators/base_platform.py class BasePlatform ( abc . ABC ): \"\"\" BasePlatform base abstraction for a platform object. Platforms can be aircraft, land vehicles, ground radar, satellites etc. Platforms have platform properties that describe the platform, for example position, velocity, etc. Platforms have platform parts which consist of controls, or sensors. \"\"\" def __init__ ( self , ** kwargs ): self . config : BasePlatformValidator = self . get_validator ( ** kwargs ) # set default parts and mutually exclusive parts if self . config . exclusive_part_dict is None or self . config . disable_exclusivity_check : self . config . exclusive_part_dict = { BaseController : MutuallyExclusiveParts ( set (), allow_other_keys = self . config . disable_exclusivity_check ), BaseSensor : MutuallyExclusiveParts ( set (), allow_other_keys = self . config . disable_exclusivity_check ), } self . _exclusive_parts = self . config . exclusive_part_dict # self._platform = plaform self . _name = self . config . platform_name self . _sensors = self . _get_part_list ( self . config . parts_list , BaseSensor ) self . _controllers = self . _get_part_list ( self . config . parts_list , BaseController ) self . verify_unique_parts () @property def get_validator ( self ) -> typing . Type [ BasePlatformValidator ]: \"\"\" get validator for this BasePlatform Returns: BasePlatformValidator -- validator the platform will use to generate a configuration \"\"\" return BasePlatformValidator def _get_part_list ( self , part_class_list , part_base_class ): \"\"\" Get a list of the platform parts after being wrapped in the base integration classes. This will do the mapping from an SimSensor to a type of BaseSensor for example. Some parts have no matching sim counterpart and will not have None in the map instead of the part name Parameters ---------- part_class_map: mapping of the part name to the pairs of base classes and config for that part parts: the parts we will wrap with the above mapping Returns ------- A list of the wrapped platform parts \"\"\" part_list = [ part_class ( self , part_config ) for part_class , part_config in part_class_list if issubclass ( part_class , part_base_class ) ] if not self . _exclusive_parts [ part_base_class ] . are_platform_parts_mutually_exclusive ( * part_list ): lister = self . _exclusive_parts [ part_base_class ] . get_duplicate_parts ( * part_list ) fail_string = \" \\n \" . join ([ str ( temp_var ) for temp_var in lister ]) raise ValueError ( f \"Tried to use mutually exclusive controller platform parts: \\n { fail_string } \" ) return part_list def verify_unique_parts ( self ): \"\"\" Verify all parts have a unique name \"\"\" for parts in [ self . _sensors , self . _controllers ]: part_names = list () for part in parts : if part . name in part_names : raise RuntimeError ( \"The \" + part . name + \" part has a unique name, but it already exists\" ) part_names . append ( part . name ) @property def name ( self ) -> str : \"\"\" name the name of this object Returns ------- str The name of this object \"\"\" return self . _name @property @abc . abstractmethod def operable ( self ) -> bool : \"\"\"Is the platform operable? Returns ------- bool Is the platform operable? \"\"\" @property def sensors ( self ): \"\"\" Sensors attached to this platform Returns ------ List list of all sensors attached to this platform \"\"\" return self . _sensors @property def controllers ( self ): \"\"\" Controllers attached to this platform Returns ------ List list of all controllers attached to this platform \"\"\" return self . _controllers controllers property readonly \u00a4 Controllers attached to this platform Returns \u00a4 List list of all controllers attached to this platform get_validator : Type [ corl . simulators . base_platform . BasePlatformValidator ] property readonly \u00a4 get validator for this BasePlatform Returns: Type Description Type[corl.simulators.base_platform.BasePlatformValidator] BasePlatformValidator -- validator the platform will use to generate a configuration name : str property readonly \u00a4 name the name of this object Returns \u00a4 str The name of this object operable : bool property readonly \u00a4 Is the platform operable? Returns \u00a4 bool Is the platform operable? sensors property readonly \u00a4 Sensors attached to this platform Returns \u00a4 List list of all sensors attached to this platform verify_unique_parts ( self ) \u00a4 Verify all parts have a unique name Source code in corl/simulators/base_platform.py def verify_unique_parts ( self ): \"\"\" Verify all parts have a unique name \"\"\" for parts in [ self . _sensors , self . _controllers ]: part_names = list () for part in parts : if part . name in part_names : raise RuntimeError ( \"The \" + part . name + \" part has a unique name, but it already exists\" ) part_names . append ( part . name ) BasePlatformValidator ( BaseModel ) pydantic-model \u00a4 BasePlatformValidator Parameters \u00a4 !!! platform_name \"str\" name of platform !!! parts_list \"typing.List[typing.Tuple]\" list of parts the agent uses to interact with the platform !!! exclusive_part_dict \"typing.Optional[typing.Dict] = None\" list of mutually exlusive parts for platform !!! disable_exclusivity_check \"bool = False\" bool determine if part exclusivity should be check Source code in corl/simulators/base_platform.py class BasePlatformValidator ( BaseModel ): \"\"\"BasePlatformValidator Parameters ---------- platform_name: str name of platform parts_list: typing.List[typing.Tuple] list of parts the agent uses to interact with the platform exclusive_part_dict: typing.Optional[typing.Dict] = None list of mutually exlusive parts for platform disable_exclusivity_check: bool = False bool determine if part exclusivity should be check \"\"\" platform_name : str parts_list : typing . List [ typing . Tuple ] exclusive_part_dict : typing . Optional [ typing . Dict ] = None disable_exclusivity_check : bool = False class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True Config \u00a4 Allow arbitrary types for Parameter Source code in corl/simulators/base_platform.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True","title":"Base platform"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform","text":"BasePlatform base abstraction for a platform object. Platforms can be aircraft, land vehicles, ground radar, satellites etc. Platforms have platform properties that describe the platform, for example position, velocity, etc. Platforms have platform parts which consist of controls, or sensors. Source code in corl/simulators/base_platform.py class BasePlatform ( abc . ABC ): \"\"\" BasePlatform base abstraction for a platform object. Platforms can be aircraft, land vehicles, ground radar, satellites etc. Platforms have platform properties that describe the platform, for example position, velocity, etc. Platforms have platform parts which consist of controls, or sensors. \"\"\" def __init__ ( self , ** kwargs ): self . config : BasePlatformValidator = self . get_validator ( ** kwargs ) # set default parts and mutually exclusive parts if self . config . exclusive_part_dict is None or self . config . disable_exclusivity_check : self . config . exclusive_part_dict = { BaseController : MutuallyExclusiveParts ( set (), allow_other_keys = self . config . disable_exclusivity_check ), BaseSensor : MutuallyExclusiveParts ( set (), allow_other_keys = self . config . disable_exclusivity_check ), } self . _exclusive_parts = self . config . exclusive_part_dict # self._platform = plaform self . _name = self . config . platform_name self . _sensors = self . _get_part_list ( self . config . parts_list , BaseSensor ) self . _controllers = self . _get_part_list ( self . config . parts_list , BaseController ) self . verify_unique_parts () @property def get_validator ( self ) -> typing . Type [ BasePlatformValidator ]: \"\"\" get validator for this BasePlatform Returns: BasePlatformValidator -- validator the platform will use to generate a configuration \"\"\" return BasePlatformValidator def _get_part_list ( self , part_class_list , part_base_class ): \"\"\" Get a list of the platform parts after being wrapped in the base integration classes. This will do the mapping from an SimSensor to a type of BaseSensor for example. Some parts have no matching sim counterpart and will not have None in the map instead of the part name Parameters ---------- part_class_map: mapping of the part name to the pairs of base classes and config for that part parts: the parts we will wrap with the above mapping Returns ------- A list of the wrapped platform parts \"\"\" part_list = [ part_class ( self , part_config ) for part_class , part_config in part_class_list if issubclass ( part_class , part_base_class ) ] if not self . _exclusive_parts [ part_base_class ] . are_platform_parts_mutually_exclusive ( * part_list ): lister = self . _exclusive_parts [ part_base_class ] . get_duplicate_parts ( * part_list ) fail_string = \" \\n \" . join ([ str ( temp_var ) for temp_var in lister ]) raise ValueError ( f \"Tried to use mutually exclusive controller platform parts: \\n { fail_string } \" ) return part_list def verify_unique_parts ( self ): \"\"\" Verify all parts have a unique name \"\"\" for parts in [ self . _sensors , self . _controllers ]: part_names = list () for part in parts : if part . name in part_names : raise RuntimeError ( \"The \" + part . name + \" part has a unique name, but it already exists\" ) part_names . append ( part . name ) @property def name ( self ) -> str : \"\"\" name the name of this object Returns ------- str The name of this object \"\"\" return self . _name @property @abc . abstractmethod def operable ( self ) -> bool : \"\"\"Is the platform operable? Returns ------- bool Is the platform operable? \"\"\" @property def sensors ( self ): \"\"\" Sensors attached to this platform Returns ------ List list of all sensors attached to this platform \"\"\" return self . _sensors @property def controllers ( self ): \"\"\" Controllers attached to this platform Returns ------ List list of all controllers attached to this platform \"\"\" return self . _controllers","title":"BasePlatform"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.controllers","text":"Controllers attached to this platform","title":"controllers"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.controllers--returns","text":"List list of all controllers attached to this platform","title":"Returns"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.get_validator","text":"get validator for this BasePlatform Returns: Type Description Type[corl.simulators.base_platform.BasePlatformValidator] BasePlatformValidator -- validator the platform will use to generate a configuration","title":"get_validator"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.name","text":"name the name of this object","title":"name"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.name--returns","text":"str The name of this object","title":"Returns"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.operable","text":"Is the platform operable?","title":"operable"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.operable--returns","text":"bool Is the platform operable?","title":"Returns"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.sensors","text":"Sensors attached to this platform","title":"sensors"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.sensors--returns","text":"List list of all sensors attached to this platform","title":"Returns"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatform.verify_unique_parts","text":"Verify all parts have a unique name Source code in corl/simulators/base_platform.py def verify_unique_parts ( self ): \"\"\" Verify all parts have a unique name \"\"\" for parts in [ self . _sensors , self . _controllers ]: part_names = list () for part in parts : if part . name in part_names : raise RuntimeError ( \"The \" + part . name + \" part has a unique name, but it already exists\" ) part_names . append ( part . name )","title":"verify_unique_parts()"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatformValidator","text":"BasePlatformValidator","title":"BasePlatformValidator"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatformValidator--parameters","text":"!!! platform_name \"str\" name of platform !!! parts_list \"typing.List[typing.Tuple]\" list of parts the agent uses to interact with the platform !!! exclusive_part_dict \"typing.Optional[typing.Dict] = None\" list of mutually exlusive parts for platform !!! disable_exclusivity_check \"bool = False\" bool determine if part exclusivity should be check Source code in corl/simulators/base_platform.py class BasePlatformValidator ( BaseModel ): \"\"\"BasePlatformValidator Parameters ---------- platform_name: str name of platform parts_list: typing.List[typing.Tuple] list of parts the agent uses to interact with the platform exclusive_part_dict: typing.Optional[typing.Dict] = None list of mutually exlusive parts for platform disable_exclusivity_check: bool = False bool determine if part exclusivity should be check \"\"\" platform_name : str parts_list : typing . List [ typing . Tuple ] exclusive_part_dict : typing . Optional [ typing . Dict ] = None disable_exclusivity_check : bool = False class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True","title":"Parameters"},{"location":"reference/simulators/base_platform/#corl.simulators.base_platform.BasePlatformValidator.Config","text":"Allow arbitrary types for Parameter Source code in corl/simulators/base_platform.py class Config : \"\"\"Allow arbitrary types for Parameter\"\"\" arbitrary_types_allowed = True","title":"Config"},{"location":"reference/simulators/base_properties/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. property definitions for sensors NoOpProp ( BoxProp ) pydantic-model \u00a4 TimeProp defines time space Source code in corl/simulators/base_properties.py class NoOpProp ( BoxProp ): \"\"\" TimeProp defines time space \"\"\" name : str = \"NoOp\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 0 , max_items = 0 )] = [] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 0 , max_items = 0 )] = [] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 0 , max_items = 0 )] = [] description : str = \"No op prop actions from this pace will not be sent to simulator\" TimeProp ( BoxProp ) pydantic-model \u00a4 TimeProp defines time space Source code in corl/simulators/base_properties.py class TimeProp ( BoxProp ): \"\"\" TimeProp defines time space \"\"\" name : str = \"time\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ np . inf ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"seconds\" ] description : str = \"The time of the sensor\"","title":"Base properties"},{"location":"reference/simulators/base_properties/#corl.simulators.base_properties.NoOpProp","text":"TimeProp defines time space Source code in corl/simulators/base_properties.py class NoOpProp ( BoxProp ): \"\"\" TimeProp defines time space \"\"\" name : str = \"NoOp\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 0 , max_items = 0 )] = [] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 0 , max_items = 0 )] = [] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 0 , max_items = 0 )] = [] description : str = \"No op prop actions from this pace will not be sent to simulator\"","title":"NoOpProp"},{"location":"reference/simulators/base_properties/#corl.simulators.base_properties.TimeProp","text":"TimeProp defines time space Source code in corl/simulators/base_properties.py class TimeProp ( BoxProp ): \"\"\" TimeProp defines time space \"\"\" name : str = \"time\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ np . inf ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"seconds\" ] description : str = \"The time of the sensor\"","title":"TimeProp"},{"location":"reference/simulators/base_simulator/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. AgentConfig ( BaseModel ) pydantic-model \u00a4 any configuration needed for the simulator to initialize this platform and configure it in the sim class a list of tuples where the first element is come python class path of a BasePart , and then the second element is a configuration dictionary for that part Source code in corl/simulators/base_simulator.py class AgentConfig ( BaseModel ): \"\"\" platform_config: any configuration needed for the simulator to initialize this platform and configure it in the sim class parts_list: a list of tuples where the first element is come python class path of a BasePart, and then the second element is a configuration dictionary for that part Arguments: BaseModel {[type]} -- [description] \"\"\" platform_config : typing . Union [ typing . Dict [ str , typing . Any ], BaseModel ] parts_list : typing . List [ typing . Tuple [ PyObject , typing . Dict [ str , typing . Any ]]] BaseSimulator ( ABC ) \u00a4 BaseSimulator is responsible for initializing the platform objects for a simulation and knowing how to setup episodes based on input parameters from a parameter provider it is also responsible for reporting the simulation state at each timestep Source code in corl/simulators/base_simulator.py class BaseSimulator ( ABC ): \"\"\" BaseSimulator is responsible for initializing the platform objects for a simulation and knowing how to setup episodes based on input parameters from a parameter provider it is also responsible for reporting the simulation state at each timestep \"\"\" def __init__ ( self , ** kwargs ): self . config = self . get_simulator_validator ( ** kwargs ) @property def get_simulator_validator ( self ) -> typing . Type [ BaseSimulatorValidator ]: \"\"\" returns the validator for the configuration options to the simulator the kwargs to this class are validated and put into a defined struct potentially raising based on invalid configurations Returns: BaseSimulatorValidator -- The validator to use for this simulation class \"\"\" return BaseSimulatorValidator @property def get_reset_validator ( self ) -> typing . Type [ BaseSimulatorResetValidator ]: \"\"\" returns the validator that can be used to validate episode parameters coming into the reset function from the environment class Returns: BaseSimulatorResetValidator -- The validator to use during resets \"\"\" return BaseSimulatorResetValidator @property def frame_rate ( self ) -> float : \"\"\"Return the frame rate (in Hz) this simulator will run at\"\"\" return self . config . frame_rate @abstractmethod def reset ( self , config : typing . Dict [ str , typing . Any ]) -> StateDict : \"\"\" reset resets the simulation and sets up a new episode Arguments: config {typing.Dict[str, typing.Any]} -- The parameters to validate and use to setup this episode Returns: StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation \"\"\" ... @abstractmethod def step ( self ) -> StateDict : \"\"\" advances the simulation platforms and returns the state Returns: StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation \"\"\" ... @abstractproperty def sim_time ( self ) -> float : \"\"\" returns the time Returns: float - time \"\"\" ... @abstractproperty def platforms ( self ) -> typing . List : \"\"\" returns a list of platforms in the simulation Returns: list of platforms \"\"\" ... @abstractmethod def mark_episode_done ( self , done_info : OrderedDict , episode_state : OrderedDict ): \"\"\" Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Arguments: done_info {OrderedDict} -- The Dict describing which Done conditions ended an episode episode_state {OrderedDict} -- The episode state at the end of the simulation \"\"\" ... @abstractmethod def save_episode_information ( self , dones , rewards , observations ): \"\"\" provides a way to save information about the current episode based on the environment Arguments: dones {[type]} -- the current done info of the step rewards {[type]} -- the reward info for this step observations {[type]} -- the observations for this step \"\"\" ... def render ( self , state , mode = \"human\" ): # pylint: disable=unused-argument \"\"\" allows you to do something to render your simulation you are responsible for checking which worker/vector index you are on \"\"\" ... def delete_platform ( self , name ): # pylint: disable=unused-argument \"\"\" provides a way to delete a platform from the simulation \"\"\" ... frame_rate : float property readonly \u00a4 Return the frame rate (in Hz) this simulator will run at get_reset_validator : Type [ corl . simulators . base_simulator . BaseSimulatorResetValidator ] property readonly \u00a4 returns the validator that can be used to validate episode parameters coming into the reset function from the environment class Returns: Type Description Type[corl.simulators.base_simulator.BaseSimulatorResetValidator] BaseSimulatorResetValidator -- The validator to use during resets get_simulator_validator : Type [ corl . simulators . base_simulator . BaseSimulatorValidator ] property readonly \u00a4 returns the validator for the configuration options to the simulator the kwargs to this class are validated and put into a defined struct potentially raising based on invalid configurations Returns: Type Description Type[corl.simulators.base_simulator.BaseSimulatorValidator] BaseSimulatorValidator -- The validator to use for this simulation class platforms : List property readonly \u00a4 returns a list of platforms in the simulation Returns: Type Description List list of platforms sim_time : float property readonly \u00a4 returns the time Returns: Type Description float float - time delete_platform ( self , name ) \u00a4 provides a way to delete a platform from the simulation Source code in corl/simulators/base_simulator.py def delete_platform ( self , name ): # pylint: disable=unused-argument \"\"\" provides a way to delete a platform from the simulation \"\"\" ... mark_episode_done ( self , done_info , episode_state ) \u00a4 Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Source code in corl/simulators/base_simulator.py @abstractmethod def mark_episode_done ( self , done_info : OrderedDict , episode_state : OrderedDict ): \"\"\" Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Arguments: done_info {OrderedDict} -- The Dict describing which Done conditions ended an episode episode_state {OrderedDict} -- The episode state at the end of the simulation \"\"\" ... render ( self , state , mode = 'human' ) \u00a4 allows you to do something to render your simulation you are responsible for checking which worker/vector index you are on Source code in corl/simulators/base_simulator.py def render ( self , state , mode = \"human\" ): # pylint: disable=unused-argument \"\"\" allows you to do something to render your simulation you are responsible for checking which worker/vector index you are on \"\"\" ... reset ( self , config ) \u00a4 reset resets the simulation and sets up a new episode Returns: Type Description StateDict StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/base_simulator.py @abstractmethod def reset ( self , config : typing . Dict [ str , typing . Any ]) -> StateDict : \"\"\" reset resets the simulation and sets up a new episode Arguments: config {typing.Dict[str, typing.Any]} -- The parameters to validate and use to setup this episode Returns: StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation \"\"\" ... save_episode_information ( self , dones , rewards , observations ) \u00a4 provides a way to save information about the current episode based on the environment Source code in corl/simulators/base_simulator.py @abstractmethod def save_episode_information ( self , dones , rewards , observations ): \"\"\" provides a way to save information about the current episode based on the environment Arguments: dones {[type]} -- the current done info of the step rewards {[type]} -- the reward info for this step observations {[type]} -- the observations for this step \"\"\" ... step ( self ) \u00a4 advances the simulation platforms and returns the state Returns: Type Description StateDict StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/base_simulator.py @abstractmethod def step ( self ) -> StateDict : \"\"\" advances the simulation platforms and returns the state Returns: StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation \"\"\" ... BaseSimulatorResetValidator ( BaseModel ) pydantic-model \u00a4 Validator to use to validate the reset input to a simulator class allows the simulator class to take EPP params and structure/validate them Note that all attributes in this validator need to survive being parsed by validation_helper_units_and_parameters. Subclasses can redefine platforms to make the typing.Any more restrictive. It must remain a dictionary with keys named for the platforms in the simulation. Source code in corl/simulators/base_simulator.py class BaseSimulatorResetValidator ( BaseModel ): \"\"\" Validator to use to validate the reset input to a simulator class allows the simulator class to take EPP params and structure/validate them Note that all attributes in this validator need to survive being parsed by validation_helper_units_and_parameters. Subclasses can redefine `platforms` to make the `typing.Any` more restrictive. It must remain a dictionary with keys named for the platforms in the simulation. \"\"\" platforms : typing . Dict [ str , typing . Any ] = {} BaseSimulatorValidator ( BaseModel ) pydantic-model \u00a4 worker_index: what worker this simulator class is running on < used for render vector_index: what vector index this simulator class is running on < used for render agent_configs: the mapping of agent names to a dict describing the platform this bool should be used to tell downstream platforms that the user wishes to disable any mutually exclusive parts checking on a platform this should pretty much only be used for behavior tree type agents frame_rate: the rate the simulator should run at (in Hz) Source code in corl/simulators/base_simulator.py class BaseSimulatorValidator ( BaseModel ): \"\"\" worker_index: what worker this simulator class is running on < used for render vector_index: what vector index this simulator class is running on < used for render agent_configs: the mapping of agent names to a dict describing the platform disable_exclusivity_check: this bool should be used to tell downstream platforms that the user wishes to disable any mutually exclusive parts checking on a platform this should pretty much only be used for behavior tree type agents frame_rate: the rate the simulator should run at (in Hz) \"\"\" worker_index : int = 0 vector_index : int = 0 agent_configs : typing . Mapping [ str , AgentConfig ] disable_exclusivity_check : bool = False frame_rate : float = 1.0 validation_helper_units_and_parameters ( value ) \u00a4 Recursively inspect a dictionary, converting ValueWithUnits and Factory Source code in corl/simulators/base_simulator.py @validate_arguments def validation_helper_units_and_parameters ( value : typing . Dict [ str , typing . Any ]) -> typing . Dict [ str , typing . Any ]: \"\"\"Recursively inspect a dictionary, converting ValueWithUnits and Factory\"\"\" output : typing . Dict [ str , typing . Any ] = {} for k , v in value . items (): try : elem = parse_obj_as ( ValueWithUnits , v ) except ValidationError : pass else : output [ k ] = elem continue try : factory = parse_obj_as ( Factory , v ) except ValidationError : pass else : output [ k ] = factory . build () continue try : output [ k ] = validation_helper_units_and_parameters ( v ) except ValidationError : pass else : continue output [ k ] = v return output","title":"Base simulator"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.AgentConfig","text":"any configuration needed for the simulator to initialize this platform and configure it in the sim class a list of tuples where the first element is come python class path of a BasePart , and then the second element is a configuration dictionary for that part Source code in corl/simulators/base_simulator.py class AgentConfig ( BaseModel ): \"\"\" platform_config: any configuration needed for the simulator to initialize this platform and configure it in the sim class parts_list: a list of tuples where the first element is come python class path of a BasePart, and then the second element is a configuration dictionary for that part Arguments: BaseModel {[type]} -- [description] \"\"\" platform_config : typing . Union [ typing . Dict [ str , typing . Any ], BaseModel ] parts_list : typing . List [ typing . Tuple [ PyObject , typing . Dict [ str , typing . Any ]]]","title":"AgentConfig"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator","text":"BaseSimulator is responsible for initializing the platform objects for a simulation and knowing how to setup episodes based on input parameters from a parameter provider it is also responsible for reporting the simulation state at each timestep Source code in corl/simulators/base_simulator.py class BaseSimulator ( ABC ): \"\"\" BaseSimulator is responsible for initializing the platform objects for a simulation and knowing how to setup episodes based on input parameters from a parameter provider it is also responsible for reporting the simulation state at each timestep \"\"\" def __init__ ( self , ** kwargs ): self . config = self . get_simulator_validator ( ** kwargs ) @property def get_simulator_validator ( self ) -> typing . Type [ BaseSimulatorValidator ]: \"\"\" returns the validator for the configuration options to the simulator the kwargs to this class are validated and put into a defined struct potentially raising based on invalid configurations Returns: BaseSimulatorValidator -- The validator to use for this simulation class \"\"\" return BaseSimulatorValidator @property def get_reset_validator ( self ) -> typing . Type [ BaseSimulatorResetValidator ]: \"\"\" returns the validator that can be used to validate episode parameters coming into the reset function from the environment class Returns: BaseSimulatorResetValidator -- The validator to use during resets \"\"\" return BaseSimulatorResetValidator @property def frame_rate ( self ) -> float : \"\"\"Return the frame rate (in Hz) this simulator will run at\"\"\" return self . config . frame_rate @abstractmethod def reset ( self , config : typing . Dict [ str , typing . Any ]) -> StateDict : \"\"\" reset resets the simulation and sets up a new episode Arguments: config {typing.Dict[str, typing.Any]} -- The parameters to validate and use to setup this episode Returns: StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation \"\"\" ... @abstractmethod def step ( self ) -> StateDict : \"\"\" advances the simulation platforms and returns the state Returns: StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation \"\"\" ... @abstractproperty def sim_time ( self ) -> float : \"\"\" returns the time Returns: float - time \"\"\" ... @abstractproperty def platforms ( self ) -> typing . List : \"\"\" returns a list of platforms in the simulation Returns: list of platforms \"\"\" ... @abstractmethod def mark_episode_done ( self , done_info : OrderedDict , episode_state : OrderedDict ): \"\"\" Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Arguments: done_info {OrderedDict} -- The Dict describing which Done conditions ended an episode episode_state {OrderedDict} -- The episode state at the end of the simulation \"\"\" ... @abstractmethod def save_episode_information ( self , dones , rewards , observations ): \"\"\" provides a way to save information about the current episode based on the environment Arguments: dones {[type]} -- the current done info of the step rewards {[type]} -- the reward info for this step observations {[type]} -- the observations for this step \"\"\" ... def render ( self , state , mode = \"human\" ): # pylint: disable=unused-argument \"\"\" allows you to do something to render your simulation you are responsible for checking which worker/vector index you are on \"\"\" ... def delete_platform ( self , name ): # pylint: disable=unused-argument \"\"\" provides a way to delete a platform from the simulation \"\"\" ...","title":"BaseSimulator"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.frame_rate","text":"Return the frame rate (in Hz) this simulator will run at","title":"frame_rate"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.get_reset_validator","text":"returns the validator that can be used to validate episode parameters coming into the reset function from the environment class Returns: Type Description Type[corl.simulators.base_simulator.BaseSimulatorResetValidator] BaseSimulatorResetValidator -- The validator to use during resets","title":"get_reset_validator"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.get_simulator_validator","text":"returns the validator for the configuration options to the simulator the kwargs to this class are validated and put into a defined struct potentially raising based on invalid configurations Returns: Type Description Type[corl.simulators.base_simulator.BaseSimulatorValidator] BaseSimulatorValidator -- The validator to use for this simulation class","title":"get_simulator_validator"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.platforms","text":"returns a list of platforms in the simulation Returns: Type Description List list of platforms","title":"platforms"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.sim_time","text":"returns the time Returns: Type Description float float - time","title":"sim_time"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.delete_platform","text":"provides a way to delete a platform from the simulation Source code in corl/simulators/base_simulator.py def delete_platform ( self , name ): # pylint: disable=unused-argument \"\"\" provides a way to delete a platform from the simulation \"\"\" ...","title":"delete_platform()"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.mark_episode_done","text":"Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Source code in corl/simulators/base_simulator.py @abstractmethod def mark_episode_done ( self , done_info : OrderedDict , episode_state : OrderedDict ): \"\"\" Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Arguments: done_info {OrderedDict} -- The Dict describing which Done conditions ended an episode episode_state {OrderedDict} -- The episode state at the end of the simulation \"\"\" ...","title":"mark_episode_done()"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.render","text":"allows you to do something to render your simulation you are responsible for checking which worker/vector index you are on Source code in corl/simulators/base_simulator.py def render ( self , state , mode = \"human\" ): # pylint: disable=unused-argument \"\"\" allows you to do something to render your simulation you are responsible for checking which worker/vector index you are on \"\"\" ...","title":"render()"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.reset","text":"reset resets the simulation and sets up a new episode Returns: Type Description StateDict StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/base_simulator.py @abstractmethod def reset ( self , config : typing . Dict [ str , typing . Any ]) -> StateDict : \"\"\" reset resets the simulation and sets up a new episode Arguments: config {typing.Dict[str, typing.Any]} -- The parameters to validate and use to setup this episode Returns: StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation \"\"\" ...","title":"reset()"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.save_episode_information","text":"provides a way to save information about the current episode based on the environment Source code in corl/simulators/base_simulator.py @abstractmethod def save_episode_information ( self , dones , rewards , observations ): \"\"\" provides a way to save information about the current episode based on the environment Arguments: dones {[type]} -- the current done info of the step rewards {[type]} -- the reward info for this step observations {[type]} -- the observations for this step \"\"\" ...","title":"save_episode_information()"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulator.step","text":"advances the simulation platforms and returns the state Returns: Type Description StateDict StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/base_simulator.py @abstractmethod def step ( self ) -> StateDict : \"\"\" advances the simulation platforms and returns the state Returns: StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation \"\"\" ...","title":"step()"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulatorResetValidator","text":"Validator to use to validate the reset input to a simulator class allows the simulator class to take EPP params and structure/validate them Note that all attributes in this validator need to survive being parsed by validation_helper_units_and_parameters. Subclasses can redefine platforms to make the typing.Any more restrictive. It must remain a dictionary with keys named for the platforms in the simulation. Source code in corl/simulators/base_simulator.py class BaseSimulatorResetValidator ( BaseModel ): \"\"\" Validator to use to validate the reset input to a simulator class allows the simulator class to take EPP params and structure/validate them Note that all attributes in this validator need to survive being parsed by validation_helper_units_and_parameters. Subclasses can redefine `platforms` to make the `typing.Any` more restrictive. It must remain a dictionary with keys named for the platforms in the simulation. \"\"\" platforms : typing . Dict [ str , typing . Any ] = {}","title":"BaseSimulatorResetValidator"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.BaseSimulatorValidator","text":"worker_index: what worker this simulator class is running on < used for render vector_index: what vector index this simulator class is running on < used for render agent_configs: the mapping of agent names to a dict describing the platform this bool should be used to tell downstream platforms that the user wishes to disable any mutually exclusive parts checking on a platform this should pretty much only be used for behavior tree type agents frame_rate: the rate the simulator should run at (in Hz) Source code in corl/simulators/base_simulator.py class BaseSimulatorValidator ( BaseModel ): \"\"\" worker_index: what worker this simulator class is running on < used for render vector_index: what vector index this simulator class is running on < used for render agent_configs: the mapping of agent names to a dict describing the platform disable_exclusivity_check: this bool should be used to tell downstream platforms that the user wishes to disable any mutually exclusive parts checking on a platform this should pretty much only be used for behavior tree type agents frame_rate: the rate the simulator should run at (in Hz) \"\"\" worker_index : int = 0 vector_index : int = 0 agent_configs : typing . Mapping [ str , AgentConfig ] disable_exclusivity_check : bool = False frame_rate : float = 1.0","title":"BaseSimulatorValidator"},{"location":"reference/simulators/base_simulator/#corl.simulators.base_simulator.validation_helper_units_and_parameters","text":"Recursively inspect a dictionary, converting ValueWithUnits and Factory Source code in corl/simulators/base_simulator.py @validate_arguments def validation_helper_units_and_parameters ( value : typing . Dict [ str , typing . Any ]) -> typing . Dict [ str , typing . Any ]: \"\"\"Recursively inspect a dictionary, converting ValueWithUnits and Factory\"\"\" output : typing . Dict [ str , typing . Any ] = {} for k , v in value . items (): try : elem = parse_obj_as ( ValueWithUnits , v ) except ValidationError : pass else : output [ k ] = elem continue try : factory = parse_obj_as ( Factory , v ) except ValidationError : pass else : output [ k ] = factory . build () continue try : output [ k ] = validation_helper_units_and_parameters ( v ) except ValidationError : pass else : continue output [ k ] = v return output","title":"validation_helper_units_and_parameters()"},{"location":"reference/simulators/common_platform_utils/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Common Platform Utils Module get_controller_by_name ( platform , name ) \u00a4 Gets a platform controller from a platfrom given the controller name Parameters \u00a4 BasePlatform platfrom to find part str name of a given controller to search for in the platform Returns \u00a4 BaseController The platform controller with the given name Source code in corl/simulators/common_platform_utils.py def get_controller_by_name ( platform : BasePlatform , name : str ) -> BaseController : \"\"\" Gets a platform controller from a platfrom given the controller name Parameters ---------- platform: BasePlatform platfrom to find part name: str name of a given controller to search for in the platform Returns ------- platform_controller: BaseController The platform controller with the given name \"\"\" return get_part_by_name ( platform , name , BaseController ) get_part_by_name ( platform , name , part_type = None ) \u00a4 Gets a platform part from a platfrom given the part name Parameters \u00a4 BasePlatform platfrom to find part str name of a given part to search for in the platform Returns \u00a4 BasePlatformPart The platform part with the given name Source code in corl/simulators/common_platform_utils.py def get_part_by_name ( platform : BasePlatform , name : str , part_type : typing . Type [ T ] = None ) -> T : \"\"\" Gets a platform part from a platfrom given the part name Parameters ---------- platform: BasePlatform platfrom to find part name: str name of a given part to search for in the platform Returns ------- platform_part: BasePlatformPart The platform part with the given name \"\"\" if part_type : def part_filter ( part ): return isinstance ( part , part_type ) all_parts = filter ( part_filter , platform . controllers + platform . sensors ) else : all_parts = platform . controllers + platform . sensors for part in all_parts : if part . name == name : return part raise RuntimeError ( f \"An attached part associated with { name } group could not be found\" ) get_platform_by_name ( state , platform_name , allow_invalid = False ) \u00a4 Gets a platform from a sim state based on a given agent id (name) Parameters \u00a4 StateDict State of current platforms in a sim step str name of a given platform to search for in the sim state Returns \u00a4 BasePlatform The platform with the given platform_name name Source code in corl/simulators/common_platform_utils.py def get_platform_by_name ( state : StateDict , platform_name : str , allow_invalid = False ) -> typing . Optional [ BasePlatform ]: \"\"\" Gets a platform from a sim state based on a given agent id (name) Parameters ---------- state: StateDict State of current platforms in a sim step platform_name: str name of a given platform to search for in the sim state Returns ------- platform: BasePlatform The platform with the given platform_name name \"\"\" platform : typing . Optional [ BasePlatform ] = None if \"_\" in platform_name : temp = platform_name . split ( \"_\" , 1 )[ 0 ] else : temp = platform_name for plat in state . sim_platforms : if plat . name == temp : platform = plat if not allow_invalid and ( platform is None or not issubclass ( platform . __class__ , BasePlatform )): raise ValueError ( f \"Could not find a platform named { platform_name } of class BasePlatform\" ) return platform get_sensor_by_name ( platform , name ) \u00a4 Gets a platform sensor from a platfrom given the sensor name Parameters \u00a4 BasePlatform platfrom to find part str name of a given sensor to search for in the platform Returns \u00a4 BaseSensor The platform sensor with the given name Source code in corl/simulators/common_platform_utils.py def get_sensor_by_name ( platform : BasePlatform , name : str ) -> BaseSensor : \"\"\" Gets a platform sensor from a platfrom given the sensor name Parameters ---------- platform: BasePlatform platfrom to find part name: str name of a given sensor to search for in the platform Returns ------- platform_sensor: BaseSensor The platform sensor with the given name \"\"\" return get_part_by_name ( platform , name , BaseSensor ) is_platform_operable ( state , platform_name ) \u00a4 Check if a platform specified by name is operable Parameters \u00a4 State Simulation state Platform_name Name of the platform checking to be alive Returns Is platform operable? Source code in corl/simulators/common_platform_utils.py def is_platform_operable ( state : StateDict , platform_name : str ) -> bool : \"\"\" Check if a platform specified by name is operable Parameters ---------- state: Simulation state platform_name: Name of the platform checking to be alive Returns Is platform operable? ------- \"\"\" platform = get_platform_by_name ( state , platform_name , allow_invalid = True ) return platform is not None and platform . operable","title":"Common platform utils"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_controller_by_name","text":"Gets a platform controller from a platfrom given the controller name","title":"get_controller_by_name()"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_controller_by_name--parameters","text":"BasePlatform platfrom to find part str name of a given controller to search for in the platform","title":"Parameters"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_controller_by_name--returns","text":"BaseController The platform controller with the given name Source code in corl/simulators/common_platform_utils.py def get_controller_by_name ( platform : BasePlatform , name : str ) -> BaseController : \"\"\" Gets a platform controller from a platfrom given the controller name Parameters ---------- platform: BasePlatform platfrom to find part name: str name of a given controller to search for in the platform Returns ------- platform_controller: BaseController The platform controller with the given name \"\"\" return get_part_by_name ( platform , name , BaseController )","title":"Returns"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_part_by_name","text":"Gets a platform part from a platfrom given the part name","title":"get_part_by_name()"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_part_by_name--parameters","text":"BasePlatform platfrom to find part str name of a given part to search for in the platform","title":"Parameters"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_part_by_name--returns","text":"BasePlatformPart The platform part with the given name Source code in corl/simulators/common_platform_utils.py def get_part_by_name ( platform : BasePlatform , name : str , part_type : typing . Type [ T ] = None ) -> T : \"\"\" Gets a platform part from a platfrom given the part name Parameters ---------- platform: BasePlatform platfrom to find part name: str name of a given part to search for in the platform Returns ------- platform_part: BasePlatformPart The platform part with the given name \"\"\" if part_type : def part_filter ( part ): return isinstance ( part , part_type ) all_parts = filter ( part_filter , platform . controllers + platform . sensors ) else : all_parts = platform . controllers + platform . sensors for part in all_parts : if part . name == name : return part raise RuntimeError ( f \"An attached part associated with { name } group could not be found\" )","title":"Returns"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_platform_by_name","text":"Gets a platform from a sim state based on a given agent id (name)","title":"get_platform_by_name()"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_platform_by_name--parameters","text":"StateDict State of current platforms in a sim step str name of a given platform to search for in the sim state","title":"Parameters"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_platform_by_name--returns","text":"BasePlatform The platform with the given platform_name name Source code in corl/simulators/common_platform_utils.py def get_platform_by_name ( state : StateDict , platform_name : str , allow_invalid = False ) -> typing . Optional [ BasePlatform ]: \"\"\" Gets a platform from a sim state based on a given agent id (name) Parameters ---------- state: StateDict State of current platforms in a sim step platform_name: str name of a given platform to search for in the sim state Returns ------- platform: BasePlatform The platform with the given platform_name name \"\"\" platform : typing . Optional [ BasePlatform ] = None if \"_\" in platform_name : temp = platform_name . split ( \"_\" , 1 )[ 0 ] else : temp = platform_name for plat in state . sim_platforms : if plat . name == temp : platform = plat if not allow_invalid and ( platform is None or not issubclass ( platform . __class__ , BasePlatform )): raise ValueError ( f \"Could not find a platform named { platform_name } of class BasePlatform\" ) return platform","title":"Returns"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_sensor_by_name","text":"Gets a platform sensor from a platfrom given the sensor name","title":"get_sensor_by_name()"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_sensor_by_name--parameters","text":"BasePlatform platfrom to find part str name of a given sensor to search for in the platform","title":"Parameters"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.get_sensor_by_name--returns","text":"BaseSensor The platform sensor with the given name Source code in corl/simulators/common_platform_utils.py def get_sensor_by_name ( platform : BasePlatform , name : str ) -> BaseSensor : \"\"\" Gets a platform sensor from a platfrom given the sensor name Parameters ---------- platform: BasePlatform platfrom to find part name: str name of a given sensor to search for in the platform Returns ------- platform_sensor: BaseSensor The platform sensor with the given name \"\"\" return get_part_by_name ( platform , name , BaseSensor )","title":"Returns"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.is_platform_operable","text":"Check if a platform specified by name is operable","title":"is_platform_operable()"},{"location":"reference/simulators/common_platform_utils/#corl.simulators.common_platform_utils.is_platform_operable--parameters","text":"State Simulation state Platform_name Name of the platform checking to be alive Returns Is platform operable? Source code in corl/simulators/common_platform_utils.py def is_platform_operable ( state : StateDict , platform_name : str ) -> bool : \"\"\" Check if a platform specified by name is operable Parameters ---------- state: Simulation state platform_name: Name of the platform checking to be alive Returns Is platform operable? ------- \"\"\" platform = get_platform_by_name ( state , platform_name , allow_invalid = True ) return platform is not None and platform . operable","title":"Parameters"},{"location":"reference/simulators/docking_1d/__init__/","text":"","title":"Docking 1D"},{"location":"reference/simulators/docking_1d/available_platforms/","text":"This module defines an enumeration of available platform types that can be referenced in the platform config. See config/tasks/docking_1d/docking1d_platform.yml. Docking1dAvailablePlatformTypes ( BaseAvailablePlatformTypes ) \u00a4 An enumeration that outlines the platform types that have been implemented Source code in corl/simulators/docking_1d/available_platforms.py class Docking1dAvailablePlatformTypes ( BaseAvailablePlatformTypes ): \"\"\" An enumeration that outlines the platform types that have been implemented \"\"\" DOCKING1D = ( 1 , ) @classmethod def ParseFromNameModel ( cls , config : dict ): \"\"\"Given a config with the keys \"model\" and \"name\" determine the PlatformType Raises: RuntimeError: if the given config doesn't have both \"name\" and \"model\" keys RuntimeError: if the \"name\" and \"model\" keys do not match a known model \"\"\" if \"name\" not in config : raise RuntimeError ( \"Attempting to parse a PlatformType from name/model config, but both are not given!\" ) if config [ \"name\" ] == \"DOCKING1D\" : return Docking1dAvailablePlatformTypes . DOCKING1D raise RuntimeError ( f 'name: { config [ \"name\" ] } and model: { config [ \"model\" ] } did not match a known platform type' )","title":"Available platforms"},{"location":"reference/simulators/docking_1d/available_platforms/#corl.simulators.docking_1d.available_platforms.Docking1dAvailablePlatformTypes","text":"An enumeration that outlines the platform types that have been implemented Source code in corl/simulators/docking_1d/available_platforms.py class Docking1dAvailablePlatformTypes ( BaseAvailablePlatformTypes ): \"\"\" An enumeration that outlines the platform types that have been implemented \"\"\" DOCKING1D = ( 1 , ) @classmethod def ParseFromNameModel ( cls , config : dict ): \"\"\"Given a config with the keys \"model\" and \"name\" determine the PlatformType Raises: RuntimeError: if the given config doesn't have both \"name\" and \"model\" keys RuntimeError: if the \"name\" and \"model\" keys do not match a known model \"\"\" if \"name\" not in config : raise RuntimeError ( \"Attempting to parse a PlatformType from name/model config, but both are not given!\" ) if config [ \"name\" ] == \"DOCKING1D\" : return Docking1dAvailablePlatformTypes . DOCKING1D raise RuntimeError ( f 'name: { config [ \"name\" ] } and model: { config [ \"model\" ] } did not match a known platform type' )","title":"Docking1dAvailablePlatformTypes"},{"location":"reference/simulators/docking_1d/controllers/","text":"This module defines the controller used by the agent to interact with its environment. Thrust1dController ( BaseController ) \u00a4 A controller to apply thrust along a single axis. Parameters \u00a4 parent_platform : Docking1dPlatform the platform to which the controller belongs config : dict contains configuration proprties control_properties : corl.libraries.property.BoxProp a class to define the acceptable bounds and units of the controller's control Source code in corl/simulators/docking_1d/controllers.py class Thrust1dController ( BaseController ): \"\"\" A controller to apply thrust along a single axis. Parameters ---------- parent_platform : Docking1dPlatform the platform to which the controller belongs config : dict contains configuration proprties control_properties : corl.libraries.property.BoxProp a class to define the acceptable bounds and units of the controller's control \"\"\" def __init__ ( self , parent_platform , config , control_properties = ThrustProp , ): # pylint: disable=W0102 self . config : Thrust1dControllerValidator # noqa # pylint: disable=undefined-variable super () . __init__ ( property_class = control_properties , parent_platform = parent_platform , config = config ) @property def name ( self ): \"\"\" Returns ------- String name of the controller \"\"\" return self . config . name def apply_control ( self , control : np . ndarray ) -> None : \"\"\" Applies control to the parent platform Parameters ---------- control ndarray describing the control to the platform \"\"\" self . parent_platform . save_action_to_platform ( action = control ) def get_applied_control ( self ) -> np . ndarray : \"\"\" Retreive the applied control to the parent platform Returns ------- np.ndarray Previously applied action \"\"\" return self . parent_platform . get_applied_action () name property readonly \u00a4 Returns \u00a4 String name of the controller apply_control ( self , control ) \u00a4 Applies control to the parent platform Parameters \u00a4 control ndarray describing the control to the platform Source code in corl/simulators/docking_1d/controllers.py def apply_control ( self , control : np . ndarray ) -> None : \"\"\" Applies control to the parent platform Parameters ---------- control ndarray describing the control to the platform \"\"\" self . parent_platform . save_action_to_platform ( action = control ) get_applied_control ( self ) \u00a4 Retreive the applied control to the parent platform Returns \u00a4 np.ndarray Previously applied action Source code in corl/simulators/docking_1d/controllers.py def get_applied_control ( self ) -> np . ndarray : \"\"\" Retreive the applied control to the parent platform Returns ------- np.ndarray Previously applied action \"\"\" return self . parent_platform . get_applied_action ()","title":"Controllers"},{"location":"reference/simulators/docking_1d/controllers/#corl.simulators.docking_1d.controllers.Thrust1dController","text":"A controller to apply thrust along a single axis.","title":"Thrust1dController"},{"location":"reference/simulators/docking_1d/controllers/#corl.simulators.docking_1d.controllers.Thrust1dController--parameters","text":"parent_platform : Docking1dPlatform the platform to which the controller belongs config : dict contains configuration proprties control_properties : corl.libraries.property.BoxProp a class to define the acceptable bounds and units of the controller's control Source code in corl/simulators/docking_1d/controllers.py class Thrust1dController ( BaseController ): \"\"\" A controller to apply thrust along a single axis. Parameters ---------- parent_platform : Docking1dPlatform the platform to which the controller belongs config : dict contains configuration proprties control_properties : corl.libraries.property.BoxProp a class to define the acceptable bounds and units of the controller's control \"\"\" def __init__ ( self , parent_platform , config , control_properties = ThrustProp , ): # pylint: disable=W0102 self . config : Thrust1dControllerValidator # noqa # pylint: disable=undefined-variable super () . __init__ ( property_class = control_properties , parent_platform = parent_platform , config = config ) @property def name ( self ): \"\"\" Returns ------- String name of the controller \"\"\" return self . config . name def apply_control ( self , control : np . ndarray ) -> None : \"\"\" Applies control to the parent platform Parameters ---------- control ndarray describing the control to the platform \"\"\" self . parent_platform . save_action_to_platform ( action = control ) def get_applied_control ( self ) -> np . ndarray : \"\"\" Retreive the applied control to the parent platform Returns ------- np.ndarray Previously applied action \"\"\" return self . parent_platform . get_applied_action ()","title":"Parameters"},{"location":"reference/simulators/docking_1d/controllers/#corl.simulators.docking_1d.controllers.Thrust1dController.name","text":"","title":"name"},{"location":"reference/simulators/docking_1d/controllers/#corl.simulators.docking_1d.controllers.Thrust1dController.name--returns","text":"String name of the controller","title":"Returns"},{"location":"reference/simulators/docking_1d/controllers/#corl.simulators.docking_1d.controllers.Thrust1dController.apply_control","text":"Applies control to the parent platform","title":"apply_control()"},{"location":"reference/simulators/docking_1d/controllers/#corl.simulators.docking_1d.controllers.Thrust1dController.apply_control--parameters","text":"control ndarray describing the control to the platform Source code in corl/simulators/docking_1d/controllers.py def apply_control ( self , control : np . ndarray ) -> None : \"\"\" Applies control to the parent platform Parameters ---------- control ndarray describing the control to the platform \"\"\" self . parent_platform . save_action_to_platform ( action = control )","title":"Parameters"},{"location":"reference/simulators/docking_1d/controllers/#corl.simulators.docking_1d.controllers.Thrust1dController.get_applied_control","text":"Retreive the applied control to the parent platform","title":"get_applied_control()"},{"location":"reference/simulators/docking_1d/controllers/#corl.simulators.docking_1d.controllers.Thrust1dController.get_applied_control--returns","text":"np.ndarray Previously applied action Source code in corl/simulators/docking_1d/controllers.py def get_applied_control ( self ) -> np . ndarray : \"\"\" Retreive the applied control to the parent platform Returns ------- np.ndarray Previously applied action \"\"\" return self . parent_platform . get_applied_action ()","title":"Returns"},{"location":"reference/simulators/docking_1d/entities/","text":"This module defines backend entities that maintain the state of a platform and defines the dynamics model used to calculate state transitions between time steps, given applied controls. In this example, the dynamics model is a 1D Double Integrator. Deputy1D \u00a4 1D point mass spacecraft with a +/- thruster and Double Integrator dynamics States x x_dot Controls thrust range = [-1, 1] Newtons Parameters \u00a4 float Mass of spacecraft in kilograms, by default 12 str Numerical integration method passed to dynamics model. Kwargs Additional keyword arguments passed to Deputy1dValidator Source code in corl/simulators/docking_1d/entities.py class Deputy1D : \"\"\" 1D point mass spacecraft with a +/- thruster and Double Integrator dynamics States x x_dot Controls thrust range = [-1, 1] Newtons Parameters ---------- m: float Mass of spacecraft in kilograms, by default 12 integration_method: str Numerical integration method passed to dynamics model. kwargs: Additional keyword arguments passed to Deputy1dValidator \"\"\" def __init__ ( self , m = 12 , integration_method = \"RK45\" , ** kwargs ): self . config = self . _get_config_validator ()( ** kwargs ) self . name = self . config . name self . dynamics = Docking1dDynamics ( m = m , integration_method = integration_method ) self . control_default = np . zeros (( 1 , )) self . control_min = - 1.0 self . control_max = 1.0 self . _state = self . _build_state () self . state_dot = np . zeros_like ( self . _state ) def __eq__ ( self , other ): if isinstance ( other , Deputy1D ): eq = ( self . velocity == other . velocity ) . all () eq = eq and ( self . position == other . position ) . all () return eq return False @classmethod def _get_config_validator ( cls ): return Deputy1dValidator def step ( self , step_size , action = None ): \"\"\" Executes a state transition simulation step for the entity Parameters ---------- step_size : float duration of simulation step in seconds action : np.ndarray, optional Control action taken by entity, by default None resulting in a control of control_default. Raises ------ KeyError Raised when action dict key not found in control map ValueError Raised when action is not one of the required types \"\"\" if action is None : control = self . control_default . copy () else : if isinstance ( action , np . ndarray ): control = action . copy () else : raise ValueError ( \"action must be type np.ndarray\" ) # enforce control bounds control = np . clip ( control , self . control_min , self . control_max ) # compute new state if dynamics were applied self . state , self . state_dot = self . dynamics . step ( step_size , self . state , control ) def _build_state ( self ): # builds initial state? state = np . array ([ self . config . x . value , self . config . xdot . value ], dtype = np . float32 ) return state @property def state ( self ) -> np . ndarray : \"\"\" Returns copy of entity's state vector Returns ------- np.ndarray copy of state vector \"\"\" return self . _state . copy () @state . setter def state ( self , value : np . ndarray ): self . _state = value . copy () @property def position ( self ): \"\"\" get 1d position vector \"\"\" position = np . zeros ( 1 ) position [ 0 ] = self . _state [ 0 ] . copy () return position @property def velocity ( self ): \"\"\" get 1d velocity vector \"\"\" velocity = np . zeros ( 1 ) velocity [ 0 ] = self . _state [ 1 ] . copy () return velocity position property readonly \u00a4 get 1d position vector state : ndarray property writable \u00a4 Returns copy of entity's state vector Returns \u00a4 np.ndarray copy of state vector velocity property readonly \u00a4 get 1d velocity vector step ( self , step_size , action = None ) \u00a4 Executes a state transition simulation step for the entity Parameters \u00a4 step_size : float duration of simulation step in seconds action : np.ndarray, optional Control action taken by entity, by default None resulting in a control of control_default. Raises \u00a4 KeyError Raised when action dict key not found in control map ValueError Raised when action is not one of the required types Source code in corl/simulators/docking_1d/entities.py def step ( self , step_size , action = None ): \"\"\" Executes a state transition simulation step for the entity Parameters ---------- step_size : float duration of simulation step in seconds action : np.ndarray, optional Control action taken by entity, by default None resulting in a control of control_default. Raises ------ KeyError Raised when action dict key not found in control map ValueError Raised when action is not one of the required types \"\"\" if action is None : control = self . control_default . copy () else : if isinstance ( action , np . ndarray ): control = action . copy () else : raise ValueError ( \"action must be type np.ndarray\" ) # enforce control bounds control = np . clip ( control , self . control_min , self . control_max ) # compute new state if dynamics were applied self . state , self . state_dot = self . dynamics . step ( step_size , self . state , control ) Deputy1dValidator ( BaseModel ) pydantic-model \u00a4 This module validates that Deputy1D configs contain a name and initial state values. Source code in corl/simulators/docking_1d/entities.py class Deputy1dValidator ( BaseModel ): \"\"\" This module validates that Deputy1D configs contain a name and initial state values. \"\"\" name : str x : ValueWithUnits = ValueWithUnits ( value = 0 ) xdot : ValueWithUnits = ValueWithUnits ( value = 0 ) Docking1dDynamics \u00a4 State transition implementation for generic Linear Ordinary Differential Equation dynamics models of the form dx/dt = Ax+Bu. Computes next state through numerical integration of differential equation. Parameters \u00a4 state_min : float or np.ndarray Minimum allowable value for the next state. State values that exceed this are clipped. When a float, represents single limit applied to entire state vector. When an ndarray, each element represents the limit to the corresponding state vector element. state_max : float or np.ndarray Maximum allowable value for the next state. State values that exceed this are clipped. When a float, represents single limit applied to entire state vector. When an ndarray, each element represents the limit to the corresponding state vector element. np.ndarray Enables circular wrapping of angles. Defines the center of circular wrap such that angles are within [center+pi, center-pi]. When None, no angle wrapping applied. When ndarray, each element defines the angle wrap center of the corresponding state element. Wrapping not applied when element is NaN. integration_method : string Numerical integration method used by dyanmics solver. One of ['RK45', 'Euler']. 'RK45' is slow but very accurate. 'Euler' is fast but very inaccurate. Source code in corl/simulators/docking_1d/entities.py class Docking1dDynamics : \"\"\" State transition implementation for generic Linear Ordinary Differential Equation dynamics models of the form dx/dt = Ax+Bu. Computes next state through numerical integration of differential equation. Parameters ---------- state_min : float or np.ndarray Minimum allowable value for the next state. State values that exceed this are clipped. When a float, represents single limit applied to entire state vector. When an ndarray, each element represents the limit to the corresponding state vector element. state_max : float or np.ndarray Maximum allowable value for the next state. State values that exceed this are clipped. When a float, represents single limit applied to entire state vector. When an ndarray, each element represents the limit to the corresponding state vector element. angle_wrap_centers: np.ndarray Enables circular wrapping of angles. Defines the center of circular wrap such that angles are within [center+pi, center-pi]. When None, no angle wrapping applied. When ndarray, each element defines the angle wrap center of the corresponding state element. Wrapping not applied when element is NaN. integration_method : string Numerical integration method used by dyanmics solver. One of ['RK45', 'Euler']. 'RK45' is slow but very accurate. 'Euler' is fast but very inaccurate. \"\"\" def __init__ ( self , state_min : Union [ float , np . ndarray ] = - np . inf , state_max : Union [ float , np . ndarray ] = np . inf , angle_wrap_centers : np . ndarray = None , m : float = 12.0 , integration_method : str = \"RK45\" , ): self . state_min = state_min self . state_max = state_max self . angle_wrap_centers = angle_wrap_centers self . m = m self . A , self . B = self . _gen_dynamics_matrices () self . integration_method = integration_method def step ( self , step_size : float , state : np . ndarray , control : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Computes the dynamics state transition from the current state and control input Parameters ---------- step_size : float Duration of the simation step in seconds. state : np.ndarray Current state of the system at the beginning of the simulation step. control : np.ndarray Control vector of the dynamics model. Returns ------- Tuple[np.ndarray, np.ndarray] tuple of the systems's next state and the state's instantaneous time derivative at the end of the step \"\"\" if self . integration_method == \"RK45\" : sol = scipy . integrate . solve_ivp ( self . compute_state_dot , ( 0 , step_size ), state , args = ( control , )) next_state = sol . y [:, - 1 ] # save last timestep of integration solution state_dot = self . compute_state_dot ( step_size , next_state , control ) else : raise ValueError ( f \"invalid integration method ' { self . integration_method } '\" ) next_state = np . clip ( next_state , self . state_min , self . state_max ) next_state = self . _wrap_angles ( next_state ) return next_state , state_dot def _wrap_angles ( self , state ): wrapped_state = state . copy () if self . angle_wrap_centers is not None : wrap_idxs = np . logical_not ( np . isnan ( self . angle_wrap_centers )) wrapped_state [ wrap_idxs ] = \\ (( wrapped_state [ wrap_idxs ] + np . pi ) % ( 2 * np . pi )) - np . pi + self . angle_wrap_centers [ wrap_idxs ] return wrapped_state def compute_state_dot ( self , t : float , state : np . ndarray , control : np . ndarray ) -> np . ndarray : # pylint: disable=unused-argument \"\"\" Computes the instataneous time derivative of the state vector Parameters ---------- t : float Time in seconds since the beginning of the simulation step. Note, this is NOT the total simulation time but the time within the individual step. state : np.ndarray Current state vector at time t. control : np.ndarray Control vector. Returns ------- np.ndarray Instantaneous time derivative of the state vector. \"\"\" state_dot = np . matmul ( self . A , state ) + np . matmul ( self . B , control ) # clip state_dot by state limits lower_bounded_states = state <= self . state_min upper_bounded_state = state >= self . state_max state_dot [ lower_bounded_states ] = np . clip ( state_dot [ lower_bounded_states ], 0 , np . inf ) state_dot [ upper_bounded_state ] = np . clip ( state_dot [ upper_bounded_state ], - np . inf , 0 ) return state_dot def _gen_dynamics_matrices ( self ): \"\"\" Returns ------- Tuple[np.ndarray, np.ndarray] The A and B matrix defining 1D Double Integrator dynamics \"\"\" m = self . m A = np . array ( [[ 0 , 1 ], [ 0 , 0 ]], dtype = np . float32 , ) B = np . array ( [[ 0 ], [ 1 / m ]], dtype = np . float32 , ) return A , B compute_state_dot ( self , t , state , control ) \u00a4 Computes the instataneous time derivative of the state vector Parameters \u00a4 t : float Time in seconds since the beginning of the simulation step. Note, this is NOT the total simulation time but the time within the individual step. state : np.ndarray Current state vector at time t. control : np.ndarray Control vector. Returns \u00a4 np.ndarray Instantaneous time derivative of the state vector. Source code in corl/simulators/docking_1d/entities.py def compute_state_dot ( self , t : float , state : np . ndarray , control : np . ndarray ) -> np . ndarray : # pylint: disable=unused-argument \"\"\" Computes the instataneous time derivative of the state vector Parameters ---------- t : float Time in seconds since the beginning of the simulation step. Note, this is NOT the total simulation time but the time within the individual step. state : np.ndarray Current state vector at time t. control : np.ndarray Control vector. Returns ------- np.ndarray Instantaneous time derivative of the state vector. \"\"\" state_dot = np . matmul ( self . A , state ) + np . matmul ( self . B , control ) # clip state_dot by state limits lower_bounded_states = state <= self . state_min upper_bounded_state = state >= self . state_max state_dot [ lower_bounded_states ] = np . clip ( state_dot [ lower_bounded_states ], 0 , np . inf ) state_dot [ upper_bounded_state ] = np . clip ( state_dot [ upper_bounded_state ], - np . inf , 0 ) return state_dot step ( self , step_size , state , control ) \u00a4 Computes the dynamics state transition from the current state and control input Parameters \u00a4 step_size : float Duration of the simation step in seconds. state : np.ndarray Current state of the system at the beginning of the simulation step. control : np.ndarray Control vector of the dynamics model. Returns \u00a4 Tuple[np.ndarray, np.ndarray] tuple of the systems's next state and the state's instantaneous time derivative at the end of the step Source code in corl/simulators/docking_1d/entities.py def step ( self , step_size : float , state : np . ndarray , control : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Computes the dynamics state transition from the current state and control input Parameters ---------- step_size : float Duration of the simation step in seconds. state : np.ndarray Current state of the system at the beginning of the simulation step. control : np.ndarray Control vector of the dynamics model. Returns ------- Tuple[np.ndarray, np.ndarray] tuple of the systems's next state and the state's instantaneous time derivative at the end of the step \"\"\" if self . integration_method == \"RK45\" : sol = scipy . integrate . solve_ivp ( self . compute_state_dot , ( 0 , step_size ), state , args = ( control , )) next_state = sol . y [:, - 1 ] # save last timestep of integration solution state_dot = self . compute_state_dot ( step_size , next_state , control ) else : raise ValueError ( f \"invalid integration method ' { self . integration_method } '\" ) next_state = np . clip ( next_state , self . state_min , self . state_max ) next_state = self . _wrap_angles ( next_state ) return next_state , state_dot","title":"Entities"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D","text":"1D point mass spacecraft with a +/- thruster and Double Integrator dynamics States x x_dot Controls thrust range = [-1, 1] Newtons","title":"Deputy1D"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D--parameters","text":"float Mass of spacecraft in kilograms, by default 12 str Numerical integration method passed to dynamics model. Kwargs Additional keyword arguments passed to Deputy1dValidator Source code in corl/simulators/docking_1d/entities.py class Deputy1D : \"\"\" 1D point mass spacecraft with a +/- thruster and Double Integrator dynamics States x x_dot Controls thrust range = [-1, 1] Newtons Parameters ---------- m: float Mass of spacecraft in kilograms, by default 12 integration_method: str Numerical integration method passed to dynamics model. kwargs: Additional keyword arguments passed to Deputy1dValidator \"\"\" def __init__ ( self , m = 12 , integration_method = \"RK45\" , ** kwargs ): self . config = self . _get_config_validator ()( ** kwargs ) self . name = self . config . name self . dynamics = Docking1dDynamics ( m = m , integration_method = integration_method ) self . control_default = np . zeros (( 1 , )) self . control_min = - 1.0 self . control_max = 1.0 self . _state = self . _build_state () self . state_dot = np . zeros_like ( self . _state ) def __eq__ ( self , other ): if isinstance ( other , Deputy1D ): eq = ( self . velocity == other . velocity ) . all () eq = eq and ( self . position == other . position ) . all () return eq return False @classmethod def _get_config_validator ( cls ): return Deputy1dValidator def step ( self , step_size , action = None ): \"\"\" Executes a state transition simulation step for the entity Parameters ---------- step_size : float duration of simulation step in seconds action : np.ndarray, optional Control action taken by entity, by default None resulting in a control of control_default. Raises ------ KeyError Raised when action dict key not found in control map ValueError Raised when action is not one of the required types \"\"\" if action is None : control = self . control_default . copy () else : if isinstance ( action , np . ndarray ): control = action . copy () else : raise ValueError ( \"action must be type np.ndarray\" ) # enforce control bounds control = np . clip ( control , self . control_min , self . control_max ) # compute new state if dynamics were applied self . state , self . state_dot = self . dynamics . step ( step_size , self . state , control ) def _build_state ( self ): # builds initial state? state = np . array ([ self . config . x . value , self . config . xdot . value ], dtype = np . float32 ) return state @property def state ( self ) -> np . ndarray : \"\"\" Returns copy of entity's state vector Returns ------- np.ndarray copy of state vector \"\"\" return self . _state . copy () @state . setter def state ( self , value : np . ndarray ): self . _state = value . copy () @property def position ( self ): \"\"\" get 1d position vector \"\"\" position = np . zeros ( 1 ) position [ 0 ] = self . _state [ 0 ] . copy () return position @property def velocity ( self ): \"\"\" get 1d velocity vector \"\"\" velocity = np . zeros ( 1 ) velocity [ 0 ] = self . _state [ 1 ] . copy () return velocity","title":"Parameters"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D.position","text":"get 1d position vector","title":"position"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D.state","text":"Returns copy of entity's state vector","title":"state"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D.state--returns","text":"np.ndarray copy of state vector","title":"Returns"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D.velocity","text":"get 1d velocity vector","title":"velocity"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D.step","text":"Executes a state transition simulation step for the entity","title":"step()"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D.step--parameters","text":"step_size : float duration of simulation step in seconds action : np.ndarray, optional Control action taken by entity, by default None resulting in a control of control_default.","title":"Parameters"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1D.step--raises","text":"KeyError Raised when action dict key not found in control map ValueError Raised when action is not one of the required types Source code in corl/simulators/docking_1d/entities.py def step ( self , step_size , action = None ): \"\"\" Executes a state transition simulation step for the entity Parameters ---------- step_size : float duration of simulation step in seconds action : np.ndarray, optional Control action taken by entity, by default None resulting in a control of control_default. Raises ------ KeyError Raised when action dict key not found in control map ValueError Raised when action is not one of the required types \"\"\" if action is None : control = self . control_default . copy () else : if isinstance ( action , np . ndarray ): control = action . copy () else : raise ValueError ( \"action must be type np.ndarray\" ) # enforce control bounds control = np . clip ( control , self . control_min , self . control_max ) # compute new state if dynamics were applied self . state , self . state_dot = self . dynamics . step ( step_size , self . state , control )","title":"Raises"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Deputy1dValidator","text":"This module validates that Deputy1D configs contain a name and initial state values. Source code in corl/simulators/docking_1d/entities.py class Deputy1dValidator ( BaseModel ): \"\"\" This module validates that Deputy1D configs contain a name and initial state values. \"\"\" name : str x : ValueWithUnits = ValueWithUnits ( value = 0 ) xdot : ValueWithUnits = ValueWithUnits ( value = 0 )","title":"Deputy1dValidator"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Docking1dDynamics","text":"State transition implementation for generic Linear Ordinary Differential Equation dynamics models of the form dx/dt = Ax+Bu. Computes next state through numerical integration of differential equation.","title":"Docking1dDynamics"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Docking1dDynamics--parameters","text":"state_min : float or np.ndarray Minimum allowable value for the next state. State values that exceed this are clipped. When a float, represents single limit applied to entire state vector. When an ndarray, each element represents the limit to the corresponding state vector element. state_max : float or np.ndarray Maximum allowable value for the next state. State values that exceed this are clipped. When a float, represents single limit applied to entire state vector. When an ndarray, each element represents the limit to the corresponding state vector element. np.ndarray Enables circular wrapping of angles. Defines the center of circular wrap such that angles are within [center+pi, center-pi]. When None, no angle wrapping applied. When ndarray, each element defines the angle wrap center of the corresponding state element. Wrapping not applied when element is NaN. integration_method : string Numerical integration method used by dyanmics solver. One of ['RK45', 'Euler']. 'RK45' is slow but very accurate. 'Euler' is fast but very inaccurate. Source code in corl/simulators/docking_1d/entities.py class Docking1dDynamics : \"\"\" State transition implementation for generic Linear Ordinary Differential Equation dynamics models of the form dx/dt = Ax+Bu. Computes next state through numerical integration of differential equation. Parameters ---------- state_min : float or np.ndarray Minimum allowable value for the next state. State values that exceed this are clipped. When a float, represents single limit applied to entire state vector. When an ndarray, each element represents the limit to the corresponding state vector element. state_max : float or np.ndarray Maximum allowable value for the next state. State values that exceed this are clipped. When a float, represents single limit applied to entire state vector. When an ndarray, each element represents the limit to the corresponding state vector element. angle_wrap_centers: np.ndarray Enables circular wrapping of angles. Defines the center of circular wrap such that angles are within [center+pi, center-pi]. When None, no angle wrapping applied. When ndarray, each element defines the angle wrap center of the corresponding state element. Wrapping not applied when element is NaN. integration_method : string Numerical integration method used by dyanmics solver. One of ['RK45', 'Euler']. 'RK45' is slow but very accurate. 'Euler' is fast but very inaccurate. \"\"\" def __init__ ( self , state_min : Union [ float , np . ndarray ] = - np . inf , state_max : Union [ float , np . ndarray ] = np . inf , angle_wrap_centers : np . ndarray = None , m : float = 12.0 , integration_method : str = \"RK45\" , ): self . state_min = state_min self . state_max = state_max self . angle_wrap_centers = angle_wrap_centers self . m = m self . A , self . B = self . _gen_dynamics_matrices () self . integration_method = integration_method def step ( self , step_size : float , state : np . ndarray , control : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Computes the dynamics state transition from the current state and control input Parameters ---------- step_size : float Duration of the simation step in seconds. state : np.ndarray Current state of the system at the beginning of the simulation step. control : np.ndarray Control vector of the dynamics model. Returns ------- Tuple[np.ndarray, np.ndarray] tuple of the systems's next state and the state's instantaneous time derivative at the end of the step \"\"\" if self . integration_method == \"RK45\" : sol = scipy . integrate . solve_ivp ( self . compute_state_dot , ( 0 , step_size ), state , args = ( control , )) next_state = sol . y [:, - 1 ] # save last timestep of integration solution state_dot = self . compute_state_dot ( step_size , next_state , control ) else : raise ValueError ( f \"invalid integration method ' { self . integration_method } '\" ) next_state = np . clip ( next_state , self . state_min , self . state_max ) next_state = self . _wrap_angles ( next_state ) return next_state , state_dot def _wrap_angles ( self , state ): wrapped_state = state . copy () if self . angle_wrap_centers is not None : wrap_idxs = np . logical_not ( np . isnan ( self . angle_wrap_centers )) wrapped_state [ wrap_idxs ] = \\ (( wrapped_state [ wrap_idxs ] + np . pi ) % ( 2 * np . pi )) - np . pi + self . angle_wrap_centers [ wrap_idxs ] return wrapped_state def compute_state_dot ( self , t : float , state : np . ndarray , control : np . ndarray ) -> np . ndarray : # pylint: disable=unused-argument \"\"\" Computes the instataneous time derivative of the state vector Parameters ---------- t : float Time in seconds since the beginning of the simulation step. Note, this is NOT the total simulation time but the time within the individual step. state : np.ndarray Current state vector at time t. control : np.ndarray Control vector. Returns ------- np.ndarray Instantaneous time derivative of the state vector. \"\"\" state_dot = np . matmul ( self . A , state ) + np . matmul ( self . B , control ) # clip state_dot by state limits lower_bounded_states = state <= self . state_min upper_bounded_state = state >= self . state_max state_dot [ lower_bounded_states ] = np . clip ( state_dot [ lower_bounded_states ], 0 , np . inf ) state_dot [ upper_bounded_state ] = np . clip ( state_dot [ upper_bounded_state ], - np . inf , 0 ) return state_dot def _gen_dynamics_matrices ( self ): \"\"\" Returns ------- Tuple[np.ndarray, np.ndarray] The A and B matrix defining 1D Double Integrator dynamics \"\"\" m = self . m A = np . array ( [[ 0 , 1 ], [ 0 , 0 ]], dtype = np . float32 , ) B = np . array ( [[ 0 ], [ 1 / m ]], dtype = np . float32 , ) return A , B","title":"Parameters"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Docking1dDynamics.compute_state_dot","text":"Computes the instataneous time derivative of the state vector","title":"compute_state_dot()"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Docking1dDynamics.compute_state_dot--parameters","text":"t : float Time in seconds since the beginning of the simulation step. Note, this is NOT the total simulation time but the time within the individual step. state : np.ndarray Current state vector at time t. control : np.ndarray Control vector.","title":"Parameters"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Docking1dDynamics.compute_state_dot--returns","text":"np.ndarray Instantaneous time derivative of the state vector. Source code in corl/simulators/docking_1d/entities.py def compute_state_dot ( self , t : float , state : np . ndarray , control : np . ndarray ) -> np . ndarray : # pylint: disable=unused-argument \"\"\" Computes the instataneous time derivative of the state vector Parameters ---------- t : float Time in seconds since the beginning of the simulation step. Note, this is NOT the total simulation time but the time within the individual step. state : np.ndarray Current state vector at time t. control : np.ndarray Control vector. Returns ------- np.ndarray Instantaneous time derivative of the state vector. \"\"\" state_dot = np . matmul ( self . A , state ) + np . matmul ( self . B , control ) # clip state_dot by state limits lower_bounded_states = state <= self . state_min upper_bounded_state = state >= self . state_max state_dot [ lower_bounded_states ] = np . clip ( state_dot [ lower_bounded_states ], 0 , np . inf ) state_dot [ upper_bounded_state ] = np . clip ( state_dot [ upper_bounded_state ], - np . inf , 0 ) return state_dot","title":"Returns"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Docking1dDynamics.step","text":"Computes the dynamics state transition from the current state and control input","title":"step()"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Docking1dDynamics.step--parameters","text":"step_size : float Duration of the simation step in seconds. state : np.ndarray Current state of the system at the beginning of the simulation step. control : np.ndarray Control vector of the dynamics model.","title":"Parameters"},{"location":"reference/simulators/docking_1d/entities/#corl.simulators.docking_1d.entities.Docking1dDynamics.step--returns","text":"Tuple[np.ndarray, np.ndarray] tuple of the systems's next state and the state's instantaneous time derivative at the end of the step Source code in corl/simulators/docking_1d/entities.py def step ( self , step_size : float , state : np . ndarray , control : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Computes the dynamics state transition from the current state and control input Parameters ---------- step_size : float Duration of the simation step in seconds. state : np.ndarray Current state of the system at the beginning of the simulation step. control : np.ndarray Control vector of the dynamics model. Returns ------- Tuple[np.ndarray, np.ndarray] tuple of the systems's next state and the state's instantaneous time derivative at the end of the step \"\"\" if self . integration_method == \"RK45\" : sol = scipy . integrate . solve_ivp ( self . compute_state_dot , ( 0 , step_size ), state , args = ( control , )) next_state = sol . y [:, - 1 ] # save last timestep of integration solution state_dot = self . compute_state_dot ( step_size , next_state , control ) else : raise ValueError ( f \"invalid integration method ' { self . integration_method } '\" ) next_state = np . clip ( next_state , self . state_min , self . state_max ) next_state = self . _wrap_angles ( next_state ) return next_state , state_dot","title":"Returns"},{"location":"reference/simulators/docking_1d/platform/","text":"This module extends corl.simulators.base_platform.BasePlatform to create a simple one dimensional docking platform. Docking1dPlatform ( BasePlatform ) \u00a4 A platform representing a spacecraft operating under Double Integrator dynamics. Allows for saving an action to the platform for when the platform needs to give an action to the environment during the environment step function Parameters \u00a4 platform_name : str Name of the platform platform : sim_entity Backend simulation entity associated with the platform platform_config : dict Platform-specific configuration dictionary Source code in corl/simulators/docking_1d/platform.py class Docking1dPlatform ( BasePlatform ): \"\"\" A platform representing a spacecraft operating under Double Integrator dynamics. Allows for saving an action to the platform for when the platform needs to give an action to the environment during the environment step function Parameters ---------- platform_name : str Name of the platform platform : sim_entity Backend simulation entity associated with the platform platform_config : dict Platform-specific configuration dictionary \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : Docking1dPlatformValidator super () . __init__ ( ** kwargs ) self . _platform = self . config . platform self . _last_applied_action = np . array ([ 0 ], dtype = np . float32 ) # thrust self . _sim_time = 0.0 @property def get_validator ( self ) -> typing . Type [ Docking1dPlatformValidator ]: return Docking1dPlatformValidator def __eq__ ( self , other ): if isinstance ( other , Docking1dPlatform ): eq = ( self . velocity == other . velocity ) . all () eq = eq and ( self . position == other . position ) . all () eq = eq and self . sim_time == other . sim_time return eq return False def get_applied_action ( self ): \"\"\" returns the action stored in this platform Returns: typing.Any -- any sort of stored action \"\"\" return self . _last_applied_action def save_action_to_platform ( self , action ): \"\"\" saves an action to the platform if it matches the action space Arguments: action typing.Any -- The action to store in the platform \"\"\" if isinstance ( action , np . ndarray ) and len ( action ) == 1 : self . _last_applied_action = action @property def position ( self ): \"\"\" The position of the platform Returns ------- np.ndarray The position vector of the platform \"\"\" return self . _platform . position @property def velocity ( self ): \"\"\" The velocity of the platform Returns ------- np.ndarray The velocity vector of the platform \"\"\" return self . _platform . velocity @property def sim_time ( self ): \"\"\" The current simulation time in seconds. Returns ------- float Current simulation time \"\"\" return self . _sim_time @sim_time . setter def sim_time ( self , time ): self . _sim_time = time @property def operable ( self ): return True get_validator : Type [ corl . simulators . docking_1d . platform . Docking1dPlatformValidator ] property readonly \u00a4 get validator for this BasePlatform Returns: Type Description Type[corl.simulators.docking_1d.platform.Docking1dPlatformValidator] BasePlatformValidator -- validator the platform will use to generate a configuration operable property readonly \u00a4 Is the platform operable? Returns \u00a4 bool Is the platform operable? position property readonly \u00a4 The position of the platform Returns \u00a4 np.ndarray The position vector of the platform sim_time property writable \u00a4 The current simulation time in seconds. Returns \u00a4 float Current simulation time velocity property readonly \u00a4 The velocity of the platform Returns \u00a4 np.ndarray The velocity vector of the platform get_applied_action ( self ) \u00a4 returns the action stored in this platform Returns: Type Description typing.Any -- any sort of stored action Source code in corl/simulators/docking_1d/platform.py def get_applied_action ( self ): \"\"\" returns the action stored in this platform Returns: typing.Any -- any sort of stored action \"\"\" return self . _last_applied_action save_action_to_platform ( self , action ) \u00a4 saves an action to the platform if it matches the action space Source code in corl/simulators/docking_1d/platform.py def save_action_to_platform ( self , action ): \"\"\" saves an action to the platform if it matches the action space Arguments: action typing.Any -- The action to store in the platform \"\"\" if isinstance ( action , np . ndarray ) and len ( action ) == 1 : self . _last_applied_action = action Docking1dPlatformValidator ( BasePlatformValidator ) pydantic-model \u00a4 Docking1dPlatformValidator Parameters \u00a4 !!! platform \"Deputy1D\" Deputy associated with the CoRL Docking1dPlatform Source code in corl/simulators/docking_1d/platform.py class Docking1dPlatformValidator ( BasePlatformValidator ): \"\"\"Docking1dPlatformValidator Parameters ---------- platform: Deputy1D Deputy associated with the CoRL Docking1dPlatform \"\"\" platform : Deputy1D","title":"Platform"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform","text":"A platform representing a spacecraft operating under Double Integrator dynamics. Allows for saving an action to the platform for when the platform needs to give an action to the environment during the environment step function","title":"Docking1dPlatform"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform--parameters","text":"platform_name : str Name of the platform platform : sim_entity Backend simulation entity associated with the platform platform_config : dict Platform-specific configuration dictionary Source code in corl/simulators/docking_1d/platform.py class Docking1dPlatform ( BasePlatform ): \"\"\" A platform representing a spacecraft operating under Double Integrator dynamics. Allows for saving an action to the platform for when the platform needs to give an action to the environment during the environment step function Parameters ---------- platform_name : str Name of the platform platform : sim_entity Backend simulation entity associated with the platform platform_config : dict Platform-specific configuration dictionary \"\"\" def __init__ ( self , ** kwargs ) -> None : self . config : Docking1dPlatformValidator super () . __init__ ( ** kwargs ) self . _platform = self . config . platform self . _last_applied_action = np . array ([ 0 ], dtype = np . float32 ) # thrust self . _sim_time = 0.0 @property def get_validator ( self ) -> typing . Type [ Docking1dPlatformValidator ]: return Docking1dPlatformValidator def __eq__ ( self , other ): if isinstance ( other , Docking1dPlatform ): eq = ( self . velocity == other . velocity ) . all () eq = eq and ( self . position == other . position ) . all () eq = eq and self . sim_time == other . sim_time return eq return False def get_applied_action ( self ): \"\"\" returns the action stored in this platform Returns: typing.Any -- any sort of stored action \"\"\" return self . _last_applied_action def save_action_to_platform ( self , action ): \"\"\" saves an action to the platform if it matches the action space Arguments: action typing.Any -- The action to store in the platform \"\"\" if isinstance ( action , np . ndarray ) and len ( action ) == 1 : self . _last_applied_action = action @property def position ( self ): \"\"\" The position of the platform Returns ------- np.ndarray The position vector of the platform \"\"\" return self . _platform . position @property def velocity ( self ): \"\"\" The velocity of the platform Returns ------- np.ndarray The velocity vector of the platform \"\"\" return self . _platform . velocity @property def sim_time ( self ): \"\"\" The current simulation time in seconds. Returns ------- float Current simulation time \"\"\" return self . _sim_time @sim_time . setter def sim_time ( self , time ): self . _sim_time = time @property def operable ( self ): return True","title":"Parameters"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.get_validator","text":"get validator for this BasePlatform Returns: Type Description Type[corl.simulators.docking_1d.platform.Docking1dPlatformValidator] BasePlatformValidator -- validator the platform will use to generate a configuration","title":"get_validator"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.operable","text":"Is the platform operable?","title":"operable"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.operable--returns","text":"bool Is the platform operable?","title":"Returns"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.position","text":"The position of the platform","title":"position"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.position--returns","text":"np.ndarray The position vector of the platform","title":"Returns"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.sim_time","text":"The current simulation time in seconds.","title":"sim_time"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.sim_time--returns","text":"float Current simulation time","title":"Returns"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.velocity","text":"The velocity of the platform","title":"velocity"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.velocity--returns","text":"np.ndarray The velocity vector of the platform","title":"Returns"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.get_applied_action","text":"returns the action stored in this platform Returns: Type Description typing.Any -- any sort of stored action Source code in corl/simulators/docking_1d/platform.py def get_applied_action ( self ): \"\"\" returns the action stored in this platform Returns: typing.Any -- any sort of stored action \"\"\" return self . _last_applied_action","title":"get_applied_action()"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatform.save_action_to_platform","text":"saves an action to the platform if it matches the action space Source code in corl/simulators/docking_1d/platform.py def save_action_to_platform ( self , action ): \"\"\" saves an action to the platform if it matches the action space Arguments: action typing.Any -- The action to store in the platform \"\"\" if isinstance ( action , np . ndarray ) and len ( action ) == 1 : self . _last_applied_action = action","title":"save_action_to_platform()"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatformValidator","text":"Docking1dPlatformValidator","title":"Docking1dPlatformValidator"},{"location":"reference/simulators/docking_1d/platform/#corl.simulators.docking_1d.platform.Docking1dPlatformValidator--parameters","text":"!!! platform \"Deputy1D\" Deputy associated with the CoRL Docking1dPlatform Source code in corl/simulators/docking_1d/platform.py class Docking1dPlatformValidator ( BasePlatformValidator ): \"\"\"Docking1dPlatformValidator Parameters ---------- platform: Deputy1D Deputy associated with the CoRL Docking1dPlatform \"\"\" platform : Deputy1D","title":"Parameters"},{"location":"reference/simulators/docking_1d/properties/","text":"This module defines the measurement and control properties for Double Integrator spacecraft sensors and controllers. PositionProp ( BoxProp ) pydantic-model \u00a4 Position sensor properties. name : str sensor property name low : list[float] minimum bounds of sensor output high : list[float] maximum bounds of sensor output unit : str unit of measurement for sensor output description : str description of sensor properties Source code in corl/simulators/docking_1d/properties.py class PositionProp ( BoxProp ): \"\"\" Position sensor properties. name : str sensor property name low : list[float] minimum bounds of sensor output high : list[float] maximum bounds of sensor output unit : str unit of measurement for sensor output description : str description of sensor properties \"\"\" name : str = \"position\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 50000.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 50000.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"m\" ] description : str = \"Position Sensor Properties\" ThrustProp ( BoxProp ) pydantic-model \u00a4 Thrust control properties. name : str control property name low : list[float] minimum bounds of control input high : list[float] maximum bounds of control input unit : str unit of measurement for control input description : str description of control properties Source code in corl/simulators/docking_1d/properties.py class ThrustProp ( BoxProp ): \"\"\" Thrust control properties. name : str control property name low : list[float] minimum bounds of control input high : list[float] maximum bounds of control input unit : str unit of measurement for control input description : str description of control properties \"\"\" name : str = \"thrust\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 1.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 1.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"newtons\" ] description : str = \"Direct Thrust Control\" VelocityProp ( BoxProp ) pydantic-model \u00a4 Velocity sensor properties. name : str sensor property name low : list[float] minimum bounds of sensor output high : list[float] maximum bounds of sensor output unit : str unit of measurement for sensor output description : str description of sensor properties Source code in corl/simulators/docking_1d/properties.py class VelocityProp ( BoxProp ): \"\"\" Velocity sensor properties. name : str sensor property name low : list[float] minimum bounds of sensor output high : list[float] maximum bounds of sensor output unit : str unit of measurement for sensor output description : str description of sensor properties \"\"\" name : str = \"velocity\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 2000.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 2000.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"m/s\" ] description : str = \"Velocity Sensor Properties\"","title":"Properties"},{"location":"reference/simulators/docking_1d/properties/#corl.simulators.docking_1d.properties.PositionProp","text":"Position sensor properties. name : str sensor property name low : list[float] minimum bounds of sensor output high : list[float] maximum bounds of sensor output unit : str unit of measurement for sensor output description : str description of sensor properties Source code in corl/simulators/docking_1d/properties.py class PositionProp ( BoxProp ): \"\"\" Position sensor properties. name : str sensor property name low : list[float] minimum bounds of sensor output high : list[float] maximum bounds of sensor output unit : str unit of measurement for sensor output description : str description of sensor properties \"\"\" name : str = \"position\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 50000.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 50000.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"m\" ] description : str = \"Position Sensor Properties\"","title":"PositionProp"},{"location":"reference/simulators/docking_1d/properties/#corl.simulators.docking_1d.properties.ThrustProp","text":"Thrust control properties. name : str control property name low : list[float] minimum bounds of control input high : list[float] maximum bounds of control input unit : str unit of measurement for control input description : str description of control properties Source code in corl/simulators/docking_1d/properties.py class ThrustProp ( BoxProp ): \"\"\" Thrust control properties. name : str control property name low : list[float] minimum bounds of control input high : list[float] maximum bounds of control input unit : str unit of measurement for control input description : str description of control properties \"\"\" name : str = \"thrust\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 1.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 1.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"newtons\" ] description : str = \"Direct Thrust Control\"","title":"ThrustProp"},{"location":"reference/simulators/docking_1d/properties/#corl.simulators.docking_1d.properties.VelocityProp","text":"Velocity sensor properties. name : str sensor property name low : list[float] minimum bounds of sensor output high : list[float] maximum bounds of sensor output unit : str unit of measurement for sensor output description : str description of sensor properties Source code in corl/simulators/docking_1d/properties.py class VelocityProp ( BoxProp ): \"\"\" Velocity sensor properties. name : str sensor property name low : list[float] minimum bounds of sensor output high : list[float] maximum bounds of sensor output unit : str unit of measurement for sensor output description : str description of sensor properties \"\"\" name : str = \"velocity\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 2000.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 2000.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"m/s\" ] description : str = \"Velocity Sensor Properties\"","title":"VelocityProp"},{"location":"reference/simulators/docking_1d/sensors/","text":"This module contains implementations of Sensors that reside on the Docking1dPlatform. PositionSensor ( BaseSensor ) \u00a4 A Sensor to measure the position of the associated Docking1dPlatform. Source code in corl/simulators/docking_1d/sensors.py class PositionSensor ( BaseSensor ): \"\"\" A Sensor to measure the position of the associated Docking1dPlatform. \"\"\" def __init__ ( self , parent_platform , config , measurement_properties = PositionProp ): super () . __init__ ( parent_platform = parent_platform , config = config , property_class = measurement_properties ) def _calculate_measurement ( self , state ): \"\"\" get measurements from the sensor \"\"\" return self . parent_platform . position VelocitySensor ( BaseSensor ) \u00a4 A Sensor to measure the velocity of the associated Docking1dPlatform. Source code in corl/simulators/docking_1d/sensors.py class VelocitySensor ( BaseSensor ): \"\"\" A Sensor to measure the velocity of the associated Docking1dPlatform. \"\"\" def __init__ ( self , parent_platform , config , measurement_properties = VelocityProp ): super () . __init__ ( parent_platform = parent_platform , config = config , property_class = measurement_properties ) def _calculate_measurement ( self , state ): \"\"\" get measurements from the sensor \"\"\" return self . parent_platform . velocity","title":"Sensors"},{"location":"reference/simulators/docking_1d/sensors/#corl.simulators.docking_1d.sensors.PositionSensor","text":"A Sensor to measure the position of the associated Docking1dPlatform. Source code in corl/simulators/docking_1d/sensors.py class PositionSensor ( BaseSensor ): \"\"\" A Sensor to measure the position of the associated Docking1dPlatform. \"\"\" def __init__ ( self , parent_platform , config , measurement_properties = PositionProp ): super () . __init__ ( parent_platform = parent_platform , config = config , property_class = measurement_properties ) def _calculate_measurement ( self , state ): \"\"\" get measurements from the sensor \"\"\" return self . parent_platform . position","title":"PositionSensor"},{"location":"reference/simulators/docking_1d/sensors/#corl.simulators.docking_1d.sensors.VelocitySensor","text":"A Sensor to measure the velocity of the associated Docking1dPlatform. Source code in corl/simulators/docking_1d/sensors.py class VelocitySensor ( BaseSensor ): \"\"\" A Sensor to measure the velocity of the associated Docking1dPlatform. \"\"\" def __init__ ( self , parent_platform , config , measurement_properties = VelocityProp ): super () . __init__ ( parent_platform = parent_platform , config = config , property_class = measurement_properties ) def _calculate_measurement ( self , state ): \"\"\" get measurements from the sensor \"\"\" return self . parent_platform . velocity","title":"VelocitySensor"},{"location":"reference/simulators/docking_1d/simulator/","text":"This module defines the Docking1dSimulator class which maintains environment objects (platforms and entities) and progresses a simulated training episode via the step() method. Docking1dSimulator ( BaseSimulator ) \u00a4 Simulator for the 1D Docking task. Interfaces Docking1dPlatform with underlying Deputy1D entities in Docking simulation. Docking1dSimulator is responsible for initializing the platform objects for a simulation and knowing how to set up episodes based on input parameters from a parameter provider. It is also responsible for reporting the simulation state at each timestep. Source code in corl/simulators/docking_1d/simulator.py class Docking1dSimulator ( BaseSimulator ): \"\"\" Simulator for the 1D Docking task. Interfaces Docking1dPlatform with underlying Deputy1D entities in Docking simulation. Docking1dSimulator is responsible for initializing the platform objects for a simulation and knowing how to set up episodes based on input parameters from a parameter provider. It is also responsible for reporting the simulation state at each timestep. \"\"\" @property def get_simulator_validator ( self ): return Docking1dSimulatorValidator @property def get_reset_validator ( self ): return Docking1dSimulatorResetValidator def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _state = StateDict () self . clock = 0.0 def reset ( self , config ): config = self . get_reset_validator ( ** config ) self . _state . clear () self . clock = 0.0 # construct entities (\"Gets the platform object associated with each simulation entity.\") self . sim_entities = {} # pylint: disable=attribute-defined-outside-init for agent_id , agent_config in self . config . agent_configs . items (): agent_reset_config = config . platforms . get ( agent_id , {}) self . sim_entities [ agent_id ] = Deputy1D ( name = agent_id , ** agent_reset_config ) # construct platforms (\"Gets the correct backend simulation entity for each agent.\") sim_platforms = [] for agent_id , entity in self . sim_entities . items (): agent_config = self . config . agent_configs [ agent_id ] sim_platforms . append ( Docking1dPlatform ( platform_name = agent_id , platform = entity , parts_list = agent_config . parts_list )) self . _state . sim_platforms = tuple ( sim_platforms ) self . update_sensor_measurements () return self . _state def step ( self ): for platform in self . _state . sim_platforms : agent_id = platform . name action = np . array ( platform . get_applied_action (), dtype = np . float32 ) entity = self . sim_entities [ agent_id ] entity . step ( action = action , step_size = self . config . step_size ) platform . sim_time = self . clock self . update_sensor_measurements () self . clock += self . config . step_size return self . _state @property def sim_time ( self ) -> float : return self . clock @property def platforms ( self ) -> typing . List : return list ( self . _state . sim_platforms ) def update_sensor_measurements ( self ): \"\"\" Update and cache all the measurements of all the sensors on each platform \"\"\" for plat in self . _state . sim_platforms : for sensor in plat . sensors : sensor . calculate_and_cache_measurement ( state = self . _state . sim_platforms ) def mark_episode_done ( self , done_info , episode_state ): pass def save_episode_information ( self , dones , rewards , observations ): pass get_reset_validator property readonly \u00a4 returns the validator that can be used to validate episode parameters coming into the reset function from the environment class Returns: Type Description BaseSimulatorResetValidator -- The validator to use during resets get_simulator_validator property readonly \u00a4 returns the validator for the configuration options to the simulator the kwargs to this class are validated and put into a defined struct potentially raising based on invalid configurations Returns: Type Description BaseSimulatorValidator -- The validator to use for this simulation class platforms : List property readonly \u00a4 returns a list of platforms in the simulation Returns: Type Description List list of platforms sim_time : float property readonly \u00a4 returns the time Returns: Type Description float float - time mark_episode_done ( self , done_info , episode_state ) \u00a4 Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Source code in corl/simulators/docking_1d/simulator.py def mark_episode_done ( self , done_info , episode_state ): pass reset ( self , config ) \u00a4 reset resets the simulation and sets up a new episode Returns: Type Description StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/docking_1d/simulator.py def reset ( self , config ): config = self . get_reset_validator ( ** config ) self . _state . clear () self . clock = 0.0 # construct entities (\"Gets the platform object associated with each simulation entity.\") self . sim_entities = {} # pylint: disable=attribute-defined-outside-init for agent_id , agent_config in self . config . agent_configs . items (): agent_reset_config = config . platforms . get ( agent_id , {}) self . sim_entities [ agent_id ] = Deputy1D ( name = agent_id , ** agent_reset_config ) # construct platforms (\"Gets the correct backend simulation entity for each agent.\") sim_platforms = [] for agent_id , entity in self . sim_entities . items (): agent_config = self . config . agent_configs [ agent_id ] sim_platforms . append ( Docking1dPlatform ( platform_name = agent_id , platform = entity , parts_list = agent_config . parts_list )) self . _state . sim_platforms = tuple ( sim_platforms ) self . update_sensor_measurements () return self . _state save_episode_information ( self , dones , rewards , observations ) \u00a4 provides a way to save information about the current episode based on the environment Source code in corl/simulators/docking_1d/simulator.py def save_episode_information ( self , dones , rewards , observations ): pass step ( self ) \u00a4 advances the simulation platforms and returns the state Returns: Type Description StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/docking_1d/simulator.py def step ( self ): for platform in self . _state . sim_platforms : agent_id = platform . name action = np . array ( platform . get_applied_action (), dtype = np . float32 ) entity = self . sim_entities [ agent_id ] entity . step ( action = action , step_size = self . config . step_size ) platform . sim_time = self . clock self . update_sensor_measurements () self . clock += self . config . step_size return self . _state update_sensor_measurements ( self ) \u00a4 Update and cache all the measurements of all the sensors on each platform Source code in corl/simulators/docking_1d/simulator.py def update_sensor_measurements ( self ): \"\"\" Update and cache all the measurements of all the sensors on each platform \"\"\" for plat in self . _state . sim_platforms : for sensor in plat . sensors : sensor . calculate_and_cache_measurement ( state = self . _state . sim_platforms ) Docking1dSimulatorResetValidator ( BaseSimulatorResetValidator ) pydantic-model \u00a4 A Validator for Docking1dSimulatorValidator reset configs Parameters \u00a4 dict Contains individual initialization dicts for each agent. Key is platform name, value is platform's initialization dict. Source code in corl/simulators/docking_1d/simulator.py class Docking1dSimulatorResetValidator ( BaseSimulatorResetValidator ): \"\"\" A Validator for Docking1dSimulatorValidator reset configs Parameters ---------- platform_config: dict Contains individual initialization dicts for each agent. Key is platform name, value is platform's initialization dict. \"\"\" platform_config : typing . Optional [ typing . Dict ] = {} Docking1dSimulatorValidator ( BaseSimulatorValidator ) pydantic-model \u00a4 A validator for the Docking1dSimulatorValidator config. step_size: A float representing how many simulated seconds pass each time the simulator updates Source code in corl/simulators/docking_1d/simulator.py class Docking1dSimulatorValidator ( BaseSimulatorValidator ): \"\"\" A validator for the Docking1dSimulatorValidator config. step_size: A float representing how many simulated seconds pass each time the simulator updates \"\"\" step_size : float","title":"Simulator"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator","text":"Simulator for the 1D Docking task. Interfaces Docking1dPlatform with underlying Deputy1D entities in Docking simulation. Docking1dSimulator is responsible for initializing the platform objects for a simulation and knowing how to set up episodes based on input parameters from a parameter provider. It is also responsible for reporting the simulation state at each timestep. Source code in corl/simulators/docking_1d/simulator.py class Docking1dSimulator ( BaseSimulator ): \"\"\" Simulator for the 1D Docking task. Interfaces Docking1dPlatform with underlying Deputy1D entities in Docking simulation. Docking1dSimulator is responsible for initializing the platform objects for a simulation and knowing how to set up episodes based on input parameters from a parameter provider. It is also responsible for reporting the simulation state at each timestep. \"\"\" @property def get_simulator_validator ( self ): return Docking1dSimulatorValidator @property def get_reset_validator ( self ): return Docking1dSimulatorResetValidator def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) self . _state = StateDict () self . clock = 0.0 def reset ( self , config ): config = self . get_reset_validator ( ** config ) self . _state . clear () self . clock = 0.0 # construct entities (\"Gets the platform object associated with each simulation entity.\") self . sim_entities = {} # pylint: disable=attribute-defined-outside-init for agent_id , agent_config in self . config . agent_configs . items (): agent_reset_config = config . platforms . get ( agent_id , {}) self . sim_entities [ agent_id ] = Deputy1D ( name = agent_id , ** agent_reset_config ) # construct platforms (\"Gets the correct backend simulation entity for each agent.\") sim_platforms = [] for agent_id , entity in self . sim_entities . items (): agent_config = self . config . agent_configs [ agent_id ] sim_platforms . append ( Docking1dPlatform ( platform_name = agent_id , platform = entity , parts_list = agent_config . parts_list )) self . _state . sim_platforms = tuple ( sim_platforms ) self . update_sensor_measurements () return self . _state def step ( self ): for platform in self . _state . sim_platforms : agent_id = platform . name action = np . array ( platform . get_applied_action (), dtype = np . float32 ) entity = self . sim_entities [ agent_id ] entity . step ( action = action , step_size = self . config . step_size ) platform . sim_time = self . clock self . update_sensor_measurements () self . clock += self . config . step_size return self . _state @property def sim_time ( self ) -> float : return self . clock @property def platforms ( self ) -> typing . List : return list ( self . _state . sim_platforms ) def update_sensor_measurements ( self ): \"\"\" Update and cache all the measurements of all the sensors on each platform \"\"\" for plat in self . _state . sim_platforms : for sensor in plat . sensors : sensor . calculate_and_cache_measurement ( state = self . _state . sim_platforms ) def mark_episode_done ( self , done_info , episode_state ): pass def save_episode_information ( self , dones , rewards , observations ): pass","title":"Docking1dSimulator"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.get_reset_validator","text":"returns the validator that can be used to validate episode parameters coming into the reset function from the environment class Returns: Type Description BaseSimulatorResetValidator -- The validator to use during resets","title":"get_reset_validator"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.get_simulator_validator","text":"returns the validator for the configuration options to the simulator the kwargs to this class are validated and put into a defined struct potentially raising based on invalid configurations Returns: Type Description BaseSimulatorValidator -- The validator to use for this simulation class","title":"get_simulator_validator"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.platforms","text":"returns a list of platforms in the simulation Returns: Type Description List list of platforms","title":"platforms"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.sim_time","text":"returns the time Returns: Type Description float float - time","title":"sim_time"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.mark_episode_done","text":"Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Source code in corl/simulators/docking_1d/simulator.py def mark_episode_done ( self , done_info , episode_state ): pass","title":"mark_episode_done()"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.reset","text":"reset resets the simulation and sets up a new episode Returns: Type Description StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/docking_1d/simulator.py def reset ( self , config ): config = self . get_reset_validator ( ** config ) self . _state . clear () self . clock = 0.0 # construct entities (\"Gets the platform object associated with each simulation entity.\") self . sim_entities = {} # pylint: disable=attribute-defined-outside-init for agent_id , agent_config in self . config . agent_configs . items (): agent_reset_config = config . platforms . get ( agent_id , {}) self . sim_entities [ agent_id ] = Deputy1D ( name = agent_id , ** agent_reset_config ) # construct platforms (\"Gets the correct backend simulation entity for each agent.\") sim_platforms = [] for agent_id , entity in self . sim_entities . items (): agent_config = self . config . agent_configs [ agent_id ] sim_platforms . append ( Docking1dPlatform ( platform_name = agent_id , platform = entity , parts_list = agent_config . parts_list )) self . _state . sim_platforms = tuple ( sim_platforms ) self . update_sensor_measurements () return self . _state","title":"reset()"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.save_episode_information","text":"provides a way to save information about the current episode based on the environment Source code in corl/simulators/docking_1d/simulator.py def save_episode_information ( self , dones , rewards , observations ): pass","title":"save_episode_information()"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.step","text":"advances the simulation platforms and returns the state Returns: Type Description StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/docking_1d/simulator.py def step ( self ): for platform in self . _state . sim_platforms : agent_id = platform . name action = np . array ( platform . get_applied_action (), dtype = np . float32 ) entity = self . sim_entities [ agent_id ] entity . step ( action = action , step_size = self . config . step_size ) platform . sim_time = self . clock self . update_sensor_measurements () self . clock += self . config . step_size return self . _state","title":"step()"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulator.update_sensor_measurements","text":"Update and cache all the measurements of all the sensors on each platform Source code in corl/simulators/docking_1d/simulator.py def update_sensor_measurements ( self ): \"\"\" Update and cache all the measurements of all the sensors on each platform \"\"\" for plat in self . _state . sim_platforms : for sensor in plat . sensors : sensor . calculate_and_cache_measurement ( state = self . _state . sim_platforms )","title":"update_sensor_measurements()"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulatorResetValidator","text":"A Validator for Docking1dSimulatorValidator reset configs","title":"Docking1dSimulatorResetValidator"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulatorResetValidator--parameters","text":"dict Contains individual initialization dicts for each agent. Key is platform name, value is platform's initialization dict. Source code in corl/simulators/docking_1d/simulator.py class Docking1dSimulatorResetValidator ( BaseSimulatorResetValidator ): \"\"\" A Validator for Docking1dSimulatorValidator reset configs Parameters ---------- platform_config: dict Contains individual initialization dicts for each agent. Key is platform name, value is platform's initialization dict. \"\"\" platform_config : typing . Optional [ typing . Dict ] = {}","title":"Parameters"},{"location":"reference/simulators/docking_1d/simulator/#corl.simulators.docking_1d.simulator.Docking1dSimulatorValidator","text":"A validator for the Docking1dSimulatorValidator config. step_size: A float representing how many simulated seconds pass each time the simulator updates Source code in corl/simulators/docking_1d/simulator.py class Docking1dSimulatorValidator ( BaseSimulatorValidator ): \"\"\" A validator for the Docking1dSimulatorValidator config. step_size: A float representing how many simulated seconds pass each time the simulator updates \"\"\" step_size : float","title":"Docking1dSimulatorValidator"},{"location":"reference/simulators/openai_gym/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"OpenAI Gym"},{"location":"reference/simulators/openai_gym/gym_available_platforms/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Provides AvailablePlatformTypes for the Open AI Gym Simulator OpenAIGymAvailablePlatformTypes ( BaseAvailablePlatformTypes ) \u00a4 Enumeration that outlines the platform types that have been implemented Source code in corl/simulators/openai_gym/gym_available_platforms.py class OpenAIGymAvailablePlatformTypes ( BaseAvailablePlatformTypes ): \"\"\"Enumeration that outlines the platform types that have been implemented \"\"\" MAIN = ( 1 , ) @classmethod def ParseFromNameModel ( cls , config : dict ) -> OpenAIGymAvailablePlatformTypes : \"\"\" This just returns the main platform type, as openai gym simulators are simple \"\"\" return OpenAIGymAvailablePlatformTypes . MAIN","title":"Gym available platforms"},{"location":"reference/simulators/openai_gym/gym_available_platforms/#corl.simulators.openai_gym.gym_available_platforms.OpenAIGymAvailablePlatformTypes","text":"Enumeration that outlines the platform types that have been implemented Source code in corl/simulators/openai_gym/gym_available_platforms.py class OpenAIGymAvailablePlatformTypes ( BaseAvailablePlatformTypes ): \"\"\"Enumeration that outlines the platform types that have been implemented \"\"\" MAIN = ( 1 , ) @classmethod def ParseFromNameModel ( cls , config : dict ) -> OpenAIGymAvailablePlatformTypes : \"\"\" This just returns the main platform type, as openai gym simulators are simple \"\"\" return OpenAIGymAvailablePlatformTypes . MAIN","title":"OpenAIGymAvailablePlatformTypes"},{"location":"reference/simulators/openai_gym/gym_controllers/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Provides controllers to pass actions to wrapped OpenAI Gym Environments OpenAIGymDuplicateController ( OpenAIGymMainController ) \u00a4 GymController implementation for passing actions to wrapped OpenAI Gym Environments Source code in corl/simulators/openai_gym/gym_controllers.py class OpenAIGymDuplicateController ( OpenAIGymMainController ): \"\"\" GymController implementation for passing actions to wrapped OpenAI Gym Environments \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return set () exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness OpenAIGymMainController ( BaseController ) \u00a4 GymController implementation for passing actions to wrapped OpenAI Gym Environments Source code in corl/simulators/openai_gym/gym_controllers.py class OpenAIGymMainController ( BaseController ): \"\"\" GymController implementation for passing actions to wrapped OpenAI Gym Environments \"\"\" def __init__ ( self , parent_platform , config = None ): act_space = parent_platform . action_space cont_prop : typing . Union [ DiscreteProp , BoxProp ] if isinstance ( act_space , gym . spaces . Discrete ): class DiscreteGymProp ( DiscreteProp ): \"\"\" DiscreteGymProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"gym controller\" unit : str = \"None\" n : int = act_space . n description : str = \"gym env action space\" cont_prop = DiscreteGymProp elif isinstance ( act_space , gym . spaces . Box ): class BoxGymProp ( BoxProp ): \"\"\" BoxGymProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"gym controller\" low : typing . List [ float ] = act_space . low . tolist () high : typing . List [ float ] = act_space . high . tolist () dtype : np . dtype = act_space . dtype unit : typing . List [ str ] = [ \"None\" ] * len ( act_space . low ) description : str = \"gym env action space\" cont_prop = BoxGymProp else : raise RuntimeError ( f \"This controller does not currently know how to handle a { type ( act_space ) } action space\" ) super () . __init__ ( parent_platform = parent_platform , config = config , property_class = cont_prop ) @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"main_controller\" } def apply_control ( self , control : np . ndarray ) -> None : self . parent_platform . save_action_to_platform ( control ) def get_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: return self . parent_platform . get_applied_action () exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness apply_control ( self , control ) \u00a4 The generic method to apply the control for this controller. Parameters \u00a4 control The control to be executed by the controller Source code in corl/simulators/openai_gym/gym_controllers.py def apply_control ( self , control : np . ndarray ) -> None : self . parent_platform . save_action_to_platform ( control ) get_applied_control ( self ) \u00a4 Get the previously applied control that was given to the apply_control function Returns previously applied control that was given to the apply_control function Source code in corl/simulators/openai_gym/gym_controllers.py def get_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: return self . parent_platform . get_applied_action ()","title":"Gym controllers"},{"location":"reference/simulators/openai_gym/gym_controllers/#corl.simulators.openai_gym.gym_controllers.OpenAIGymDuplicateController","text":"GymController implementation for passing actions to wrapped OpenAI Gym Environments Source code in corl/simulators/openai_gym/gym_controllers.py class OpenAIGymDuplicateController ( OpenAIGymMainController ): \"\"\" GymController implementation for passing actions to wrapped OpenAI Gym Environments \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return set ()","title":"OpenAIGymDuplicateController"},{"location":"reference/simulators/openai_gym/gym_controllers/#corl.simulators.openai_gym.gym_controllers.OpenAIGymDuplicateController.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/openai_gym/gym_controllers/#corl.simulators.openai_gym.gym_controllers.OpenAIGymMainController","text":"GymController implementation for passing actions to wrapped OpenAI Gym Environments Source code in corl/simulators/openai_gym/gym_controllers.py class OpenAIGymMainController ( BaseController ): \"\"\" GymController implementation for passing actions to wrapped OpenAI Gym Environments \"\"\" def __init__ ( self , parent_platform , config = None ): act_space = parent_platform . action_space cont_prop : typing . Union [ DiscreteProp , BoxProp ] if isinstance ( act_space , gym . spaces . Discrete ): class DiscreteGymProp ( DiscreteProp ): \"\"\" DiscreteGymProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"gym controller\" unit : str = \"None\" n : int = act_space . n description : str = \"gym env action space\" cont_prop = DiscreteGymProp elif isinstance ( act_space , gym . spaces . Box ): class BoxGymProp ( BoxProp ): \"\"\" BoxGymProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"gym controller\" low : typing . List [ float ] = act_space . low . tolist () high : typing . List [ float ] = act_space . high . tolist () dtype : np . dtype = act_space . dtype unit : typing . List [ str ] = [ \"None\" ] * len ( act_space . low ) description : str = \"gym env action space\" cont_prop = BoxGymProp else : raise RuntimeError ( f \"This controller does not currently know how to handle a { type ( act_space ) } action space\" ) super () . __init__ ( parent_platform = parent_platform , config = config , property_class = cont_prop ) @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"main_controller\" } def apply_control ( self , control : np . ndarray ) -> None : self . parent_platform . save_action_to_platform ( control ) def get_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: return self . parent_platform . get_applied_action ()","title":"OpenAIGymMainController"},{"location":"reference/simulators/openai_gym/gym_controllers/#corl.simulators.openai_gym.gym_controllers.OpenAIGymMainController.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/openai_gym/gym_controllers/#corl.simulators.openai_gym.gym_controllers.OpenAIGymMainController.apply_control","text":"The generic method to apply the control for this controller.","title":"apply_control()"},{"location":"reference/simulators/openai_gym/gym_controllers/#corl.simulators.openai_gym.gym_controllers.OpenAIGymMainController.apply_control--parameters","text":"control The control to be executed by the controller Source code in corl/simulators/openai_gym/gym_controllers.py def apply_control ( self , control : np . ndarray ) -> None : self . parent_platform . save_action_to_platform ( control )","title":"Parameters"},{"location":"reference/simulators/openai_gym/gym_controllers/#corl.simulators.openai_gym.gym_controllers.OpenAIGymMainController.get_applied_control","text":"Get the previously applied control that was given to the apply_control function Returns previously applied control that was given to the apply_control function Source code in corl/simulators/openai_gym/gym_controllers.py def get_applied_control ( self ) -> typing . Union [ np . ndarray , numbers . Number ]: return self . parent_platform . get_applied_action ()","title":"get_applied_control()"},{"location":"reference/simulators/openai_gym/gym_sensors/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Sensors for OpenAIGymSimulator OpenAiGymRepeatedStateSensor ( BaseSensor ) \u00a4 Sensor that reports the observation of a given platform gym environment Source code in corl/simulators/openai_gym/gym_sensors.py class OpenAiGymRepeatedStateSensor ( BaseSensor ): \"\"\" Sensor that reports the observation of a given platform gym environment \"\"\" def __init__ ( self , parent_platform , config = None ): obs_space = parent_platform . observation_space class GymSensorProp ( BoxProp ): \"\"\" GymSensorProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"GymStateSensor\" low : typing . List [ float ] = obs_space . low . tolist () high : typing . List [ float ] = obs_space . high . tolist () unit : typing . List [ str ] = [ \"None\" ] * len ( obs_space . low ) description : str = \"Gym Space\" class GymSensorRepeatedProp ( RepeatedProp ): \"\"\" GymSensorProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"GymStateRepeatedSensor\" max_len : int = 10 child_space : typing . Dict [ str , GymSensorProp ] = { \"GymState\" : GymSensorProp ()} # type: ignore description : str = \"Gym Space Repeated\" super () . __init__ ( config = config , parent_platform = parent_platform , property_class = GymSensorRepeatedProp ) self . obs_buffer = RingBuffer ( capacity = self . measurement_properties . max_len , dtype = np . ndarray ) @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"state_sensor\" } def _calculate_measurement ( self , state ): self . obs_buffer . append ( state . obs [ self . parent_platform . name ]) ret = map ( lambda x : { \"GymState\" : x }, self . obs_buffer ) return list ( ret ) exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness OpenAiGymStateSensor ( BaseSensor ) \u00a4 Sensor that reports the observation of a given platform gym environment Source code in corl/simulators/openai_gym/gym_sensors.py class OpenAiGymStateSensor ( BaseSensor ): \"\"\" Sensor that reports the observation of a given platform gym environment \"\"\" def __init__ ( self , parent_platform , config = None ): obs_space = parent_platform . observation_space class GymSensorProp ( BoxProp ): \"\"\" GymSensorProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"GymStateSensor\" low : typing . List [ float ] = obs_space . low . tolist () high : typing . List [ float ] = obs_space . high . tolist () unit : typing . List [ str ] = [ \"None\" ] * len ( obs_space . low ) description : str = \"Gym Space\" super () . __init__ ( parent_platform = parent_platform , config = config , property_class = GymSensorProp ) @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"state_sensor\" } def _calculate_measurement ( self , state ): return state . obs [ self . parent_platform . name ] exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness","title":"Gym sensors"},{"location":"reference/simulators/openai_gym/gym_sensors/#corl.simulators.openai_gym.gym_sensors.OpenAiGymRepeatedStateSensor","text":"Sensor that reports the observation of a given platform gym environment Source code in corl/simulators/openai_gym/gym_sensors.py class OpenAiGymRepeatedStateSensor ( BaseSensor ): \"\"\" Sensor that reports the observation of a given platform gym environment \"\"\" def __init__ ( self , parent_platform , config = None ): obs_space = parent_platform . observation_space class GymSensorProp ( BoxProp ): \"\"\" GymSensorProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"GymStateSensor\" low : typing . List [ float ] = obs_space . low . tolist () high : typing . List [ float ] = obs_space . high . tolist () unit : typing . List [ str ] = [ \"None\" ] * len ( obs_space . low ) description : str = \"Gym Space\" class GymSensorRepeatedProp ( RepeatedProp ): \"\"\" GymSensorProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"GymStateRepeatedSensor\" max_len : int = 10 child_space : typing . Dict [ str , GymSensorProp ] = { \"GymState\" : GymSensorProp ()} # type: ignore description : str = \"Gym Space Repeated\" super () . __init__ ( config = config , parent_platform = parent_platform , property_class = GymSensorRepeatedProp ) self . obs_buffer = RingBuffer ( capacity = self . measurement_properties . max_len , dtype = np . ndarray ) @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"state_sensor\" } def _calculate_measurement ( self , state ): self . obs_buffer . append ( state . obs [ self . parent_platform . name ]) ret = map ( lambda x : { \"GymState\" : x }, self . obs_buffer ) return list ( ret )","title":"OpenAiGymRepeatedStateSensor"},{"location":"reference/simulators/openai_gym/gym_sensors/#corl.simulators.openai_gym.gym_sensors.OpenAiGymRepeatedStateSensor.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/openai_gym/gym_sensors/#corl.simulators.openai_gym.gym_sensors.OpenAiGymStateSensor","text":"Sensor that reports the observation of a given platform gym environment Source code in corl/simulators/openai_gym/gym_sensors.py class OpenAiGymStateSensor ( BaseSensor ): \"\"\" Sensor that reports the observation of a given platform gym environment \"\"\" def __init__ ( self , parent_platform , config = None ): obs_space = parent_platform . observation_space class GymSensorProp ( BoxProp ): \"\"\" GymSensorProp can be updated via config and valdidate by pydantic \"\"\" name : str = \"GymStateSensor\" low : typing . List [ float ] = obs_space . low . tolist () high : typing . List [ float ] = obs_space . high . tolist () unit : typing . List [ str ] = [ \"None\" ] * len ( obs_space . low ) description : str = \"Gym Space\" super () . __init__ ( parent_platform = parent_platform , config = config , property_class = GymSensorProp ) @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"state_sensor\" } def _calculate_measurement ( self , state ): return state . obs [ self . parent_platform . name ]","title":"OpenAiGymStateSensor"},{"location":"reference/simulators/openai_gym/gym_sensors/#corl.simulators.openai_gym.gym_sensors.OpenAiGymStateSensor.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/openai_gym/gym_simulator/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Base Simulator and Platform for Toy Openai Environments This mainly shows a \"how to use example\" and provide an setup to unit test with GymAgentConfig ( AgentConfig ) pydantic-model \u00a4 any configuration needed for the simulator to initialize this platform and configure it in the sim class a list of tuples where the first element is come python class path of a BasePart , and then the second element is a configuration dictionary for that part Source code in corl/simulators/openai_gym/gym_simulator.py class GymAgentConfig ( AgentConfig ): \"\"\" platform_config: any configuration needed for the simulator to initialize this platform and configure it in the sim class parts_list: a list of tuples where the first element is come python class path of a BasePart, and then the second element is a configuration dictionary for that part Arguments: BaseModel {[type]} -- [description] \"\"\" platform_config : GymPlatformConfig GymPlatformConfig ( BaseModel ) pydantic-model \u00a4 GymPlatformSimConfig Source code in corl/simulators/openai_gym/gym_simulator.py class GymPlatformConfig ( BaseModel ): \"\"\" GymPlatformSimConfig \"\"\" platform_class : PyObject GymPlatformValidator ( BasePlatformValidator ) pydantic-model \u00a4 GymPlatformValidator Parameters \u00a4 !!! platform \"gym.Env\" Gym env associated with GymPlatform Source code in corl/simulators/openai_gym/gym_simulator.py class GymPlatformValidator ( BasePlatformValidator ): \"\"\"GymPlatformValidator Parameters ---------- platform: gym.Env Gym env associated with GymPlatform \"\"\" platform : gym . Env OpenAIGymSimulator ( BaseSimulator ) \u00a4 Simulator backend for running openai Gyms Source code in corl/simulators/openai_gym/gym_simulator.py class OpenAIGymSimulator ( BaseSimulator ): \"\"\" Simulator backend for running openai Gyms \"\"\" @property def get_simulator_validator ( self ) -> typing . Type [ OpenAIGymSimulatorValidator ]: \"\"\"Return validator\"\"\" return OpenAIGymSimulatorValidator def __init__ ( self , ** kwargs ) -> None : self . config : OpenAIGymSimulatorValidator super () . __init__ ( ** kwargs ) self . _state = StateDict () self . gym_env_dict = {} for agent_name in self . config . agent_configs : env = gym . make ( self . config . gym_env , ** self . config . gym_configs ) for wrapper_cls in self . config . wrappers : env = wrapper_cls ( env ) self . gym_env_dict [ agent_name ] = env self . gym_env_dict [ agent_name ] . seed ( self . config . seed ) self . sim_platforms : typing . List = [] self . _time = 0.0 def get_platforms ( self ): \"\"\" gets the current state of the simulation and makes the sim platforms Returns: typing.List[OpenAiGymPlatform] -- the list of openai gym platforms \"\"\" sim_platforms = [] for agent_name , agent_env in self . gym_env_dict . items (): sim_platforms . append ( self . config . agent_configs [ agent_name ] . platform_config . platform_class ( platform_name = agent_name , platform = agent_env , parts_list = self . config . agent_configs [ agent_name ] . parts_list , disable_exclusivity_check = self . config . disable_exclusivity_check , ) ) return sim_platforms def update_sensor_measurements ( self ): \"\"\" Update and caches all the measurements of all the sensors on each platform \"\"\" for plat in self . sim_platforms : for sensor in plat . sensors : sensor . calculate_and_cache_measurement ( state = self . _state ) def reset ( self , config ): self . _time = 0.0 self . _state . clear () self . _state . obs = {} self . _state . rewards = {} self . _state . dones = {} self . _state . info = {} for agent_name , agent_env in self . gym_env_dict . items (): self . _state . obs [ agent_name ] = agent_env . reset () self . sim_platforms = self . get_platforms () self . update_sensor_measurements () return self . _state def step ( self ): for sim_platform in self . sim_platforms : agent_name = sim_platform . name if sim_platform . operable : tmp = self . gym_env_dict [ agent_name ] . step ( sim_platform . get_applied_action ()) self . _state . obs [ agent_name ] = tmp [ 0 ] self . _state . rewards [ agent_name ] = tmp [ 1 ] self . _state . dones [ agent_name ] = tmp [ 2 ] self . _state . info [ agent_name ] = tmp [ 3 ] if self . _state . dones [ agent_name ]: sim_platform . operable = False self . update_sensor_measurements () self . _time += 1 return self . _state @property def sim_time ( self ) -> float : return self . _time @property def platforms ( self ) -> typing . List : return self . sim_platforms def mark_episode_done ( self , done_info , episode_state ): pass def save_episode_information ( self , dones , rewards , observations ): pass def render ( self , state , mode = \"human\" ): # pylint: disable=unused-argument \"\"\"only render first environment \"\"\" agent = self . gym_env_dict . keys ()[ 0 ] self . gym_env_dict [ agent ] . render ( mode ) get_simulator_validator : Type [ corl . simulators . openai_gym . gym_simulator . OpenAIGymSimulatorValidator ] property readonly \u00a4 Return validator platforms : List property readonly \u00a4 returns a list of platforms in the simulation Returns: Type Description List list of platforms sim_time : float property readonly \u00a4 returns the time Returns: Type Description float float - time get_platforms ( self ) \u00a4 gets the current state of the simulation and makes the sim platforms Returns: Type Description typing.List[OpenAiGymPlatform] -- the list of openai gym platforms Source code in corl/simulators/openai_gym/gym_simulator.py def get_platforms ( self ): \"\"\" gets the current state of the simulation and makes the sim platforms Returns: typing.List[OpenAiGymPlatform] -- the list of openai gym platforms \"\"\" sim_platforms = [] for agent_name , agent_env in self . gym_env_dict . items (): sim_platforms . append ( self . config . agent_configs [ agent_name ] . platform_config . platform_class ( platform_name = agent_name , platform = agent_env , parts_list = self . config . agent_configs [ agent_name ] . parts_list , disable_exclusivity_check = self . config . disable_exclusivity_check , ) ) return sim_platforms mark_episode_done ( self , done_info , episode_state ) \u00a4 Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Source code in corl/simulators/openai_gym/gym_simulator.py def mark_episode_done ( self , done_info , episode_state ): pass render ( self , state , mode = 'human' ) \u00a4 only render first environment Source code in corl/simulators/openai_gym/gym_simulator.py def render ( self , state , mode = \"human\" ): # pylint: disable=unused-argument \"\"\"only render first environment \"\"\" agent = self . gym_env_dict . keys ()[ 0 ] self . gym_env_dict [ agent ] . render ( mode ) reset ( self , config ) \u00a4 reset resets the simulation and sets up a new episode Returns: Type Description StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/openai_gym/gym_simulator.py def reset ( self , config ): self . _time = 0.0 self . _state . clear () self . _state . obs = {} self . _state . rewards = {} self . _state . dones = {} self . _state . info = {} for agent_name , agent_env in self . gym_env_dict . items (): self . _state . obs [ agent_name ] = agent_env . reset () self . sim_platforms = self . get_platforms () self . update_sensor_measurements () return self . _state save_episode_information ( self , dones , rewards , observations ) \u00a4 provides a way to save information about the current episode based on the environment Source code in corl/simulators/openai_gym/gym_simulator.py def save_episode_information ( self , dones , rewards , observations ): pass step ( self ) \u00a4 advances the simulation platforms and returns the state Returns: Type Description StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/openai_gym/gym_simulator.py def step ( self ): for sim_platform in self . sim_platforms : agent_name = sim_platform . name if sim_platform . operable : tmp = self . gym_env_dict [ agent_name ] . step ( sim_platform . get_applied_action ()) self . _state . obs [ agent_name ] = tmp [ 0 ] self . _state . rewards [ agent_name ] = tmp [ 1 ] self . _state . dones [ agent_name ] = tmp [ 2 ] self . _state . info [ agent_name ] = tmp [ 3 ] if self . _state . dones [ agent_name ]: sim_platform . operable = False self . update_sensor_measurements () self . _time += 1 return self . _state update_sensor_measurements ( self ) \u00a4 Update and caches all the measurements of all the sensors on each platform Source code in corl/simulators/openai_gym/gym_simulator.py def update_sensor_measurements ( self ): \"\"\" Update and caches all the measurements of all the sensors on each platform \"\"\" for plat in self . sim_platforms : for sensor in plat . sensors : sensor . calculate_and_cache_measurement ( state = self . _state ) OpenAIGymSimulatorValidator ( BaseSimulatorValidator ) pydantic-model \u00a4 Validator for OpenAIGymSimulatorValidator the name of a gym environment registered to the gym registry Source code in corl/simulators/openai_gym/gym_simulator.py class OpenAIGymSimulatorValidator ( BaseSimulatorValidator ): # pylint: disable=too-few-public-methods \"\"\" Validator for OpenAIGymSimulatorValidator gym_env: the name of a gym environment registered to the gym registry \"\"\" # todo: maybe switch this to a PyObject and do a validator that it # implements gym.core.Env gym_env : str gym_configs : typing . Mapping [ str , typing . Optional [ typing . Union [ bool , float , int , str ]]] = {} seed : int = 1 agent_configs : typing . Mapping [ str , GymAgentConfig ] wrappers : typing . List [ PyObject ] = [] OpenAiGymInclusivePartsPlatform ( OpenAiGymPlatform ) \u00a4 The OpenAiGymInclusivePartsPlatform mirrors OpenAiGymPlatform but without mutually exclusive parts Source code in corl/simulators/openai_gym/gym_simulator.py class OpenAiGymInclusivePartsPlatform ( OpenAiGymPlatform ): \"\"\" The OpenAiGymInclusivePartsPlatform mirrors OpenAiGymPlatform but without mutually exclusive parts \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) if isinstance ( self . action_space , gym . spaces . Discrete ): self . _last_applied_action = 0 elif isinstance ( self . action_space , gym . spaces . Box ): self . _last_applied_action = self . action_space . low self . _operable = True OpenAiGymPlatform ( BasePlatform ) \u00a4 The OpenAiGymPlatform wraps some gym environment as it's platform and allows for saving an action to the platform for when the platform needs to give an action to the environment during the environment step function Source code in corl/simulators/openai_gym/gym_simulator.py class OpenAiGymPlatform ( BasePlatform ): \"\"\" The OpenAiGymPlatform wraps some gym environment as it's platform and allows for saving an action to the platform for when the platform needs to give an action to the environment during the environment step function \"\"\" def __init__ ( self , ** kwargs ): kwargs [ \"exclusive_part_dict\" ] = { BaseController : MutuallyExclusiveParts ({ \"main_controller\" }), BaseSensor : MutuallyExclusiveParts ({ \"state_sensor\" }) } # hack to get this working until platforms are fixed self . config : GymPlatformValidator = self . get_validator ( ** kwargs ) self . _platform = self . config . platform super () . __init__ ( ** kwargs ) if isinstance ( self . action_space , gym . spaces . Discrete ): self . _last_applied_action = 0 elif isinstance ( self . action_space , gym . spaces . Box ): self . _last_applied_action = self . action_space . low self . _operable = True @property def get_validator ( self ) -> typing . Type [ GymPlatformValidator ]: return GymPlatformValidator @property def observation_space ( self ): \"\"\" Provides the observation space for a sensor to use Returns: gym.Space -- the observation space of the platform gym environment \"\"\" return self . _platform . observation_space @property def action_space ( self ): \"\"\" Provides the action space for a controller to use Returns: gym.Space -- the action space of the platform gym environment \"\"\" return self . _platform . action_space def get_applied_action ( self ): \"\"\"returns the action stored in this platform Returns: typing.Any -- any sort of stored action \"\"\" return self . _last_applied_action def save_action_to_platform ( self , action ): \"\"\" saves an action to the platform if it matches the action space Arguments: action typing.Any -- The action to store in the platform Raises: RuntimeError: if the action attempted to be stored does not match the environments action space \"\"\" if not self . action_space . contains ( action ): raise RuntimeError ( \"Error: action attempting to be stored in platform does not match platforms action space\" ) self . _last_applied_action = action @property def operable ( self ): return self . _operable @operable . setter def operable ( self , value ): self . _operable = value action_space property readonly \u00a4 Provides the action space for a controller to use Returns: Type Description gym.Space -- the action space of the platform gym environment get_validator : Type [ corl . simulators . openai_gym . gym_simulator . GymPlatformValidator ] property readonly \u00a4 get validator for this BasePlatform Returns: Type Description Type[corl.simulators.openai_gym.gym_simulator.GymPlatformValidator] BasePlatformValidator -- validator the platform will use to generate a configuration observation_space property readonly \u00a4 Provides the observation space for a sensor to use Returns: Type Description gym.Space -- the observation space of the platform gym environment operable property writable \u00a4 Is the platform operable? Returns \u00a4 bool Is the platform operable? get_applied_action ( self ) \u00a4 returns the action stored in this platform Returns: Type Description typing.Any -- any sort of stored action Source code in corl/simulators/openai_gym/gym_simulator.py def get_applied_action ( self ): \"\"\"returns the action stored in this platform Returns: typing.Any -- any sort of stored action \"\"\" return self . _last_applied_action save_action_to_platform ( self , action ) \u00a4 saves an action to the platform if it matches the action space Exceptions: Type Description RuntimeError if the action attempted to be stored does not match the environments action space Source code in corl/simulators/openai_gym/gym_simulator.py def save_action_to_platform ( self , action ): \"\"\" saves an action to the platform if it matches the action space Arguments: action typing.Any -- The action to store in the platform Raises: RuntimeError: if the action attempted to be stored does not match the environments action space \"\"\" if not self . action_space . contains ( action ): raise RuntimeError ( \"Error: action attempting to be stored in platform does not match platforms action space\" ) self . _last_applied_action = action","title":"Gym simulator"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.GymAgentConfig","text":"any configuration needed for the simulator to initialize this platform and configure it in the sim class a list of tuples where the first element is come python class path of a BasePart , and then the second element is a configuration dictionary for that part Source code in corl/simulators/openai_gym/gym_simulator.py class GymAgentConfig ( AgentConfig ): \"\"\" platform_config: any configuration needed for the simulator to initialize this platform and configure it in the sim class parts_list: a list of tuples where the first element is come python class path of a BasePart, and then the second element is a configuration dictionary for that part Arguments: BaseModel {[type]} -- [description] \"\"\" platform_config : GymPlatformConfig","title":"GymAgentConfig"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.GymPlatformConfig","text":"GymPlatformSimConfig Source code in corl/simulators/openai_gym/gym_simulator.py class GymPlatformConfig ( BaseModel ): \"\"\" GymPlatformSimConfig \"\"\" platform_class : PyObject","title":"GymPlatformConfig"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.GymPlatformValidator","text":"GymPlatformValidator","title":"GymPlatformValidator"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.GymPlatformValidator--parameters","text":"!!! platform \"gym.Env\" Gym env associated with GymPlatform Source code in corl/simulators/openai_gym/gym_simulator.py class GymPlatformValidator ( BasePlatformValidator ): \"\"\"GymPlatformValidator Parameters ---------- platform: gym.Env Gym env associated with GymPlatform \"\"\" platform : gym . Env","title":"Parameters"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator","text":"Simulator backend for running openai Gyms Source code in corl/simulators/openai_gym/gym_simulator.py class OpenAIGymSimulator ( BaseSimulator ): \"\"\" Simulator backend for running openai Gyms \"\"\" @property def get_simulator_validator ( self ) -> typing . Type [ OpenAIGymSimulatorValidator ]: \"\"\"Return validator\"\"\" return OpenAIGymSimulatorValidator def __init__ ( self , ** kwargs ) -> None : self . config : OpenAIGymSimulatorValidator super () . __init__ ( ** kwargs ) self . _state = StateDict () self . gym_env_dict = {} for agent_name in self . config . agent_configs : env = gym . make ( self . config . gym_env , ** self . config . gym_configs ) for wrapper_cls in self . config . wrappers : env = wrapper_cls ( env ) self . gym_env_dict [ agent_name ] = env self . gym_env_dict [ agent_name ] . seed ( self . config . seed ) self . sim_platforms : typing . List = [] self . _time = 0.0 def get_platforms ( self ): \"\"\" gets the current state of the simulation and makes the sim platforms Returns: typing.List[OpenAiGymPlatform] -- the list of openai gym platforms \"\"\" sim_platforms = [] for agent_name , agent_env in self . gym_env_dict . items (): sim_platforms . append ( self . config . agent_configs [ agent_name ] . platform_config . platform_class ( platform_name = agent_name , platform = agent_env , parts_list = self . config . agent_configs [ agent_name ] . parts_list , disable_exclusivity_check = self . config . disable_exclusivity_check , ) ) return sim_platforms def update_sensor_measurements ( self ): \"\"\" Update and caches all the measurements of all the sensors on each platform \"\"\" for plat in self . sim_platforms : for sensor in plat . sensors : sensor . calculate_and_cache_measurement ( state = self . _state ) def reset ( self , config ): self . _time = 0.0 self . _state . clear () self . _state . obs = {} self . _state . rewards = {} self . _state . dones = {} self . _state . info = {} for agent_name , agent_env in self . gym_env_dict . items (): self . _state . obs [ agent_name ] = agent_env . reset () self . sim_platforms = self . get_platforms () self . update_sensor_measurements () return self . _state def step ( self ): for sim_platform in self . sim_platforms : agent_name = sim_platform . name if sim_platform . operable : tmp = self . gym_env_dict [ agent_name ] . step ( sim_platform . get_applied_action ()) self . _state . obs [ agent_name ] = tmp [ 0 ] self . _state . rewards [ agent_name ] = tmp [ 1 ] self . _state . dones [ agent_name ] = tmp [ 2 ] self . _state . info [ agent_name ] = tmp [ 3 ] if self . _state . dones [ agent_name ]: sim_platform . operable = False self . update_sensor_measurements () self . _time += 1 return self . _state @property def sim_time ( self ) -> float : return self . _time @property def platforms ( self ) -> typing . List : return self . sim_platforms def mark_episode_done ( self , done_info , episode_state ): pass def save_episode_information ( self , dones , rewards , observations ): pass def render ( self , state , mode = \"human\" ): # pylint: disable=unused-argument \"\"\"only render first environment \"\"\" agent = self . gym_env_dict . keys ()[ 0 ] self . gym_env_dict [ agent ] . render ( mode )","title":"OpenAIGymSimulator"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.get_simulator_validator","text":"Return validator","title":"get_simulator_validator"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.platforms","text":"returns a list of platforms in the simulation Returns: Type Description List list of platforms","title":"platforms"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.sim_time","text":"returns the time Returns: Type Description float float - time","title":"sim_time"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.get_platforms","text":"gets the current state of the simulation and makes the sim platforms Returns: Type Description typing.List[OpenAiGymPlatform] -- the list of openai gym platforms Source code in corl/simulators/openai_gym/gym_simulator.py def get_platforms ( self ): \"\"\" gets the current state of the simulation and makes the sim platforms Returns: typing.List[OpenAiGymPlatform] -- the list of openai gym platforms \"\"\" sim_platforms = [] for agent_name , agent_env in self . gym_env_dict . items (): sim_platforms . append ( self . config . agent_configs [ agent_name ] . platform_config . platform_class ( platform_name = agent_name , platform = agent_env , parts_list = self . config . agent_configs [ agent_name ] . parts_list , disable_exclusivity_check = self . config . disable_exclusivity_check , ) ) return sim_platforms","title":"get_platforms()"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.mark_episode_done","text":"Takes in the done_info specifying how the episode completed and does any book keeping around ending an episode Source code in corl/simulators/openai_gym/gym_simulator.py def mark_episode_done ( self , done_info , episode_state ): pass","title":"mark_episode_done()"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.render","text":"only render first environment Source code in corl/simulators/openai_gym/gym_simulator.py def render ( self , state , mode = \"human\" ): # pylint: disable=unused-argument \"\"\"only render first environment \"\"\" agent = self . gym_env_dict . keys ()[ 0 ] self . gym_env_dict [ agent ] . render ( mode )","title":"render()"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.reset","text":"reset resets the simulation and sets up a new episode Returns: Type Description StateDict -- The simulation state, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/openai_gym/gym_simulator.py def reset ( self , config ): self . _time = 0.0 self . _state . clear () self . _state . obs = {} self . _state . rewards = {} self . _state . dones = {} self . _state . info = {} for agent_name , agent_env in self . gym_env_dict . items (): self . _state . obs [ agent_name ] = agent_env . reset () self . sim_platforms = self . get_platforms () self . update_sensor_measurements () return self . _state","title":"reset()"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.save_episode_information","text":"provides a way to save information about the current episode based on the environment Source code in corl/simulators/openai_gym/gym_simulator.py def save_episode_information ( self , dones , rewards , observations ): pass","title":"save_episode_information()"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.step","text":"advances the simulation platforms and returns the state Returns: Type Description StateDict -- The state after the simulation updates, has a .sim_platforms attr to access the platforms made by the simulation Source code in corl/simulators/openai_gym/gym_simulator.py def step ( self ): for sim_platform in self . sim_platforms : agent_name = sim_platform . name if sim_platform . operable : tmp = self . gym_env_dict [ agent_name ] . step ( sim_platform . get_applied_action ()) self . _state . obs [ agent_name ] = tmp [ 0 ] self . _state . rewards [ agent_name ] = tmp [ 1 ] self . _state . dones [ agent_name ] = tmp [ 2 ] self . _state . info [ agent_name ] = tmp [ 3 ] if self . _state . dones [ agent_name ]: sim_platform . operable = False self . update_sensor_measurements () self . _time += 1 return self . _state","title":"step()"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulator.update_sensor_measurements","text":"Update and caches all the measurements of all the sensors on each platform Source code in corl/simulators/openai_gym/gym_simulator.py def update_sensor_measurements ( self ): \"\"\" Update and caches all the measurements of all the sensors on each platform \"\"\" for plat in self . sim_platforms : for sensor in plat . sensors : sensor . calculate_and_cache_measurement ( state = self . _state )","title":"update_sensor_measurements()"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAIGymSimulatorValidator","text":"Validator for OpenAIGymSimulatorValidator the name of a gym environment registered to the gym registry Source code in corl/simulators/openai_gym/gym_simulator.py class OpenAIGymSimulatorValidator ( BaseSimulatorValidator ): # pylint: disable=too-few-public-methods \"\"\" Validator for OpenAIGymSimulatorValidator gym_env: the name of a gym environment registered to the gym registry \"\"\" # todo: maybe switch this to a PyObject and do a validator that it # implements gym.core.Env gym_env : str gym_configs : typing . Mapping [ str , typing . Optional [ typing . Union [ bool , float , int , str ]]] = {} seed : int = 1 agent_configs : typing . Mapping [ str , GymAgentConfig ] wrappers : typing . List [ PyObject ] = []","title":"OpenAIGymSimulatorValidator"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymInclusivePartsPlatform","text":"The OpenAiGymInclusivePartsPlatform mirrors OpenAiGymPlatform but without mutually exclusive parts Source code in corl/simulators/openai_gym/gym_simulator.py class OpenAiGymInclusivePartsPlatform ( OpenAiGymPlatform ): \"\"\" The OpenAiGymInclusivePartsPlatform mirrors OpenAiGymPlatform but without mutually exclusive parts \"\"\" def __init__ ( self , ** kwargs ): super () . __init__ ( ** kwargs ) if isinstance ( self . action_space , gym . spaces . Discrete ): self . _last_applied_action = 0 elif isinstance ( self . action_space , gym . spaces . Box ): self . _last_applied_action = self . action_space . low self . _operable = True","title":"OpenAiGymInclusivePartsPlatform"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymPlatform","text":"The OpenAiGymPlatform wraps some gym environment as it's platform and allows for saving an action to the platform for when the platform needs to give an action to the environment during the environment step function Source code in corl/simulators/openai_gym/gym_simulator.py class OpenAiGymPlatform ( BasePlatform ): \"\"\" The OpenAiGymPlatform wraps some gym environment as it's platform and allows for saving an action to the platform for when the platform needs to give an action to the environment during the environment step function \"\"\" def __init__ ( self , ** kwargs ): kwargs [ \"exclusive_part_dict\" ] = { BaseController : MutuallyExclusiveParts ({ \"main_controller\" }), BaseSensor : MutuallyExclusiveParts ({ \"state_sensor\" }) } # hack to get this working until platforms are fixed self . config : GymPlatformValidator = self . get_validator ( ** kwargs ) self . _platform = self . config . platform super () . __init__ ( ** kwargs ) if isinstance ( self . action_space , gym . spaces . Discrete ): self . _last_applied_action = 0 elif isinstance ( self . action_space , gym . spaces . Box ): self . _last_applied_action = self . action_space . low self . _operable = True @property def get_validator ( self ) -> typing . Type [ GymPlatformValidator ]: return GymPlatformValidator @property def observation_space ( self ): \"\"\" Provides the observation space for a sensor to use Returns: gym.Space -- the observation space of the platform gym environment \"\"\" return self . _platform . observation_space @property def action_space ( self ): \"\"\" Provides the action space for a controller to use Returns: gym.Space -- the action space of the platform gym environment \"\"\" return self . _platform . action_space def get_applied_action ( self ): \"\"\"returns the action stored in this platform Returns: typing.Any -- any sort of stored action \"\"\" return self . _last_applied_action def save_action_to_platform ( self , action ): \"\"\" saves an action to the platform if it matches the action space Arguments: action typing.Any -- The action to store in the platform Raises: RuntimeError: if the action attempted to be stored does not match the environments action space \"\"\" if not self . action_space . contains ( action ): raise RuntimeError ( \"Error: action attempting to be stored in platform does not match platforms action space\" ) self . _last_applied_action = action @property def operable ( self ): return self . _operable @operable . setter def operable ( self , value ): self . _operable = value","title":"OpenAiGymPlatform"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymPlatform.action_space","text":"Provides the action space for a controller to use Returns: Type Description gym.Space -- the action space of the platform gym environment","title":"action_space"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymPlatform.get_validator","text":"get validator for this BasePlatform Returns: Type Description Type[corl.simulators.openai_gym.gym_simulator.GymPlatformValidator] BasePlatformValidator -- validator the platform will use to generate a configuration","title":"get_validator"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymPlatform.observation_space","text":"Provides the observation space for a sensor to use Returns: Type Description gym.Space -- the observation space of the platform gym environment","title":"observation_space"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymPlatform.operable","text":"Is the platform operable?","title":"operable"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymPlatform.operable--returns","text":"bool Is the platform operable?","title":"Returns"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymPlatform.get_applied_action","text":"returns the action stored in this platform Returns: Type Description typing.Any -- any sort of stored action Source code in corl/simulators/openai_gym/gym_simulator.py def get_applied_action ( self ): \"\"\"returns the action stored in this platform Returns: typing.Any -- any sort of stored action \"\"\" return self . _last_applied_action","title":"get_applied_action()"},{"location":"reference/simulators/openai_gym/gym_simulator/#corl.simulators.openai_gym.gym_simulator.OpenAiGymPlatform.save_action_to_platform","text":"saves an action to the platform if it matches the action space Exceptions: Type Description RuntimeError if the action attempted to be stored does not match the environments action space Source code in corl/simulators/openai_gym/gym_simulator.py def save_action_to_platform ( self , action ): \"\"\" saves an action to the platform if it matches the action space Arguments: action typing.Any -- The action to store in the platform Raises: RuntimeError: if the action attempted to be stored does not match the environments action space \"\"\" if not self . action_space . contains ( action ): raise RuntimeError ( \"Error: action attempting to be stored in platform does not match platforms action space\" ) self . _last_applied_action = action","title":"save_action_to_platform()"},{"location":"reference/simulators/six_dof/__init__/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details.","title":"Six DOF"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Base Integration Controls Module BasePitchController ( BaseController , ABC ) \u00a4 [summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BasePitchController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"pitch_control\" } exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness BasePitchRollController ( BaseController , ABC ) \u00a4 [summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BasePitchRollController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"pitch_control\" , \"roll_control\" } exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness BaseRollController ( BaseController , ABC ) \u00a4 [summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BaseRollController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"roll_control\" } exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness BaseRollPitchYawSpeedController ( BaseController , ABC ) \u00a4 BaseRollPitchYawSpeedController abstraction of controlling an aircraft Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BaseRollPitchYawSpeedController ( BaseController , abc . ABC ): \"\"\" BaseRollPitchYawSpeedController abstraction of controlling an aircraft \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"roll_control\" , \"pitch_control\" , \"yaw_control\" , \"speed_control\" } exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness BaseSpeedController ( BaseController , ABC ) \u00a4 [summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BaseSpeedController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"speed_control\" } exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness BaseYawController ( BaseController , ABC ) \u00a4 [summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BaseYawController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"yaw_control\" } exclusiveness : Set [ str ] property readonly \u00a4 Return exclusiveness","title":"Base six dof controllers"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BasePitchController","text":"[summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BasePitchController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"pitch_control\" }","title":"BasePitchController"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BasePitchController.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BasePitchRollController","text":"[summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BasePitchRollController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"pitch_control\" , \"roll_control\" }","title":"BasePitchRollController"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BasePitchRollController.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BaseRollController","text":"[summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BaseRollController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"roll_control\" }","title":"BaseRollController"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BaseRollController.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BaseRollPitchYawSpeedController","text":"BaseRollPitchYawSpeedController abstraction of controlling an aircraft Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BaseRollPitchYawSpeedController ( BaseController , abc . ABC ): \"\"\" BaseRollPitchYawSpeedController abstraction of controlling an aircraft \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"roll_control\" , \"pitch_control\" , \"yaw_control\" , \"speed_control\" }","title":"BaseRollPitchYawSpeedController"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BaseRollPitchYawSpeedController.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BaseSpeedController","text":"[summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BaseSpeedController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"speed_control\" }","title":"BaseSpeedController"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BaseSpeedController.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BaseYawController","text":"[summary] Returns: Type Description [type] -- [description] Source code in corl/simulators/six_dof/base_six_dof_controllers.py class BaseYawController ( BaseController , abc . ABC ): \"\"\"[summary] Arguments: BaseController {[type]} -- [description] abc {[type]} -- [description] Returns: [type] -- [description] \"\"\" @property def exclusiveness ( self ) -> typing . Set [ str ]: \"\"\"Return exclusiveness\"\"\" return { \"yaw_control\" }","title":"BaseYawController"},{"location":"reference/simulators/six_dof/base_six_dof_controllers/#corl.simulators.six_dof.base_six_dof_controllers.BaseYawController.exclusiveness","text":"Return exclusiveness","title":"exclusiveness"},{"location":"reference/simulators/six_dof/base_six_dof_platform/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. Abstaction Class for 6DOF platform types that can provide some properties for common usage Base6DOFPlatform ( BasePlatform ) \u00a4 Further abstraction of BasePlatform, this adds some common 6dof properties that may be useful for dealing with simulators using aircraft Source code in corl/simulators/six_dof/base_six_dof_platform.py class Base6DOFPlatform ( BasePlatform ): \"\"\" Further abstraction of BasePlatform, this adds some common 6dof properties that may be useful for dealing with simulators using aircraft \"\"\" def __init__ ( self , platform_name , platform , parts_list , exclusive_part_dict = None , disable_exclusivity_check = False ): if exclusive_part_dict is None : exclusive_part_dict = { BaseController : MutuallyExclusiveParts ({ \"yaw_control\" , \"pitch_control\" , \"roll_control\" , \"speed_control\" }, allow_other_keys = True ), BaseSensor : MutuallyExclusiveParts ({}, allow_other_keys = True ) } super () . __init__ ( platform_name = platform_name , platform = platform , parts_list = parts_list , exclusive_part_dict = exclusive_part_dict , disable_exclusivity_check = disable_exclusivity_check ) @property @abc . abstractmethod def position ( self ) -> np . ndarray : \"\"\" The position of the object in space. Details on the position format are provided by position_properties Returns ------- np.ndarray The position of the object in space \"\"\" ... position_properties = six_dof_props . LatLonAltProp () @property @abc . abstractmethod def orientation ( self ) -> np . ndarray : \"\"\" The orientation of the platform. For orientation formatting see orientation_properties Returns ------- np.ndarray The orientation of the platform \"\"\" ... orientation_properties = six_dof_props . OrientationProp () @property @abc . abstractmethod def velocity_ned ( self ) -> np . ndarray : \"\"\"get the velocity in true airspeed NED (m/s) Returns: np.ndarray -- The velocity in true airspeed NED (m/s) \"\"\" ... velocity_ned_properties = six_dof_props . VelocityNEDProp () angular_velocity_properties = six_dof_props . VelocityNEDProp ( description = \"angular velocity for yaw rate, pitch rate and roll rate respectively\" ) @property @abc . abstractmethod def acceleration_ned ( self ) -> np . ndarray : \"\"\"gets the acceleration in the NED Returns: np.ndarray -- The acceleration in the NED \"\"\" ... acceleration_ned_properties = six_dof_props . AccelerationNEDProp () @property @abc . abstractmethod def speed ( self ) -> np . ndarray : \"\"\"Get the speed of the platform Returns: np.ndarray -- The true airspeed of the platform in m/s \"\"\" ... speed_properties = six_dof_props . TrueAirSpeedProp ( name = \"speed\" , high = [ 1700.0 ], unit = [ \"mpstas\" ], description = \"true airspeed in m/s\" ) @property @abc . abstractmethod def controllers ( self ) -> typing . Tuple [ BaseController , ... ]: \"\"\" The controllers for this platform. This controls properties of the platform itself related to movement. For example hold heading, set throttle to X, pitch at this rate, etc. \"\"\" ... @property @abc . abstractmethod def sensors ( self ) -> typing . Tuple [ BaseSensor , ... ]: \"\"\" A list of the sensors for this platform. Sensors could be altitude, airspeed, etc. \"\"\" ... acceleration_ned : ndarray property readonly \u00a4 gets the acceleration in the NED Returns: Type Description ndarray np.ndarray -- The acceleration in the NED controllers : Tuple [ corl . simulators . base_parts . BaseController , ... ] property readonly \u00a4 The controllers for this platform. This controls properties of the platform itself related to movement. For example hold heading, set throttle to X, pitch at this rate, etc. orientation : ndarray property readonly \u00a4 The orientation of the platform. For orientation formatting see orientation_properties Returns \u00a4 np.ndarray The orientation of the platform position : ndarray property readonly \u00a4 The position of the object in space. Details on the position format are provided by position_properties Returns \u00a4 np.ndarray The position of the object in space sensors : Tuple [ corl . simulators . base_parts . BaseSensor , ... ] property readonly \u00a4 A list of the sensors for this platform. Sensors could be altitude, airspeed, etc. speed : ndarray property readonly \u00a4 Get the speed of the platform Returns: Type Description ndarray np.ndarray -- The true airspeed of the platform in m/s velocity_ned : ndarray property readonly \u00a4 get the velocity in true airspeed NED (m/s) Returns: Type Description ndarray np.ndarray -- The velocity in true airspeed NED (m/s)","title":"Base six dof platform"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform","text":"Further abstraction of BasePlatform, this adds some common 6dof properties that may be useful for dealing with simulators using aircraft Source code in corl/simulators/six_dof/base_six_dof_platform.py class Base6DOFPlatform ( BasePlatform ): \"\"\" Further abstraction of BasePlatform, this adds some common 6dof properties that may be useful for dealing with simulators using aircraft \"\"\" def __init__ ( self , platform_name , platform , parts_list , exclusive_part_dict = None , disable_exclusivity_check = False ): if exclusive_part_dict is None : exclusive_part_dict = { BaseController : MutuallyExclusiveParts ({ \"yaw_control\" , \"pitch_control\" , \"roll_control\" , \"speed_control\" }, allow_other_keys = True ), BaseSensor : MutuallyExclusiveParts ({}, allow_other_keys = True ) } super () . __init__ ( platform_name = platform_name , platform = platform , parts_list = parts_list , exclusive_part_dict = exclusive_part_dict , disable_exclusivity_check = disable_exclusivity_check ) @property @abc . abstractmethod def position ( self ) -> np . ndarray : \"\"\" The position of the object in space. Details on the position format are provided by position_properties Returns ------- np.ndarray The position of the object in space \"\"\" ... position_properties = six_dof_props . LatLonAltProp () @property @abc . abstractmethod def orientation ( self ) -> np . ndarray : \"\"\" The orientation of the platform. For orientation formatting see orientation_properties Returns ------- np.ndarray The orientation of the platform \"\"\" ... orientation_properties = six_dof_props . OrientationProp () @property @abc . abstractmethod def velocity_ned ( self ) -> np . ndarray : \"\"\"get the velocity in true airspeed NED (m/s) Returns: np.ndarray -- The velocity in true airspeed NED (m/s) \"\"\" ... velocity_ned_properties = six_dof_props . VelocityNEDProp () angular_velocity_properties = six_dof_props . VelocityNEDProp ( description = \"angular velocity for yaw rate, pitch rate and roll rate respectively\" ) @property @abc . abstractmethod def acceleration_ned ( self ) -> np . ndarray : \"\"\"gets the acceleration in the NED Returns: np.ndarray -- The acceleration in the NED \"\"\" ... acceleration_ned_properties = six_dof_props . AccelerationNEDProp () @property @abc . abstractmethod def speed ( self ) -> np . ndarray : \"\"\"Get the speed of the platform Returns: np.ndarray -- The true airspeed of the platform in m/s \"\"\" ... speed_properties = six_dof_props . TrueAirSpeedProp ( name = \"speed\" , high = [ 1700.0 ], unit = [ \"mpstas\" ], description = \"true airspeed in m/s\" ) @property @abc . abstractmethod def controllers ( self ) -> typing . Tuple [ BaseController , ... ]: \"\"\" The controllers for this platform. This controls properties of the platform itself related to movement. For example hold heading, set throttle to X, pitch at this rate, etc. \"\"\" ... @property @abc . abstractmethod def sensors ( self ) -> typing . Tuple [ BaseSensor , ... ]: \"\"\" A list of the sensors for this platform. Sensors could be altitude, airspeed, etc. \"\"\" ...","title":"Base6DOFPlatform"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.acceleration_ned","text":"gets the acceleration in the NED Returns: Type Description ndarray np.ndarray -- The acceleration in the NED","title":"acceleration_ned"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.controllers","text":"The controllers for this platform. This controls properties of the platform itself related to movement. For example hold heading, set throttle to X, pitch at this rate, etc.","title":"controllers"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.orientation","text":"The orientation of the platform. For orientation formatting see orientation_properties","title":"orientation"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.orientation--returns","text":"np.ndarray The orientation of the platform","title":"Returns"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.position","text":"The position of the object in space. Details on the position format are provided by position_properties","title":"position"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.position--returns","text":"np.ndarray The position of the object in space","title":"Returns"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.sensors","text":"A list of the sensors for this platform. Sensors could be altitude, airspeed, etc.","title":"sensors"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.speed","text":"Get the speed of the platform Returns: Type Description ndarray np.ndarray -- The true airspeed of the platform in m/s","title":"speed"},{"location":"reference/simulators/six_dof/base_six_dof_platform/#corl.simulators.six_dof.base_six_dof_platform.Base6DOFPlatform.velocity_ned","text":"get the velocity in true airspeed NED (m/s) Returns: Type Description ndarray np.ndarray -- The velocity in true airspeed NED (m/s)","title":"velocity_ned"},{"location":"reference/simulators/six_dof/base_six_dof_properties/","text":"Air Force Research Laboratory (AFRL) Autonomous Capabilities Team (ACT3) Reinforcement Learning (RL) Core. This is a US Government Work not subject to copyright protection in the US. The use, dissemination or disclosure of data in this file is subject to limitation or restriction. See accompanying README and LICENSE for details. property definitions AccelerationNEDProp ( BoxProp ) pydantic-model \u00a4 Acceleration NED space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AccelerationNEDProp ( BoxProp ): \"\"\" Acceleration NED space \"\"\" name : str = \"acceleration_ned\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - 3000.0 ] * 3 high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ 3000.0 ] * 3 unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"m/s^2\" ] * 3 description : str = \"accleration in true airspeed (m/s^2) along north, east, down axis\" AltitudePropFeet ( BoxProp ) pydantic-model \u00a4 Altitude space meters ft Source code in corl/simulators/six_dof/base_six_dof_properties.py class AltitudePropFeet ( BoxProp ): \"\"\" Altitude space meters ft \"\"\" name : str = \"altitude\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"ft\" ] description : str = \"true altitude of the platform in ft\" AltitudePropMeters ( BoxProp ) pydantic-model \u00a4 Altitude space meters Source code in corl/simulators/six_dof/base_six_dof_properties.py class AltitudePropMeters ( BoxProp ): \"\"\" Altitude space meters \"\"\" name : str = \"altitude\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"meters\" ] description : str = \"true altitude of the platform in meters\" AltitudeRateProp ( BoxProp ) pydantic-model \u00a4 Altitude rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AltitudeRateProp ( BoxProp ): \"\"\" Altitude rate space \"\"\" name : str = \"altitude_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] description : str = \"true altitude rate of the platform\" AngleOfAttackProp ( BoxProp ) pydantic-model \u00a4 Angle of attack space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AngleOfAttackProp ( BoxProp ): \"\"\" Angle of attack space \"\"\" name : str = \"angle_of_attack\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"The angle of attack of the platform in degrees\" AngleOfAttackRateProp ( BoxProp ) pydantic-model \u00a4 Angle of attack rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AngleOfAttackRateProp ( BoxProp ): \"\"\" Angle of attack rate space \"\"\" name : str = \"angle_of_attack_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"The angle of attack rate of the platform in degrees/s\" AngleOfSideSlipProp ( BoxProp ) pydantic-model \u00a4 Angle of slide slip Source code in corl/simulators/six_dof/base_six_dof_properties.py class AngleOfSideSlipProp ( BoxProp ): \"\"\" Angle of slide slip \"\"\" name : str = \"angle_of_side_slip\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 90.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 90.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"The angle of side slip of the platform in degrees\" AngleOfSideSlipRateProp ( BoxProp ) pydantic-model \u00a4 Angle of slide slip rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AngleOfSideSlipRateProp ( BoxProp ): \"\"\" Angle of slide slip rate space \"\"\" name : str = \"angle_of_side_slip_rate_dps\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"The angle of side slip rate of the platform in degrees/sec\" FlightPathAngleProp ( BoxProp ) pydantic-model \u00a4 Flight path angle space in degrees Source code in corl/simulators/six_dof/base_six_dof_properties.py class FlightPathAngleProp ( BoxProp ): \"\"\" Flight path angle space in degrees \"\"\" name : str = \"flight_path_angle_deg\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"flight path angle of the platform in degrees\" FlightPathAngleRadProp ( BoxProp ) pydantic-model \u00a4 Flight path angle in radians Source code in corl/simulators/six_dof/base_six_dof_properties.py class FlightPathAngleRadProp ( BoxProp ): \"\"\" Flight path angle in radians \"\"\" name : str = \"flight_path_angle_rad\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - np . pi / 2 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ np . pi / 2 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"rad\" ] description : str = \"flight path angle of the platform in rad\" FuelProp ( BoxProp ) pydantic-model \u00a4 Fuel space in fuel / total fuel for the platform Source code in corl/simulators/six_dof/base_six_dof_properties.py class FuelProp ( BoxProp ): \"\"\" Fuel space in fuel / total fuel for the platform \"\"\" name : str = \"fuel_percentage\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 1.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"fraction\" ] description : str = \"ratio of remaining fuel / total fuel, 0 is empty and 1 is full\" FuelWeightProp ( BoxProp ) pydantic-model \u00a4 Fuel weight space Source code in corl/simulators/six_dof/base_six_dof_properties.py class FuelWeightProp ( BoxProp ): \"\"\" Fuel weight space \"\"\" name : str = \"fuel_weight_lbs\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 10000.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"lbs\" ] description : str = \"Fuel weight\" GloadNxProp ( GloadProp ) pydantic-model \u00a4 Gload space in x Source code in corl/simulators/six_dof/base_six_dof_properties.py class GloadNxProp ( GloadProp ): \"\"\" Gload space in x \"\"\" name : str = \"g_load NX\" GloadNyProp ( GloadProp ) pydantic-model \u00a4 Gload space in y Source code in corl/simulators/six_dof/base_six_dof_properties.py class GloadNyProp ( GloadProp ): \"\"\" Gload space in y \"\"\" name : str = \"g_load NY\" GloadNzProp ( GloadProp ) pydantic-model \u00a4 Gload space in z Source code in corl/simulators/six_dof/base_six_dof_properties.py class GloadNzProp ( GloadProp ): \"\"\" Gload space in z \"\"\" name : str = \"g_load NZ\" GloadProp ( BoxProp ) pydantic-model \u00a4 Gload space Source code in corl/simulators/six_dof/base_six_dof_properties.py class GloadProp ( BoxProp ): \"\"\" Gload space \"\"\" name : str = \"g_load\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 20.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 20.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"G\" ] description : str = \"G load on the platform\" KcasProp ( BoxProp ) pydantic-model \u00a4 Kcas speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class KcasProp ( BoxProp ): \"\"\" Kcas speed space \"\"\" name : str = \"speed_kcas\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"kcas\" ] description : str = \"KCAS (in knots)\" KiasProp ( BoxProp ) pydantic-model \u00a4 Kias speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class KiasProp ( BoxProp ): \"\"\" Kias speed space \"\"\" name : str = \"speed_kias\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"kias\" ] description : str = \"KIAS (in knots)\" KtasProp ( BoxProp ) pydantic-model \u00a4 Ktas speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class KtasProp ( BoxProp ): \"\"\" Ktas speed space \"\"\" name : str = \"speed_ktas\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"ktas\" ] description : str = \"KTAS (in knots)\" LatLonAltProp ( BoxProp ) pydantic-model \u00a4 Lat Lon Alt space Source code in corl/simulators/six_dof/base_six_dof_properties.py class LatLonAltProp ( BoxProp ): \"\"\" Lat Lon Alt space \"\"\" name : str = \"position\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - 90.0 , - 180.0 , 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ 90.0 , 180.0 , 0.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"deg\" , \"deg\" , \"m\" ] description : str = \"Geodetic Latitude, Longitude, Altitude. Altitude is measured above WGS-84 ellipsoid\" LatLonProp ( BoxProp ) pydantic-model \u00a4 Lat Lon space Source code in corl/simulators/six_dof/base_six_dof_properties.py class LatLonProp ( BoxProp ): \"\"\" Lat Lon space \"\"\" name : str = \"LatLon\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 2 , max_items = 2 )] = [ - 90.0 , - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 2 , max_items = 2 )] = [ 90.0 , 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 2 , max_items = 2 )] = [ \"deg\" , \"deg\" ] description : str = \"Lat Lon\" MachProp ( BoxProp ) pydantic-model \u00a4 Mach speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class MachProp ( BoxProp ): \"\"\" Mach speed space \"\"\" name : str = \"speed_mach\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"Ma\" ] description : str = \"speed of the platform in Mach\" OrientationProp ( BoxProp ) pydantic-model \u00a4 Orientation space Source code in corl/simulators/six_dof/base_six_dof_properties.py class OrientationProp ( BoxProp ): \"\"\" Orientation space \"\"\" name : str = \"orientation\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - np . pi ] * 3 high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ np . pi ] * 3 unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"rad\" ] * 3 description : str = \"yaw/heading, pitch, roll. Orientation is relative to the NED frame\" OrientationRateProp ( BoxProp ) pydantic-model \u00a4 Orientation rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class OrientationRateProp ( BoxProp ): \"\"\" Orientation rate space \"\"\" name : str = \"orientation_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - 3 * np . pi , - 3 * np . pi , - 3 * np . pi ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ 3 * np . pi , 3 * np . pi , 3 * np . pi ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"rad/s\" ] * 3 description : str = \"yaw rate, pitch rate, roll rate\" PitchProp ( BoxProp ) pydantic-model \u00a4 Pitch space Source code in corl/simulators/six_dof/base_six_dof_properties.py class PitchProp ( BoxProp ): \"\"\" Pitch space \"\"\" name : str = \"orientation_pitch\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"Get RPM Pitch (deg)\" PitchRateProp ( BoxProp ) pydantic-model \u00a4 Pitch rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class PitchRateProp ( BoxProp ): \"\"\" Pitch rate space \"\"\" name : str = \"orientation_pitch_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 3 * 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 3 * 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"Get RPM pitch rate (deg/sec)\" RollProp ( BoxProp ) pydantic-model \u00a4 Roll space Source code in corl/simulators/six_dof/base_six_dof_properties.py class RollProp ( BoxProp ): \"\"\" Roll space \"\"\" name : str = \"orientation_roll\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"Get RPM Roll (deg)\" RollRateProp ( BoxProp ) pydantic-model \u00a4 Roll rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class RollRateProp ( BoxProp ): \"\"\" Roll rate space \"\"\" name : str = \"orientation_roll_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 3 * 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 3 * 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"Get RPM roll rate (deg/sec)\" TrueAirSpeedProp ( BoxProp ) pydantic-model \u00a4 True airspeed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class TrueAirSpeedProp ( BoxProp ): \"\"\" True airspeed space \"\"\" name : str = \"true_air_speed_fts\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"fpstas\" ] description : str = \"true airspeed of the platform in feet per second\" VelocityNEDProp ( BoxProp ) pydantic-model \u00a4 Velocity NED space Source code in corl/simulators/six_dof/base_six_dof_properties.py class VelocityNEDProp ( BoxProp ): \"\"\" Velocity NED space \"\"\" name : str = \"velocity_ned\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - 3000.0 ] * 3 high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ 3000.0 ] * 3 unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"mpstas\" ] * 3 description : str = \"velocity in true airspeed (m/s) along north, east, down axis\" WindDirectionProp ( BoxProp ) pydantic-model \u00a4 Wind direction space Source code in corl/simulators/six_dof/base_six_dof_properties.py class WindDirectionProp ( BoxProp ): \"\"\" Wind direction space \"\"\" # https://docs.google.com/spreadsheets/d/1L7D4uqVQzY7rODqtnumB0Kv0-veQ1bHIItLWWrpOOmA/edit#gid=0 name : str = \"wind_direction_deg\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 360.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"Wind Direction in Degrees of the Platform\" WindSpeedProp ( BoxProp ) pydantic-model \u00a4 Wind speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class WindSpeedProp ( BoxProp ): \"\"\" Wind speed space \"\"\" # https://docs.google.com/spreadsheets/d/1L7D4uqVQzY7rODqtnumB0Kv0-veQ1bHIItLWWrpOOmA/edit#gid=0 name : str = \"wind_speed_kts\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 200.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"kts\" ] description : str = \"Wind speed in kts of the Platform\" YawProp ( BoxProp ) pydantic-model \u00a4 Yaw space Source code in corl/simulators/six_dof/base_six_dof_properties.py class YawProp ( BoxProp ): \"\"\" Yaw space \"\"\" name : str = \"orientation_yaw\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"Get RPM Yaw (deg)\" YawRateProp ( BoxProp ) pydantic-model \u00a4 Yaw rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class YawRateProp ( BoxProp ): \"\"\" Yaw rate space \"\"\" name : str = \"orientation_yaw_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 3 * 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 3 * 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"Get RPM yaw rate (deg/sec)\"","title":"Base six dof properties"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.AccelerationNEDProp","text":"Acceleration NED space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AccelerationNEDProp ( BoxProp ): \"\"\" Acceleration NED space \"\"\" name : str = \"acceleration_ned\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - 3000.0 ] * 3 high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ 3000.0 ] * 3 unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"m/s^2\" ] * 3 description : str = \"accleration in true airspeed (m/s^2) along north, east, down axis\"","title":"AccelerationNEDProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.AltitudePropFeet","text":"Altitude space meters ft Source code in corl/simulators/six_dof/base_six_dof_properties.py class AltitudePropFeet ( BoxProp ): \"\"\" Altitude space meters ft \"\"\" name : str = \"altitude\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"ft\" ] description : str = \"true altitude of the platform in ft\"","title":"AltitudePropFeet"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.AltitudePropMeters","text":"Altitude space meters Source code in corl/simulators/six_dof/base_six_dof_properties.py class AltitudePropMeters ( BoxProp ): \"\"\" Altitude space meters \"\"\" name : str = \"altitude\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"meters\" ] description : str = \"true altitude of the platform in meters\"","title":"AltitudePropMeters"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.AltitudeRateProp","text":"Altitude rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AltitudeRateProp ( BoxProp ): \"\"\" Altitude rate space \"\"\" name : str = \"altitude_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] description : str = \"true altitude rate of the platform\"","title":"AltitudeRateProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.AngleOfAttackProp","text":"Angle of attack space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AngleOfAttackProp ( BoxProp ): \"\"\" Angle of attack space \"\"\" name : str = \"angle_of_attack\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"The angle of attack of the platform in degrees\"","title":"AngleOfAttackProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.AngleOfAttackRateProp","text":"Angle of attack rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AngleOfAttackRateProp ( BoxProp ): \"\"\" Angle of attack rate space \"\"\" name : str = \"angle_of_attack_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"The angle of attack rate of the platform in degrees/s\"","title":"AngleOfAttackRateProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.AngleOfSideSlipProp","text":"Angle of slide slip Source code in corl/simulators/six_dof/base_six_dof_properties.py class AngleOfSideSlipProp ( BoxProp ): \"\"\" Angle of slide slip \"\"\" name : str = \"angle_of_side_slip\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 90.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 90.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"The angle of side slip of the platform in degrees\"","title":"AngleOfSideSlipProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.AngleOfSideSlipRateProp","text":"Angle of slide slip rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class AngleOfSideSlipRateProp ( BoxProp ): \"\"\" Angle of slide slip rate space \"\"\" name : str = \"angle_of_side_slip_rate_dps\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"The angle of side slip rate of the platform in degrees/sec\"","title":"AngleOfSideSlipRateProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.FlightPathAngleProp","text":"Flight path angle space in degrees Source code in corl/simulators/six_dof/base_six_dof_properties.py class FlightPathAngleProp ( BoxProp ): \"\"\" Flight path angle space in degrees \"\"\" name : str = \"flight_path_angle_deg\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"flight path angle of the platform in degrees\"","title":"FlightPathAngleProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.FlightPathAngleRadProp","text":"Flight path angle in radians Source code in corl/simulators/six_dof/base_six_dof_properties.py class FlightPathAngleRadProp ( BoxProp ): \"\"\" Flight path angle in radians \"\"\" name : str = \"flight_path_angle_rad\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - np . pi / 2 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ np . pi / 2 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"rad\" ] description : str = \"flight path angle of the platform in rad\"","title":"FlightPathAngleRadProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.FuelProp","text":"Fuel space in fuel / total fuel for the platform Source code in corl/simulators/six_dof/base_six_dof_properties.py class FuelProp ( BoxProp ): \"\"\" Fuel space in fuel / total fuel for the platform \"\"\" name : str = \"fuel_percentage\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 1.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"fraction\" ] description : str = \"ratio of remaining fuel / total fuel, 0 is empty and 1 is full\"","title":"FuelProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.FuelWeightProp","text":"Fuel weight space Source code in corl/simulators/six_dof/base_six_dof_properties.py class FuelWeightProp ( BoxProp ): \"\"\" Fuel weight space \"\"\" name : str = \"fuel_weight_lbs\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 10000.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"lbs\" ] description : str = \"Fuel weight\"","title":"FuelWeightProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.GloadNxProp","text":"Gload space in x Source code in corl/simulators/six_dof/base_six_dof_properties.py class GloadNxProp ( GloadProp ): \"\"\" Gload space in x \"\"\" name : str = \"g_load NX\"","title":"GloadNxProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.GloadNyProp","text":"Gload space in y Source code in corl/simulators/six_dof/base_six_dof_properties.py class GloadNyProp ( GloadProp ): \"\"\" Gload space in y \"\"\" name : str = \"g_load NY\"","title":"GloadNyProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.GloadNzProp","text":"Gload space in z Source code in corl/simulators/six_dof/base_six_dof_properties.py class GloadNzProp ( GloadProp ): \"\"\" Gload space in z \"\"\" name : str = \"g_load NZ\"","title":"GloadNzProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.GloadProp","text":"Gload space Source code in corl/simulators/six_dof/base_six_dof_properties.py class GloadProp ( BoxProp ): \"\"\" Gload space \"\"\" name : str = \"g_load\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 20.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 20.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"G\" ] description : str = \"G load on the platform\"","title":"GloadProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.KcasProp","text":"Kcas speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class KcasProp ( BoxProp ): \"\"\" Kcas speed space \"\"\" name : str = \"speed_kcas\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"kcas\" ] description : str = \"KCAS (in knots)\"","title":"KcasProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.KiasProp","text":"Kias speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class KiasProp ( BoxProp ): \"\"\" Kias speed space \"\"\" name : str = \"speed_kias\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"kias\" ] description : str = \"KIAS (in knots)\"","title":"KiasProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.KtasProp","text":"Ktas speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class KtasProp ( BoxProp ): \"\"\" Ktas speed space \"\"\" name : str = \"speed_ktas\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"ktas\" ] description : str = \"KTAS (in knots)\"","title":"KtasProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.LatLonAltProp","text":"Lat Lon Alt space Source code in corl/simulators/six_dof/base_six_dof_properties.py class LatLonAltProp ( BoxProp ): \"\"\" Lat Lon Alt space \"\"\" name : str = \"position\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - 90.0 , - 180.0 , 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ 90.0 , 180.0 , 0.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"deg\" , \"deg\" , \"m\" ] description : str = \"Geodetic Latitude, Longitude, Altitude. Altitude is measured above WGS-84 ellipsoid\"","title":"LatLonAltProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.LatLonProp","text":"Lat Lon space Source code in corl/simulators/six_dof/base_six_dof_properties.py class LatLonProp ( BoxProp ): \"\"\" Lat Lon space \"\"\" name : str = \"LatLon\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 2 , max_items = 2 )] = [ - 90.0 , - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 2 , max_items = 2 )] = [ 90.0 , 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 2 , max_items = 2 )] = [ \"deg\" , \"deg\" ] description : str = \"Lat Lon\"","title":"LatLonProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.MachProp","text":"Mach speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class MachProp ( BoxProp ): \"\"\" Mach speed space \"\"\" name : str = \"speed_mach\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"Ma\" ] description : str = \"speed of the platform in Mach\"","title":"MachProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.OrientationProp","text":"Orientation space Source code in corl/simulators/six_dof/base_six_dof_properties.py class OrientationProp ( BoxProp ): \"\"\" Orientation space \"\"\" name : str = \"orientation\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - np . pi ] * 3 high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ np . pi ] * 3 unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"rad\" ] * 3 description : str = \"yaw/heading, pitch, roll. Orientation is relative to the NED frame\"","title":"OrientationProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.OrientationRateProp","text":"Orientation rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class OrientationRateProp ( BoxProp ): \"\"\" Orientation rate space \"\"\" name : str = \"orientation_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - 3 * np . pi , - 3 * np . pi , - 3 * np . pi ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ 3 * np . pi , 3 * np . pi , 3 * np . pi ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"rad/s\" ] * 3 description : str = \"yaw rate, pitch rate, roll rate\"","title":"OrientationRateProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.PitchProp","text":"Pitch space Source code in corl/simulators/six_dof/base_six_dof_properties.py class PitchProp ( BoxProp ): \"\"\" Pitch space \"\"\" name : str = \"orientation_pitch\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"Get RPM Pitch (deg)\"","title":"PitchProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.PitchRateProp","text":"Pitch rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class PitchRateProp ( BoxProp ): \"\"\" Pitch rate space \"\"\" name : str = \"orientation_pitch_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 3 * 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 3 * 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"Get RPM pitch rate (deg/sec)\"","title":"PitchRateProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.RollProp","text":"Roll space Source code in corl/simulators/six_dof/base_six_dof_properties.py class RollProp ( BoxProp ): \"\"\" Roll space \"\"\" name : str = \"orientation_roll\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"Get RPM Roll (deg)\"","title":"RollProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.RollRateProp","text":"Roll rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class RollRateProp ( BoxProp ): \"\"\" Roll rate space \"\"\" name : str = \"orientation_roll_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 3 * 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 3 * 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"Get RPM roll rate (deg/sec)\"","title":"RollRateProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.TrueAirSpeedProp","text":"True airspeed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class TrueAirSpeedProp ( BoxProp ): \"\"\" True airspeed space \"\"\" name : str = \"true_air_speed_fts\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"fpstas\" ] description : str = \"true airspeed of the platform in feet per second\"","title":"TrueAirSpeedProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.VelocityNEDProp","text":"Velocity NED space Source code in corl/simulators/six_dof/base_six_dof_properties.py class VelocityNEDProp ( BoxProp ): \"\"\" Velocity NED space \"\"\" name : str = \"velocity_ned\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ - 3000.0 ] * 3 high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 3 , max_items = 3 )] = [ 3000.0 ] * 3 unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 3 , max_items = 3 )] = [ \"mpstas\" ] * 3 description : str = \"velocity in true airspeed (m/s) along north, east, down axis\"","title":"VelocityNEDProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.WindDirectionProp","text":"Wind direction space Source code in corl/simulators/six_dof/base_six_dof_properties.py class WindDirectionProp ( BoxProp ): \"\"\" Wind direction space \"\"\" # https://docs.google.com/spreadsheets/d/1L7D4uqVQzY7rODqtnumB0Kv0-veQ1bHIItLWWrpOOmA/edit#gid=0 name : str = \"wind_direction_deg\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 360.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"Wind Direction in Degrees of the Platform\"","title":"WindDirectionProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.WindSpeedProp","text":"Wind speed space Source code in corl/simulators/six_dof/base_six_dof_properties.py class WindSpeedProp ( BoxProp ): \"\"\" Wind speed space \"\"\" # https://docs.google.com/spreadsheets/d/1L7D4uqVQzY7rODqtnumB0Kv0-veQ1bHIItLWWrpOOmA/edit#gid=0 name : str = \"wind_speed_kts\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 0.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 200.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"kts\" ] description : str = \"Wind speed in kts of the Platform\"","title":"WindSpeedProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.YawProp","text":"Yaw space Source code in corl/simulators/six_dof/base_six_dof_properties.py class YawProp ( BoxProp ): \"\"\" Yaw space \"\"\" name : str = \"orientation_yaw\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg\" ] description : str = \"Get RPM Yaw (deg)\"","title":"YawProp"},{"location":"reference/simulators/six_dof/base_six_dof_properties/#corl.simulators.six_dof.base_six_dof_properties.YawRateProp","text":"Yaw rate space Source code in corl/simulators/six_dof/base_six_dof_properties.py class YawRateProp ( BoxProp ): \"\"\" Yaw rate space \"\"\" name : str = \"orientation_yaw_rate\" low : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ - 3 * 180.0 ] high : Annotated [ typing . List [ StrictFloat ], Field ( min_items = 1 , max_items = 1 )] = [ 3 * 180.0 ] unit : Annotated [ typing . List [ StrictStr ], Field ( min_items = 1 , max_items = 1 )] = [ \"deg/s\" ] description : str = \"Get RPM yaw rate (deg/sec)\"","title":"YawRateProp"},{"location":"tasks/docking_1d/","text":"corl 1D Docking Example \u00a4 Intro \u00a4 This document explains how to quickly get started working with the corl. It will walk you through installation of the corl repo and dependencies and how to launch a training loop for a simple environment: 1D Docking. The code for the 1D Docking environment serves as a documented example of how to interface with the corl framework to create a custom environment. Installation \u00a4 First clone the corl git repository ( https://git.act3-ace.com/act3-rl/corl ) to your working directory. For example: git clone https://git.act3-ace.com/act3-rl/corl.git Navigate to the root of the local repository and use pip to install corl dependencies with the following commands. Note: setting up a project-specific environment is recommended before running these commands. pip install -e path/to/corl pip install -r path/to/corl/requirements.txt pip install tensorflow Training \u00a4 To launch a training loop, the module /corl/train.py is used. This module must be passed the necessary config files at launch. From the root of the repository, execute the following command: python corl/train_rl.py --cfg config/experiments/docking_1d.yml --compute-platform local The config path after the --config flag defines the task, while the three strings following the -ac flag define the agent name, agent config path, and platform config path respectively. Multiple -ac flags may be used to define multiagent environments.","title":"Docking 1D"},{"location":"tasks/docking_1d/#corl-1d-docking-example","text":"","title":"corl 1D Docking Example"},{"location":"tasks/docking_1d/#intro","text":"This document explains how to quickly get started working with the corl. It will walk you through installation of the corl repo and dependencies and how to launch a training loop for a simple environment: 1D Docking. The code for the 1D Docking environment serves as a documented example of how to interface with the corl framework to create a custom environment.","title":"Intro"},{"location":"tasks/docking_1d/#installation","text":"First clone the corl git repository ( https://git.act3-ace.com/act3-rl/corl ) to your working directory. For example: git clone https://git.act3-ace.com/act3-rl/corl.git Navigate to the root of the local repository and use pip to install corl dependencies with the following commands. Note: setting up a project-specific environment is recommended before running these commands. pip install -e path/to/corl pip install -r path/to/corl/requirements.txt pip install tensorflow","title":"Installation"},{"location":"tasks/docking_1d/#training","text":"To launch a training loop, the module /corl/train.py is used. This module must be passed the necessary config files at launch. From the root of the repository, execute the following command: python corl/train_rl.py --cfg config/experiments/docking_1d.yml --compute-platform local The config path after the --config flag defines the task, while the three strings following the -ac flag define the agent name, agent config path, and platform config path respectively. Multiple -ac flags may be used to define multiagent environments.","title":"Training"},{"location":"coverage/","text":".md-content { max-width: none !important; } article h1, article > a { display: none; } var coviframe = document.getElementById(\"coviframe\"); function resizeIframe() { coviframe.style.height = coviframe.contentWindow.document.documentElement.offsetHeight + 'px'; } coviframe.contentWindow.document.body.onclick = function() { coviframe.contentWindow.location.reload(); }","title":"Coverage"}]}